2017-10-11T11:25:40.044560: step 7801, loss 0.110104, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:40.127054: step 7802, loss 0.211401, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:40.210650: step 7803, loss 0.148702, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:40.290534: step 7804, loss 0.220891, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:40.371112: step 7805, loss 0.206547, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:40.453701: step 7806, loss 0.17727, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:40.534680: step 7807, loss 0.169449, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:40.618147: step 7808, loss 0.152293, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:40.696844: step 7809, loss 0.155224, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:40.774100: step 7810, loss 0.202276, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:40.857140: step 7811, loss 0.250964, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:40.940633: step 7812, loss 0.174526, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.022274: step 7813, loss 0.177012, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.105209: step 7814, loss 0.173607, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.186708: step 7815, loss 0.0928425, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:41.272806: step 7816, loss 0.233019, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:41.352740: step 7817, loss 0.216013, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:41.435611: step 7818, loss 0.182501, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:41.517047: step 7819, loss 0.250467, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.600792: step 7820, loss 0.210898, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:41.685466: step 7821, loss 0.184569, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.765564: step 7822, loss 0.203315, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:41.849685: step 7823, loss 0.218737, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:41.933152: step 7824, loss 0.323311, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:42.016718: step 7825, loss 0.0796852, acc 1, learning_rate 0.0001
2017-10-11T11:25:42.098447: step 7826, loss 0.196882, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:42.181188: step 7827, loss 0.195171, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:42.261936: step 7828, loss 0.212219, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:42.344179: step 7829, loss 0.0922585, acc 1, learning_rate 0.0001
2017-10-11T11:25:42.423946: step 7830, loss 0.277526, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:42.511317: step 7831, loss 0.143247, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:42.593685: step 7832, loss 0.122076, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:42.677613: step 7833, loss 0.105392, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:42.757421: step 7834, loss 0.244078, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:42.840894: step 7835, loss 0.137973, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:42.922276: step 7836, loss 0.245331, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:43.004104: step 7837, loss 0.274245, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:43.086675: step 7838, loss 0.185935, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:43.168187: step 7839, loss 0.0970907, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:43.238937: step 7840, loss 0.276881, acc 0.882353, learning_rate 0.0001
