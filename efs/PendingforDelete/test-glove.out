
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=50
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=50
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=100
NUM_FILTERS=128

Loading data...
Vocabulary Size: 46116
Train/Dev split: 6259/695
Writing to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767

Load glove file /home/xxliu10/bigdata/glove.twitter.27B.100d.txt
glove file has been loaded

2017-09-08T16:58:01.435874: step 1, loss 8.0963, acc 0.140625
2017-09-08T16:58:02.139711: step 2, loss 5.84495, acc 0.1875
2017-09-08T16:58:02.842046: step 3, loss 4.70679, acc 0.21875
2017-09-08T16:58:03.575655: step 4, loss 4.78139, acc 0.125
2017-09-08T16:58:04.236034: step 5, loss 4.13191, acc 0.328125
2017-09-08T16:58:04.913185: step 6, loss 2.90513, acc 0.390625
2017-09-08T16:58:05.694615: step 7, loss 4.51716, acc 0.296875
2017-09-08T16:58:06.364847: step 8, loss 4.17256, acc 0.3125
2017-09-08T16:58:06.961592: step 9, loss 4.23886, acc 0.296875
2017-09-08T16:58:07.681249: step 10, loss 3.41726, acc 0.28125
2017-09-08T16:58:08.344061: step 11, loss 4.18203, acc 0.28125
2017-09-08T16:58:08.982147: step 12, loss 3.3436, acc 0.46875
2017-09-08T16:58:09.694302: step 13, loss 3.88659, acc 0.359375
2017-09-08T16:58:10.432918: step 14, loss 3.87959, acc 0.25
2017-09-08T16:58:11.116536: step 15, loss 2.79833, acc 0.4375
2017-09-08T16:58:11.829630: step 16, loss 4.27869, acc 0.21875
2017-09-08T16:58:12.498812: step 17, loss 3.70323, acc 0.34375
2017-09-08T16:58:13.224837: step 18, loss 3.03083, acc 0.390625
2017-09-08T16:58:13.873185: step 19, loss 2.93541, acc 0.328125
2017-09-08T16:58:14.552708: step 20, loss 3.17351, acc 0.390625
2017-09-08T16:58:15.041979: step 21, loss 2.18843, acc 0.40625
2017-09-08T16:58:15.691940: step 22, loss 2.8889, acc 0.5
2017-09-08T16:58:16.329386: step 23, loss 3.50311, acc 0.34375
2017-09-08T16:58:16.986743: step 24, loss 3.20895, acc 0.359375
2017-09-08T16:58:17.688570: step 25, loss 2.58017, acc 0.46875
2017-09-08T16:58:18.405783: step 26, loss 3.1399, acc 0.375
2017-09-08T16:58:19.001904: step 27, loss 3.17133, acc 0.34375
2017-09-08T16:58:19.720164: step 28, loss 2.53178, acc 0.40625
2017-09-08T16:58:20.293640: step 29, loss 2.41781, acc 0.484375
2017-09-08T16:58:20.802389: step 30, loss 2.28496, acc 0.4375
2017-09-08T16:58:21.374134: step 31, loss 3.26973, acc 0.359375
2017-09-08T16:58:22.037326: step 32, loss 2.30951, acc 0.5
2017-09-08T16:58:22.716674: step 33, loss 2.6835, acc 0.390625
2017-09-08T16:58:23.491712: step 34, loss 2.99172, acc 0.375
2017-09-08T16:58:24.181047: step 35, loss 2.32588, acc 0.53125
2017-09-08T16:58:24.965069: step 36, loss 2.49421, acc 0.484375
2017-09-08T16:58:25.729255: step 37, loss 1.96119, acc 0.5
2017-09-08T16:58:26.517459: step 38, loss 1.85336, acc 0.5625
2017-09-08T16:58:27.259039: step 39, loss 1.66204, acc 0.5
2017-09-08T16:58:28.000990: step 40, loss 2.46769, acc 0.484375
2017-09-08T16:58:28.687087: step 41, loss 2.26299, acc 0.53125
2017-09-08T16:58:29.447944: step 42, loss 2.49847, acc 0.421875
2017-09-08T16:58:30.222464: step 43, loss 1.89423, acc 0.59375
2017-09-08T16:58:30.963267: step 44, loss 2.40669, acc 0.515625
2017-09-08T16:58:31.693507: step 45, loss 2.06038, acc 0.515625
2017-09-08T16:58:32.551274: step 46, loss 2.44017, acc 0.4375
2017-09-08T16:58:33.254754: step 47, loss 1.69594, acc 0.625
2017-09-08T16:58:34.017037: step 48, loss 1.68605, acc 0.53125
2017-09-08T16:58:34.677447: step 49, loss 1.17753, acc 0.6875
2017-09-08T16:58:35.276723: step 50, loss 1.97383, acc 0.515625

Evaluation:
2017-09-08T16:58:36.029804: step 50, loss 0.534093, acc 0.8

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-50

2017-09-08T16:58:39.802644: step 51, loss 1.88606, acc 0.53125
2017-09-08T16:58:40.608260: step 52, loss 1.92787, acc 0.5
2017-09-08T16:58:41.347528: step 53, loss 1.87886, acc 0.609375
2017-09-08T16:58:42.003882: step 54, loss 1.74089, acc 0.53125
2017-09-08T16:58:42.642037: step 55, loss 2.10169, acc 0.515625
2017-09-08T16:58:43.285697: step 56, loss 1.72097, acc 0.546875
2017-09-08T16:58:43.964572: step 57, loss 1.85065, acc 0.5
2017-09-08T16:58:44.630360: step 58, loss 1.85182, acc 0.546875
2017-09-08T16:58:45.339350: step 59, loss 1.56431, acc 0.640625
2017-09-08T16:58:46.056301: step 60, loss 2.34446, acc 0.546875
2017-09-08T16:58:46.835977: step 61, loss 1.68892, acc 0.59375
2017-09-08T16:58:47.501697: step 62, loss 1.48059, acc 0.546875
2017-09-08T16:58:48.207379: step 63, loss 1.97265, acc 0.5
2017-09-08T16:58:49.008125: step 64, loss 1.11916, acc 0.640625
2017-09-08T16:58:49.730648: step 65, loss 1.47535, acc 0.59375
2017-09-08T16:58:50.439527: step 66, loss 1.67164, acc 0.5625
2017-09-08T16:58:51.136110: step 67, loss 1.88582, acc 0.546875
2017-09-08T16:58:52.023488: step 68, loss 1.13379, acc 0.703125
2017-09-08T16:58:52.778630: step 69, loss 1.2813, acc 0.65625
2017-09-08T16:58:53.477161: step 70, loss 1.42533, acc 0.578125
2017-09-08T16:58:54.214928: step 71, loss 1.37203, acc 0.53125
2017-09-08T16:58:54.972387: step 72, loss 1.07988, acc 0.65625
2017-09-08T16:58:55.639640: step 73, loss 1.80073, acc 0.609375
2017-09-08T16:58:56.248341: step 74, loss 1.00052, acc 0.703125
2017-09-08T16:58:56.843488: step 75, loss 1.65577, acc 0.625
2017-09-08T16:58:57.512981: step 76, loss 1.01442, acc 0.6875
2017-09-08T16:58:58.182818: step 77, loss 1.20745, acc 0.671875
2017-09-08T16:58:58.915530: step 78, loss 1.45637, acc 0.625
2017-09-08T16:58:59.608587: step 79, loss 1.501, acc 0.65625
2017-09-08T16:59:00.314364: step 80, loss 0.859403, acc 0.71875
2017-09-08T16:59:00.972759: step 81, loss 1.00845, acc 0.71875
2017-09-08T16:59:01.699733: step 82, loss 1.10139, acc 0.703125
2017-09-08T16:59:02.380748: step 83, loss 0.978764, acc 0.65625
2017-09-08T16:59:03.124801: step 84, loss 1.6346, acc 0.65625
2017-09-08T16:59:03.821978: step 85, loss 0.866994, acc 0.78125
2017-09-08T16:59:04.546626: step 86, loss 1.60627, acc 0.578125
2017-09-08T16:59:05.252835: step 87, loss 1.29825, acc 0.65625
2017-09-08T16:59:05.919491: step 88, loss 1.17213, acc 0.734375
2017-09-08T16:59:06.561211: step 89, loss 0.957927, acc 0.640625
2017-09-08T16:59:07.014524: step 90, loss 0.707478, acc 0.75
2017-09-08T16:59:07.726451: step 91, loss 0.742477, acc 0.75
2017-09-08T16:59:08.451001: step 92, loss 1.41273, acc 0.640625
2017-09-08T16:59:09.229182: step 93, loss 1.32726, acc 0.65625
2017-09-08T16:59:10.003213: step 94, loss 0.921567, acc 0.703125
2017-09-08T16:59:10.778009: step 95, loss 1.29818, acc 0.640625
2017-09-08T16:59:11.528255: step 96, loss 1.13418, acc 0.6875
2017-09-08T16:59:12.284841: step 97, loss 0.931423, acc 0.71875
2017-09-08T16:59:12.970849: step 98, loss 0.733862, acc 0.764706
2017-09-08T16:59:13.713174: step 99, loss 0.866332, acc 0.765625
2017-09-08T16:59:14.374621: step 100, loss 1.13953, acc 0.71875

Evaluation:
2017-09-08T16:59:15.037114: step 100, loss 0.312003, acc 0.882014

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-100

2017-09-08T16:59:18.261499: step 101, loss 0.856018, acc 0.765625
2017-09-08T16:59:18.908717: step 102, loss 0.945202, acc 0.703125
2017-09-08T16:59:19.594759: step 103, loss 0.846621, acc 0.734375
2017-09-08T16:59:20.308817: step 104, loss 1.3537, acc 0.65625
2017-09-08T16:59:21.073213: step 105, loss 0.740401, acc 0.78125
2017-09-08T16:59:21.833079: step 106, loss 0.810302, acc 0.734375
2017-09-08T16:59:22.674989: step 107, loss 0.960863, acc 0.71875
2017-09-08T16:59:23.435057: step 108, loss 0.918642, acc 0.75
2017-09-08T16:59:24.252030: step 109, loss 0.923662, acc 0.71875
2017-09-08T16:59:25.024962: step 110, loss 0.647526, acc 0.734375
2017-09-08T16:59:25.809975: step 111, loss 0.543454, acc 0.796875
2017-09-08T16:59:26.570868: step 112, loss 0.687349, acc 0.78125
2017-09-08T16:59:27.312430: step 113, loss 0.726804, acc 0.734375
2017-09-08T16:59:27.984901: step 114, loss 0.567522, acc 0.796875
2017-09-08T16:59:28.695255: step 115, loss 0.725143, acc 0.78125
2017-09-08T16:59:29.435382: step 116, loss 0.686183, acc 0.796875
2017-09-08T16:59:30.126196: step 117, loss 0.958808, acc 0.6875
2017-09-08T16:59:30.821501: step 118, loss 0.785715, acc 0.796875
2017-09-08T16:59:31.488099: step 119, loss 0.799315, acc 0.734375
2017-09-08T16:59:32.179988: step 120, loss 0.992013, acc 0.78125
2017-09-08T16:59:32.880168: step 121, loss 1.12135, acc 0.71875
2017-09-08T16:59:33.442074: step 122, loss 0.846411, acc 0.78125
2017-09-08T16:59:34.253898: step 123, loss 0.574953, acc 0.828125
2017-09-08T16:59:34.901888: step 124, loss 0.944797, acc 0.78125
2017-09-08T16:59:35.578497: step 125, loss 0.76947, acc 0.75
2017-09-08T16:59:36.298115: step 126, loss 0.759089, acc 0.734375
2017-09-08T16:59:37.050317: step 127, loss 0.654699, acc 0.796875
2017-09-08T16:59:37.755492: step 128, loss 0.971196, acc 0.703125
2017-09-08T16:59:38.504524: step 129, loss 0.866731, acc 0.703125
2017-09-08T16:59:39.241134: step 130, loss 0.666666, acc 0.75
2017-09-08T16:59:40.071380: step 131, loss 0.727258, acc 0.765625
2017-09-08T16:59:40.743293: step 132, loss 0.584715, acc 0.8125
2017-09-08T16:59:41.401821: step 133, loss 0.6396, acc 0.75
2017-09-08T16:59:42.030469: step 134, loss 0.678017, acc 0.796875
2017-09-08T16:59:42.739611: step 135, loss 0.792818, acc 0.78125
2017-09-08T16:59:43.428850: step 136, loss 0.442809, acc 0.90625
2017-09-08T16:59:44.160391: step 137, loss 0.661703, acc 0.8125
2017-09-08T16:59:44.836243: step 138, loss 0.386526, acc 0.890625
2017-09-08T16:59:45.599123: step 139, loss 0.562331, acc 0.84375
2017-09-08T16:59:46.306078: step 140, loss 0.838742, acc 0.75
2017-09-08T16:59:47.044718: step 141, loss 0.432475, acc 0.828125
2017-09-08T16:59:47.762250: step 142, loss 0.649725, acc 0.796875
2017-09-08T16:59:48.526665: step 143, loss 0.828266, acc 0.828125
2017-09-08T16:59:49.282710: step 144, loss 0.566288, acc 0.796875
2017-09-08T16:59:50.044255: step 145, loss 0.558496, acc 0.78125
2017-09-08T16:59:50.763372: step 146, loss 0.853813, acc 0.796875
2017-09-08T16:59:51.563005: step 147, loss 0.795789, acc 0.75
2017-09-08T16:59:52.173458: step 148, loss 0.391606, acc 0.859375
2017-09-08T16:59:52.863375: step 149, loss 0.546689, acc 0.875
2017-09-08T16:59:53.487355: step 150, loss 0.854809, acc 0.71875

Evaluation:
2017-09-08T16:59:54.136925: step 150, loss 0.266115, acc 0.916547

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-150

2017-09-08T16:59:56.450964: step 151, loss 0.524674, acc 0.859375
2017-09-08T16:59:57.005800: step 152, loss 0.548002, acc 0.828125
2017-09-08T16:59:57.724812: step 153, loss 0.5952, acc 0.84375
2017-09-08T16:59:58.465237: step 154, loss 1.15252, acc 0.71875
2017-09-08T16:59:59.208981: step 155, loss 0.740519, acc 0.71875
2017-09-08T16:59:59.985063: step 156, loss 0.520113, acc 0.828125
2017-09-08T17:00:00.753227: step 157, loss 0.54239, acc 0.84375
2017-09-08T17:00:01.495207: step 158, loss 0.539213, acc 0.84375
2017-09-08T17:00:02.142184: step 159, loss 0.416178, acc 0.890625
2017-09-08T17:00:02.996552: step 160, loss 0.84776, acc 0.734375
2017-09-08T17:00:03.716144: step 161, loss 0.611296, acc 0.8125
2017-09-08T17:00:04.453233: step 162, loss 0.300396, acc 0.90625
2017-09-08T17:00:05.016301: step 163, loss 1.01365, acc 0.71875
2017-09-08T17:00:05.699227: step 164, loss 0.631973, acc 0.84375
2017-09-08T17:00:06.401738: step 165, loss 0.891879, acc 0.765625
2017-09-08T17:00:07.206524: step 166, loss 0.436791, acc 0.859375
2017-09-08T17:00:07.921152: step 167, loss 0.710353, acc 0.75
2017-09-08T17:00:08.719827: step 168, loss 0.589383, acc 0.78125
2017-09-08T17:00:09.483025: step 169, loss 0.983752, acc 0.734375
2017-09-08T17:00:10.274541: step 170, loss 0.541739, acc 0.8125
2017-09-08T17:00:10.989798: step 171, loss 0.707066, acc 0.78125
2017-09-08T17:00:11.724975: step 172, loss 0.700998, acc 0.765625
2017-09-08T17:00:12.443149: step 173, loss 0.705197, acc 0.84375
2017-09-08T17:00:13.159264: step 174, loss 0.682309, acc 0.828125
2017-09-08T17:00:13.888020: step 175, loss 0.746362, acc 0.734375
2017-09-08T17:00:14.665860: step 176, loss 0.825547, acc 0.765625
2017-09-08T17:00:15.369772: step 177, loss 0.878868, acc 0.75
2017-09-08T17:00:16.060304: step 178, loss 0.694394, acc 0.8125
2017-09-08T17:00:16.781454: step 179, loss 0.783507, acc 0.828125
2017-09-08T17:00:17.486602: step 180, loss 0.551691, acc 0.828125
2017-09-08T17:00:18.515078: step 181, loss 0.495751, acc 0.828125
2017-09-08T17:00:19.260525: step 182, loss 0.706872, acc 0.765625
2017-09-08T17:00:20.029841: step 183, loss 0.64498, acc 0.828125
2017-09-08T17:00:20.756596: step 184, loss 0.571207, acc 0.859375
2017-09-08T17:00:21.506202: step 185, loss 0.230462, acc 0.875
2017-09-08T17:00:22.195681: step 186, loss 0.58137, acc 0.859375
2017-09-08T17:00:22.813778: step 187, loss 0.850638, acc 0.765625
2017-09-08T17:00:23.459549: step 188, loss 0.590527, acc 0.8125
2017-09-08T17:00:24.218519: step 189, loss 0.55594, acc 0.859375
2017-09-08T17:00:24.955216: step 190, loss 0.653456, acc 0.78125
2017-09-08T17:00:25.662349: step 191, loss 0.635438, acc 0.8125
2017-09-08T17:00:26.330328: step 192, loss 0.664323, acc 0.84375
2017-09-08T17:00:27.033188: step 193, loss 0.562652, acc 0.828125
2017-09-08T17:00:27.804235: step 194, loss 0.676264, acc 0.8125
2017-09-08T17:00:28.513261: step 195, loss 0.615222, acc 0.859375
2017-09-08T17:00:29.049614: step 196, loss 0.612568, acc 0.823529
2017-09-08T17:00:29.743888: step 197, loss 0.663532, acc 0.78125
2017-09-08T17:00:30.443378: step 198, loss 0.275899, acc 0.921875
2017-09-08T17:00:31.159679: step 199, loss 0.270176, acc 0.890625
2017-09-08T17:00:31.846894: step 200, loss 0.383694, acc 0.828125

Evaluation:
2017-09-08T17:00:32.581976: step 200, loss 0.246831, acc 0.922302

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-200

2017-09-08T17:00:36.002663: step 201, loss 0.392404, acc 0.859375
2017-09-08T17:00:36.781538: step 202, loss 0.458137, acc 0.859375
2017-09-08T17:00:37.509391: step 203, loss 0.308865, acc 0.890625
2017-09-08T17:00:38.223459: step 204, loss 0.428572, acc 0.890625
2017-09-08T17:00:38.965077: step 205, loss 0.470913, acc 0.84375
2017-09-08T17:00:39.679472: step 206, loss 0.287537, acc 0.890625
2017-09-08T17:00:40.463028: step 207, loss 0.299023, acc 0.890625
2017-09-08T17:00:41.191711: step 208, loss 0.33842, acc 0.859375
2017-09-08T17:00:41.939140: step 209, loss 0.281448, acc 0.921875
2017-09-08T17:00:42.688832: step 210, loss 0.697982, acc 0.796875
2017-09-08T17:00:43.455764: step 211, loss 0.461664, acc 0.875
2017-09-08T17:00:44.344913: step 212, loss 0.331521, acc 0.875
2017-09-08T17:00:45.224765: step 213, loss 0.488849, acc 0.828125
2017-09-08T17:00:45.938032: step 214, loss 0.241425, acc 0.921875
2017-09-08T17:00:46.643202: step 215, loss 0.566981, acc 0.84375
2017-09-08T17:00:47.335601: step 216, loss 0.598511, acc 0.78125
2017-09-08T17:00:47.895677: step 217, loss 0.53674, acc 0.8125
2017-09-08T17:00:48.658760: step 218, loss 0.362098, acc 0.90625
2017-09-08T17:00:49.425449: step 219, loss 0.378901, acc 0.890625
2017-09-08T17:00:50.194510: step 220, loss 0.763063, acc 0.796875
2017-09-08T17:00:50.954238: step 221, loss 0.199845, acc 0.90625
2017-09-08T17:00:51.725634: step 222, loss 0.386307, acc 0.90625
2017-09-08T17:00:52.462907: step 223, loss 0.317885, acc 0.875
2017-09-08T17:00:53.195392: step 224, loss 0.458304, acc 0.875
2017-09-08T17:00:53.908789: step 225, loss 0.357301, acc 0.84375
2017-09-08T17:00:54.654398: step 226, loss 0.434164, acc 0.84375
2017-09-08T17:00:55.275222: step 227, loss 0.509536, acc 0.859375
2017-09-08T17:00:55.936068: step 228, loss 0.556841, acc 0.875
2017-09-08T17:00:56.544236: step 229, loss 0.244406, acc 0.890625
2017-09-08T17:00:57.220935: step 230, loss 0.440787, acc 0.828125
2017-09-08T17:00:57.933440: step 231, loss 0.313616, acc 0.859375
2017-09-08T17:00:58.664404: step 232, loss 0.424434, acc 0.859375
2017-09-08T17:00:59.531510: step 233, loss 0.287266, acc 0.890625
2017-09-08T17:01:00.304782: step 234, loss 0.283199, acc 0.90625
2017-09-08T17:01:01.054680: step 235, loss 0.397203, acc 0.890625
2017-09-08T17:01:01.627528: step 236, loss 0.555083, acc 0.84375
2017-09-08T17:01:02.339627: step 237, loss 0.511828, acc 0.875
2017-09-08T17:01:02.958885: step 238, loss 0.40052, acc 0.875
2017-09-08T17:01:03.532092: step 239, loss 0.366895, acc 0.875
2017-09-08T17:01:04.170845: step 240, loss 0.316514, acc 0.875
2017-09-08T17:01:04.870597: step 241, loss 0.579972, acc 0.796875
2017-09-08T17:01:05.638428: step 242, loss 0.453968, acc 0.84375
2017-09-08T17:01:06.400237: step 243, loss 0.459339, acc 0.859375
2017-09-08T17:01:07.067572: step 244, loss 0.294258, acc 0.890625
2017-09-08T17:01:07.782966: step 245, loss 0.531431, acc 0.875
2017-09-08T17:01:09.357290: step 246, loss 0.567154, acc 0.8125
2017-09-08T17:01:10.111969: step 247, loss 0.456361, acc 0.890625
2017-09-08T17:01:10.810745: step 248, loss 0.377322, acc 0.90625
2017-09-08T17:01:11.600864: step 249, loss 0.582551, acc 0.84375
2017-09-08T17:01:12.358934: step 250, loss 0.308673, acc 0.859375

Evaluation:
2017-09-08T17:01:13.150762: step 250, loss 0.233217, acc 0.920863

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-250

2017-09-08T17:01:16.841975: step 251, loss 0.500653, acc 0.84375
2017-09-08T17:01:17.479472: step 252, loss 0.38971, acc 0.90625
2017-09-08T17:01:18.159266: step 253, loss 0.516511, acc 0.84375
2017-09-08T17:01:18.933260: step 254, loss 0.413574, acc 0.84375
2017-09-08T17:01:19.612097: step 255, loss 0.522168, acc 0.84375
2017-09-08T17:01:20.499267: step 256, loss 0.58549, acc 0.84375
2017-09-08T17:01:21.292179: step 257, loss 0.257075, acc 0.890625
2017-09-08T17:01:22.060878: step 258, loss 0.302616, acc 0.890625
2017-09-08T17:01:22.859838: step 259, loss 0.63019, acc 0.75
2017-09-08T17:01:23.579378: step 260, loss 0.379868, acc 0.890625
2017-09-08T17:01:24.326382: step 261, loss 0.542056, acc 0.828125
2017-09-08T17:01:25.061502: step 262, loss 0.391279, acc 0.84375
2017-09-08T17:01:25.687551: step 263, loss 0.347183, acc 0.890625
2017-09-08T17:01:26.198497: step 264, loss 0.134026, acc 0.96875
2017-09-08T17:01:26.823280: step 265, loss 0.284971, acc 0.890625
2017-09-08T17:01:27.478520: step 266, loss 0.302314, acc 0.890625
2017-09-08T17:01:28.120556: step 267, loss 0.282012, acc 0.890625
2017-09-08T17:01:28.872161: step 268, loss 0.182448, acc 0.890625
2017-09-08T17:01:29.590348: step 269, loss 0.585763, acc 0.796875
2017-09-08T17:01:30.325180: step 270, loss 0.227101, acc 0.921875
2017-09-08T17:01:31.094804: step 271, loss 0.345338, acc 0.875
2017-09-08T17:01:31.815546: step 272, loss 0.270291, acc 0.921875
2017-09-08T17:01:32.670899: step 273, loss 0.341528, acc 0.875
2017-09-08T17:01:33.464729: step 274, loss 0.312384, acc 0.875
2017-09-08T17:01:34.202900: step 275, loss 0.193053, acc 0.90625
2017-09-08T17:01:34.969153: step 276, loss 0.284819, acc 0.9375
2017-09-08T17:01:35.693323: step 277, loss 0.577611, acc 0.84375
2017-09-08T17:01:36.454453: step 278, loss 0.410653, acc 0.875
2017-09-08T17:01:37.221620: step 279, loss 0.285673, acc 0.9375
2017-09-08T17:01:37.944195: step 280, loss 0.378121, acc 0.9375
2017-09-08T17:01:38.783114: step 281, loss 0.546888, acc 0.8125
2017-09-08T17:01:39.475044: step 282, loss 0.257152, acc 0.90625
2017-09-08T17:01:40.204420: step 283, loss 0.641216, acc 0.84375
2017-09-08T17:01:40.954701: step 284, loss 0.390248, acc 0.859375
2017-09-08T17:01:41.732323: step 285, loss 0.320972, acc 0.890625
2017-09-08T17:01:42.449678: step 286, loss 0.451807, acc 0.890625
2017-09-08T17:01:43.122905: step 287, loss 0.34558, acc 0.859375
2017-09-08T17:01:43.632180: step 288, loss 0.64866, acc 0.8125
2017-09-08T17:01:44.207148: step 289, loss 0.405119, acc 0.875
2017-09-08T17:01:44.764511: step 290, loss 0.410916, acc 0.875
2017-09-08T17:01:45.520402: step 291, loss 0.133937, acc 0.9375
2017-09-08T17:01:46.326655: step 292, loss 0.393771, acc 0.890625
2017-09-08T17:01:47.080872: step 293, loss 0.530391, acc 0.859375
2017-09-08T17:01:47.740808: step 294, loss 0.342532, acc 0.941176
2017-09-08T17:01:48.553960: step 295, loss 0.251582, acc 0.921875
2017-09-08T17:01:49.322311: step 296, loss 0.257055, acc 0.90625
2017-09-08T17:01:50.015015: step 297, loss 0.318055, acc 0.890625
2017-09-08T17:01:50.686697: step 298, loss 0.446085, acc 0.875
2017-09-08T17:01:51.407083: step 299, loss 0.175849, acc 0.9375
2017-09-08T17:01:52.077964: step 300, loss 0.337212, acc 0.859375

Evaluation:
2017-09-08T17:01:52.815863: step 300, loss 0.213641, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-300

2017-09-08T17:01:55.681736: step 301, loss 0.226997, acc 0.9375
2017-09-08T17:01:56.249751: step 302, loss 0.232059, acc 0.921875
2017-09-08T17:01:56.877354: step 303, loss 0.201406, acc 0.921875
2017-09-08T17:01:57.669467: step 304, loss 0.173167, acc 0.921875
2017-09-08T17:01:58.311609: step 305, loss 0.244412, acc 0.921875
2017-09-08T17:01:59.006520: step 306, loss 0.269988, acc 0.9375
2017-09-08T17:01:59.694190: step 307, loss 0.133745, acc 0.953125
2017-09-08T17:02:00.379075: step 308, loss 0.397928, acc 0.859375
2017-09-08T17:02:01.112535: step 309, loss 0.178727, acc 0.9375
2017-09-08T17:02:01.883126: step 310, loss 0.429351, acc 0.875
2017-09-08T17:02:02.620182: step 311, loss 0.325288, acc 0.875
2017-09-08T17:02:03.379121: step 312, loss 0.223886, acc 0.921875
2017-09-08T17:02:04.143110: step 313, loss 0.35569, acc 0.859375
2017-09-08T17:02:04.895549: step 314, loss 0.129625, acc 0.953125
2017-09-08T17:02:05.589829: step 315, loss 0.194395, acc 0.9375
2017-09-08T17:02:06.283559: step 316, loss 0.308584, acc 0.90625
2017-09-08T17:02:07.043241: step 317, loss 0.43303, acc 0.859375
2017-09-08T17:02:07.769865: step 318, loss 0.134242, acc 0.96875
2017-09-08T17:02:08.488650: step 319, loss 0.22837, acc 0.953125
2017-09-08T17:02:09.249010: step 320, loss 0.240799, acc 0.921875
2017-09-08T17:02:10.021836: step 321, loss 0.178107, acc 0.9375
2017-09-08T17:02:10.710539: step 322, loss 0.178915, acc 0.9375
2017-09-08T17:02:11.438152: step 323, loss 0.326072, acc 0.84375
2017-09-08T17:02:12.092296: step 324, loss 0.239752, acc 0.921875
2017-09-08T17:02:12.759388: step 325, loss 0.289392, acc 0.90625
2017-09-08T17:02:13.436838: step 326, loss 0.211086, acc 0.9375
2017-09-08T17:02:14.134858: step 327, loss 0.342538, acc 0.875
2017-09-08T17:02:14.811927: step 328, loss 0.196627, acc 0.921875
2017-09-08T17:02:15.579190: step 329, loss 0.349122, acc 0.875
2017-09-08T17:02:16.310545: step 330, loss 0.475505, acc 0.859375
2017-09-08T17:02:17.058710: step 331, loss 0.20248, acc 0.953125
2017-09-08T17:02:17.825119: step 332, loss 0.312093, acc 0.90625
2017-09-08T17:02:18.555118: step 333, loss 0.341545, acc 0.875
2017-09-08T17:02:19.358675: step 334, loss 0.157132, acc 0.953125
2017-09-08T17:02:20.116201: step 335, loss 0.255856, acc 0.9375
2017-09-08T17:02:20.907670: step 336, loss 0.0919652, acc 0.984375
2017-09-08T17:02:21.635785: step 337, loss 0.15238, acc 0.96875
2017-09-08T17:02:22.446731: step 338, loss 0.240218, acc 0.875
2017-09-08T17:02:23.147139: step 339, loss 0.224535, acc 0.890625
2017-09-08T17:02:23.839123: step 340, loss 0.222725, acc 0.9375
2017-09-08T17:02:24.533807: step 341, loss 0.352743, acc 0.921875
2017-09-08T17:02:25.208502: step 342, loss 0.166396, acc 0.9375
2017-09-08T17:02:25.885597: step 343, loss 0.124911, acc 0.953125
2017-09-08T17:02:26.608096: step 344, loss 0.248871, acc 0.90625
2017-09-08T17:02:27.320355: step 345, loss 0.19973, acc 0.921875
2017-09-08T17:02:28.030239: step 346, loss 0.349853, acc 0.90625
2017-09-08T17:02:28.739487: step 347, loss 0.13243, acc 0.953125
2017-09-08T17:02:29.529743: step 348, loss 0.359235, acc 0.859375
2017-09-08T17:02:30.344151: step 349, loss 0.27341, acc 0.921875
2017-09-08T17:02:31.087005: step 350, loss 0.150668, acc 0.9375

Evaluation:
2017-09-08T17:02:31.904911: step 350, loss 0.219346, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-350

2017-09-08T17:02:35.193016: step 351, loss 0.403945, acc 0.859375
2017-09-08T17:02:35.872002: step 352, loss 0.260829, acc 0.890625
2017-09-08T17:02:36.459003: step 353, loss 0.291485, acc 0.890625
2017-09-08T17:02:37.035501: step 354, loss 0.240823, acc 0.9375
2017-09-08T17:02:37.664207: step 355, loss 0.189942, acc 0.921875
2017-09-08T17:02:38.341552: step 356, loss 0.122105, acc 0.96875
2017-09-08T17:02:39.010578: step 357, loss 0.56239, acc 0.828125
2017-09-08T17:02:39.639399: step 358, loss 0.240044, acc 0.90625
2017-09-08T17:02:40.414955: step 359, loss 0.14656, acc 0.96875
2017-09-08T17:02:41.150105: step 360, loss 0.344072, acc 0.84375
2017-09-08T17:02:41.768448: step 361, loss 0.2561, acc 0.921875
2017-09-08T17:02:42.448999: step 362, loss 0.282997, acc 0.875
2017-09-08T17:02:43.097725: step 363, loss 0.123426, acc 0.953125
2017-09-08T17:02:43.824485: step 364, loss 0.397496, acc 0.84375
2017-09-08T17:02:44.561036: step 365, loss 0.222733, acc 0.90625
2017-09-08T17:02:45.281777: step 366, loss 0.438526, acc 0.859375
2017-09-08T17:02:46.049424: step 367, loss 0.391283, acc 0.84375
2017-09-08T17:02:46.810623: step 368, loss 0.263425, acc 0.921875
2017-09-08T17:02:47.571008: step 369, loss 0.305798, acc 0.921875
2017-09-08T17:02:48.356701: step 370, loss 0.313526, acc 0.90625
2017-09-08T17:02:49.047347: step 371, loss 0.326071, acc 0.890625
2017-09-08T17:02:49.759399: step 372, loss 0.284582, acc 0.875
2017-09-08T17:02:50.474265: step 373, loss 0.18682, acc 0.921875
2017-09-08T17:02:51.192013: step 374, loss 0.550599, acc 0.828125
2017-09-08T17:02:51.883292: step 375, loss 0.342845, acc 0.921875
2017-09-08T17:02:52.608604: step 376, loss 0.309569, acc 0.859375
2017-09-08T17:02:53.315856: step 377, loss 0.396223, acc 0.921875
2017-09-08T17:02:54.056286: step 378, loss 0.297042, acc 0.890625
2017-09-08T17:02:54.923618: step 379, loss 0.174023, acc 0.9375
2017-09-08T17:02:55.735383: step 380, loss 0.260413, acc 0.90625
2017-09-08T17:02:56.575910: step 381, loss 0.223917, acc 0.9375
2017-09-08T17:02:57.285850: step 382, loss 0.325499, acc 0.9375
2017-09-08T17:02:58.098642: step 383, loss 0.196696, acc 0.9375
2017-09-08T17:02:58.865348: step 384, loss 0.248458, acc 0.9375
2017-09-08T17:02:59.632543: step 385, loss 0.124501, acc 0.96875
2017-09-08T17:03:00.316881: step 386, loss 0.245664, acc 0.921875
2017-09-08T17:03:01.007646: step 387, loss 0.28125, acc 0.90625
2017-09-08T17:03:01.716672: step 388, loss 0.367356, acc 0.875
2017-09-08T17:03:02.486510: step 389, loss 0.52071, acc 0.84375
2017-09-08T17:03:03.143628: step 390, loss 0.162884, acc 0.9375
2017-09-08T17:03:03.830264: step 391, loss 0.192909, acc 0.90625
2017-09-08T17:03:04.434236: step 392, loss 0.198182, acc 0.921569
2017-09-08T17:03:05.110757: step 393, loss 0.111096, acc 0.984375
2017-09-08T17:03:05.761444: step 394, loss 0.230547, acc 0.90625
2017-09-08T17:03:06.488312: step 395, loss 0.258737, acc 0.921875
2017-09-08T17:03:07.216383: step 396, loss 0.258395, acc 0.90625
2017-09-08T17:03:07.889024: step 397, loss 0.0600882, acc 0.96875
2017-09-08T17:03:08.648422: step 398, loss 0.194113, acc 0.9375
2017-09-08T17:03:09.391570: step 399, loss 0.122118, acc 0.96875
2017-09-08T17:03:09.861381: step 400, loss 0.299939, acc 0.921875

Evaluation:
2017-09-08T17:03:10.667393: step 400, loss 0.206951, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-400

2017-09-08T17:03:14.317078: step 401, loss 0.184358, acc 0.921875
2017-09-08T17:03:14.979849: step 402, loss 0.301508, acc 0.875
2017-09-08T17:03:15.552396: step 403, loss 0.129961, acc 0.953125
2017-09-08T17:03:16.156571: step 404, loss 0.157229, acc 0.9375
2017-09-08T17:03:16.752155: step 405, loss 0.129449, acc 0.9375
2017-09-08T17:03:17.453011: step 406, loss 0.148806, acc 0.9375
2017-09-08T17:03:18.183746: step 407, loss 0.145023, acc 0.9375
2017-09-08T17:03:18.990834: step 408, loss 0.26603, acc 0.921875
2017-09-08T17:03:19.812084: step 409, loss 0.238745, acc 0.90625
2017-09-08T17:03:20.433114: step 410, loss 0.201104, acc 0.921875
2017-09-08T17:03:21.138575: step 411, loss 0.217736, acc 0.90625
2017-09-08T17:03:21.815768: step 412, loss 0.327058, acc 0.9375
2017-09-08T17:03:22.486174: step 413, loss 0.197281, acc 0.9375
2017-09-08T17:03:23.121825: step 414, loss 0.210731, acc 0.890625
2017-09-08T17:03:23.774792: step 415, loss 0.27865, acc 0.9375
2017-09-08T17:03:24.401860: step 416, loss 0.202635, acc 0.9375
2017-09-08T17:03:25.033851: step 417, loss 0.187608, acc 0.90625
2017-09-08T17:03:25.558583: step 418, loss 0.262868, acc 0.921875
2017-09-08T17:03:26.330811: step 419, loss 0.166475, acc 0.953125
2017-09-08T17:03:27.129856: step 420, loss 0.122175, acc 0.9375
2017-09-08T17:03:27.873166: step 421, loss 0.231064, acc 0.921875
2017-09-08T17:03:28.587804: step 422, loss 0.0698398, acc 0.96875
2017-09-08T17:03:29.322354: step 423, loss 0.297367, acc 0.90625
2017-09-08T17:03:30.112364: step 424, loss 0.207294, acc 0.90625
2017-09-08T17:03:30.784912: step 425, loss 0.337259, acc 0.875
2017-09-08T17:03:31.582805: step 426, loss 0.213766, acc 0.9375
2017-09-08T17:03:32.397671: step 427, loss 0.156473, acc 0.953125
2017-09-08T17:03:33.219151: step 428, loss 0.234, acc 0.90625
2017-09-08T17:03:33.875863: step 429, loss 0.178064, acc 0.90625
2017-09-08T17:03:34.571261: step 430, loss 0.144634, acc 0.9375
2017-09-08T17:03:35.299273: step 431, loss 0.207686, acc 0.921875
2017-09-08T17:03:36.004426: step 432, loss 0.208141, acc 0.921875
2017-09-08T17:03:36.737807: step 433, loss 0.116759, acc 0.953125
2017-09-08T17:03:37.499153: step 434, loss 0.0892738, acc 0.953125
2017-09-08T17:03:38.220734: step 435, loss 0.185598, acc 0.921875
2017-09-08T17:03:38.972678: step 436, loss 0.271726, acc 0.890625
2017-09-08T17:03:39.827083: step 437, loss 0.0343717, acc 1
2017-09-08T17:03:40.624537: step 438, loss 0.150034, acc 0.953125
2017-09-08T17:03:41.470253: step 439, loss 0.464796, acc 0.921875
2017-09-08T17:03:42.274676: step 440, loss 0.280596, acc 0.875
2017-09-08T17:03:43.040683: step 441, loss 0.13093, acc 0.9375
2017-09-08T17:03:43.719772: step 442, loss 0.301633, acc 0.90625
2017-09-08T17:03:44.431883: step 443, loss 0.155091, acc 0.9375
2017-09-08T17:03:45.066024: step 444, loss 0.155684, acc 0.96875
2017-09-08T17:03:45.693871: step 445, loss 0.260058, acc 0.890625
2017-09-08T17:03:46.362663: step 446, loss 0.127838, acc 0.9375
2017-09-08T17:03:47.027921: step 447, loss 0.180812, acc 0.953125
2017-09-08T17:03:47.646529: step 448, loss 0.138092, acc 0.953125
2017-09-08T17:03:48.335980: step 449, loss 0.174566, acc 0.96875
2017-09-08T17:03:49.003301: step 450, loss 0.139106, acc 0.921875

Evaluation:
2017-09-08T17:03:49.771274: step 450, loss 0.199706, acc 0.945324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-450

2017-09-08T17:03:52.920573: step 451, loss 0.0620454, acc 0.984375
2017-09-08T17:03:53.783544: step 452, loss 0.162684, acc 0.9375
2017-09-08T17:03:54.478373: step 453, loss 0.0577742, acc 1
2017-09-08T17:03:55.186856: step 454, loss 0.265496, acc 0.90625
2017-09-08T17:03:56.174302: step 455, loss 0.19637, acc 0.921875
2017-09-08T17:03:56.837556: step 456, loss 0.208421, acc 0.921875
2017-09-08T17:03:57.523748: step 457, loss 0.154851, acc 0.953125
2017-09-08T17:03:58.281971: step 458, loss 0.135662, acc 0.953125
2017-09-08T17:03:59.042912: step 459, loss 0.095113, acc 0.96875
2017-09-08T17:03:59.794664: step 460, loss 0.134919, acc 0.953125
2017-09-08T17:04:00.567771: step 461, loss 0.18313, acc 0.921875
2017-09-08T17:04:01.315063: step 462, loss 0.2115, acc 0.9375
2017-09-08T17:04:02.160246: step 463, loss 0.1348, acc 0.953125
2017-09-08T17:04:02.980436: step 464, loss 0.253681, acc 0.921875
2017-09-08T17:04:03.751786: step 465, loss 0.269674, acc 0.890625
2017-09-08T17:04:04.551804: step 466, loss 0.171494, acc 0.9375
2017-09-08T17:04:05.318531: step 467, loss 0.1673, acc 0.9375
2017-09-08T17:04:06.079545: step 468, loss 0.176075, acc 0.9375
2017-09-08T17:04:06.897684: step 469, loss 0.277201, acc 0.90625
2017-09-08T17:04:07.744414: step 470, loss 0.109445, acc 0.953125
2017-09-08T17:04:08.476791: step 471, loss 0.234107, acc 0.921875
2017-09-08T17:04:09.211836: step 472, loss 0.104237, acc 0.9375
2017-09-08T17:04:09.904847: step 473, loss 0.24508, acc 0.9375
2017-09-08T17:04:10.585909: step 474, loss 0.264844, acc 0.9375
2017-09-08T17:04:11.301135: step 475, loss 0.0981407, acc 0.96875
2017-09-08T17:04:11.995761: step 476, loss 0.182446, acc 0.921875
2017-09-08T17:04:12.708934: step 477, loss 0.296403, acc 0.90625
2017-09-08T17:04:13.492674: step 478, loss 0.178358, acc 0.9375
2017-09-08T17:04:14.217587: step 479, loss 0.242294, acc 0.9375
2017-09-08T17:04:14.981253: step 480, loss 0.269809, acc 0.859375
2017-09-08T17:04:15.733556: step 481, loss 0.214254, acc 0.921875
2017-09-08T17:04:16.437867: step 482, loss 0.200435, acc 0.921875
2017-09-08T17:04:17.276828: step 483, loss 0.0969435, acc 0.96875
2017-09-08T17:04:18.013576: step 484, loss 0.123288, acc 0.953125
2017-09-08T17:04:18.806683: step 485, loss 0.177084, acc 0.921875
2017-09-08T17:04:19.610373: step 486, loss 0.167727, acc 0.96875
2017-09-08T17:04:20.375938: step 487, loss 0.263184, acc 0.890625
2017-09-08T17:04:20.973412: step 488, loss 0.137435, acc 0.953125
2017-09-08T17:04:21.926017: step 489, loss 0.118354, acc 0.953125
2017-09-08T17:04:22.588857: step 490, loss 0.188904, acc 0.921569
2017-09-08T17:04:23.321566: step 491, loss 0.122017, acc 0.953125
2017-09-08T17:04:23.945320: step 492, loss 0.0381546, acc 1
2017-09-08T17:04:24.609666: step 493, loss 0.169139, acc 0.9375
2017-09-08T17:04:25.101201: step 494, loss 0.168027, acc 0.953125
2017-09-08T17:04:25.732351: step 495, loss 0.270213, acc 0.921875
2017-09-08T17:04:26.383956: step 496, loss 0.245352, acc 0.875
2017-09-08T17:04:27.043807: step 497, loss 0.16752, acc 0.921875
2017-09-08T17:04:27.756505: step 498, loss 0.086307, acc 0.96875
2017-09-08T17:04:28.502376: step 499, loss 0.154656, acc 0.90625
2017-09-08T17:04:29.307850: step 500, loss 0.305843, acc 0.9375

Evaluation:
2017-09-08T17:04:30.105935: step 500, loss 0.193538, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-500

2017-09-08T17:04:33.324919: step 501, loss 0.201001, acc 0.9375
2017-09-08T17:04:33.930518: step 502, loss 0.200053, acc 0.921875
2017-09-08T17:04:34.472373: step 503, loss 0.14617, acc 0.921875
2017-09-08T17:04:35.168740: step 504, loss 0.0521421, acc 0.984375
2017-09-08T17:04:35.755524: step 505, loss 0.102938, acc 0.984375
2017-09-08T17:04:36.296162: step 506, loss 0.199792, acc 0.90625
2017-09-08T17:04:36.885149: step 507, loss 0.206162, acc 0.921875
2017-09-08T17:04:37.543815: step 508, loss 0.245079, acc 0.90625
2017-09-08T17:04:38.320597: step 509, loss 0.0696715, acc 0.984375
2017-09-08T17:04:39.031384: step 510, loss 0.114703, acc 0.953125
2017-09-08T17:04:39.823267: step 511, loss 0.0959772, acc 0.96875
2017-09-08T17:04:40.554861: step 512, loss 0.2527, acc 0.921875
2017-09-08T17:04:41.359229: step 513, loss 0.100766, acc 0.96875
2017-09-08T17:04:42.121504: step 514, loss 0.173029, acc 0.953125
2017-09-08T17:04:42.868702: step 515, loss 0.135367, acc 0.953125
2017-09-08T17:04:43.638026: step 516, loss 0.213985, acc 0.953125
2017-09-08T17:04:44.327214: step 517, loss 0.102732, acc 0.96875
2017-09-08T17:04:44.996390: step 518, loss 0.314611, acc 0.921875
2017-09-08T17:04:45.716466: step 519, loss 0.192391, acc 0.921875
2017-09-08T17:04:46.379978: step 520, loss 0.0940921, acc 0.96875
2017-09-08T17:04:46.952030: step 521, loss 0.107917, acc 0.953125
2017-09-08T17:04:47.616635: step 522, loss 0.10829, acc 0.96875
2017-09-08T17:04:48.373674: step 523, loss 0.118429, acc 0.953125
2017-09-08T17:04:49.164730: step 524, loss 0.16155, acc 0.9375
2017-09-08T17:04:49.929644: step 525, loss 0.0732084, acc 0.984375
2017-09-08T17:04:50.708162: step 526, loss 0.0756015, acc 0.984375
2017-09-08T17:04:51.453850: step 527, loss 0.207434, acc 0.921875
2017-09-08T17:04:52.233193: step 528, loss 0.173473, acc 0.9375
2017-09-08T17:04:53.047015: step 529, loss 0.263531, acc 0.890625
2017-09-08T17:04:53.681748: step 530, loss 0.114902, acc 0.953125
2017-09-08T17:04:54.471261: step 531, loss 0.142879, acc 0.9375
2017-09-08T17:04:55.180728: step 532, loss 0.0990157, acc 0.96875
2017-09-08T17:04:55.870756: step 533, loss 0.100691, acc 0.984375
2017-09-08T17:04:56.578550: step 534, loss 0.150641, acc 0.9375
2017-09-08T17:04:57.220816: step 535, loss 0.204794, acc 0.90625
2017-09-08T17:04:57.892329: step 536, loss 0.063014, acc 0.984375
2017-09-08T17:04:58.711016: step 537, loss 0.107745, acc 0.96875
2017-09-08T17:04:59.597744: step 538, loss 0.18086, acc 0.90625
2017-09-08T17:05:00.439431: step 539, loss 0.0889503, acc 0.96875
2017-09-08T17:05:01.153658: step 540, loss 0.170124, acc 0.921875
2017-09-08T17:05:01.893574: step 541, loss 0.205514, acc 0.9375
2017-09-08T17:05:02.656616: step 542, loss 0.0797626, acc 0.96875
2017-09-08T17:05:03.426699: step 543, loss 0.0864315, acc 0.9375
2017-09-08T17:05:04.181794: step 544, loss 0.134624, acc 0.96875
2017-09-08T17:05:04.964037: step 545, loss 0.117498, acc 0.96875
2017-09-08T17:05:05.688738: step 546, loss 0.203187, acc 0.9375
2017-09-08T17:05:06.357733: step 547, loss 0.0609968, acc 0.984375
2017-09-08T17:05:06.986528: step 548, loss 0.353468, acc 0.875
2017-09-08T17:05:07.672794: step 549, loss 0.164696, acc 0.953125
2017-09-08T17:05:08.320379: step 550, loss 0.089309, acc 0.953125

Evaluation:
2017-09-08T17:05:08.971799: step 550, loss 0.20502, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-550

2017-09-08T17:05:12.998594: step 551, loss 0.223401, acc 0.90625
2017-09-08T17:05:13.793356: step 552, loss 0.11922, acc 0.96875
2017-09-08T17:05:14.494733: step 553, loss 0.0765937, acc 0.984375
2017-09-08T17:05:15.204477: step 554, loss 0.134444, acc 0.984375
2017-09-08T17:05:15.867819: step 555, loss 0.10653, acc 0.953125
2017-09-08T17:05:16.571033: step 556, loss 0.176324, acc 0.953125
2017-09-08T17:05:17.256213: step 557, loss 0.18195, acc 0.9375
2017-09-08T17:05:17.931559: step 558, loss 0.183908, acc 0.9375
2017-09-08T17:05:18.640470: step 559, loss 0.163485, acc 0.953125
2017-09-08T17:05:19.323331: step 560, loss 0.168801, acc 0.96875
2017-09-08T17:05:20.053144: step 561, loss 0.140324, acc 0.921875
2017-09-08T17:05:20.850906: step 562, loss 0.0965194, acc 0.96875
2017-09-08T17:05:21.565134: step 563, loss 0.294404, acc 0.875
2017-09-08T17:05:22.412609: step 564, loss 0.159567, acc 0.921875
2017-09-08T17:05:23.177221: step 565, loss 0.0826194, acc 0.953125
2017-09-08T17:05:23.877847: step 566, loss 0.0742954, acc 0.96875
2017-09-08T17:05:24.575731: step 567, loss 0.119404, acc 0.984375
2017-09-08T17:05:25.299732: step 568, loss 0.133017, acc 0.96875
2017-09-08T17:05:26.063980: step 569, loss 0.10508, acc 0.96875
2017-09-08T17:05:26.783212: step 570, loss 0.0634675, acc 0.984375
2017-09-08T17:05:27.759260: step 571, loss 0.0902955, acc 0.96875
2017-09-08T17:05:28.516867: step 572, loss 0.165563, acc 0.921875
2017-09-08T17:05:29.234509: step 573, loss 0.185613, acc 0.953125
2017-09-08T17:05:30.036156: step 574, loss 0.168612, acc 0.9375
2017-09-08T17:05:30.797404: step 575, loss 0.034604, acc 1
2017-09-08T17:05:31.576810: step 576, loss 0.126345, acc 0.953125
2017-09-08T17:05:32.389472: step 577, loss 0.0910606, acc 0.96875
2017-09-08T17:05:33.135406: step 578, loss 0.106475, acc 0.96875
2017-09-08T17:05:33.828165: step 579, loss 0.148304, acc 0.953125
2017-09-08T17:05:34.572591: step 580, loss 0.126135, acc 0.96875
2017-09-08T17:05:35.374873: step 581, loss 0.149772, acc 0.921875
2017-09-08T17:05:36.100149: step 582, loss 0.0825123, acc 1
2017-09-08T17:05:36.872649: step 583, loss 0.159917, acc 0.953125
2017-09-08T17:05:37.661168: step 584, loss 0.24417, acc 0.921875
2017-09-08T17:05:38.400927: step 585, loss 0.119813, acc 0.96875
2017-09-08T17:05:39.205140: step 586, loss 0.0442049, acc 1
2017-09-08T17:05:39.910198: step 587, loss 0.0776294, acc 0.984375
2017-09-08T17:05:40.533641: step 588, loss 0.0508117, acc 0.980392
2017-09-08T17:05:41.254165: step 589, loss 0.0927497, acc 0.953125
2017-09-08T17:05:41.907124: step 590, loss 0.0681933, acc 0.96875
2017-09-08T17:05:42.608586: step 591, loss 0.109521, acc 0.953125
2017-09-08T17:05:43.171753: step 592, loss 0.233359, acc 0.90625
2017-09-08T17:05:43.813640: step 593, loss 0.0635394, acc 0.984375
2017-09-08T17:05:44.492542: step 594, loss 0.142583, acc 0.96875
2017-09-08T17:05:45.283928: step 595, loss 0.0975071, acc 0.96875
2017-09-08T17:05:45.996873: step 596, loss 0.0840319, acc 0.96875
2017-09-08T17:05:46.732604: step 597, loss 0.0883665, acc 0.96875
2017-09-08T17:05:47.357700: step 598, loss 0.188304, acc 0.953125
2017-09-08T17:05:48.011178: step 599, loss 0.154252, acc 0.921875
2017-09-08T17:05:48.717357: step 600, loss 0.114681, acc 0.96875

Evaluation:
2017-09-08T17:05:49.481990: step 600, loss 0.177354, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-600

2017-09-08T17:05:52.586252: step 601, loss 0.203412, acc 0.921875
2017-09-08T17:05:53.249955: step 602, loss 0.119051, acc 0.953125
2017-09-08T17:05:53.956006: step 603, loss 0.161425, acc 0.953125
2017-09-08T17:05:54.655730: step 604, loss 0.148294, acc 0.953125
2017-09-08T17:05:55.365041: step 605, loss 0.0623852, acc 0.984375
2017-09-08T17:05:56.140825: step 606, loss 0.0764645, acc 0.96875
2017-09-08T17:05:56.922675: step 607, loss 0.0553551, acc 0.984375
2017-09-08T17:05:57.653683: step 608, loss 0.0604404, acc 0.984375
2017-09-08T17:05:58.339157: step 609, loss 0.124307, acc 0.984375
2017-09-08T17:05:59.046644: step 610, loss 0.13097, acc 0.96875
2017-09-08T17:05:59.815954: step 611, loss 0.0708595, acc 0.984375
2017-09-08T17:06:00.579237: step 612, loss 0.199386, acc 0.9375
2017-09-08T17:06:01.333213: step 613, loss 0.127686, acc 0.96875
2017-09-08T17:06:02.057643: step 614, loss 0.104024, acc 0.953125
2017-09-08T17:06:02.753322: step 615, loss 0.0916963, acc 0.96875
2017-09-08T17:06:03.467208: step 616, loss 0.207214, acc 0.9375
2017-09-08T17:06:04.155456: step 617, loss 0.163709, acc 0.9375
2017-09-08T17:06:04.868539: step 618, loss 0.155037, acc 0.953125
2017-09-08T17:06:05.703561: step 619, loss 0.0337162, acc 1
2017-09-08T17:06:06.439221: step 620, loss 0.0446542, acc 1
2017-09-08T17:06:07.182528: step 621, loss 0.15439, acc 0.96875
2017-09-08T17:06:07.960892: step 622, loss 0.162465, acc 0.953125
2017-09-08T17:06:08.726868: step 623, loss 0.112137, acc 0.953125
2017-09-08T17:06:09.487615: step 624, loss 0.173061, acc 0.96875
2017-09-08T17:06:10.258272: step 625, loss 0.173505, acc 0.9375
2017-09-08T17:06:11.030944: step 626, loss 0.149427, acc 0.984375
2017-09-08T17:06:11.753581: step 627, loss 0.0428235, acc 1
2017-09-08T17:06:12.694272: step 628, loss 0.0832923, acc 0.953125
2017-09-08T17:06:13.380007: step 629, loss 0.254347, acc 0.921875
2017-09-08T17:06:13.983900: step 630, loss 0.0669312, acc 0.984375
2017-09-08T17:06:14.743472: step 631, loss 0.235261, acc 0.953125
2017-09-08T17:06:15.454410: step 632, loss 0.232092, acc 0.953125
2017-09-08T17:06:16.170385: step 633, loss 0.0900824, acc 0.984375
2017-09-08T17:06:16.953925: step 634, loss 0.198435, acc 0.921875
2017-09-08T17:06:17.698665: step 635, loss 0.146166, acc 0.953125
2017-09-08T17:06:18.413996: step 636, loss 0.0733866, acc 0.96875
2017-09-08T17:06:19.140678: step 637, loss 0.194081, acc 0.9375
2017-09-08T17:06:19.903273: step 638, loss 0.0860198, acc 0.96875
2017-09-08T17:06:20.639634: step 639, loss 0.220674, acc 0.9375
2017-09-08T17:06:21.418564: step 640, loss 0.143704, acc 0.953125
2017-09-08T17:06:22.194719: step 641, loss 0.0741981, acc 0.96875
2017-09-08T17:06:23.023671: step 642, loss 0.133304, acc 0.96875
2017-09-08T17:06:23.851094: step 643, loss 0.0324275, acc 1
2017-09-08T17:06:24.686226: step 644, loss 0.23781, acc 0.921875
2017-09-08T17:06:25.482168: step 645, loss 0.0450994, acc 0.984375
2017-09-08T17:06:26.147943: step 646, loss 0.0684232, acc 0.984375
2017-09-08T17:06:26.811240: step 647, loss 0.0465126, acc 0.984375
2017-09-08T17:06:27.433492: step 648, loss 0.0564575, acc 0.984375
2017-09-08T17:06:28.134286: step 649, loss 0.168242, acc 0.90625
2017-09-08T17:06:28.796262: step 650, loss 0.0533936, acc 0.96875

Evaluation:
2017-09-08T17:06:29.529565: step 650, loss 0.18343, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-650

2017-09-08T17:06:33.758663: step 651, loss 0.162571, acc 0.96875
2017-09-08T17:06:34.369995: step 652, loss 0.114047, acc 0.953125
2017-09-08T17:06:35.000362: step 653, loss 0.186707, acc 0.90625
2017-09-08T17:06:35.684195: step 654, loss 0.144199, acc 0.953125
2017-09-08T17:06:36.297688: step 655, loss 0.0696101, acc 0.96875
2017-09-08T17:06:36.972061: step 656, loss 0.104046, acc 0.96875
2017-09-08T17:06:37.658128: step 657, loss 0.184247, acc 0.921875
2017-09-08T17:06:38.273057: step 658, loss 0.0704243, acc 0.984375
2017-09-08T17:06:39.008744: step 659, loss 0.0439557, acc 1
2017-09-08T17:06:39.645792: step 660, loss 0.129588, acc 0.96875
2017-09-08T17:06:40.376025: step 661, loss 0.0821923, acc 0.96875
2017-09-08T17:06:41.163386: step 662, loss 0.109129, acc 0.953125
2017-09-08T17:06:41.863293: step 663, loss 0.210387, acc 0.953125
2017-09-08T17:06:42.621020: step 664, loss 0.045718, acc 0.984375
2017-09-08T17:06:43.306768: step 665, loss 0.157777, acc 0.96875
2017-09-08T17:06:44.007263: step 666, loss 0.109558, acc 0.953125
2017-09-08T17:06:44.717340: step 667, loss 0.127031, acc 0.96875
2017-09-08T17:06:45.464741: step 668, loss 0.0975097, acc 0.96875
2017-09-08T17:06:46.258168: step 669, loss 0.108772, acc 0.96875
2017-09-08T17:06:47.041415: step 670, loss 0.163636, acc 0.9375
2017-09-08T17:06:47.825245: step 671, loss 0.0969555, acc 0.953125
2017-09-08T17:06:48.640525: step 672, loss 0.112785, acc 0.984375
2017-09-08T17:06:49.406820: step 673, loss 0.160155, acc 0.953125
2017-09-08T17:06:50.212012: step 674, loss 0.15154, acc 0.96875
2017-09-08T17:06:50.953048: step 675, loss 0.0855082, acc 0.96875
2017-09-08T17:06:51.674647: step 676, loss 0.128754, acc 0.9375
2017-09-08T17:06:52.440313: step 677, loss 0.291671, acc 0.9375
2017-09-08T17:06:53.151997: step 678, loss 0.249168, acc 0.9375
2017-09-08T17:06:53.866912: step 679, loss 0.0665509, acc 0.984375
2017-09-08T17:06:54.605709: step 680, loss 0.073811, acc 0.96875
2017-09-08T17:06:55.357846: step 681, loss 0.185445, acc 0.96875
2017-09-08T17:06:56.135428: step 682, loss 0.117005, acc 0.96875
2017-09-08T17:06:56.921739: step 683, loss 0.170779, acc 0.9375
2017-09-08T17:06:57.686030: step 684, loss 0.108538, acc 0.96875
2017-09-08T17:06:58.490052: step 685, loss 0.133681, acc 0.984375
2017-09-08T17:06:59.095512: step 686, loss 0.0792625, acc 0.980392
2017-09-08T17:06:59.832823: step 687, loss 0.0399808, acc 0.984375
2017-09-08T17:07:00.570547: step 688, loss 0.0475183, acc 1
2017-09-08T17:07:01.381247: step 689, loss 0.0826294, acc 0.984375
2017-09-08T17:07:02.171837: step 690, loss 0.125054, acc 0.9375
2017-09-08T17:07:02.961770: step 691, loss 0.0895679, acc 0.96875
2017-09-08T17:07:03.728575: step 692, loss 0.0443731, acc 1
2017-09-08T17:07:04.538700: step 693, loss 0.133167, acc 0.953125
2017-09-08T17:07:05.344636: step 694, loss 0.109309, acc 0.953125
2017-09-08T17:07:06.121175: step 695, loss 0.052575, acc 0.984375
2017-09-08T17:07:06.815649: step 696, loss 0.189206, acc 0.9375
2017-09-08T17:07:07.495986: step 697, loss 0.140695, acc 0.96875
2017-09-08T17:07:08.226062: step 698, loss 0.0624991, acc 0.984375
2017-09-08T17:07:08.923705: step 699, loss 0.114214, acc 0.984375
2017-09-08T17:07:09.650572: step 700, loss 0.0797225, acc 0.984375

Evaluation:
2017-09-08T17:07:10.335660: step 700, loss 0.186829, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-700

2017-09-08T17:07:12.863668: step 701, loss 0.03887, acc 0.984375
2017-09-08T17:07:13.490041: step 702, loss 0.0519605, acc 0.96875
2017-09-08T17:07:14.199258: step 703, loss 0.0854755, acc 0.984375
2017-09-08T17:07:14.893737: step 704, loss 0.118149, acc 0.96875
2017-09-08T17:07:15.522323: step 705, loss 0.0257976, acc 1
2017-09-08T17:07:16.264618: step 706, loss 0.0576318, acc 0.984375
2017-09-08T17:07:16.966441: step 707, loss 0.0216592, acc 1
2017-09-08T17:07:17.700941: step 708, loss 0.0203413, acc 1
2017-09-08T17:07:18.393784: step 709, loss 0.0756427, acc 0.96875
2017-09-08T17:07:19.049990: step 710, loss 0.0788596, acc 0.984375
2017-09-08T17:07:19.695167: step 711, loss 0.15093, acc 0.953125
2017-09-08T17:07:20.474037: step 712, loss 0.0949874, acc 0.96875
2017-09-08T17:07:21.209806: step 713, loss 0.168637, acc 0.953125
2017-09-08T17:07:21.954683: step 714, loss 0.130512, acc 0.96875
2017-09-08T17:07:22.657801: step 715, loss 0.11623, acc 0.953125
2017-09-08T17:07:23.341495: step 716, loss 0.110778, acc 0.953125
2017-09-08T17:07:24.029746: step 717, loss 0.0510766, acc 1
2017-09-08T17:07:25.096089: step 718, loss 0.167392, acc 0.953125
2017-09-08T17:07:25.862154: step 719, loss 0.0813015, acc 0.96875
2017-09-08T17:07:26.613969: step 720, loss 0.0627699, acc 0.984375
2017-09-08T17:07:27.312918: step 721, loss 0.190083, acc 0.9375
2017-09-08T17:07:28.071568: step 722, loss 0.0504071, acc 0.984375
2017-09-08T17:07:28.842221: step 723, loss 0.0818781, acc 0.96875
2017-09-08T17:07:29.617510: step 724, loss 0.112645, acc 0.96875
2017-09-08T17:07:30.363525: step 725, loss 0.0951959, acc 0.96875
2017-09-08T17:07:31.168702: step 726, loss 0.111687, acc 0.9375
2017-09-08T17:07:31.901988: step 727, loss 0.212974, acc 0.90625
2017-09-08T17:07:32.647597: step 728, loss 0.0487942, acc 0.984375
2017-09-08T17:07:33.402293: step 729, loss 0.100204, acc 0.953125
2017-09-08T17:07:34.153198: step 730, loss 0.12961, acc 0.96875
2017-09-08T17:07:35.084814: step 731, loss 0.112104, acc 0.96875
2017-09-08T17:07:35.866558: step 732, loss 0.113425, acc 0.96875
2017-09-08T17:07:36.636945: step 733, loss 0.0490785, acc 0.984375
2017-09-08T17:07:37.365818: step 734, loss 0.0605642, acc 0.984375
2017-09-08T17:07:38.099587: step 735, loss 0.117894, acc 0.96875
2017-09-08T17:07:38.816637: step 736, loss 0.0859445, acc 0.953125
2017-09-08T17:07:39.560124: step 737, loss 0.0671379, acc 0.96875
2017-09-08T17:07:40.265163: step 738, loss 0.0713254, acc 0.96875
2017-09-08T17:07:40.995594: step 739, loss 0.0217238, acc 1
2017-09-08T17:07:41.720246: step 740, loss 0.120244, acc 0.953125
2017-09-08T17:07:42.633514: step 741, loss 0.0832363, acc 0.96875
2017-09-08T17:07:43.423713: step 742, loss 0.166915, acc 0.953125
2017-09-08T17:07:44.177799: step 743, loss 0.051488, acc 0.984375
2017-09-08T17:07:44.987364: step 744, loss 0.0410407, acc 0.984375
2017-09-08T17:07:45.686915: step 745, loss 0.125406, acc 0.984375
2017-09-08T17:07:46.402232: step 746, loss 0.0932085, acc 0.953125
2017-09-08T17:07:47.100430: step 747, loss 0.036849, acc 0.984375
2017-09-08T17:07:47.687164: step 748, loss 0.125089, acc 0.9375
2017-09-08T17:07:48.221492: step 749, loss 0.0893423, acc 0.984375
2017-09-08T17:07:48.868434: step 750, loss 0.218988, acc 0.921875

Evaluation:
2017-09-08T17:07:49.655893: step 750, loss 0.190603, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-750

2017-09-08T17:07:52.983801: step 751, loss 0.0560699, acc 0.984375
2017-09-08T17:07:53.728588: step 752, loss 0.119027, acc 0.96875
2017-09-08T17:07:54.368119: step 753, loss 0.123486, acc 0.984375
2017-09-08T17:07:55.103805: step 754, loss 0.04904, acc 0.984375
2017-09-08T17:07:55.863333: step 755, loss 0.0683437, acc 0.984375
2017-09-08T17:07:56.608486: step 756, loss 0.127731, acc 0.9375
2017-09-08T17:07:57.435063: step 757, loss 0.110258, acc 0.9375
2017-09-08T17:07:58.114165: step 758, loss 0.0583152, acc 0.984375
2017-09-08T17:07:58.793307: step 759, loss 0.0948661, acc 0.96875
2017-09-08T17:07:59.305368: step 760, loss 0.128052, acc 0.953125
2017-09-08T17:07:59.875251: step 761, loss 0.0923678, acc 0.96875
2017-09-08T17:08:00.709801: step 762, loss 0.0980206, acc 0.96875
2017-09-08T17:08:01.434776: step 763, loss 0.0471384, acc 0.984375
2017-09-08T17:08:02.177853: step 764, loss 0.0196995, acc 1
2017-09-08T17:08:02.874416: step 765, loss 0.0304368, acc 1
2017-09-08T17:08:03.561505: step 766, loss 0.0396001, acc 0.984375
2017-09-08T17:08:04.297469: step 767, loss 0.067235, acc 0.96875
2017-09-08T17:08:05.023099: step 768, loss 0.0512254, acc 1
2017-09-08T17:08:05.778519: step 769, loss 0.0728685, acc 0.96875
2017-09-08T17:08:06.517045: step 770, loss 0.0504672, acc 0.984375
2017-09-08T17:08:07.270389: step 771, loss 0.0869467, acc 0.984375
2017-09-08T17:08:08.102987: step 772, loss 0.028773, acc 1
2017-09-08T17:08:08.832306: step 773, loss 0.0327172, acc 1
2017-09-08T17:08:09.560501: step 774, loss 0.0838534, acc 0.96875
2017-09-08T17:08:10.263117: step 775, loss 0.0426008, acc 0.984375
2017-09-08T17:08:11.009454: step 776, loss 0.0411838, acc 1
2017-09-08T17:08:11.684192: step 777, loss 0.0845798, acc 0.953125
2017-09-08T17:08:12.384093: step 778, loss 0.105564, acc 0.96875
2017-09-08T17:08:13.117387: step 779, loss 0.14314, acc 0.953125
2017-09-08T17:08:13.885886: step 780, loss 0.142006, acc 0.96875
2017-09-08T17:08:14.509167: step 781, loss 0.0238366, acc 1
2017-09-08T17:08:15.250562: step 782, loss 0.109155, acc 0.953125
2017-09-08T17:08:15.844059: step 783, loss 0.268922, acc 0.9375
2017-09-08T17:08:16.487618: step 784, loss 0.0352353, acc 0.980392
2017-09-08T17:08:17.240067: step 785, loss 0.0922394, acc 0.96875
2017-09-08T17:08:18.022988: step 786, loss 0.0307173, acc 0.984375
2017-09-08T17:08:18.813414: step 787, loss 0.115817, acc 0.96875
2017-09-08T17:08:19.543418: step 788, loss 0.0651037, acc 0.984375
2017-09-08T17:08:20.378176: step 789, loss 0.122336, acc 0.953125
2017-09-08T17:08:21.111623: step 790, loss 0.0727799, acc 0.96875
2017-09-08T17:08:21.925459: step 791, loss 0.129552, acc 0.953125
2017-09-08T17:08:22.617412: step 792, loss 0.134135, acc 0.9375
2017-09-08T17:08:23.341217: step 793, loss 0.138757, acc 0.953125
2017-09-08T17:08:24.103919: step 794, loss 0.0949797, acc 0.984375
2017-09-08T17:08:24.814208: step 795, loss 0.0246037, acc 0.984375
2017-09-08T17:08:25.577077: step 796, loss 0.0282857, acc 1
2017-09-08T17:08:26.293300: step 797, loss 0.0320932, acc 1
2017-09-08T17:08:27.029021: step 798, loss 0.035706, acc 0.984375
2017-09-08T17:08:27.623699: step 799, loss 0.107269, acc 0.984375
2017-09-08T17:08:28.285138: step 800, loss 0.0548259, acc 0.984375

Evaluation:
2017-09-08T17:08:29.090850: step 800, loss 0.193576, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-800

2017-09-08T17:08:32.990730: step 801, loss 0.0586681, acc 0.984375
2017-09-08T17:08:33.593027: step 802, loss 0.0184883, acc 1
2017-09-08T17:08:34.337262: step 803, loss 0.0862584, acc 0.984375
2017-09-08T17:08:34.966063: step 804, loss 0.0422273, acc 1
2017-09-08T17:08:35.648780: step 805, loss 0.152626, acc 0.9375
2017-09-08T17:08:36.431247: step 806, loss 0.0274138, acc 1
2017-09-08T17:08:37.128725: step 807, loss 0.0887557, acc 0.984375
2017-09-08T17:08:37.970428: step 808, loss 0.128366, acc 0.96875
2017-09-08T17:08:38.621040: step 809, loss 0.131503, acc 0.984375
2017-09-08T17:08:39.249687: step 810, loss 0.0712843, acc 0.984375
2017-09-08T17:08:40.008340: step 811, loss 0.0849742, acc 0.96875
2017-09-08T17:08:40.705714: step 812, loss 0.0894477, acc 0.96875
2017-09-08T17:08:41.443216: step 813, loss 0.0929404, acc 0.984375
2017-09-08T17:08:42.170433: step 814, loss 0.0368226, acc 0.984375
2017-09-08T17:08:42.931578: step 815, loss 0.142252, acc 0.953125
2017-09-08T17:08:43.731754: step 816, loss 0.0561952, acc 0.96875
2017-09-08T17:08:44.482223: step 817, loss 0.201292, acc 0.96875
2017-09-08T17:08:45.245619: step 818, loss 0.0170032, acc 1
2017-09-08T17:08:46.068996: step 819, loss 0.10901, acc 0.96875
2017-09-08T17:08:46.902054: step 820, loss 0.0710605, acc 0.96875
2017-09-08T17:08:47.732849: step 821, loss 0.128939, acc 0.953125
2017-09-08T17:08:48.459541: step 822, loss 0.0560488, acc 0.984375
2017-09-08T17:08:49.227023: step 823, loss 0.0129898, acc 1
2017-09-08T17:08:49.984890: step 824, loss 0.0743945, acc 0.96875
2017-09-08T17:08:50.755659: step 825, loss 0.315749, acc 0.921875
2017-09-08T17:08:51.449042: step 826, loss 0.115913, acc 0.96875
2017-09-08T17:08:52.222687: step 827, loss 0.0256749, acc 1
2017-09-08T17:08:52.934646: step 828, loss 0.107865, acc 0.984375
2017-09-08T17:08:53.707955: step 829, loss 0.106877, acc 0.953125
2017-09-08T17:08:54.467423: step 830, loss 0.150181, acc 0.9375
2017-09-08T17:08:55.186962: step 831, loss 0.0884665, acc 0.984375
2017-09-08T17:08:56.026612: step 832, loss 0.0709532, acc 0.96875
2017-09-08T17:08:56.751568: step 833, loss 0.126031, acc 0.984375
2017-09-08T17:08:57.547706: step 834, loss 0.0961406, acc 0.984375
2017-09-08T17:08:58.375551: step 835, loss 0.0483976, acc 0.984375
2017-09-08T17:08:59.086414: step 836, loss 0.0834091, acc 0.96875
2017-09-08T17:08:59.766922: step 837, loss 0.0627461, acc 0.984375
2017-09-08T17:09:00.513809: step 838, loss 0.118279, acc 0.953125
2017-09-08T17:09:01.305191: step 839, loss 0.159188, acc 0.96875
2017-09-08T17:09:02.044695: step 840, loss 0.0211846, acc 1
2017-09-08T17:09:02.645511: step 841, loss 0.126873, acc 0.953125
2017-09-08T17:09:03.375582: step 842, loss 0.0650043, acc 0.984375
2017-09-08T17:09:04.038423: step 843, loss 0.0202739, acc 1
2017-09-08T17:09:04.819433: step 844, loss 0.0530753, acc 0.984375
2017-09-08T17:09:05.682891: step 845, loss 0.0817343, acc 0.96875
2017-09-08T17:09:06.375576: step 846, loss 0.101496, acc 0.953125
2017-09-08T17:09:07.131503: step 847, loss 0.0586091, acc 0.984375
2017-09-08T17:09:07.812296: step 848, loss 0.0219349, acc 0.984375
2017-09-08T17:09:08.600181: step 849, loss 0.149401, acc 0.9375
2017-09-08T17:09:09.342436: step 850, loss 0.145333, acc 0.953125

Evaluation:
2017-09-08T17:09:10.048760: step 850, loss 0.184306, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-850

2017-09-08T17:09:12.671894: step 851, loss 0.0683464, acc 0.96875
2017-09-08T17:09:13.362907: step 852, loss 0.0313444, acc 1
2017-09-08T17:09:14.038794: step 853, loss 0.0190597, acc 1
2017-09-08T17:09:14.783520: step 854, loss 0.110284, acc 0.984375
2017-09-08T17:09:15.537013: step 855, loss 0.0577028, acc 0.984375
2017-09-08T17:09:16.344582: step 856, loss 0.0488213, acc 0.984375
2017-09-08T17:09:17.046715: step 857, loss 0.0302994, acc 1
2017-09-08T17:09:17.859936: step 858, loss 0.0692974, acc 0.984375
2017-09-08T17:09:18.664486: step 859, loss 0.0758241, acc 0.984375
2017-09-08T17:09:19.366469: step 860, loss 0.0329065, acc 0.984375
2017-09-08T17:09:20.065301: step 861, loss 0.0723488, acc 0.984375
2017-09-08T17:09:20.735943: step 862, loss 0.0423491, acc 0.984375
2017-09-08T17:09:21.456354: step 863, loss 0.0193267, acc 1
2017-09-08T17:09:22.184538: step 864, loss 0.0217285, acc 1
2017-09-08T17:09:22.905132: step 865, loss 0.15682, acc 0.953125
2017-09-08T17:09:23.668548: step 866, loss 0.0497288, acc 1
2017-09-08T17:09:24.361168: step 867, loss 0.219603, acc 0.9375
2017-09-08T17:09:25.069864: step 868, loss 0.130638, acc 0.96875
2017-09-08T17:09:25.862058: step 869, loss 0.0776891, acc 0.953125
2017-09-08T17:09:26.610049: step 870, loss 0.0592582, acc 0.984375
2017-09-08T17:09:27.332423: step 871, loss 0.0758932, acc 0.96875
2017-09-08T17:09:27.998530: step 872, loss 0.140403, acc 0.96875
2017-09-08T17:09:28.757543: step 873, loss 0.108554, acc 0.96875
2017-09-08T17:09:29.529213: step 874, loss 0.136079, acc 0.953125
2017-09-08T17:09:30.266453: step 875, loss 0.149851, acc 0.953125
2017-09-08T17:09:31.100331: step 876, loss 0.120447, acc 0.984375
2017-09-08T17:09:31.833693: step 877, loss 0.0354439, acc 1
2017-09-08T17:09:32.605650: step 878, loss 0.0128721, acc 1
2017-09-08T17:09:33.341490: step 879, loss 0.0445699, acc 0.984375
2017-09-08T17:09:33.968760: step 880, loss 0.109392, acc 0.953125
2017-09-08T17:09:34.666923: step 881, loss 0.0214695, acc 1
2017-09-08T17:09:35.340969: step 882, loss 0.0722272, acc 0.980392
2017-09-08T17:09:36.092232: step 883, loss 0.0293905, acc 1
2017-09-08T17:09:36.804792: step 884, loss 0.0340771, acc 0.984375
2017-09-08T17:09:37.578707: step 885, loss 0.0632286, acc 0.96875
2017-09-08T17:09:38.367606: step 886, loss 0.0281591, acc 1
2017-09-08T17:09:39.110431: step 887, loss 0.0297218, acc 1
2017-09-08T17:09:39.869373: step 888, loss 0.0405883, acc 0.984375
2017-09-08T17:09:40.685788: step 889, loss 0.125005, acc 0.953125
2017-09-08T17:09:41.429809: step 890, loss 0.0374467, acc 1
2017-09-08T17:09:42.146894: step 891, loss 0.0347762, acc 1
2017-09-08T17:09:42.932415: step 892, loss 0.0366982, acc 0.984375
2017-09-08T17:09:43.799473: step 893, loss 0.0470517, acc 0.984375
2017-09-08T17:09:44.603500: step 894, loss 0.0222883, acc 1
2017-09-08T17:09:45.227375: step 895, loss 0.096306, acc 0.96875
2017-09-08T17:09:45.912035: step 896, loss 0.0450204, acc 0.984375
2017-09-08T17:09:46.639006: step 897, loss 0.0142402, acc 1
2017-09-08T17:09:47.369231: step 898, loss 0.150915, acc 0.9375
2017-09-08T17:09:48.146149: step 899, loss 0.0543831, acc 0.96875
2017-09-08T17:09:48.932542: step 900, loss 0.0432399, acc 1

Evaluation:
2017-09-08T17:09:49.664947: step 900, loss 0.18982, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-900

2017-09-08T17:09:53.613936: step 901, loss 0.0303902, acc 1
2017-09-08T17:09:54.295855: step 902, loss 0.0876115, acc 0.96875
2017-09-08T17:09:55.057940: step 903, loss 0.0305063, acc 1
2017-09-08T17:09:55.818691: step 904, loss 0.179664, acc 0.9375
2017-09-08T17:09:56.636640: step 905, loss 0.125053, acc 0.953125
2017-09-08T17:09:57.431581: step 906, loss 0.0664529, acc 0.96875
2017-09-08T17:09:58.058775: step 907, loss 0.0761917, acc 0.984375
2017-09-08T17:09:58.674657: step 908, loss 0.0435034, acc 1
2017-09-08T17:09:59.408837: step 909, loss 0.103369, acc 0.953125
2017-09-08T17:10:00.047391: step 910, loss 0.0189297, acc 1
2017-09-08T17:10:00.712504: step 911, loss 0.0387654, acc 0.984375
2017-09-08T17:10:01.354761: step 912, loss 0.0199303, acc 1
2017-09-08T17:10:02.074940: step 913, loss 0.0456149, acc 1
2017-09-08T17:10:02.851652: step 914, loss 0.0310499, acc 1
2017-09-08T17:10:03.573238: step 915, loss 0.0419728, acc 1
2017-09-08T17:10:04.323176: step 916, loss 0.0843377, acc 0.96875
2017-09-08T17:10:05.060408: step 917, loss 0.0372534, acc 1
2017-09-08T17:10:05.792135: step 918, loss 0.0732826, acc 0.984375
2017-09-08T17:10:06.574375: step 919, loss 0.0591945, acc 0.984375
2017-09-08T17:10:07.318217: step 920, loss 0.122474, acc 0.96875
2017-09-08T17:10:08.059154: step 921, loss 0.126743, acc 0.953125
2017-09-08T17:10:08.758993: step 922, loss 0.0302513, acc 1
2017-09-08T17:10:09.322117: step 923, loss 0.0481567, acc 0.984375
2017-09-08T17:10:09.977131: step 924, loss 0.0679974, acc 0.984375
2017-09-08T17:10:10.652534: step 925, loss 0.111143, acc 0.96875
2017-09-08T17:10:11.296706: step 926, loss 0.0202995, acc 0.984375
2017-09-08T17:10:11.966687: step 927, loss 0.0312559, acc 1
2017-09-08T17:10:12.710196: step 928, loss 0.0200647, acc 1
2017-09-08T17:10:13.481846: step 929, loss 0.0910397, acc 0.96875
2017-09-08T17:10:14.202325: step 930, loss 0.0636394, acc 0.96875
2017-09-08T17:10:15.056078: step 931, loss 0.120684, acc 0.96875
2017-09-08T17:10:15.847755: step 932, loss 0.115426, acc 0.96875
2017-09-08T17:10:16.615522: step 933, loss 0.0487074, acc 1
2017-09-08T17:10:17.449492: step 934, loss 0.0849737, acc 0.96875
2017-09-08T17:10:18.205806: step 935, loss 0.0840582, acc 0.953125
2017-09-08T17:10:18.927766: step 936, loss 0.0417259, acc 0.984375
2017-09-08T17:10:19.741729: step 937, loss 0.011878, acc 1
2017-09-08T17:10:20.420683: step 938, loss 0.194046, acc 0.953125
2017-09-08T17:10:21.201834: step 939, loss 0.0764492, acc 0.96875
2017-09-08T17:10:21.974414: step 940, loss 0.0524701, acc 0.96875
2017-09-08T17:10:22.710692: step 941, loss 0.0470022, acc 0.984375
2017-09-08T17:10:23.449321: step 942, loss 0.04054, acc 1
2017-09-08T17:10:24.138058: step 943, loss 0.10019, acc 0.96875
2017-09-08T17:10:24.884993: step 944, loss 0.014617, acc 1
2017-09-08T17:10:25.708990: step 945, loss 0.0553945, acc 0.96875
2017-09-08T17:10:26.444420: step 946, loss 0.12093, acc 0.953125
2017-09-08T17:10:27.305807: step 947, loss 0.108441, acc 0.96875
2017-09-08T17:10:28.090674: step 948, loss 0.0761584, acc 0.96875
2017-09-08T17:10:28.856711: step 949, loss 0.0645575, acc 0.953125
2017-09-08T17:10:29.638716: step 950, loss 0.022645, acc 1

Evaluation:
2017-09-08T17:10:30.368407: step 950, loss 0.185661, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-950

2017-09-08T17:10:33.169550: step 951, loss 0.0930919, acc 0.984375
2017-09-08T17:10:33.803726: step 952, loss 0.0618865, acc 0.96875
2017-09-08T17:10:34.559848: step 953, loss 0.0463541, acc 1
2017-09-08T17:10:35.284082: step 954, loss 0.0303043, acc 0.984375
2017-09-08T17:10:36.002838: step 955, loss 0.0309587, acc 1
2017-09-08T17:10:36.752212: step 956, loss 0.0763769, acc 0.96875
2017-09-08T17:10:37.469259: step 957, loss 0.0687201, acc 0.984375
2017-09-08T17:10:38.257964: step 958, loss 0.0607073, acc 0.984375
2017-09-08T17:10:39.019361: step 959, loss 0.0589965, acc 0.984375
2017-09-08T17:10:39.654945: step 960, loss 0.10466, acc 0.96875
2017-09-08T17:10:40.344362: step 961, loss 0.0683903, acc 0.96875
2017-09-08T17:10:40.969265: step 962, loss 0.0655964, acc 0.984375
2017-09-08T17:10:42.043574: step 963, loss 0.0329735, acc 1
2017-09-08T17:10:42.740445: step 964, loss 0.112993, acc 0.953125
2017-09-08T17:10:43.506269: step 965, loss 0.0307779, acc 0.984375
2017-09-08T17:10:44.249592: step 966, loss 0.0192215, acc 1
2017-09-08T17:10:45.087481: step 967, loss 0.0742148, acc 0.953125
2017-09-08T17:10:45.821473: step 968, loss 0.0151407, acc 1
2017-09-08T17:10:46.575759: step 969, loss 0.077111, acc 0.984375
2017-09-08T17:10:47.341443: step 970, loss 0.0540393, acc 0.984375
2017-09-08T17:10:48.130896: step 971, loss 0.174361, acc 0.953125
2017-09-08T17:10:48.866436: step 972, loss 0.0764684, acc 0.984375
2017-09-08T17:10:49.594243: step 973, loss 0.0284504, acc 1
2017-09-08T17:10:50.385115: step 974, loss 0.165634, acc 0.921875
2017-09-08T17:10:51.020263: step 975, loss 0.0288826, acc 1
2017-09-08T17:10:51.797811: step 976, loss 0.0260691, acc 1
2017-09-08T17:10:52.579082: step 977, loss 0.153186, acc 0.9375
2017-09-08T17:10:53.294869: step 978, loss 0.0951883, acc 0.96875
2017-09-08T17:10:54.063851: step 979, loss 0.0239316, acc 1
2017-09-08T17:10:54.698158: step 980, loss 0.187015, acc 0.921569
2017-09-08T17:10:55.407068: step 981, loss 0.0506372, acc 0.96875
2017-09-08T17:10:56.124281: step 982, loss 0.0638136, acc 0.984375
2017-09-08T17:10:57.097038: step 983, loss 0.0393642, acc 0.984375
2017-09-08T17:10:57.830777: step 984, loss 0.0931439, acc 0.96875
2017-09-08T17:10:58.577933: step 985, loss 0.0379067, acc 0.984375
2017-09-08T17:10:59.333610: step 986, loss 0.0689117, acc 0.984375
2017-09-08T17:11:00.095454: step 987, loss 0.057233, acc 0.984375
2017-09-08T17:11:00.915034: step 988, loss 0.044814, acc 0.984375
2017-09-08T17:11:01.628953: step 989, loss 0.0302628, acc 1
2017-09-08T17:11:02.391311: step 990, loss 0.0578731, acc 0.96875
2017-09-08T17:11:03.270872: step 991, loss 0.0917261, acc 0.984375
2017-09-08T17:11:04.062397: step 992, loss 0.0184532, acc 1
2017-09-08T17:11:04.796560: step 993, loss 0.0437577, acc 0.984375
2017-09-08T17:11:05.465762: step 994, loss 0.043187, acc 0.984375
2017-09-08T17:11:06.202782: step 995, loss 0.06634, acc 0.984375
2017-09-08T17:11:06.993075: step 996, loss 0.0336038, acc 1
2017-09-08T17:11:07.753233: step 997, loss 0.0139192, acc 1
2017-09-08T17:11:08.485413: step 998, loss 0.026712, acc 1
2017-09-08T17:11:09.278855: step 999, loss 0.0338801, acc 0.984375
2017-09-08T17:11:10.023155: step 1000, loss 0.0443193, acc 0.984375

Evaluation:
2017-09-08T17:11:10.843716: step 1000, loss 0.190776, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1000

2017-09-08T17:11:13.890221: step 1001, loss 0.0400469, acc 0.984375
2017-09-08T17:11:14.489787: step 1002, loss 0.0558626, acc 0.984375
2017-09-08T17:11:15.172422: step 1003, loss 0.0362263, acc 0.984375
2017-09-08T17:11:15.908245: step 1004, loss 0.0266723, acc 0.984375
2017-09-08T17:11:16.512379: step 1005, loss 0.114903, acc 0.953125
2017-09-08T17:11:17.363151: step 1006, loss 0.0890954, acc 0.96875
2017-09-08T17:11:18.022946: step 1007, loss 0.0322797, acc 1
2017-09-08T17:11:18.823258: step 1008, loss 0.0587335, acc 0.96875
2017-09-08T17:11:19.527307: step 1009, loss 0.00967849, acc 1
2017-09-08T17:11:20.221417: step 1010, loss 0.0420641, acc 0.984375
2017-09-08T17:11:20.920819: step 1011, loss 0.0421906, acc 0.984375
2017-09-08T17:11:21.520307: step 1012, loss 0.0239517, acc 1
2017-09-08T17:11:22.236439: step 1013, loss 0.0311958, acc 1
2017-09-08T17:11:22.947077: step 1014, loss 0.0929474, acc 0.984375
2017-09-08T17:11:23.660605: step 1015, loss 0.0746968, acc 0.96875
2017-09-08T17:11:24.422236: step 1016, loss 0.0750491, acc 0.984375
2017-09-08T17:11:25.152942: step 1017, loss 0.0156954, acc 1
2017-09-08T17:11:25.869404: step 1018, loss 0.0397854, acc 1
2017-09-08T17:11:26.618418: step 1019, loss 0.0395028, acc 0.984375
2017-09-08T17:11:27.425374: step 1020, loss 0.150482, acc 0.96875
2017-09-08T17:11:28.155086: step 1021, loss 0.0828039, acc 0.96875
2017-09-08T17:11:28.971277: step 1022, loss 0.0530566, acc 0.984375
2017-09-08T17:11:29.699906: step 1023, loss 0.02458, acc 1
2017-09-08T17:11:30.431000: step 1024, loss 0.041266, acc 0.984375
2017-09-08T17:11:31.149224: step 1025, loss 0.0448591, acc 0.984375
2017-09-08T17:11:31.853848: step 1026, loss 0.0632303, acc 0.984375
2017-09-08T17:11:32.657775: step 1027, loss 0.0221648, acc 1
2017-09-08T17:11:33.423751: step 1028, loss 0.0203099, acc 1
2017-09-08T17:11:34.159144: step 1029, loss 0.0208292, acc 1
2017-09-08T17:11:34.923854: step 1030, loss 0.0381208, acc 0.984375
2017-09-08T17:11:35.699543: step 1031, loss 0.0189543, acc 1
2017-09-08T17:11:36.513736: step 1032, loss 0.0789835, acc 0.96875
2017-09-08T17:11:37.231021: step 1033, loss 0.0578596, acc 0.953125
2017-09-08T17:11:38.072404: step 1034, loss 0.0797484, acc 0.96875
2017-09-08T17:11:38.813457: step 1035, loss 0.0225316, acc 0.984375
2017-09-08T17:11:39.638264: step 1036, loss 0.0887614, acc 0.96875
2017-09-08T17:11:40.360930: step 1037, loss 0.159162, acc 0.96875
2017-09-08T17:11:41.205268: step 1038, loss 0.0379812, acc 0.984375
2017-09-08T17:11:41.996057: step 1039, loss 0.0740931, acc 0.96875
2017-09-08T17:11:42.752533: step 1040, loss 0.0663153, acc 0.984375
2017-09-08T17:11:43.510382: step 1041, loss 0.0261645, acc 0.984375
2017-09-08T17:11:44.300521: step 1042, loss 0.0153441, acc 1
2017-09-08T17:11:45.005891: step 1043, loss 0.0934107, acc 0.984375
2017-09-08T17:11:45.718402: step 1044, loss 0.0176861, acc 1
2017-09-08T17:11:46.442709: step 1045, loss 0.0374687, acc 0.984375
2017-09-08T17:11:47.139240: step 1046, loss 0.0966475, acc 0.96875
2017-09-08T17:11:47.863669: step 1047, loss 0.133289, acc 0.953125
2017-09-08T17:11:48.566360: step 1048, loss 0.0331771, acc 1
2017-09-08T17:11:49.369311: step 1049, loss 0.0699508, acc 0.96875
2017-09-08T17:11:50.152632: step 1050, loss 0.0401296, acc 0.984375

Evaluation:
2017-09-08T17:11:50.960952: step 1050, loss 0.195608, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1050

2017-09-08T17:11:54.818415: step 1051, loss 0.0660225, acc 0.96875
2017-09-08T17:11:55.621510: step 1052, loss 0.012278, acc 1
2017-09-08T17:11:56.384885: step 1053, loss 0.0859722, acc 0.96875
2017-09-08T17:11:57.238587: step 1054, loss 0.0565791, acc 0.984375
2017-09-08T17:11:57.915451: step 1055, loss 0.0291656, acc 1
2017-09-08T17:11:58.615849: step 1056, loss 0.0471503, acc 0.96875
2017-09-08T17:11:59.361713: step 1057, loss 0.253922, acc 0.90625
2017-09-08T17:12:00.087787: step 1058, loss 0.050311, acc 0.984375
2017-09-08T17:12:00.730473: step 1059, loss 0.0673881, acc 0.984375
2017-09-08T17:12:01.391260: step 1060, loss 0.1111, acc 0.96875
2017-09-08T17:12:02.084294: step 1061, loss 0.0601409, acc 0.96875
2017-09-08T17:12:02.878218: step 1062, loss 0.0590693, acc 0.984375
2017-09-08T17:12:03.645167: step 1063, loss 0.0973372, acc 0.96875
2017-09-08T17:12:04.438902: step 1064, loss 0.031096, acc 1
2017-09-08T17:12:05.207201: step 1065, loss 0.0886677, acc 0.984375
2017-09-08T17:12:05.978321: step 1066, loss 0.129441, acc 0.953125
2017-09-08T17:12:06.703391: step 1067, loss 0.0565357, acc 0.984375
2017-09-08T17:12:07.432772: step 1068, loss 0.0958266, acc 0.984375
2017-09-08T17:12:08.156887: step 1069, loss 0.0955027, acc 0.96875
2017-09-08T17:12:08.930272: step 1070, loss 0.0443101, acc 1
2017-09-08T17:12:09.617980: step 1071, loss 0.0501157, acc 0.984375
2017-09-08T17:12:10.378009: step 1072, loss 0.0127337, acc 1
2017-09-08T17:12:11.045181: step 1073, loss 0.116782, acc 0.96875
2017-09-08T17:12:11.603653: step 1074, loss 0.0278891, acc 1
2017-09-08T17:12:12.206127: step 1075, loss 0.0844277, acc 0.953125
2017-09-08T17:12:12.947949: step 1076, loss 0.0977646, acc 0.953125
2017-09-08T17:12:13.705331: step 1077, loss 0.0562012, acc 0.984375
2017-09-08T17:12:14.352358: step 1078, loss 0.086359, acc 0.960784
2017-09-08T17:12:15.240860: step 1079, loss 0.100383, acc 0.953125
2017-09-08T17:12:15.997431: step 1080, loss 0.160288, acc 0.953125
2017-09-08T17:12:16.765643: step 1081, loss 0.0458203, acc 0.984375
2017-09-08T17:12:17.542607: step 1082, loss 0.0799436, acc 0.984375
2017-09-08T17:12:18.284429: step 1083, loss 0.0338867, acc 0.984375
2017-09-08T17:12:19.056303: step 1084, loss 0.0389178, acc 0.984375
2017-09-08T17:12:19.889405: step 1085, loss 0.0538346, acc 0.984375
2017-09-08T17:12:20.667797: step 1086, loss 0.0302943, acc 1
2017-09-08T17:12:21.411196: step 1087, loss 0.0764431, acc 0.96875
2017-09-08T17:12:22.320200: step 1088, loss 0.0191255, acc 1
2017-09-08T17:12:23.014543: step 1089, loss 0.0810952, acc 0.96875
2017-09-08T17:12:23.747942: step 1090, loss 0.0436731, acc 0.984375
2017-09-08T17:12:24.440559: step 1091, loss 0.0711364, acc 0.96875
2017-09-08T17:12:25.150248: step 1092, loss 0.122015, acc 0.953125
2017-09-08T17:12:25.867255: step 1093, loss 0.0675789, acc 0.984375
2017-09-08T17:12:26.641739: step 1094, loss 0.0371022, acc 1
2017-09-08T17:12:27.473117: step 1095, loss 0.067289, acc 0.96875
2017-09-08T17:12:28.245442: step 1096, loss 0.0264871, acc 1
2017-09-08T17:12:29.069495: step 1097, loss 0.0724729, acc 0.96875
2017-09-08T17:12:29.820084: step 1098, loss 0.0580205, acc 0.984375
2017-09-08T17:12:30.615757: step 1099, loss 0.0107937, acc 1
2017-09-08T17:12:31.459314: step 1100, loss 0.0347924, acc 0.984375

Evaluation:
2017-09-08T17:12:32.025372: step 1100, loss 0.179347, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1100

2017-09-08T17:12:34.736608: step 1101, loss 0.113103, acc 0.96875
2017-09-08T17:12:35.439420: step 1102, loss 0.0390448, acc 0.984375
2017-09-08T17:12:36.088986: step 1103, loss 0.0701076, acc 0.96875
2017-09-08T17:12:36.735351: step 1104, loss 0.023744, acc 1
2017-09-08T17:12:37.432965: step 1105, loss 0.0341703, acc 1
2017-09-08T17:12:38.267422: step 1106, loss 0.0264556, acc 1
2017-09-08T17:12:38.972483: step 1107, loss 0.074238, acc 0.96875
2017-09-08T17:12:39.660455: step 1108, loss 0.0633068, acc 0.984375
2017-09-08T17:12:40.329631: step 1109, loss 0.0272958, acc 1
2017-09-08T17:12:40.988873: step 1110, loss 0.0699729, acc 0.96875
2017-09-08T17:12:41.737313: step 1111, loss 0.0507439, acc 1
2017-09-08T17:12:42.421940: step 1112, loss 0.097101, acc 0.96875
2017-09-08T17:12:43.194378: step 1113, loss 0.00888338, acc 1
2017-09-08T17:12:43.934949: step 1114, loss 0.0228934, acc 1
2017-09-08T17:12:44.678411: step 1115, loss 0.00427495, acc 1
2017-09-08T17:12:45.434945: step 1116, loss 0.0164722, acc 1
2017-09-08T17:12:46.018545: step 1117, loss 0.130966, acc 0.96875
2017-09-08T17:12:46.643565: step 1118, loss 0.0124033, acc 1
2017-09-08T17:12:47.346668: step 1119, loss 0.0364387, acc 1
2017-09-08T17:12:48.052237: step 1120, loss 0.0117511, acc 1
2017-09-08T17:12:48.781826: step 1121, loss 0.0124903, acc 1
2017-09-08T17:12:49.559022: step 1122, loss 0.0294424, acc 0.984375
2017-09-08T17:12:50.267302: step 1123, loss 0.0287461, acc 0.984375
2017-09-08T17:12:51.079936: step 1124, loss 0.0278133, acc 1
2017-09-08T17:12:51.900842: step 1125, loss 0.080797, acc 0.984375
2017-09-08T17:12:52.636685: step 1126, loss 0.0373249, acc 0.984375
2017-09-08T17:12:53.433540: step 1127, loss 0.10934, acc 0.953125
2017-09-08T17:12:54.165276: step 1128, loss 0.0567727, acc 0.984375
2017-09-08T17:12:54.970871: step 1129, loss 0.0769406, acc 0.96875
2017-09-08T17:12:55.721651: step 1130, loss 0.0792781, acc 0.96875
2017-09-08T17:12:56.507068: step 1131, loss 0.0338173, acc 0.984375
2017-09-08T17:12:57.212651: step 1132, loss 0.0180231, acc 1
2017-09-08T17:12:57.943190: step 1133, loss 0.016135, acc 1
2017-09-08T17:12:58.624480: step 1134, loss 0.044279, acc 0.984375
2017-09-08T17:12:59.343921: step 1135, loss 0.0267239, acc 0.984375
2017-09-08T17:13:00.128303: step 1136, loss 0.0288947, acc 1
2017-09-08T17:13:00.752892: step 1137, loss 0.0176617, acc 1
2017-09-08T17:13:01.295013: step 1138, loss 0.0368164, acc 1
2017-09-08T17:13:02.071311: step 1139, loss 0.0850437, acc 0.984375
2017-09-08T17:13:02.868874: step 1140, loss 0.0447518, acc 1
2017-09-08T17:13:03.617669: step 1141, loss 0.144008, acc 0.96875
2017-09-08T17:13:04.404317: step 1142, loss 0.00854174, acc 1
2017-09-08T17:13:05.121862: step 1143, loss 0.116306, acc 0.953125
2017-09-08T17:13:05.908420: step 1144, loss 0.0346298, acc 0.984375
2017-09-08T17:13:06.712911: step 1145, loss 0.0410234, acc 0.984375
2017-09-08T17:13:07.526918: step 1146, loss 0.0151245, acc 1
2017-09-08T17:13:08.360503: step 1147, loss 0.121223, acc 0.953125
2017-09-08T17:13:09.118656: step 1148, loss 0.0115977, acc 1
2017-09-08T17:13:09.885933: step 1149, loss 0.0430547, acc 0.984375
2017-09-08T17:13:10.540228: step 1150, loss 0.0431271, acc 0.984375

Evaluation:
2017-09-08T17:13:11.277188: step 1150, loss 0.181834, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1150

2017-09-08T17:13:15.467113: step 1151, loss 0.0380348, acc 0.984375
2017-09-08T17:13:16.161042: step 1152, loss 0.0772582, acc 0.96875
2017-09-08T17:13:16.862939: step 1153, loss 0.016024, acc 1
2017-09-08T17:13:17.539692: step 1154, loss 0.011358, acc 1
2017-09-08T17:13:18.194858: step 1155, loss 0.0509772, acc 0.984375
2017-09-08T17:13:18.868915: step 1156, loss 0.142684, acc 0.96875
2017-09-08T17:13:19.603723: step 1157, loss 0.0791004, acc 0.984375
2017-09-08T17:13:20.269211: step 1158, loss 0.0392822, acc 0.96875
2017-09-08T17:13:20.916081: step 1159, loss 0.0370919, acc 1
2017-09-08T17:13:21.615994: step 1160, loss 0.0460792, acc 0.984375
2017-09-08T17:13:22.332011: step 1161, loss 0.0690658, acc 0.984375
2017-09-08T17:13:23.093742: step 1162, loss 0.0190035, acc 1
2017-09-08T17:13:23.838477: step 1163, loss 0.0179225, acc 1
2017-09-08T17:13:24.563559: step 1164, loss 0.0399366, acc 1
2017-09-08T17:13:25.259408: step 1165, loss 0.0301405, acc 1
2017-09-08T17:13:25.969831: step 1166, loss 0.0185115, acc 1
2017-09-08T17:13:26.750652: step 1167, loss 0.0944097, acc 0.953125
2017-09-08T17:13:27.504458: step 1168, loss 0.0481416, acc 0.984375
2017-09-08T17:13:28.219435: step 1169, loss 0.0334947, acc 0.984375
2017-09-08T17:13:28.871999: step 1170, loss 0.0317398, acc 0.984375
2017-09-08T17:13:29.587561: step 1171, loss 0.147449, acc 0.9375
2017-09-08T17:13:30.297039: step 1172, loss 0.00390852, acc 1
2017-09-08T17:13:31.147711: step 1173, loss 0.0189058, acc 1
2017-09-08T17:13:31.920971: step 1174, loss 0.0425028, acc 0.984375
2017-09-08T17:13:32.614678: step 1175, loss 0.0179662, acc 1
2017-09-08T17:13:33.324137: step 1176, loss 0.0310042, acc 0.980392
2017-09-08T17:13:34.046632: step 1177, loss 0.0711719, acc 0.953125
2017-09-08T17:13:34.772090: step 1178, loss 0.0442284, acc 0.96875
2017-09-08T17:13:35.507062: step 1179, loss 0.024969, acc 1
2017-09-08T17:13:36.220903: step 1180, loss 0.0976573, acc 0.96875
2017-09-08T17:13:37.042006: step 1181, loss 0.0339558, acc 0.984375
2017-09-08T17:13:37.728343: step 1182, loss 0.0773184, acc 0.96875
2017-09-08T17:13:38.530662: step 1183, loss 0.03877, acc 0.96875
2017-09-08T17:13:39.281334: step 1184, loss 0.00989959, acc 1
2017-09-08T17:13:40.053494: step 1185, loss 0.0616313, acc 0.984375
2017-09-08T17:13:40.823302: step 1186, loss 0.0590554, acc 0.984375
2017-09-08T17:13:41.604719: step 1187, loss 0.11279, acc 0.953125
2017-09-08T17:13:42.384590: step 1188, loss 0.0259241, acc 0.984375
2017-09-08T17:13:43.119419: step 1189, loss 0.0444375, acc 0.984375
2017-09-08T17:13:43.887012: step 1190, loss 0.0865358, acc 0.953125
2017-09-08T17:13:44.622680: step 1191, loss 0.0379424, acc 0.984375
2017-09-08T17:13:45.387029: step 1192, loss 0.0595348, acc 0.96875
2017-09-08T17:13:46.109888: step 1193, loss 0.0301732, acc 0.984375
2017-09-08T17:13:46.880371: step 1194, loss 0.049453, acc 0.984375
2017-09-08T17:13:47.637421: step 1195, loss 0.0305363, acc 1
2017-09-08T17:13:48.719382: step 1196, loss 0.0102962, acc 1
2017-09-08T17:13:49.428593: step 1197, loss 0.0504843, acc 0.96875
2017-09-08T17:13:50.150826: step 1198, loss 0.0522188, acc 0.96875
2017-09-08T17:13:50.957306: step 1199, loss 0.0295217, acc 0.984375
2017-09-08T17:13:51.709400: step 1200, loss 0.0708576, acc 0.96875

Evaluation:
2017-09-08T17:13:52.645156: step 1200, loss 0.185738, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1200

2017-09-08T17:13:55.642640: step 1201, loss 0.012895, acc 1
2017-09-08T17:13:56.443197: step 1202, loss 0.0195524, acc 1
2017-09-08T17:13:57.176831: step 1203, loss 0.16299, acc 0.96875
2017-09-08T17:13:57.881252: step 1204, loss 0.0254234, acc 1
2017-09-08T17:13:58.655695: step 1205, loss 0.00872022, acc 1
2017-09-08T17:13:59.287803: step 1206, loss 0.0426848, acc 0.984375
2017-09-08T17:14:00.032670: step 1207, loss 0.0343918, acc 0.96875
2017-09-08T17:14:00.797385: step 1208, loss 0.0292516, acc 0.984375
2017-09-08T17:14:01.603765: step 1209, loss 0.016442, acc 1
2017-09-08T17:14:02.318791: step 1210, loss 0.0466032, acc 0.984375
2017-09-08T17:14:03.016120: step 1211, loss 0.0552191, acc 0.984375
2017-09-08T17:14:03.741292: step 1212, loss 0.0286913, acc 0.984375
2017-09-08T17:14:04.528683: step 1213, loss 0.0486263, acc 0.984375
2017-09-08T17:14:05.314868: step 1214, loss 0.00748617, acc 1
2017-09-08T17:14:06.149081: step 1215, loss 0.0653955, acc 0.96875
2017-09-08T17:14:07.035495: step 1216, loss 0.0621413, acc 0.984375
2017-09-08T17:14:07.830410: step 1217, loss 0.0782685, acc 0.953125
2017-09-08T17:14:08.587900: step 1218, loss 0.120422, acc 0.953125
2017-09-08T17:14:09.295257: step 1219, loss 0.0631964, acc 0.984375
2017-09-08T17:14:10.016208: step 1220, loss 0.0259701, acc 0.984375
2017-09-08T17:14:10.800779: step 1221, loss 0.113041, acc 0.96875
2017-09-08T17:14:11.525720: step 1222, loss 0.0101887, acc 1
2017-09-08T17:14:12.211901: step 1223, loss 0.0580949, acc 0.984375
2017-09-08T17:14:12.885385: step 1224, loss 0.0516652, acc 0.984375
2017-09-08T17:14:13.500956: step 1225, loss 0.0679265, acc 0.984375
2017-09-08T17:14:14.311361: step 1226, loss 0.0568525, acc 0.96875
2017-09-08T17:14:14.956735: step 1227, loss 0.0676813, acc 0.96875
2017-09-08T17:14:15.707817: step 1228, loss 0.031014, acc 1
2017-09-08T17:14:16.394891: step 1229, loss 0.0102464, acc 1
2017-09-08T17:14:17.130176: step 1230, loss 0.0861668, acc 0.984375
2017-09-08T17:14:17.864173: step 1231, loss 0.034702, acc 0.984375
2017-09-08T17:14:18.631262: step 1232, loss 0.0497831, acc 0.984375
2017-09-08T17:14:19.336266: step 1233, loss 0.0209439, acc 1
2017-09-08T17:14:20.083532: step 1234, loss 0.0405848, acc 0.984375
2017-09-08T17:14:20.843467: step 1235, loss 0.0167976, acc 1
2017-09-08T17:14:21.569278: step 1236, loss 0.0621462, acc 0.984375
2017-09-08T17:14:22.300438: step 1237, loss 0.0652303, acc 0.984375
2017-09-08T17:14:22.990889: step 1238, loss 0.0241757, acc 1
2017-09-08T17:14:23.665666: step 1239, loss 0.0849396, acc 0.984375
2017-09-08T17:14:24.423929: step 1240, loss 0.0335569, acc 0.984375
2017-09-08T17:14:25.254937: step 1241, loss 0.0443548, acc 0.984375
2017-09-08T17:14:26.052579: step 1242, loss 0.0511963, acc 0.96875
2017-09-08T17:14:26.838212: step 1243, loss 0.0135995, acc 1
2017-09-08T17:14:27.600230: step 1244, loss 0.0696452, acc 0.96875
2017-09-08T17:14:28.354575: step 1245, loss 0.0219392, acc 1
2017-09-08T17:14:29.100389: step 1246, loss 0.0272944, acc 1
2017-09-08T17:14:29.894267: step 1247, loss 0.00753358, acc 1
2017-09-08T17:14:30.635392: step 1248, loss 0.0563525, acc 0.984375
2017-09-08T17:14:31.378612: step 1249, loss 0.0292911, acc 0.984375
2017-09-08T17:14:32.127362: step 1250, loss 0.0837485, acc 0.96875

Evaluation:
2017-09-08T17:14:32.789977: step 1250, loss 0.1892, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1250

2017-09-08T17:14:36.080036: step 1251, loss 0.020913, acc 1
2017-09-08T17:14:36.956483: step 1252, loss 0.0181444, acc 1
2017-09-08T17:14:37.661723: step 1253, loss 0.186648, acc 0.96875
2017-09-08T17:14:38.547961: step 1254, loss 0.0479604, acc 0.984375
2017-09-08T17:14:39.259489: step 1255, loss 0.0179294, acc 1
2017-09-08T17:14:39.998417: step 1256, loss 0.0242623, acc 0.984375
2017-09-08T17:14:40.673521: step 1257, loss 0.0553109, acc 0.984375
2017-09-08T17:14:41.367088: step 1258, loss 0.0467598, acc 0.984375
2017-09-08T17:14:41.994372: step 1259, loss 0.0403698, acc 0.984375
2017-09-08T17:14:42.733951: step 1260, loss 0.0391563, acc 0.984375
2017-09-08T17:14:43.534925: step 1261, loss 0.0496861, acc 0.96875
2017-09-08T17:14:44.184182: step 1262, loss 0.0176548, acc 1
2017-09-08T17:14:44.944434: step 1263, loss 0.0325246, acc 0.984375
2017-09-08T17:14:45.704040: step 1264, loss 0.0298384, acc 0.984375
2017-09-08T17:14:46.522686: step 1265, loss 0.0181297, acc 1
2017-09-08T17:14:47.419330: step 1266, loss 0.11396, acc 0.984375
2017-09-08T17:14:48.086404: step 1267, loss 0.0312978, acc 1
2017-09-08T17:14:48.880875: step 1268, loss 0.101847, acc 0.984375
2017-09-08T17:14:49.627190: step 1269, loss 0.0262703, acc 0.984375
2017-09-08T17:14:50.404622: step 1270, loss 0.0571493, acc 0.984375
2017-09-08T17:14:51.208523: step 1271, loss 0.0141026, acc 1
2017-09-08T17:14:51.991198: step 1272, loss 0.0362688, acc 0.984375
2017-09-08T17:14:52.848722: step 1273, loss 0.0289297, acc 1
2017-09-08T17:14:53.498029: step 1274, loss 0.0663206, acc 0.960784
2017-09-08T17:14:54.243945: step 1275, loss 0.0460679, acc 0.984375
2017-09-08T17:14:55.018288: step 1276, loss 0.0377544, acc 0.984375
2017-09-08T17:14:55.804741: step 1277, loss 0.0891141, acc 0.96875
2017-09-08T17:14:56.605695: step 1278, loss 0.0125738, acc 1
2017-09-08T17:14:57.358096: step 1279, loss 0.126921, acc 0.96875
2017-09-08T17:14:58.026768: step 1280, loss 0.0219076, acc 1
2017-09-08T17:14:58.735946: step 1281, loss 0.010941, acc 1
2017-09-08T17:14:59.417686: step 1282, loss 0.0953235, acc 0.96875
2017-09-08T17:15:00.130309: step 1283, loss 0.0564178, acc 0.984375
2017-09-08T17:15:00.782103: step 1284, loss 0.025231, acc 1
2017-09-08T17:15:01.433472: step 1285, loss 0.0249253, acc 1
2017-09-08T17:15:01.954874: step 1286, loss 0.0380626, acc 0.96875
2017-09-08T17:15:02.694426: step 1287, loss 0.0122015, acc 1
2017-09-08T17:15:03.492897: step 1288, loss 0.0197756, acc 1
2017-09-08T17:15:04.272555: step 1289, loss 0.0302098, acc 0.984375
2017-09-08T17:15:05.072144: step 1290, loss 0.0435839, acc 0.984375
2017-09-08T17:15:05.845126: step 1291, loss 0.0487658, acc 0.984375
2017-09-08T17:15:06.557252: step 1292, loss 0.0702999, acc 0.96875
2017-09-08T17:15:07.344459: step 1293, loss 0.0109844, acc 1
2017-09-08T17:15:08.115320: step 1294, loss 0.00302361, acc 1
2017-09-08T17:15:08.908526: step 1295, loss 0.0670079, acc 0.96875
2017-09-08T17:15:09.640342: step 1296, loss 0.0838018, acc 0.953125
2017-09-08T17:15:10.390785: step 1297, loss 0.0387851, acc 0.984375
2017-09-08T17:15:11.186577: step 1298, loss 0.00765882, acc 1
2017-09-08T17:15:12.055511: step 1299, loss 0.0188026, acc 1
2017-09-08T17:15:12.862745: step 1300, loss 0.0427825, acc 0.96875

Evaluation:
2017-09-08T17:15:13.501446: step 1300, loss 0.205307, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1300

2017-09-08T17:15:17.586503: step 1301, loss 0.00631978, acc 1
2017-09-08T17:15:18.354033: step 1302, loss 0.0722875, acc 0.96875
2017-09-08T17:15:19.157720: step 1303, loss 0.0929702, acc 0.96875
2017-09-08T17:15:19.834445: step 1304, loss 0.0176341, acc 1
2017-09-08T17:15:20.513249: step 1305, loss 0.10426, acc 0.96875
2017-09-08T17:15:21.183087: step 1306, loss 0.0111059, acc 1
2017-09-08T17:15:21.925825: step 1307, loss 0.0610534, acc 0.984375
2017-09-08T17:15:22.552938: step 1308, loss 0.0239715, acc 0.984375
2017-09-08T17:15:23.221496: step 1309, loss 0.0256581, acc 1
2017-09-08T17:15:23.857168: step 1310, loss 0.0227978, acc 0.984375
2017-09-08T17:15:24.541417: step 1311, loss 0.121702, acc 0.984375
2017-09-08T17:15:25.238920: step 1312, loss 0.0682457, acc 0.984375
2017-09-08T17:15:26.008730: step 1313, loss 0.0106674, acc 1
2017-09-08T17:15:26.801078: step 1314, loss 0.0882185, acc 0.96875
2017-09-08T17:15:27.594665: step 1315, loss 0.0457223, acc 0.984375
2017-09-08T17:15:28.363110: step 1316, loss 0.0260438, acc 0.984375
2017-09-08T17:15:29.104243: step 1317, loss 0.0393637, acc 0.984375
2017-09-08T17:15:29.867147: step 1318, loss 0.0438401, acc 0.96875
2017-09-08T17:15:30.699264: step 1319, loss 0.0278801, acc 1
2017-09-08T17:15:31.493368: step 1320, loss 0.0588443, acc 0.96875
2017-09-08T17:15:32.319980: step 1321, loss 0.010756, acc 1
2017-09-08T17:15:33.015520: step 1322, loss 0.0352554, acc 0.984375
2017-09-08T17:15:33.710025: step 1323, loss 0.0788755, acc 0.984375
2017-09-08T17:15:34.491839: step 1324, loss 0.0725813, acc 0.96875
2017-09-08T17:15:35.248453: step 1325, loss 0.127332, acc 0.96875
2017-09-08T17:15:36.002836: step 1326, loss 0.00885272, acc 1
2017-09-08T17:15:36.855255: step 1327, loss 0.00967136, acc 1
2017-09-08T17:15:37.571311: step 1328, loss 0.0184676, acc 1
2017-09-08T17:15:38.294144: step 1329, loss 0.0428975, acc 0.96875
2017-09-08T17:15:39.013339: step 1330, loss 0.00963055, acc 1
2017-09-08T17:15:39.843459: step 1331, loss 0.0772986, acc 0.96875
2017-09-08T17:15:40.594606: step 1332, loss 0.0665963, acc 0.984375
2017-09-08T17:15:41.333922: step 1333, loss 0.0316558, acc 1
2017-09-08T17:15:42.174796: step 1334, loss 0.0504933, acc 0.984375
2017-09-08T17:15:42.972787: step 1335, loss 0.0999706, acc 0.96875
2017-09-08T17:15:43.764065: step 1336, loss 0.0155628, acc 1
2017-09-08T17:15:44.490928: step 1337, loss 0.133696, acc 0.96875
2017-09-08T17:15:45.184976: step 1338, loss 0.073812, acc 0.96875
2017-09-08T17:15:45.877240: step 1339, loss 0.00839343, acc 1
2017-09-08T17:15:46.548689: step 1340, loss 0.0165363, acc 1
2017-09-08T17:15:47.308294: step 1341, loss 0.139447, acc 0.96875
2017-09-08T17:15:48.128731: step 1342, loss 0.155121, acc 0.953125
2017-09-08T17:15:48.906021: step 1343, loss 0.01056, acc 1
2017-09-08T17:15:49.731890: step 1344, loss 0.0105345, acc 1
2017-09-08T17:15:50.478445: step 1345, loss 0.0197992, acc 1
2017-09-08T17:15:51.312755: step 1346, loss 0.0257116, acc 0.984375
2017-09-08T17:15:52.069521: step 1347, loss 0.0240431, acc 0.984375
2017-09-08T17:15:52.859383: step 1348, loss 0.0166684, acc 1
2017-09-08T17:15:53.599085: step 1349, loss 0.0644417, acc 0.984375
2017-09-08T17:15:54.136388: step 1350, loss 0.0880359, acc 0.953125

Evaluation:
2017-09-08T17:15:54.889045: step 1350, loss 0.197412, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1350

2017-09-08T17:15:57.608050: step 1351, loss 0.0967292, acc 0.984375
2017-09-08T17:15:58.395018: step 1352, loss 0.0131589, acc 1
2017-09-08T17:15:59.106954: step 1353, loss 0.0144508, acc 1
2017-09-08T17:15:59.826304: step 1354, loss 0.0341531, acc 0.984375
2017-09-08T17:16:00.535538: step 1355, loss 0.0412986, acc 0.984375
2017-09-08T17:16:01.215069: step 1356, loss 0.0184866, acc 1
2017-09-08T17:16:01.853115: step 1357, loss 0.0776032, acc 0.96875
2017-09-08T17:16:02.576038: step 1358, loss 0.0810138, acc 0.96875
2017-09-08T17:16:03.347723: step 1359, loss 0.0596034, acc 0.96875
2017-09-08T17:16:04.041658: step 1360, loss 0.053105, acc 0.984375
2017-09-08T17:16:04.810705: step 1361, loss 0.0955788, acc 0.96875
2017-09-08T17:16:05.539751: step 1362, loss 0.017686, acc 1
2017-09-08T17:16:06.432267: step 1363, loss 0.00768385, acc 1
2017-09-08T17:16:07.163025: step 1364, loss 0.0397688, acc 0.984375
2017-09-08T17:16:07.881913: step 1365, loss 0.0299673, acc 1
2017-09-08T17:16:08.573191: step 1366, loss 0.0126327, acc 1
2017-09-08T17:16:09.261824: step 1367, loss 0.138698, acc 0.953125
2017-09-08T17:16:10.019284: step 1368, loss 0.0725132, acc 0.96875
2017-09-08T17:16:10.700696: step 1369, loss 0.0855161, acc 0.96875
2017-09-08T17:16:11.528422: step 1370, loss 0.0108315, acc 1
2017-09-08T17:16:12.310004: step 1371, loss 0.0168975, acc 1
2017-09-08T17:16:12.945371: step 1372, loss 0.027606, acc 1
2017-09-08T17:16:13.745831: step 1373, loss 0.0288379, acc 1
2017-09-08T17:16:14.534226: step 1374, loss 0.0149511, acc 1
2017-09-08T17:16:15.281441: step 1375, loss 0.0829728, acc 0.96875
2017-09-08T17:16:15.965037: step 1376, loss 0.0842179, acc 0.953125
2017-09-08T17:16:16.735605: step 1377, loss 0.00696514, acc 1
2017-09-08T17:16:17.487465: step 1378, loss 0.0299257, acc 1
2017-09-08T17:16:18.265988: step 1379, loss 0.0155558, acc 1
2017-09-08T17:16:19.137641: step 1380, loss 0.0246637, acc 1
2017-09-08T17:16:20.271291: step 1381, loss 0.106666, acc 0.984375
2017-09-08T17:16:21.004980: step 1382, loss 0.0339225, acc 0.984375
2017-09-08T17:16:21.722514: step 1383, loss 0.12502, acc 0.953125
2017-09-08T17:16:22.488490: step 1384, loss 0.0151197, acc 1
2017-09-08T17:16:23.204613: step 1385, loss 0.0677862, acc 0.96875
2017-09-08T17:16:24.009791: step 1386, loss 0.00645087, acc 1
2017-09-08T17:16:24.817087: step 1387, loss 0.0101081, acc 1
2017-09-08T17:16:25.555268: step 1388, loss 0.00859069, acc 1
2017-09-08T17:16:26.309749: step 1389, loss 0.0619549, acc 0.984375
2017-09-08T17:16:27.093885: step 1390, loss 0.00623403, acc 1
2017-09-08T17:16:27.891757: step 1391, loss 0.00895444, acc 1
2017-09-08T17:16:28.676932: step 1392, loss 0.0253702, acc 0.984375
2017-09-08T17:16:29.356483: step 1393, loss 0.053802, acc 0.984375
2017-09-08T17:16:30.135801: step 1394, loss 0.0140052, acc 1
2017-09-08T17:16:30.943013: step 1395, loss 0.0321528, acc 0.96875
2017-09-08T17:16:31.614951: step 1396, loss 0.00757868, acc 1
2017-09-08T17:16:32.295533: step 1397, loss 0.0507981, acc 0.96875
2017-09-08T17:16:32.978428: step 1398, loss 0.0136892, acc 1
2017-09-08T17:16:33.531015: step 1399, loss 0.0826096, acc 0.984375
2017-09-08T17:16:34.064123: step 1400, loss 0.00978824, acc 1

Evaluation:
2017-09-08T17:16:34.859485: step 1400, loss 0.180697, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1400

2017-09-08T17:16:38.742994: step 1401, loss 0.125641, acc 0.984375
2017-09-08T17:16:39.565854: step 1402, loss 0.0228871, acc 1
2017-09-08T17:16:40.349606: step 1403, loss 0.04942, acc 0.984375
2017-09-08T17:16:41.112190: step 1404, loss 0.0411693, acc 0.984375
2017-09-08T17:16:41.821643: step 1405, loss 0.00454941, acc 1
2017-09-08T17:16:42.616775: step 1406, loss 0.0709067, acc 0.984375
2017-09-08T17:16:43.245823: step 1407, loss 0.13032, acc 0.96875
2017-09-08T17:16:43.782753: step 1408, loss 0.0183248, acc 1
2017-09-08T17:16:44.419215: step 1409, loss 0.0084119, acc 1
2017-09-08T17:16:45.117048: step 1410, loss 0.0285965, acc 1
2017-09-08T17:16:45.817158: step 1411, loss 0.0571753, acc 0.984375
2017-09-08T17:16:46.692867: step 1412, loss 0.0127476, acc 1
2017-09-08T17:16:47.653127: step 1413, loss 0.0117217, acc 1
2017-09-08T17:16:48.381023: step 1414, loss 0.120474, acc 0.96875
2017-09-08T17:16:49.251158: step 1415, loss 0.0353452, acc 0.96875
2017-09-08T17:16:50.015176: step 1416, loss 0.00872281, acc 1
2017-09-08T17:16:50.707022: step 1417, loss 0.00597784, acc 1
2017-09-08T17:16:51.543215: step 1418, loss 0.0165806, acc 1
2017-09-08T17:16:52.325934: step 1419, loss 0.0115532, acc 1
2017-09-08T17:16:53.146770: step 1420, loss 0.020804, acc 0.984375
2017-09-08T17:16:53.920592: step 1421, loss 0.0702694, acc 0.984375
2017-09-08T17:16:54.665026: step 1422, loss 0.10172, acc 0.984375
2017-09-08T17:16:55.301020: step 1423, loss 0.106822, acc 0.953125
2017-09-08T17:16:56.043590: step 1424, loss 0.0540061, acc 0.984375
2017-09-08T17:16:56.745905: step 1425, loss 0.0247676, acc 1
2017-09-08T17:16:57.392576: step 1426, loss 0.0476049, acc 0.984375
2017-09-08T17:16:58.135544: step 1427, loss 0.0412826, acc 0.984375
2017-09-08T17:16:58.988048: step 1428, loss 0.00715718, acc 1
2017-09-08T17:16:59.741041: step 1429, loss 0.01658, acc 1
2017-09-08T17:17:00.477813: step 1430, loss 0.0182304, acc 1
2017-09-08T17:17:01.195717: step 1431, loss 0.016443, acc 1
2017-09-08T17:17:01.941543: step 1432, loss 0.0262245, acc 0.984375
2017-09-08T17:17:02.681931: step 1433, loss 0.0338178, acc 1
2017-09-08T17:17:03.513164: step 1434, loss 0.0421635, acc 0.96875
2017-09-08T17:17:04.275872: step 1435, loss 0.0735771, acc 0.96875
2017-09-08T17:17:05.014446: step 1436, loss 0.0698763, acc 0.953125
2017-09-08T17:17:05.753728: step 1437, loss 0.0270649, acc 1
2017-09-08T17:17:06.489067: step 1438, loss 0.0692479, acc 0.96875
2017-09-08T17:17:07.260135: step 1439, loss 0.0379512, acc 0.984375
2017-09-08T17:17:07.911114: step 1440, loss 0.0696703, acc 0.96875
2017-09-08T17:17:08.594881: step 1441, loss 0.00659942, acc 1
2017-09-08T17:17:09.214776: step 1442, loss 0.0671763, acc 0.96875
2017-09-08T17:17:09.888405: step 1443, loss 0.00808592, acc 1
2017-09-08T17:17:10.585395: step 1444, loss 0.0913496, acc 0.984375
2017-09-08T17:17:11.349266: step 1445, loss 0.0735071, acc 0.96875
2017-09-08T17:17:12.151307: step 1446, loss 0.00681942, acc 1
2017-09-08T17:17:12.914785: step 1447, loss 0.00935971, acc 1
2017-09-08T17:17:13.676659: step 1448, loss 0.00705759, acc 1
2017-09-08T17:17:14.435033: step 1449, loss 0.103073, acc 0.984375
2017-09-08T17:17:15.156202: step 1450, loss 0.0056557, acc 1

Evaluation:
2017-09-08T17:17:15.942584: step 1450, loss 0.184516, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1450

2017-09-08T17:17:18.687464: step 1451, loss 0.0847267, acc 0.984375
2017-09-08T17:17:19.427316: step 1452, loss 0.0367584, acc 0.984375
2017-09-08T17:17:20.180071: step 1453, loss 0.0469063, acc 0.984375
2017-09-08T17:17:20.845071: step 1454, loss 0.0125636, acc 1
2017-09-08T17:17:21.581107: step 1455, loss 0.00680838, acc 1
2017-09-08T17:17:22.351678: step 1456, loss 0.0253162, acc 0.984375
2017-09-08T17:17:23.020858: step 1457, loss 0.0676891, acc 0.984375
2017-09-08T17:17:23.702516: step 1458, loss 0.0374122, acc 0.984375
2017-09-08T17:17:24.436076: step 1459, loss 0.0236389, acc 1
2017-09-08T17:17:25.117855: step 1460, loss 0.0128522, acc 1
2017-09-08T17:17:25.853124: step 1461, loss 0.0724576, acc 0.984375
2017-09-08T17:17:26.572443: step 1462, loss 0.0890767, acc 0.96875
2017-09-08T17:17:27.375530: step 1463, loss 0.0360185, acc 0.984375
2017-09-08T17:17:28.156696: step 1464, loss 0.115634, acc 0.96875
2017-09-08T17:17:28.865942: step 1465, loss 0.0571328, acc 0.96875
2017-09-08T17:17:29.640943: step 1466, loss 0.00609053, acc 1
2017-09-08T17:17:30.467907: step 1467, loss 0.0650895, acc 0.984375
2017-09-08T17:17:31.301349: step 1468, loss 0.0298632, acc 0.984375
2017-09-08T17:17:32.064060: step 1469, loss 0.0205729, acc 0.984375
2017-09-08T17:17:32.716856: step 1470, loss 0.0176136, acc 1
2017-09-08T17:17:33.480251: step 1471, loss 0.0238335, acc 0.984375
2017-09-08T17:17:34.369111: step 1472, loss 0.00406699, acc 1
2017-09-08T17:17:35.155066: step 1473, loss 0.0395583, acc 0.984375
2017-09-08T17:17:35.930426: step 1474, loss 0.130762, acc 0.96875
2017-09-08T17:17:36.724479: step 1475, loss 0.00744023, acc 1
2017-09-08T17:17:37.493832: step 1476, loss 0.0279304, acc 0.984375
2017-09-08T17:17:38.314904: step 1477, loss 0.00312094, acc 1
2017-09-08T17:17:39.166317: step 1478, loss 0.0842682, acc 0.953125
2017-09-08T17:17:40.029213: step 1479, loss 0.0773088, acc 0.96875
2017-09-08T17:17:40.972788: step 1480, loss 0.0097328, acc 1
2017-09-08T17:17:41.741963: step 1481, loss 0.0397062, acc 0.984375
2017-09-08T17:17:42.459346: step 1482, loss 0.0115042, acc 1
2017-09-08T17:17:43.189712: step 1483, loss 0.0823268, acc 0.96875
2017-09-08T17:17:43.915813: step 1484, loss 0.019995, acc 1
2017-09-08T17:17:44.609067: step 1485, loss 0.0571689, acc 0.984375
2017-09-08T17:17:45.346261: step 1486, loss 0.0739397, acc 0.96875
2017-09-08T17:17:46.107477: step 1487, loss 0.042873, acc 0.984375
2017-09-08T17:17:46.803746: step 1488, loss 0.0615298, acc 0.984375
2017-09-08T17:17:47.660162: step 1489, loss 0.0344971, acc 0.984375
2017-09-08T17:17:48.438243: step 1490, loss 0.0181643, acc 1
2017-09-08T17:17:49.098217: step 1491, loss 0.0593368, acc 0.984375
2017-09-08T17:17:49.787056: step 1492, loss 0.0439093, acc 0.984375
2017-09-08T17:17:50.441341: step 1493, loss 0.00643804, acc 1
2017-09-08T17:17:51.220736: step 1494, loss 0.0570733, acc 0.984375
2017-09-08T17:17:52.040723: step 1495, loss 0.00849984, acc 1
2017-09-08T17:17:52.807237: step 1496, loss 0.0144316, acc 1
2017-09-08T17:17:53.496282: step 1497, loss 0.0379493, acc 0.984375
2017-09-08T17:17:54.213032: step 1498, loss 0.036231, acc 0.984375
2017-09-08T17:17:54.839965: step 1499, loss 0.0104898, acc 1
2017-09-08T17:17:55.489773: step 1500, loss 0.054373, acc 0.984375

Evaluation:
2017-09-08T17:17:56.127394: step 1500, loss 0.183113, acc 0.942446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1500

2017-09-08T17:17:59.477532: step 1501, loss 0.102043, acc 0.96875
2017-09-08T17:18:00.276766: step 1502, loss 0.0702512, acc 0.96875
2017-09-08T17:18:01.081342: step 1503, loss 0.0081355, acc 1
2017-09-08T17:18:01.788150: step 1504, loss 0.00720308, acc 1
2017-09-08T17:18:02.542303: step 1505, loss 0.00782257, acc 1
2017-09-08T17:18:03.271869: step 1506, loss 0.108863, acc 0.984375
2017-09-08T17:18:03.873023: step 1507, loss 0.0338052, acc 0.984375
2017-09-08T17:18:04.549593: step 1508, loss 0.0146708, acc 1
2017-09-08T17:18:05.241951: step 1509, loss 0.0106738, acc 1
2017-09-08T17:18:05.780634: step 1510, loss 0.068632, acc 0.984375
2017-09-08T17:18:06.300061: step 1511, loss 0.0273586, acc 0.984375
2017-09-08T17:18:06.824514: step 1512, loss 0.0627731, acc 0.96875
2017-09-08T17:18:07.435050: step 1513, loss 0.0721967, acc 0.953125
2017-09-08T17:18:08.162190: step 1514, loss 0.043754, acc 0.984375
2017-09-08T17:18:08.866168: step 1515, loss 0.041262, acc 0.984375
2017-09-08T17:18:09.555394: step 1516, loss 0.100345, acc 0.984375
2017-09-08T17:18:10.343540: step 1517, loss 0.0101106, acc 1
2017-09-08T17:18:11.198780: step 1518, loss 0.0369273, acc 0.984375
2017-09-08T17:18:12.059402: step 1519, loss 0.024145, acc 1
2017-09-08T17:18:12.829541: step 1520, loss 0.00462154, acc 1
2017-09-08T17:18:13.602815: step 1521, loss 0.00593976, acc 1
2017-09-08T17:18:14.504628: step 1522, loss 0.0524569, acc 0.953125
2017-09-08T17:18:15.339938: step 1523, loss 0.0107457, acc 1
2017-09-08T17:18:16.165199: step 1524, loss 0.0208254, acc 1
2017-09-08T17:18:16.908924: step 1525, loss 0.0345652, acc 0.984375
2017-09-08T17:18:17.676619: step 1526, loss 0.148128, acc 0.953125
2017-09-08T17:18:18.398149: step 1527, loss 0.00746805, acc 1
2017-09-08T17:18:19.081740: step 1528, loss 0.0633814, acc 0.96875
2017-09-08T17:18:19.810523: step 1529, loss 0.0378262, acc 1
2017-09-08T17:18:20.609976: step 1530, loss 0.0139967, acc 1
2017-09-08T17:18:21.312429: step 1531, loss 0.0122178, acc 1
2017-09-08T17:18:22.003400: step 1532, loss 0.0260835, acc 0.984375
2017-09-08T17:18:22.737316: step 1533, loss 0.0231135, acc 1
2017-09-08T17:18:23.495162: step 1534, loss 0.144228, acc 0.921875
2017-09-08T17:18:24.384154: step 1535, loss 0.0648551, acc 0.984375
2017-09-08T17:18:25.170584: step 1536, loss 0.0935155, acc 0.96875
2017-09-08T17:18:25.906303: step 1537, loss 0.102514, acc 0.984375
2017-09-08T17:18:26.698463: step 1538, loss 0.0980527, acc 0.96875
2017-09-08T17:18:27.468462: step 1539, loss 0.0722999, acc 0.96875
2017-09-08T17:18:28.232216: step 1540, loss 0.0586391, acc 0.96875
2017-09-08T17:18:28.956568: step 1541, loss 0.0557405, acc 0.984375
2017-09-08T17:18:29.648126: step 1542, loss 0.0484428, acc 0.96875
2017-09-08T17:18:30.257644: step 1543, loss 0.02215, acc 1
2017-09-08T17:18:30.872016: step 1544, loss 0.0560793, acc 0.984375
2017-09-08T17:18:31.459923: step 1545, loss 0.0153778, acc 1
2017-09-08T17:18:32.138760: step 1546, loss 0.00728313, acc 1
2017-09-08T17:18:32.867984: step 1547, loss 0.0677093, acc 0.984375
2017-09-08T17:18:33.593395: step 1548, loss 0.0421944, acc 0.984375
2017-09-08T17:18:34.318644: step 1549, loss 0.0478256, acc 0.984375
2017-09-08T17:18:35.076582: step 1550, loss 0.0161059, acc 1

Evaluation:
2017-09-08T17:18:35.769868: step 1550, loss 0.190661, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1550

2017-09-08T17:18:40.193671: step 1551, loss 0.0673757, acc 0.96875
2017-09-08T17:18:40.954258: step 1552, loss 0.0072726, acc 1
2017-09-08T17:18:41.774192: step 1553, loss 0.0261548, acc 1
2017-09-08T17:18:42.544978: step 1554, loss 0.0427381, acc 0.984375
2017-09-08T17:18:43.318801: step 1555, loss 0.0143141, acc 1
2017-09-08T17:18:44.081139: step 1556, loss 0.0156294, acc 1
2017-09-08T17:18:44.846830: step 1557, loss 0.108193, acc 0.984375
2017-09-08T17:18:45.579429: step 1558, loss 0.00534459, acc 1
2017-09-08T17:18:46.251423: step 1559, loss 0.0992893, acc 0.984375
2017-09-08T17:18:46.879677: step 1560, loss 0.0302207, acc 0.984375
2017-09-08T17:18:47.551976: step 1561, loss 0.0749938, acc 0.984375
2017-09-08T17:18:48.206028: step 1562, loss 0.033764, acc 1
2017-09-08T17:18:48.929923: step 1563, loss 0.00706856, acc 1
2017-09-08T17:18:49.617403: step 1564, loss 0.0114556, acc 1
2017-09-08T17:18:50.513226: step 1565, loss 0.0104956, acc 1
2017-09-08T17:18:51.231401: step 1566, loss 0.0226028, acc 0.984375
2017-09-08T17:18:51.937463: step 1567, loss 0.0500489, acc 0.96875
2017-09-08T17:18:52.579262: step 1568, loss 0.104498, acc 0.960784
2017-09-08T17:18:53.199040: step 1569, loss 0.0121655, acc 1
2017-09-08T17:18:53.915922: step 1570, loss 0.0103992, acc 1
2017-09-08T17:18:54.503260: step 1571, loss 0.06244, acc 0.984375
2017-09-08T17:18:55.215706: step 1572, loss 0.0410146, acc 0.984375
2017-09-08T17:18:55.933675: step 1573, loss 0.0119562, acc 1
2017-09-08T17:18:56.580639: step 1574, loss 0.00960607, acc 1
2017-09-08T17:18:57.270933: step 1575, loss 0.0169643, acc 0.984375
2017-09-08T17:18:58.000867: step 1576, loss 0.025917, acc 1
2017-09-08T17:18:58.751732: step 1577, loss 0.0600499, acc 0.96875
2017-09-08T17:18:59.593651: step 1578, loss 0.0401453, acc 0.96875
2017-09-08T17:19:00.348023: step 1579, loss 0.0702182, acc 0.984375
2017-09-08T17:19:01.220182: step 1580, loss 0.0775611, acc 0.96875
2017-09-08T17:19:02.261420: step 1581, loss 0.0765739, acc 0.984375
2017-09-08T17:19:03.029689: step 1582, loss 0.00742724, acc 1
2017-09-08T17:19:03.857146: step 1583, loss 0.0243712, acc 0.984375
2017-09-08T17:19:04.736502: step 1584, loss 0.0364845, acc 0.984375
2017-09-08T17:19:05.597667: step 1585, loss 0.10687, acc 0.953125
2017-09-08T17:19:06.348165: step 1586, loss 0.0276347, acc 0.984375
2017-09-08T17:19:06.950251: step 1587, loss 0.033, acc 0.96875
2017-09-08T17:19:07.702582: step 1588, loss 0.0488936, acc 0.984375
2017-09-08T17:19:08.443042: step 1589, loss 0.0592882, acc 0.96875
2017-09-08T17:19:09.180191: step 1590, loss 0.00810006, acc 1
2017-09-08T17:19:09.911538: step 1591, loss 0.0215027, acc 0.984375
2017-09-08T17:19:10.621436: step 1592, loss 0.0268497, acc 0.984375
2017-09-08T17:19:11.428252: step 1593, loss 0.0560453, acc 0.984375
2017-09-08T17:19:12.180179: step 1594, loss 0.0177092, acc 1
2017-09-08T17:19:12.953054: step 1595, loss 0.00571365, acc 1
2017-09-08T17:19:13.730325: step 1596, loss 0.0334285, acc 0.984375
2017-09-08T17:19:14.450119: step 1597, loss 0.0794647, acc 0.96875
2017-09-08T17:19:15.247309: step 1598, loss 0.0616763, acc 0.984375
2017-09-08T17:19:15.994605: step 1599, loss 0.0509273, acc 0.984375
2017-09-08T17:19:16.664523: step 1600, loss 0.00886323, acc 1

Evaluation:
2017-09-08T17:19:17.345397: step 1600, loss 0.199328, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1600

2017-09-08T17:19:20.042139: step 1601, loss 0.0200789, acc 1
2017-09-08T17:19:20.818977: step 1602, loss 0.0196015, acc 1
2017-09-08T17:19:21.519056: step 1603, loss 0.0171731, acc 1
2017-09-08T17:19:22.390504: step 1604, loss 0.0544446, acc 0.984375
2017-09-08T17:19:23.148693: step 1605, loss 0.0196055, acc 1
2017-09-08T17:19:23.966290: step 1606, loss 0.0205067, acc 1
2017-09-08T17:19:24.714229: step 1607, loss 0.0127825, acc 1
2017-09-08T17:19:25.572740: step 1608, loss 0.0502771, acc 0.96875
2017-09-08T17:19:26.310842: step 1609, loss 0.0559155, acc 0.96875
2017-09-08T17:19:27.026666: step 1610, loss 0.0395913, acc 0.984375
2017-09-08T17:19:27.727249: step 1611, loss 0.0616265, acc 0.984375
2017-09-08T17:19:28.361564: step 1612, loss 0.10122, acc 0.96875
2017-09-08T17:19:29.026000: step 1613, loss 0.00802316, acc 1
2017-09-08T17:19:29.865785: step 1614, loss 0.262265, acc 0.921875
2017-09-08T17:19:30.592450: step 1615, loss 0.0599305, acc 0.984375
2017-09-08T17:19:31.408458: step 1616, loss 0.0104583, acc 1
2017-09-08T17:19:32.128681: step 1617, loss 0.0318116, acc 0.96875
2017-09-08T17:19:32.858611: step 1618, loss 0.0848888, acc 0.953125
2017-09-08T17:19:33.591381: step 1619, loss 0.00873831, acc 1
2017-09-08T17:19:34.357968: step 1620, loss 0.0194244, acc 1
2017-09-08T17:19:35.109566: step 1621, loss 0.00656344, acc 1
2017-09-08T17:19:35.876671: step 1622, loss 0.0255798, acc 0.984375
2017-09-08T17:19:36.651979: step 1623, loss 0.0921108, acc 0.984375
2017-09-08T17:19:37.452396: step 1624, loss 0.00629863, acc 1
2017-09-08T17:19:38.259954: step 1625, loss 0.0569781, acc 0.984375
2017-09-08T17:19:39.046611: step 1626, loss 0.0775043, acc 0.984375
2017-09-08T17:19:39.799452: step 1627, loss 0.00440993, acc 1
2017-09-08T17:19:40.583262: step 1628, loss 0.00517773, acc 1
2017-09-08T17:19:41.311377: step 1629, loss 0.0129517, acc 1
2017-09-08T17:19:42.020969: step 1630, loss 0.0858192, acc 0.984375
2017-09-08T17:19:42.736541: step 1631, loss 0.0300176, acc 0.984375
2017-09-08T17:19:43.485313: step 1632, loss 0.0760125, acc 0.984375
2017-09-08T17:19:44.232233: step 1633, loss 0.0861428, acc 0.984375
2017-09-08T17:19:45.049063: step 1634, loss 0.0773517, acc 0.984375
2017-09-08T17:19:45.760074: step 1635, loss 0.0144983, acc 1
2017-09-08T17:19:46.582486: step 1636, loss 0.055062, acc 0.953125
2017-09-08T17:19:47.582266: step 1637, loss 0.0539257, acc 0.96875
2017-09-08T17:19:48.317902: step 1638, loss 0.0217482, acc 1
2017-09-08T17:19:49.064943: step 1639, loss 0.0282238, acc 0.984375
2017-09-08T17:19:49.841572: step 1640, loss 0.0733916, acc 0.984375
2017-09-08T17:19:50.567635: step 1641, loss 0.057957, acc 0.96875
2017-09-08T17:19:51.279737: step 1642, loss 0.00714954, acc 1
2017-09-08T17:19:51.881107: step 1643, loss 0.135238, acc 0.984375
2017-09-08T17:19:52.690951: step 1644, loss 0.01626, acc 1
2017-09-08T17:19:53.509083: step 1645, loss 0.00779068, acc 1
2017-09-08T17:19:54.261089: step 1646, loss 0.00821694, acc 1
2017-09-08T17:19:54.972724: step 1647, loss 0.0906374, acc 0.96875
2017-09-08T17:19:55.797753: step 1648, loss 0.0543703, acc 0.96875
2017-09-08T17:19:56.505926: step 1649, loss 0.071718, acc 0.96875
2017-09-08T17:19:57.162400: step 1650, loss 0.0130488, acc 1

Evaluation:
2017-09-08T17:19:57.961854: step 1650, loss 0.193028, acc 0.943885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1650

2017-09-08T17:20:01.645047: step 1651, loss 0.0789644, acc 0.984375
2017-09-08T17:20:02.338622: step 1652, loss 0.0138081, acc 1
2017-09-08T17:20:03.101873: step 1653, loss 0.0736835, acc 0.984375
2017-09-08T17:20:03.864842: step 1654, loss 0.021265, acc 1
2017-09-08T17:20:04.566178: step 1655, loss 0.0105415, acc 1
2017-09-08T17:20:05.264770: step 1656, loss 0.0289313, acc 0.984375
2017-09-08T17:20:06.011320: step 1657, loss 0.0106413, acc 1
2017-09-08T17:20:06.698776: step 1658, loss 0.0379669, acc 0.984375
2017-09-08T17:20:07.431976: step 1659, loss 0.031, acc 0.984375
2017-09-08T17:20:08.069816: step 1660, loss 0.0398842, acc 0.984375
2017-09-08T17:20:08.687494: step 1661, loss 0.0179537, acc 1
2017-09-08T17:20:09.336009: step 1662, loss 0.0061352, acc 1
2017-09-08T17:20:09.943602: step 1663, loss 0.0117022, acc 1
2017-09-08T17:20:10.588155: step 1664, loss 0.0567246, acc 0.984375
2017-09-08T17:20:11.377079: step 1665, loss 0.0124626, acc 1
2017-09-08T17:20:12.029462: step 1666, loss 0.0328375, acc 0.980392
2017-09-08T17:20:12.817877: step 1667, loss 0.0660314, acc 0.96875
2017-09-08T17:20:13.510723: step 1668, loss 0.082688, acc 0.96875
2017-09-08T17:20:14.132271: step 1669, loss 0.0149295, acc 1
2017-09-08T17:20:14.851983: step 1670, loss 0.0139328, acc 1
2017-09-08T17:20:15.612111: step 1671, loss 0.101005, acc 0.984375
2017-09-08T17:20:16.370952: step 1672, loss 0.0129289, acc 1
2017-09-08T17:20:17.115626: step 1673, loss 0.0113914, acc 1
2017-09-08T17:20:17.866694: step 1674, loss 0.018377, acc 1
2017-09-08T17:20:18.593506: step 1675, loss 0.0322994, acc 0.984375
2017-09-08T17:20:19.245530: step 1676, loss 0.0775777, acc 0.96875
2017-09-08T17:20:20.002519: step 1677, loss 0.0497147, acc 0.96875
2017-09-08T17:20:20.781396: step 1678, loss 0.0227262, acc 0.984375
2017-09-08T17:20:21.549039: step 1679, loss 0.00823871, acc 1
2017-09-08T17:20:22.280495: step 1680, loss 0.048691, acc 0.984375
2017-09-08T17:20:23.084193: step 1681, loss 0.00470427, acc 1
2017-09-08T17:20:23.861185: step 1682, loss 0.115474, acc 0.953125
2017-09-08T17:20:24.653515: step 1683, loss 0.0403192, acc 0.984375
2017-09-08T17:20:25.436247: step 1684, loss 0.0507187, acc 0.984375
2017-09-08T17:20:26.281640: step 1685, loss 0.00372253, acc 1
2017-09-08T17:20:27.011003: step 1686, loss 0.0228458, acc 0.984375
2017-09-08T17:20:27.807136: step 1687, loss 0.011596, acc 1
2017-09-08T17:20:28.503600: step 1688, loss 0.0211596, acc 0.984375
2017-09-08T17:20:29.246259: step 1689, loss 0.101418, acc 0.96875
2017-09-08T17:20:29.906513: step 1690, loss 0.00593011, acc 1
2017-09-08T17:20:30.581205: step 1691, loss 0.0478422, acc 0.96875
2017-09-08T17:20:31.339090: step 1692, loss 0.0113716, acc 1
2017-09-08T17:20:32.044341: step 1693, loss 0.0226783, acc 1
2017-09-08T17:20:32.747540: step 1694, loss 0.00616394, acc 1
2017-09-08T17:20:33.515629: step 1695, loss 0.0257728, acc 0.984375
2017-09-08T17:20:34.196228: step 1696, loss 0.0857449, acc 0.96875
2017-09-08T17:20:34.966996: step 1697, loss 0.0160933, acc 1
2017-09-08T17:20:35.812139: step 1698, loss 0.0112519, acc 1
2017-09-08T17:20:36.670050: step 1699, loss 0.00978938, acc 1
2017-09-08T17:20:37.407223: step 1700, loss 0.0184115, acc 0.984375

Evaluation:
2017-09-08T17:20:38.246077: step 1700, loss 0.194157, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1700

2017-09-08T17:20:41.176736: step 1701, loss 0.0902713, acc 0.953125
2017-09-08T17:20:41.864973: step 1702, loss 0.00991506, acc 1
2017-09-08T17:20:42.631211: step 1703, loss 0.0369324, acc 0.984375
2017-09-08T17:20:43.334973: step 1704, loss 0.0265815, acc 0.984375
2017-09-08T17:20:44.081744: step 1705, loss 0.00669806, acc 1
2017-09-08T17:20:44.886515: step 1706, loss 0.0111645, acc 1
2017-09-08T17:20:45.631729: step 1707, loss 0.109669, acc 0.96875
2017-09-08T17:20:46.452445: step 1708, loss 0.0347416, acc 0.984375
2017-09-08T17:20:47.186010: step 1709, loss 0.0796719, acc 0.984375
2017-09-08T17:20:48.011085: step 1710, loss 0.00766989, acc 1
2017-09-08T17:20:48.779702: step 1711, loss 0.021452, acc 1
2017-09-08T17:20:49.540660: step 1712, loss 0.00373418, acc 1
2017-09-08T17:20:50.192198: step 1713, loss 0.00841001, acc 1
2017-09-08T17:20:50.797879: step 1714, loss 0.0115913, acc 1
2017-09-08T17:20:51.327231: step 1715, loss 0.10344, acc 0.96875
2017-09-08T17:20:51.988367: step 1716, loss 0.0134937, acc 1
2017-09-08T17:20:52.620894: step 1717, loss 0.0315891, acc 0.984375
2017-09-08T17:20:53.892679: step 1718, loss 0.008101, acc 1
2017-09-08T17:20:54.649614: step 1719, loss 0.0144001, acc 1
2017-09-08T17:20:55.393309: step 1720, loss 0.0185357, acc 1
2017-09-08T17:20:56.190281: step 1721, loss 0.026073, acc 0.984375
2017-09-08T17:20:56.977018: step 1722, loss 0.0593503, acc 0.984375
2017-09-08T17:20:57.737162: step 1723, loss 0.104363, acc 0.96875
2017-09-08T17:20:58.493527: step 1724, loss 0.00628828, acc 1
2017-09-08T17:20:59.324197: step 1725, loss 0.0191256, acc 0.984375
2017-09-08T17:21:00.166448: step 1726, loss 0.0173515, acc 1
2017-09-08T17:21:00.956094: step 1727, loss 0.028999, acc 0.984375
2017-09-08T17:21:01.841689: step 1728, loss 0.116751, acc 0.953125
2017-09-08T17:21:02.593893: step 1729, loss 0.0499557, acc 0.984375
2017-09-08T17:21:03.358845: step 1730, loss 0.00861657, acc 1
2017-09-08T17:21:04.079155: step 1731, loss 0.0105646, acc 1
2017-09-08T17:21:04.874956: step 1732, loss 0.0341866, acc 0.984375
2017-09-08T17:21:05.662823: step 1733, loss 0.0568332, acc 0.984375
2017-09-08T17:21:06.419316: step 1734, loss 0.0181327, acc 1
2017-09-08T17:21:07.174520: step 1735, loss 0.00428467, acc 1
2017-09-08T17:21:07.995449: step 1736, loss 0.0540829, acc 0.984375
2017-09-08T17:21:08.646500: step 1737, loss 0.00416492, acc 1
2017-09-08T17:21:09.338256: step 1738, loss 0.034701, acc 0.984375
2017-09-08T17:21:10.061088: step 1739, loss 0.0108737, acc 1
2017-09-08T17:21:10.864161: step 1740, loss 0.0504243, acc 0.96875
2017-09-08T17:21:11.700977: step 1741, loss 0.00439491, acc 1
2017-09-08T17:21:12.460079: step 1742, loss 0.196453, acc 0.953125
2017-09-08T17:21:13.245432: step 1743, loss 0.0560688, acc 0.984375
2017-09-08T17:21:13.954703: step 1744, loss 0.00962661, acc 1
2017-09-08T17:21:14.685733: step 1745, loss 0.00989498, acc 1
2017-09-08T17:21:15.392630: step 1746, loss 0.162645, acc 0.953125
2017-09-08T17:21:16.111322: step 1747, loss 0.0339077, acc 0.984375
2017-09-08T17:21:16.784018: step 1748, loss 0.0260015, acc 0.984375
2017-09-08T17:21:17.495319: step 1749, loss 0.101281, acc 0.96875
2017-09-08T17:21:18.177273: step 1750, loss 0.0557438, acc 0.96875

Evaluation:
2017-09-08T17:21:18.846555: step 1750, loss 0.195477, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1750

2017-09-08T17:21:22.263106: step 1751, loss 0.0210967, acc 0.984375
2017-09-08T17:21:22.956519: step 1752, loss 0.0554543, acc 0.984375
2017-09-08T17:21:23.950953: step 1753, loss 0.0451742, acc 0.984375
2017-09-08T17:21:24.778147: step 1754, loss 0.0614874, acc 0.96875
2017-09-08T17:21:25.602087: step 1755, loss 0.0301295, acc 0.984375
2017-09-08T17:21:26.369698: step 1756, loss 0.0117713, acc 1
2017-09-08T17:21:27.147994: step 1757, loss 0.00264086, acc 1
2017-09-08T17:21:27.839581: step 1758, loss 0.0201972, acc 1
2017-09-08T17:21:28.603585: step 1759, loss 0.0291757, acc 0.984375
2017-09-08T17:21:29.388722: step 1760, loss 0.0371063, acc 1
2017-09-08T17:21:30.097339: step 1761, loss 0.0683587, acc 0.984375
2017-09-08T17:21:30.715952: step 1762, loss 0.0157195, acc 1
2017-09-08T17:21:31.308812: step 1763, loss 0.0425575, acc 0.984375
2017-09-08T17:21:31.871507: step 1764, loss 0.0843094, acc 0.980392
2017-09-08T17:21:32.571719: step 1765, loss 0.0152573, acc 1
2017-09-08T17:21:33.090712: step 1766, loss 0.0193855, acc 1
2017-09-08T17:21:34.062966: step 1767, loss 0.0441376, acc 0.96875
2017-09-08T17:21:34.867278: step 1768, loss 0.0988009, acc 0.96875
2017-09-08T17:21:35.647894: step 1769, loss 0.00464743, acc 1
2017-09-08T17:21:36.410582: step 1770, loss 0.0280015, acc 0.984375
2017-09-08T17:21:37.139433: step 1771, loss 0.0194388, acc 1
2017-09-08T17:21:37.935880: step 1772, loss 0.00961643, acc 1
2017-09-08T17:21:38.743349: step 1773, loss 0.021368, acc 1
2017-09-08T17:21:39.451732: step 1774, loss 0.0229797, acc 0.984375
2017-09-08T17:21:40.212881: step 1775, loss 0.0625167, acc 0.96875
2017-09-08T17:21:41.039108: step 1776, loss 0.0120023, acc 1
2017-09-08T17:21:41.797389: step 1777, loss 0.0141618, acc 1
2017-09-08T17:21:42.556416: step 1778, loss 0.00707696, acc 1
2017-09-08T17:21:43.278118: step 1779, loss 0.0764369, acc 0.96875
2017-09-08T17:21:44.013150: step 1780, loss 0.00883324, acc 1
2017-09-08T17:21:44.691404: step 1781, loss 0.102858, acc 0.984375
2017-09-08T17:21:45.496302: step 1782, loss 0.0158818, acc 1
2017-09-08T17:21:46.275300: step 1783, loss 0.0527288, acc 0.96875
2017-09-08T17:21:47.097487: step 1784, loss 0.0095645, acc 1
2017-09-08T17:21:47.944775: step 1785, loss 0.00310636, acc 1
2017-09-08T17:21:48.679426: step 1786, loss 0.0913378, acc 0.96875
2017-09-08T17:21:49.429068: step 1787, loss 0.0722415, acc 0.984375
2017-09-08T17:21:50.150093: step 1788, loss 0.0840775, acc 0.96875
2017-09-08T17:21:50.965166: step 1789, loss 0.0332204, acc 0.984375
2017-09-08T17:21:51.716930: step 1790, loss 0.0340249, acc 0.984375
2017-09-08T17:21:52.535398: step 1791, loss 0.026415, acc 0.984375
2017-09-08T17:21:53.342774: step 1792, loss 0.00715474, acc 1
2017-09-08T17:21:54.152383: step 1793, loss 0.0532056, acc 0.984375
2017-09-08T17:21:54.910260: step 1794, loss 0.00725248, acc 1
2017-09-08T17:21:55.645394: step 1795, loss 0.0664971, acc 0.96875
2017-09-08T17:21:56.451820: step 1796, loss 0.0982504, acc 0.96875
2017-09-08T17:21:57.272872: step 1797, loss 0.0416915, acc 0.984375
2017-09-08T17:21:57.888623: step 1798, loss 0.0891599, acc 0.984375
2017-09-08T17:21:58.632836: step 1799, loss 0.00187144, acc 1
2017-09-08T17:21:59.318105: step 1800, loss 0.0546257, acc 0.984375

Evaluation:
2017-09-08T17:22:00.082840: step 1800, loss 0.1878, acc 0.945324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1800

2017-09-08T17:22:03.744012: step 1801, loss 0.117704, acc 0.96875
2017-09-08T17:22:04.484780: step 1802, loss 0.0236181, acc 0.984375
2017-09-08T17:22:05.213210: step 1803, loss 0.0118168, acc 1
2017-09-08T17:22:05.976480: step 1804, loss 0.0151857, acc 1
2017-09-08T17:22:06.735847: step 1805, loss 0.136242, acc 0.96875
2017-09-08T17:22:07.514593: step 1806, loss 0.0427472, acc 0.984375
2017-09-08T17:22:08.282264: step 1807, loss 0.0588194, acc 0.984375
2017-09-08T17:22:09.007585: step 1808, loss 0.0251908, acc 0.984375
2017-09-08T17:22:09.778678: step 1809, loss 0.0436822, acc 0.984375
2017-09-08T17:22:10.576867: step 1810, loss 0.00696677, acc 1
2017-09-08T17:22:11.407535: step 1811, loss 0.0218501, acc 1
2017-09-08T17:22:12.134407: step 1812, loss 0.06654, acc 0.96875
2017-09-08T17:22:12.807997: step 1813, loss 0.00741228, acc 1
2017-09-08T17:22:13.443587: step 1814, loss 0.0450694, acc 0.984375
2017-09-08T17:22:14.086449: step 1815, loss 0.0910134, acc 0.96875
2017-09-08T17:22:14.755229: step 1816, loss 0.0639737, acc 0.96875
2017-09-08T17:22:15.540598: step 1817, loss 0.0730142, acc 0.96875
2017-09-08T17:22:16.300332: step 1818, loss 0.00313485, acc 1
2017-09-08T17:22:17.153128: step 1819, loss 0.0225583, acc 0.984375
2017-09-08T17:22:17.885644: step 1820, loss 0.00895866, acc 1
2017-09-08T17:22:18.578898: step 1821, loss 0.0179539, acc 1
2017-09-08T17:22:19.798218: step 1822, loss 0.0329692, acc 0.984375
2017-09-08T17:22:20.596298: step 1823, loss 0.0308419, acc 1
2017-09-08T17:22:21.324947: step 1824, loss 0.0051928, acc 1
2017-09-08T17:22:22.100370: step 1825, loss 0.0070278, acc 1
2017-09-08T17:22:22.840245: step 1826, loss 0.170521, acc 0.953125
2017-09-08T17:22:23.577686: step 1827, loss 0.0621778, acc 0.96875
2017-09-08T17:22:24.378353: step 1828, loss 0.041761, acc 0.984375
2017-09-08T17:22:25.120089: step 1829, loss 0.0339955, acc 0.984375
2017-09-08T17:22:25.860524: step 1830, loss 0.0812918, acc 0.96875
2017-09-08T17:22:26.667610: step 1831, loss 0.055161, acc 0.984375
2017-09-08T17:22:27.449796: step 1832, loss 0.0126563, acc 1
2017-09-08T17:22:28.151223: step 1833, loss 0.0433288, acc 0.984375
2017-09-08T17:22:28.830177: step 1834, loss 0.0216749, acc 1
2017-09-08T17:22:29.555565: step 1835, loss 0.0365614, acc 0.984375
2017-09-08T17:22:30.203908: step 1836, loss 0.112052, acc 0.9375
2017-09-08T17:22:30.770190: step 1837, loss 0.0327641, acc 1
2017-09-08T17:22:31.355737: step 1838, loss 0.0243389, acc 0.984375
2017-09-08T17:22:32.098319: step 1839, loss 0.00959433, acc 1
2017-09-08T17:22:32.792848: step 1840, loss 0.0360304, acc 0.984375
2017-09-08T17:22:33.581138: step 1841, loss 0.110603, acc 0.96875
2017-09-08T17:22:34.367046: step 1842, loss 0.0847701, acc 0.96875
2017-09-08T17:22:35.105192: step 1843, loss 0.00548536, acc 1
2017-09-08T17:22:35.839893: step 1844, loss 0.0605975, acc 0.96875
2017-09-08T17:22:36.644395: step 1845, loss 0.0971186, acc 0.984375
2017-09-08T17:22:37.398454: step 1846, loss 0.00672513, acc 1
2017-09-08T17:22:38.179382: step 1847, loss 0.00426935, acc 1
2017-09-08T17:22:38.901367: step 1848, loss 0.133363, acc 0.984375
2017-09-08T17:22:39.601301: step 1849, loss 0.0175741, acc 0.984375
2017-09-08T17:22:40.349534: step 1850, loss 0.00593215, acc 1

Evaluation:
2017-09-08T17:22:41.047276: step 1850, loss 0.196007, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1850

2017-09-08T17:22:43.518430: step 1851, loss 0.020485, acc 0.984375
2017-09-08T17:22:44.352953: step 1852, loss 0.0461984, acc 0.984375
2017-09-08T17:22:45.015134: step 1853, loss 0.153396, acc 0.9375
2017-09-08T17:22:45.742492: step 1854, loss 0.0231285, acc 0.984375
2017-09-08T17:22:46.445794: step 1855, loss 0.00782799, acc 1
2017-09-08T17:22:47.213784: step 1856, loss 0.0121174, acc 1
2017-09-08T17:22:48.039808: step 1857, loss 0.00414441, acc 1
2017-09-08T17:22:48.793627: step 1858, loss 0.0359192, acc 0.984375
2017-09-08T17:22:49.648372: step 1859, loss 0.0119716, acc 1
2017-09-08T17:22:50.393707: step 1860, loss 0.0357161, acc 0.984375
2017-09-08T17:22:51.172853: step 1861, loss 0.149616, acc 0.953125
2017-09-08T17:22:51.823876: step 1862, loss 0.0140692, acc 1
2017-09-08T17:22:52.504961: step 1863, loss 0.00300952, acc 1
2017-09-08T17:22:53.219795: step 1864, loss 0.0330893, acc 0.984375
2017-09-08T17:22:53.939284: step 1865, loss 0.00679077, acc 1
2017-09-08T17:22:54.544327: step 1866, loss 0.0103755, acc 1
2017-09-08T17:22:55.154175: step 1867, loss 0.0401999, acc 0.984375
2017-09-08T17:22:55.898035: step 1868, loss 0.00580246, acc 1
2017-09-08T17:22:56.712940: step 1869, loss 0.138354, acc 0.96875
2017-09-08T17:22:57.465849: step 1870, loss 0.0138526, acc 1
2017-09-08T17:22:58.260659: step 1871, loss 0.0784298, acc 0.984375
2017-09-08T17:22:59.053242: step 1872, loss 0.0253397, acc 0.984375
2017-09-08T17:22:59.823025: step 1873, loss 0.00403332, acc 1
2017-09-08T17:23:00.548476: step 1874, loss 0.00514919, acc 1
2017-09-08T17:23:01.390967: step 1875, loss 0.0170725, acc 0.984375
2017-09-08T17:23:02.207133: step 1876, loss 0.147162, acc 0.953125
2017-09-08T17:23:03.044228: step 1877, loss 0.0420466, acc 0.984375
2017-09-08T17:23:03.793345: step 1878, loss 0.0461236, acc 0.984375
2017-09-08T17:23:04.791574: step 1879, loss 0.0237252, acc 0.984375
2017-09-08T17:23:05.574329: step 1880, loss 0.0109015, acc 1
2017-09-08T17:23:06.356974: step 1881, loss 0.00830408, acc 1
2017-09-08T17:23:07.144730: step 1882, loss 0.0855925, acc 0.984375
2017-09-08T17:23:07.908623: step 1883, loss 0.0872406, acc 0.953125
2017-09-08T17:23:08.681777: step 1884, loss 0.0104213, acc 1
2017-09-08T17:23:09.415631: step 1885, loss 0.0464916, acc 0.984375
2017-09-08T17:23:10.142765: step 1886, loss 0.0300295, acc 0.984375
2017-09-08T17:23:10.878256: step 1887, loss 0.0417536, acc 0.984375
2017-09-08T17:23:11.685950: step 1888, loss 0.015971, acc 1
2017-09-08T17:23:12.502330: step 1889, loss 0.0248545, acc 1
2017-09-08T17:23:13.322580: step 1890, loss 0.0150223, acc 1
2017-09-08T17:23:14.115781: step 1891, loss 0.0451143, acc 0.984375
2017-09-08T17:23:14.957673: step 1892, loss 0.0331153, acc 0.984375
2017-09-08T17:23:15.772797: step 1893, loss 0.061943, acc 0.96875
2017-09-08T17:23:16.613852: step 1894, loss 0.0127648, acc 1
2017-09-08T17:23:17.330502: step 1895, loss 0.0272621, acc 0.984375
2017-09-08T17:23:18.070412: step 1896, loss 0.00679278, acc 1
2017-09-08T17:23:18.773358: step 1897, loss 0.0582657, acc 0.96875
2017-09-08T17:23:19.393876: step 1898, loss 0.027617, acc 0.984375
2017-09-08T17:23:20.032574: step 1899, loss 0.0106091, acc 1
2017-09-08T17:23:20.665639: step 1900, loss 0.0291161, acc 1

Evaluation:
2017-09-08T17:23:21.334606: step 1900, loss 0.193761, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1900

2017-09-08T17:23:25.117033: step 1901, loss 0.00690295, acc 1
2017-09-08T17:23:25.856438: step 1902, loss 0.0188981, acc 0.984375
2017-09-08T17:23:26.670921: step 1903, loss 0.0355302, acc 0.984375
2017-09-08T17:23:27.490105: step 1904, loss 0.00458919, acc 1
2017-09-08T17:23:28.217221: step 1905, loss 0.00927528, acc 1
2017-09-08T17:23:29.098354: step 1906, loss 0.0332361, acc 0.984375
2017-09-08T17:23:29.868608: step 1907, loss 0.0255235, acc 1
2017-09-08T17:23:30.729554: step 1908, loss 0.0674973, acc 0.96875
2017-09-08T17:23:31.511459: step 1909, loss 0.0131544, acc 1
2017-09-08T17:23:32.203109: step 1910, loss 0.0123188, acc 1
2017-09-08T17:23:32.973581: step 1911, loss 0.0382259, acc 0.984375
2017-09-08T17:23:33.717969: step 1912, loss 0.0320608, acc 0.984375
2017-09-08T17:23:34.481605: step 1913, loss 0.0651386, acc 0.96875
2017-09-08T17:23:35.058502: step 1914, loss 0.00651625, acc 1
2017-09-08T17:23:35.606264: step 1915, loss 0.122978, acc 0.953125
2017-09-08T17:23:36.327168: step 1916, loss 0.0342709, acc 0.984375
2017-09-08T17:23:36.977641: step 1917, loss 0.0199323, acc 1
2017-09-08T17:23:37.695092: step 1918, loss 0.155398, acc 0.9375
2017-09-08T17:23:38.442936: step 1919, loss 0.008569, acc 1
2017-09-08T17:23:39.189462: step 1920, loss 0.00782357, acc 1
2017-09-08T17:23:40.051703: step 1921, loss 0.0149321, acc 1
2017-09-08T17:23:40.757726: step 1922, loss 0.0670581, acc 0.984375
2017-09-08T17:23:41.465927: step 1923, loss 0.00587259, acc 1
2017-09-08T17:23:42.208283: step 1924, loss 0.00720794, acc 1
2017-09-08T17:23:42.982880: step 1925, loss 0.0229748, acc 0.984375
2017-09-08T17:23:43.728483: step 1926, loss 0.0656334, acc 0.96875
2017-09-08T17:23:44.474849: step 1927, loss 0.00722393, acc 1
2017-09-08T17:23:45.198191: step 1928, loss 0.0545748, acc 0.96875
2017-09-08T17:23:45.889627: step 1929, loss 0.0788505, acc 0.96875
2017-09-08T17:23:46.538024: step 1930, loss 0.0638698, acc 0.984375
2017-09-08T17:23:47.227651: step 1931, loss 0.00619214, acc 1
2017-09-08T17:23:47.862129: step 1932, loss 0.0406066, acc 0.984375
2017-09-08T17:23:48.717918: step 1933, loss 0.016048, acc 1
2017-09-08T17:23:49.471478: step 1934, loss 0.00657729, acc 1
2017-09-08T17:23:50.164206: step 1935, loss 0.01286, acc 1
2017-09-08T17:23:50.836980: step 1936, loss 0.016647, acc 1
2017-09-08T17:23:51.556964: step 1937, loss 0.0428171, acc 0.984375
2017-09-08T17:23:52.310847: step 1938, loss 0.0509762, acc 0.984375
2017-09-08T17:23:53.034607: step 1939, loss 0.076764, acc 0.96875
2017-09-08T17:23:53.860448: step 1940, loss 0.00752818, acc 1
2017-09-08T17:23:54.656724: step 1941, loss 0.0647298, acc 0.984375
2017-09-08T17:23:55.457895: step 1942, loss 0.0281564, acc 0.984375
2017-09-08T17:23:56.307796: step 1943, loss 0.00172693, acc 1
2017-09-08T17:23:57.046378: step 1944, loss 0.0264867, acc 0.984375
2017-09-08T17:23:57.774189: step 1945, loss 0.0366411, acc 0.984375
2017-09-08T17:23:58.595674: step 1946, loss 0.011473, acc 1
2017-09-08T17:23:59.397100: step 1947, loss 0.0169258, acc 0.984375
2017-09-08T17:24:00.218333: step 1948, loss 0.00599292, acc 1
2017-09-08T17:24:00.988031: step 1949, loss 0.00866448, acc 1
2017-09-08T17:24:01.728222: step 1950, loss 0.0267492, acc 0.984375

Evaluation:
2017-09-08T17:24:02.457261: step 1950, loss 0.192554, acc 0.943885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-1950

2017-09-08T17:24:05.338456: step 1951, loss 0.00620552, acc 1
2017-09-08T17:24:06.083491: step 1952, loss 0.105973, acc 0.96875
2017-09-08T17:24:06.831515: step 1953, loss 0.0281825, acc 0.984375
2017-09-08T17:24:07.650994: step 1954, loss 0.0459668, acc 0.984375
2017-09-08T17:24:08.462548: step 1955, loss 0.0415803, acc 0.984375
2017-09-08T17:24:09.193775: step 1956, loss 0.144208, acc 0.96875
2017-09-08T17:24:09.933283: step 1957, loss 0.148453, acc 0.96875
2017-09-08T17:24:10.670668: step 1958, loss 0.0401235, acc 0.984375
2017-09-08T17:24:11.453325: step 1959, loss 0.0153162, acc 1
2017-09-08T17:24:12.133716: step 1960, loss 0.0133542, acc 1
2017-09-08T17:24:12.945171: step 1961, loss 0.0109974, acc 1
2017-09-08T17:24:13.661119: step 1962, loss 0.0254684, acc 0.984375
2017-09-08T17:24:14.373238: step 1963, loss 0.00574147, acc 1
2017-09-08T17:24:15.278245: step 1964, loss 0.0741692, acc 0.984375
2017-09-08T17:24:16.069567: step 1965, loss 0.0764869, acc 0.96875
2017-09-08T17:24:16.904770: step 1966, loss 0.0093808, acc 1
2017-09-08T17:24:17.595635: step 1967, loss 0.00366745, acc 1
2017-09-08T17:24:18.348267: step 1968, loss 0.00918526, acc 1
2017-09-08T17:24:19.119314: step 1969, loss 0.0139361, acc 1
2017-09-08T17:24:19.894909: step 1970, loss 0.0122199, acc 1
2017-09-08T17:24:20.704861: step 1971, loss 0.0730613, acc 0.96875
2017-09-08T17:24:21.466434: step 1972, loss 0.073472, acc 0.984375
2017-09-08T17:24:22.246619: step 1973, loss 0.0127312, acc 1
2017-09-08T17:24:22.961447: step 1974, loss 0.0301949, acc 1
2017-09-08T17:24:23.626180: step 1975, loss 0.00797511, acc 1
2017-09-08T17:24:24.250036: step 1976, loss 0.00235797, acc 1
2017-09-08T17:24:24.965136: step 1977, loss 0.00554727, acc 1
2017-09-08T17:24:25.822221: step 1978, loss 0.00804323, acc 1
2017-09-08T17:24:26.718060: step 1979, loss 0.0172117, acc 1
2017-09-08T17:24:27.401306: step 1980, loss 0.0962687, acc 0.96875
2017-09-08T17:24:27.995007: step 1981, loss 0.0585782, acc 0.984375
2017-09-08T17:24:28.706500: step 1982, loss 0.0347374, acc 0.984375
2017-09-08T17:24:29.436035: step 1983, loss 0.0376936, acc 0.984375
2017-09-08T17:24:30.151352: step 1984, loss 0.00584899, acc 1
2017-09-08T17:24:30.868540: step 1985, loss 0.0494102, acc 0.984375
2017-09-08T17:24:31.662789: step 1986, loss 0.00318973, acc 1
2017-09-08T17:24:32.420203: step 1987, loss 0.065521, acc 0.96875
2017-09-08T17:24:33.159363: step 1988, loss 0.0232575, acc 0.984375
2017-09-08T17:24:33.917116: step 1989, loss 0.163063, acc 0.96875
2017-09-08T17:24:34.763771: step 1990, loss 0.0100074, acc 1
2017-09-08T17:24:35.511605: step 1991, loss 0.0168441, acc 1
2017-09-08T17:24:36.229367: step 1992, loss 0.0548164, acc 0.96875
2017-09-08T17:24:37.034394: step 1993, loss 0.0159328, acc 1
2017-09-08T17:24:37.832940: step 1994, loss 0.0305502, acc 0.984375
2017-09-08T17:24:38.615856: step 1995, loss 0.00390486, acc 1
2017-09-08T17:24:39.388922: step 1996, loss 0.0323351, acc 0.984375
2017-09-08T17:24:40.224398: step 1997, loss 0.00433766, acc 1
2017-09-08T17:24:41.017181: step 1998, loss 0.0972727, acc 0.96875
2017-09-08T17:24:41.845887: step 1999, loss 0.0180802, acc 1
2017-09-08T17:24:42.482108: step 2000, loss 0.00537415, acc 1

Evaluation:
2017-09-08T17:24:43.211536: step 2000, loss 0.191703, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2000

2017-09-08T17:24:46.463656: step 2001, loss 0.0196537, acc 0.984375
2017-09-08T17:24:47.254594: step 2002, loss 0.0144686, acc 1
2017-09-08T17:24:48.118067: step 2003, loss 0.0901552, acc 0.953125
2017-09-08T17:24:48.800663: step 2004, loss 0.0281032, acc 0.984375
2017-09-08T17:24:49.476060: step 2005, loss 0.0407995, acc 0.984375
2017-09-08T17:24:50.127491: step 2006, loss 0.00445402, acc 1
2017-09-08T17:24:50.829461: step 2007, loss 0.00620287, acc 1
2017-09-08T17:24:51.564067: step 2008, loss 0.0660162, acc 0.984375
2017-09-08T17:24:52.298433: step 2009, loss 0.0493402, acc 0.984375
2017-09-08T17:24:53.028545: step 2010, loss 0.0348165, acc 0.984375
2017-09-08T17:24:53.821571: step 2011, loss 0.00600104, acc 1
2017-09-08T17:24:54.539808: step 2012, loss 0.00617088, acc 1
2017-09-08T17:24:55.285133: step 2013, loss 0.00617775, acc 1
2017-09-08T17:24:55.983774: step 2014, loss 0.0727324, acc 0.953125
2017-09-08T17:24:56.652277: step 2015, loss 0.00870023, acc 1
2017-09-08T17:24:57.484753: step 2016, loss 0.00450025, acc 1
2017-09-08T17:24:58.058955: step 2017, loss 0.113338, acc 0.96875
2017-09-08T17:24:58.682949: step 2018, loss 0.0697948, acc 0.96875
2017-09-08T17:24:59.413400: step 2019, loss 0.0479645, acc 0.984375
2017-09-08T17:25:00.060976: step 2020, loss 0.0459047, acc 0.984375
2017-09-08T17:25:00.785058: step 2021, loss 0.0717088, acc 0.984375
2017-09-08T17:25:01.584227: step 2022, loss 0.0631451, acc 0.96875
2017-09-08T17:25:02.351510: step 2023, loss 0.0940544, acc 0.96875
2017-09-08T17:25:03.183216: step 2024, loss 0.00693928, acc 1
2017-09-08T17:25:04.077870: step 2025, loss 0.00416696, acc 1
2017-09-08T17:25:04.795347: step 2026, loss 0.0114296, acc 1
2017-09-08T17:25:05.543001: step 2027, loss 0.00286033, acc 1
2017-09-08T17:25:06.330937: step 2028, loss 0.0197497, acc 0.984375
2017-09-08T17:25:07.054582: step 2029, loss 0.0241013, acc 0.984375
2017-09-08T17:25:07.836943: step 2030, loss 0.0710043, acc 0.96875
2017-09-08T17:25:08.614143: step 2031, loss 0.00389143, acc 1
2017-09-08T17:25:09.479261: step 2032, loss 0.040726, acc 0.984375
2017-09-08T17:25:10.241468: step 2033, loss 0.0146219, acc 1
2017-09-08T17:25:10.995422: step 2034, loss 0.04376, acc 0.984375
2017-09-08T17:25:11.819773: step 2035, loss 0.00734082, acc 1
2017-09-08T17:25:12.619754: step 2036, loss 0.0084395, acc 1
2017-09-08T17:25:13.395530: step 2037, loss 0.0527531, acc 0.953125
2017-09-08T17:25:14.156067: step 2038, loss 0.012813, acc 1
2017-09-08T17:25:14.887105: step 2039, loss 0.0459221, acc 0.984375
2017-09-08T17:25:15.667140: step 2040, loss 0.0387065, acc 0.984375
2017-09-08T17:25:16.373898: step 2041, loss 0.0257703, acc 0.984375
2017-09-08T17:25:17.055265: step 2042, loss 0.0203303, acc 1
2017-09-08T17:25:17.854041: step 2043, loss 0.00515951, acc 1
2017-09-08T17:25:18.596489: step 2044, loss 0.0135196, acc 1
2017-09-08T17:25:19.369415: step 2045, loss 0.0131341, acc 1
2017-09-08T17:25:20.121919: step 2046, loss 0.0139017, acc 1
2017-09-08T17:25:20.892950: step 2047, loss 0.0400495, acc 0.984375
2017-09-08T17:25:21.603901: step 2048, loss 0.0159475, acc 0.984375
2017-09-08T17:25:22.361788: step 2049, loss 0.0301369, acc 0.984375
2017-09-08T17:25:23.092412: step 2050, loss 0.00734323, acc 1

Evaluation:
2017-09-08T17:25:23.852836: step 2050, loss 0.185615, acc 0.942446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2050

2017-09-08T17:25:28.267741: step 2051, loss 0.00774488, acc 1
2017-09-08T17:25:29.061481: step 2052, loss 0.0514515, acc 0.984375
2017-09-08T17:25:29.835693: step 2053, loss 0.00793892, acc 1
2017-09-08T17:25:30.677520: step 2054, loss 0.00344769, acc 1
2017-09-08T17:25:31.407347: step 2055, loss 0.00874949, acc 1
2017-09-08T17:25:32.121340: step 2056, loss 0.00869994, acc 1
2017-09-08T17:25:32.813939: step 2057, loss 0.0641208, acc 0.984375
2017-09-08T17:25:33.475722: step 2058, loss 0.0109368, acc 1
2017-09-08T17:25:34.197541: step 2059, loss 0.00262129, acc 1
2017-09-08T17:25:34.972345: step 2060, loss 0.0565584, acc 0.984375
2017-09-08T17:25:35.777549: step 2061, loss 0.00390506, acc 1
2017-09-08T17:25:36.537782: step 2062, loss 0.0505781, acc 0.984375
2017-09-08T17:25:37.249849: step 2063, loss 0.0353811, acc 0.984375
2017-09-08T17:25:37.935095: step 2064, loss 0.0235926, acc 0.984375
2017-09-08T17:25:38.652179: step 2065, loss 0.0191133, acc 0.984375
2017-09-08T17:25:39.279764: step 2066, loss 0.0585075, acc 0.984375
2017-09-08T17:25:39.937392: step 2067, loss 0.0382376, acc 0.984375
2017-09-08T17:25:40.671364: step 2068, loss 0.0238868, acc 1
2017-09-08T17:25:41.370659: step 2069, loss 0.00849191, acc 1
2017-09-08T17:25:42.436450: step 2070, loss 0.00741941, acc 1
2017-09-08T17:25:43.217103: step 2071, loss 0.00314174, acc 1
2017-09-08T17:25:43.989915: step 2072, loss 0.00590584, acc 1
2017-09-08T17:25:44.842903: step 2073, loss 0.0426932, acc 0.984375
2017-09-08T17:25:45.606777: step 2074, loss 0.00948261, acc 1
2017-09-08T17:25:46.363092: step 2075, loss 0.0231347, acc 0.984375
2017-09-08T17:25:47.119031: step 2076, loss 0.0208517, acc 0.984375
2017-09-08T17:25:47.886313: step 2077, loss 0.0157265, acc 1
2017-09-08T17:25:48.736834: step 2078, loss 0.0313744, acc 0.984375
2017-09-08T17:25:49.457939: step 2079, loss 0.133088, acc 0.953125
2017-09-08T17:25:50.198323: step 2080, loss 0.00865137, acc 1
2017-09-08T17:25:51.010813: step 2081, loss 0.105755, acc 0.96875
2017-09-08T17:25:51.782443: step 2082, loss 0.00251953, acc 1
2017-09-08T17:25:52.462688: step 2083, loss 0.0704589, acc 0.984375
2017-09-08T17:25:53.241087: step 2084, loss 0.014696, acc 1
2017-09-08T17:25:54.057676: step 2085, loss 0.011089, acc 1
2017-09-08T17:25:54.968398: step 2086, loss 0.0570824, acc 0.96875
2017-09-08T17:25:55.728521: step 2087, loss 0.0421636, acc 0.984375
2017-09-08T17:25:56.565183: step 2088, loss 0.0185433, acc 0.984375
2017-09-08T17:25:57.359334: step 2089, loss 0.0130791, acc 1
2017-09-08T17:25:58.125312: step 2090, loss 0.0638291, acc 0.984375
2017-09-08T17:25:58.900308: step 2091, loss 0.0046206, acc 1
2017-09-08T17:25:59.693017: step 2092, loss 0.0518944, acc 0.984375
2017-09-08T17:26:00.423057: step 2093, loss 0.043046, acc 0.984375
2017-09-08T17:26:01.160293: step 2094, loss 0.00481872, acc 1
2017-09-08T17:26:01.896112: step 2095, loss 0.00638078, acc 1
2017-09-08T17:26:02.706337: step 2096, loss 0.0274997, acc 0.984375
2017-09-08T17:26:03.346718: step 2097, loss 0.00511539, acc 1
2017-09-08T17:26:04.064751: step 2098, loss 0.0386726, acc 1
2017-09-08T17:26:04.702802: step 2099, loss 0.00148803, acc 1
2017-09-08T17:26:05.359630: step 2100, loss 0.0120777, acc 1

Evaluation:
2017-09-08T17:26:05.977199: step 2100, loss 0.189673, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2100

2017-09-08T17:26:09.209362: step 2101, loss 0.0128217, acc 1
2017-09-08T17:26:09.929200: step 2102, loss 0.00572866, acc 1
2017-09-08T17:26:10.652052: step 2103, loss 0.0121777, acc 1
2017-09-08T17:26:11.328816: step 2104, loss 0.0097563, acc 1
2017-09-08T17:26:12.089722: step 2105, loss 0.0206347, acc 0.984375
2017-09-08T17:26:12.811947: step 2106, loss 0.0211074, acc 0.984375
2017-09-08T17:26:13.638155: step 2107, loss 0.0452019, acc 0.984375
2017-09-08T17:26:14.479695: step 2108, loss 0.00719756, acc 1
2017-09-08T17:26:15.334908: step 2109, loss 0.00389384, acc 1
2017-09-08T17:26:16.152568: step 2110, loss 0.0322874, acc 0.984375
2017-09-08T17:26:17.077954: step 2111, loss 0.00442885, acc 1
2017-09-08T17:26:17.763686: step 2112, loss 0.00248701, acc 1
2017-09-08T17:26:18.461242: step 2113, loss 0.0262275, acc 0.984375
2017-09-08T17:26:19.282273: step 2114, loss 0.0134572, acc 1
2017-09-08T17:26:20.034892: step 2115, loss 0.00333953, acc 1
2017-09-08T17:26:20.774420: step 2116, loss 0.00977179, acc 1
2017-09-08T17:26:21.474426: step 2117, loss 0.0193325, acc 1
2017-09-08T17:26:22.179843: step 2118, loss 0.0154367, acc 1
2017-09-08T17:26:22.862011: step 2119, loss 0.0185134, acc 1
2017-09-08T17:26:23.522638: step 2120, loss 0.0454966, acc 0.984375
2017-09-08T17:26:24.229744: step 2121, loss 0.00840443, acc 1
2017-09-08T17:26:25.000256: step 2122, loss 0.00257476, acc 1
2017-09-08T17:26:25.753675: step 2123, loss 0.0277381, acc 0.984375
2017-09-08T17:26:26.491604: step 2124, loss 0.112077, acc 0.96875
2017-09-08T17:26:27.142570: step 2125, loss 0.0484689, acc 0.96875
2017-09-08T17:26:27.769328: step 2126, loss 0.0176191, acc 0.984375
2017-09-08T17:26:28.462896: step 2127, loss 0.00243016, acc 1
2017-09-08T17:26:29.120822: step 2128, loss 0.00455177, acc 1
2017-09-08T17:26:29.844662: step 2129, loss 0.163815, acc 0.9375
2017-09-08T17:26:30.565235: step 2130, loss 0.0408177, acc 0.96875
2017-09-08T17:26:31.328854: step 2131, loss 0.0251664, acc 0.984375
2017-09-08T17:26:32.159467: step 2132, loss 0.0331793, acc 0.96875
2017-09-08T17:26:33.009844: step 2133, loss 0.055917, acc 0.984375
2017-09-08T17:26:33.709195: step 2134, loss 0.0149348, acc 1
2017-09-08T17:26:34.485824: step 2135, loss 0.00392232, acc 1
2017-09-08T17:26:35.246531: step 2136, loss 0.0309753, acc 0.984375
2017-09-08T17:26:36.007354: step 2137, loss 0.0747066, acc 0.984375
2017-09-08T17:26:36.793869: step 2138, loss 0.0073831, acc 1
2017-09-08T17:26:37.530672: step 2139, loss 0.00756557, acc 1
2017-09-08T17:26:38.331859: step 2140, loss 0.0101265, acc 1
2017-09-08T17:26:39.056508: step 2141, loss 0.00634929, acc 1
2017-09-08T17:26:39.844079: step 2142, loss 0.00306566, acc 1
2017-09-08T17:26:40.648484: step 2143, loss 0.00783994, acc 1
2017-09-08T17:26:41.402791: step 2144, loss 0.00462766, acc 1
2017-09-08T17:26:42.166763: step 2145, loss 0.030776, acc 0.984375
2017-09-08T17:26:42.981035: step 2146, loss 0.0215554, acc 0.984375
2017-09-08T17:26:43.737473: step 2147, loss 0.0171199, acc 1
2017-09-08T17:26:44.465181: step 2148, loss 0.00637792, acc 1
2017-09-08T17:26:45.316233: step 2149, loss 0.120991, acc 0.96875
2017-09-08T17:26:45.997427: step 2150, loss 0.0279643, acc 0.984375

Evaluation:
2017-09-08T17:26:46.650080: step 2150, loss 0.192013, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2150

2017-09-08T17:26:50.504206: step 2151, loss 0.105874, acc 0.953125
2017-09-08T17:26:51.223067: step 2152, loss 0.00962899, acc 1
2017-09-08T17:26:51.918067: step 2153, loss 0.0577815, acc 0.984375
2017-09-08T17:26:52.674958: step 2154, loss 0.0263881, acc 0.984375
2017-09-08T17:26:53.467195: step 2155, loss 0.0387256, acc 0.984375
2017-09-08T17:26:54.150013: step 2156, loss 0.285452, acc 0.960784
2017-09-08T17:26:54.935174: step 2157, loss 0.056049, acc 0.96875
2017-09-08T17:26:55.714431: step 2158, loss 0.00662261, acc 1
2017-09-08T17:26:56.543456: step 2159, loss 0.0111159, acc 1
2017-09-08T17:26:57.103661: step 2160, loss 0.0126315, acc 1
2017-09-08T17:26:57.791477: step 2161, loss 0.0205528, acc 1
2017-09-08T17:26:58.434275: step 2162, loss 0.0075467, acc 1
2017-09-08T17:26:59.101864: step 2163, loss 0.0329202, acc 0.984375
2017-09-08T17:26:59.789275: step 2164, loss 0.0246017, acc 0.984375
2017-09-08T17:27:00.538052: step 2165, loss 0.0111711, acc 1
2017-09-08T17:27:01.268440: step 2166, loss 0.00220813, acc 1
2017-09-08T17:27:01.974046: step 2167, loss 0.015249, acc 1
2017-09-08T17:27:02.745610: step 2168, loss 0.0159742, acc 1
2017-09-08T17:27:03.441825: step 2169, loss 0.00290101, acc 1
2017-09-08T17:27:04.176714: step 2170, loss 0.00514862, acc 1
2017-09-08T17:27:04.962630: step 2171, loss 0.0282041, acc 0.984375
2017-09-08T17:27:05.739355: step 2172, loss 0.012546, acc 1
2017-09-08T17:27:06.556247: step 2173, loss 0.009337, acc 1
2017-09-08T17:27:07.310228: step 2174, loss 0.158211, acc 0.96875
2017-09-08T17:27:08.084534: step 2175, loss 0.118293, acc 0.984375
2017-09-08T17:27:08.832165: step 2176, loss 0.00732649, acc 1
2017-09-08T17:27:09.591150: step 2177, loss 0.00989986, acc 1
2017-09-08T17:27:10.357022: step 2178, loss 0.101848, acc 0.953125
2017-09-08T17:27:11.032310: step 2179, loss 0.00346153, acc 1
2017-09-08T17:27:11.795280: step 2180, loss 0.00701342, acc 1
2017-09-08T17:27:12.546164: step 2181, loss 0.0991849, acc 0.96875
2017-09-08T17:27:13.267497: step 2182, loss 0.0121151, acc 1
2017-09-08T17:27:14.061270: step 2183, loss 0.0841694, acc 0.96875
2017-09-08T17:27:14.780662: step 2184, loss 0.0517164, acc 0.96875
2017-09-08T17:27:15.480942: step 2185, loss 0.00715588, acc 1
2017-09-08T17:27:16.184153: step 2186, loss 0.00473512, acc 1
2017-09-08T17:27:16.951107: step 2187, loss 0.0877919, acc 0.984375
2017-09-08T17:27:17.719264: step 2188, loss 0.0232223, acc 0.984375
2017-09-08T17:27:18.375246: step 2189, loss 0.00275181, acc 1
2017-09-08T17:27:19.160708: step 2190, loss 0.0105024, acc 1
2017-09-08T17:27:19.997288: step 2191, loss 0.0136854, acc 1
2017-09-08T17:27:20.714693: step 2192, loss 0.0239211, acc 0.984375
2017-09-08T17:27:21.465452: step 2193, loss 0.00602991, acc 1
2017-09-08T17:27:22.102362: step 2194, loss 0.0907805, acc 0.96875
2017-09-08T17:27:22.957013: step 2195, loss 0.0177683, acc 0.984375
2017-09-08T17:27:23.686161: step 2196, loss 0.0227352, acc 0.984375
2017-09-08T17:27:24.444213: step 2197, loss 0.0345339, acc 0.984375
2017-09-08T17:27:25.280015: step 2198, loss 0.0292734, acc 0.984375
2017-09-08T17:27:26.039103: step 2199, loss 0.0214134, acc 1
2017-09-08T17:27:26.857203: step 2200, loss 0.0844707, acc 0.96875

Evaluation:
2017-09-08T17:27:27.558192: step 2200, loss 0.19855, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2200

2017-09-08T17:27:30.460853: step 2201, loss 0.00968142, acc 1
2017-09-08T17:27:31.258452: step 2202, loss 0.0332843, acc 0.984375
2017-09-08T17:27:32.014600: step 2203, loss 0.0181133, acc 0.984375
2017-09-08T17:27:32.852946: step 2204, loss 0.00599311, acc 1
2017-09-08T17:27:33.711010: step 2205, loss 0.0514345, acc 0.984375
2017-09-08T17:27:34.441186: step 2206, loss 0.00429599, acc 1
2017-09-08T17:27:35.182715: step 2207, loss 0.0360508, acc 0.96875
2017-09-08T17:27:35.951553: step 2208, loss 0.00203309, acc 1
2017-09-08T17:27:36.692535: step 2209, loss 0.0126422, acc 1
2017-09-08T17:27:37.504985: step 2210, loss 0.0101933, acc 1
2017-09-08T17:27:38.273709: step 2211, loss 0.00459516, acc 1
2017-09-08T17:27:38.973469: step 2212, loss 0.0104037, acc 1
2017-09-08T17:27:39.715447: step 2213, loss 0.0265373, acc 1
2017-09-08T17:27:40.441796: step 2214, loss 0.0845834, acc 0.96875
2017-09-08T17:27:41.197728: step 2215, loss 0.015165, acc 1
2017-09-08T17:27:41.982043: step 2216, loss 0.00878368, acc 1
2017-09-08T17:27:43.257496: step 2217, loss 0.00610015, acc 1
2017-09-08T17:27:43.921026: step 2218, loss 0.0735397, acc 0.984375
2017-09-08T17:27:44.621037: step 2219, loss 0.123368, acc 0.96875
2017-09-08T17:27:45.293888: step 2220, loss 0.00377496, acc 1
2017-09-08T17:27:46.044123: step 2221, loss 0.102684, acc 0.984375
2017-09-08T17:27:46.818985: step 2222, loss 0.0836508, acc 0.953125
2017-09-08T17:27:47.551065: step 2223, loss 0.0883359, acc 0.984375
2017-09-08T17:27:48.305817: step 2224, loss 0.0276981, acc 1
2017-09-08T17:27:49.080369: step 2225, loss 0.00544474, acc 1
2017-09-08T17:27:49.835738: step 2226, loss 0.0515871, acc 0.984375
2017-09-08T17:27:50.564374: step 2227, loss 0.00469932, acc 1
2017-09-08T17:27:51.364097: step 2228, loss 0.0629211, acc 0.984375
2017-09-08T17:27:52.124368: step 2229, loss 0.0344057, acc 0.984375
2017-09-08T17:27:53.206603: step 2230, loss 0.0103624, acc 1
2017-09-08T17:27:53.935207: step 2231, loss 0.0175874, acc 1
2017-09-08T17:27:54.710642: step 2232, loss 0.00736806, acc 1
2017-09-08T17:27:55.520734: step 2233, loss 0.00897896, acc 1
2017-09-08T17:27:56.294904: step 2234, loss 0.0158531, acc 0.984375
2017-09-08T17:27:57.069447: step 2235, loss 0.0586174, acc 0.984375
2017-09-08T17:27:57.822799: step 2236, loss 0.0906861, acc 0.96875
2017-09-08T17:27:58.564291: step 2237, loss 0.00780685, acc 1
2017-09-08T17:27:59.285769: step 2238, loss 0.122345, acc 0.96875
2017-09-08T17:28:00.007473: step 2239, loss 0.00644268, acc 1
2017-09-08T17:28:00.808801: step 2240, loss 0.00328596, acc 1
2017-09-08T17:28:01.587663: step 2241, loss 0.0369147, acc 0.984375
2017-09-08T17:28:02.307827: step 2242, loss 0.0444807, acc 0.984375
2017-09-08T17:28:03.061175: step 2243, loss 0.00825059, acc 1
2017-09-08T17:28:03.847012: step 2244, loss 0.00671524, acc 1
2017-09-08T17:28:04.528020: step 2245, loss 0.0137827, acc 1
2017-09-08T17:28:05.311583: step 2246, loss 0.015697, acc 1
2017-09-08T17:28:06.056044: step 2247, loss 0.0149482, acc 1
2017-09-08T17:28:06.827491: step 2248, loss 0.00215203, acc 1
2017-09-08T17:28:07.450267: step 2249, loss 0.0185646, acc 1
2017-09-08T17:28:08.139944: step 2250, loss 0.0316793, acc 0.96875

Evaluation:
2017-09-08T17:28:08.836269: step 2250, loss 0.201466, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2250

2017-09-08T17:28:12.056103: step 2251, loss 0.116396, acc 0.96875
2017-09-08T17:28:12.882807: step 2252, loss 0.0122657, acc 1
2017-09-08T17:28:13.647669: step 2253, loss 0.0291295, acc 0.984375
2017-09-08T17:28:14.402552: step 2254, loss 0.0120846, acc 1
2017-09-08T17:28:15.169604: step 2255, loss 0.0129631, acc 1
2017-09-08T17:28:15.976929: step 2256, loss 0.0107319, acc 1
2017-09-08T17:28:16.791694: step 2257, loss 0.0485998, acc 0.984375
2017-09-08T17:28:17.493195: step 2258, loss 0.00237496, acc 1
2017-09-08T17:28:18.283425: step 2259, loss 0.0328844, acc 0.984375
2017-09-08T17:28:19.052200: step 2260, loss 0.00787334, acc 1
2017-09-08T17:28:19.735954: step 2261, loss 0.00203456, acc 1
2017-09-08T17:28:20.427395: step 2262, loss 0.00489265, acc 1
2017-09-08T17:28:21.067733: step 2263, loss 0.00374849, acc 1
2017-09-08T17:28:21.799578: step 2264, loss 0.0178574, acc 1
2017-09-08T17:28:22.510782: step 2265, loss 0.0206098, acc 1
2017-09-08T17:28:23.244249: step 2266, loss 0.0734213, acc 0.984375
2017-09-08T17:28:23.849871: step 2267, loss 0.0794939, acc 0.984375
2017-09-08T17:28:24.448280: step 2268, loss 0.0148526, acc 0.984375
2017-09-08T17:28:25.000106: step 2269, loss 0.0877547, acc 0.96875
2017-09-08T17:28:25.661440: step 2270, loss 0.0529425, acc 0.984375
2017-09-08T17:28:26.272944: step 2271, loss 0.0114176, acc 1
2017-09-08T17:28:26.953387: step 2272, loss 0.0373765, acc 0.984375
2017-09-08T17:28:27.615823: step 2273, loss 0.00653441, acc 1
2017-09-08T17:28:28.344585: step 2274, loss 0.00238951, acc 1
2017-09-08T17:28:29.124669: step 2275, loss 0.0147092, acc 1
2017-09-08T17:28:29.893035: step 2276, loss 0.0650435, acc 0.96875
2017-09-08T17:28:30.656987: step 2277, loss 0.0236667, acc 0.984375
2017-09-08T17:28:31.427258: step 2278, loss 0.00187064, acc 1
2017-09-08T17:28:32.196758: step 2279, loss 0.00134296, acc 1
2017-09-08T17:28:32.911253: step 2280, loss 0.0199442, acc 0.984375
2017-09-08T17:28:33.665735: step 2281, loss 0.0161257, acc 1
2017-09-08T17:28:34.436796: step 2282, loss 0.0467782, acc 0.984375
2017-09-08T17:28:35.180060: step 2283, loss 0.00504507, acc 1
2017-09-08T17:28:35.868787: step 2284, loss 0.069305, acc 0.984375
2017-09-08T17:28:36.666526: step 2285, loss 0.020117, acc 0.984375
2017-09-08T17:28:37.468273: step 2286, loss 0.00565022, acc 1
2017-09-08T17:28:38.260976: step 2287, loss 0.00742341, acc 1
2017-09-08T17:28:39.061210: step 2288, loss 0.0676549, acc 0.984375
2017-09-08T17:28:39.878785: step 2289, loss 0.0429606, acc 0.96875
2017-09-08T17:28:40.670598: step 2290, loss 0.00456265, acc 1
2017-09-08T17:28:41.476083: step 2291, loss 0.0886268, acc 0.984375
2017-09-08T17:28:42.267393: step 2292, loss 0.0045506, acc 1
2017-09-08T17:28:43.046013: step 2293, loss 0.0446661, acc 0.984375
2017-09-08T17:28:43.827450: step 2294, loss 0.0339141, acc 0.984375
2017-09-08T17:28:44.602539: step 2295, loss 0.00272656, acc 1
2017-09-08T17:28:45.316259: step 2296, loss 0.0270624, acc 0.984375
2017-09-08T17:28:46.031827: step 2297, loss 0.00597508, acc 1
2017-09-08T17:28:46.753012: step 2298, loss 0.0382578, acc 0.984375
2017-09-08T17:28:47.560026: step 2299, loss 0.0202217, acc 0.984375
2017-09-08T17:28:48.276823: step 2300, loss 0.0257035, acc 0.984375

Evaluation:
2017-09-08T17:28:48.878939: step 2300, loss 0.195514, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2300

2017-09-08T17:28:52.645442: step 2301, loss 0.0248162, acc 0.984375
2017-09-08T17:28:53.457892: step 2302, loss 0.12779, acc 0.953125
2017-09-08T17:28:54.289807: step 2303, loss 0.0211164, acc 0.984375
2017-09-08T17:28:55.107291: step 2304, loss 0.00945357, acc 1
2017-09-08T17:28:55.939217: step 2305, loss 0.0590562, acc 0.984375
2017-09-08T17:28:56.673515: step 2306, loss 0.00681595, acc 1
2017-09-08T17:28:57.473331: step 2307, loss 0.0355585, acc 0.984375
2017-09-08T17:28:58.205771: step 2308, loss 0.0846194, acc 0.96875
2017-09-08T17:28:59.000793: step 2309, loss 0.0050121, acc 1
2017-09-08T17:28:59.781165: step 2310, loss 0.0533269, acc 0.984375
2017-09-08T17:29:00.461488: step 2311, loss 0.0115373, acc 1
2017-09-08T17:29:01.164789: step 2312, loss 0.00571689, acc 1
2017-09-08T17:29:01.873991: step 2313, loss 0.0595763, acc 0.984375
2017-09-08T17:29:02.625680: step 2314, loss 0.0679421, acc 0.984375
2017-09-08T17:29:03.453632: step 2315, loss 0.0102692, acc 1
2017-09-08T17:29:04.246817: step 2316, loss 0.00802186, acc 1
2017-09-08T17:29:05.004527: step 2317, loss 0.0199072, acc 0.984375
2017-09-08T17:29:05.790278: step 2318, loss 0.0257309, acc 1
2017-09-08T17:29:06.484775: step 2319, loss 0.0559766, acc 0.984375
2017-09-08T17:29:07.196972: step 2320, loss 0.0171719, acc 1
2017-09-08T17:29:07.871242: step 2321, loss 0.0158829, acc 0.984375
2017-09-08T17:29:08.746500: step 2322, loss 0.00418798, acc 1
2017-09-08T17:29:09.471997: step 2323, loss 0.108322, acc 0.96875
2017-09-08T17:29:10.249433: step 2324, loss 0.0408579, acc 0.96875
2017-09-08T17:29:10.938258: step 2325, loss 0.0371857, acc 0.984375
2017-09-08T17:29:11.689704: step 2326, loss 0.0190392, acc 1
2017-09-08T17:29:12.385123: step 2327, loss 0.0965253, acc 0.953125
2017-09-08T17:29:13.000317: step 2328, loss 0.0237157, acc 1
2017-09-08T17:29:13.483053: step 2329, loss 0.0083901, acc 1
2017-09-08T17:29:14.148881: step 2330, loss 0.0343395, acc 0.96875
2017-09-08T17:29:14.884503: step 2331, loss 0.00575929, acc 1
2017-09-08T17:29:15.618485: step 2332, loss 0.00508954, acc 1
2017-09-08T17:29:16.377110: step 2333, loss 0.0780519, acc 0.96875
2017-09-08T17:29:17.160889: step 2334, loss 0.0479855, acc 0.984375
2017-09-08T17:29:17.936056: step 2335, loss 0.0214978, acc 0.984375
2017-09-08T17:29:18.651958: step 2336, loss 0.0289023, acc 0.984375
2017-09-08T17:29:19.464986: step 2337, loss 0.0388991, acc 0.984375
2017-09-08T17:29:20.222632: step 2338, loss 0.0108163, acc 1
2017-09-08T17:29:20.971607: step 2339, loss 0.0124587, acc 1
2017-09-08T17:29:21.690758: step 2340, loss 0.0618582, acc 0.984375
2017-09-08T17:29:22.442604: step 2341, loss 0.00340703, acc 1
2017-09-08T17:29:23.218171: step 2342, loss 0.0109484, acc 1
2017-09-08T17:29:24.520615: step 2343, loss 0.0363586, acc 0.984375
2017-09-08T17:29:25.330201: step 2344, loss 0.0154787, acc 1
2017-09-08T17:29:26.144006: step 2345, loss 0.0327552, acc 0.984375
2017-09-08T17:29:26.961607: step 2346, loss 0.0122692, acc 1
2017-09-08T17:29:27.740415: step 2347, loss 0.00429867, acc 1
2017-09-08T17:29:28.457411: step 2348, loss 0.00691544, acc 1
2017-09-08T17:29:29.355731: step 2349, loss 0.0332135, acc 0.984375
2017-09-08T17:29:30.066283: step 2350, loss 0.0483022, acc 0.984375

Evaluation:
2017-09-08T17:29:30.742092: step 2350, loss 0.197858, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2350

2017-09-08T17:29:33.866986: step 2351, loss 0.0711555, acc 0.96875
2017-09-08T17:29:34.383478: step 2352, loss 0.00225681, acc 1
2017-09-08T17:29:35.038238: step 2353, loss 0.0689244, acc 0.984375
2017-09-08T17:29:35.761689: step 2354, loss 0.0529978, acc 0.96875
2017-09-08T17:29:36.561528: step 2355, loss 0.0079268, acc 1
2017-09-08T17:29:37.342736: step 2356, loss 0.0202218, acc 1
2017-09-08T17:29:38.082074: step 2357, loss 0.01213, acc 1
2017-09-08T17:29:38.838433: step 2358, loss 0.0621567, acc 0.984375
2017-09-08T17:29:39.553787: step 2359, loss 0.00404905, acc 1
2017-09-08T17:29:40.289219: step 2360, loss 0.0776886, acc 0.96875
2017-09-08T17:29:41.107257: step 2361, loss 0.0276144, acc 0.984375
2017-09-08T17:29:41.847002: step 2362, loss 0.035299, acc 0.984375
2017-09-08T17:29:42.534822: step 2363, loss 0.0217445, acc 0.984375
2017-09-08T17:29:43.290957: step 2364, loss 0.068176, acc 0.984375
2017-09-08T17:29:44.117490: step 2365, loss 0.0303768, acc 1
2017-09-08T17:29:44.833593: step 2366, loss 0.0173502, acc 1
2017-09-08T17:29:45.595171: step 2367, loss 0.0206858, acc 0.984375
2017-09-08T17:29:46.443775: step 2368, loss 0.00250292, acc 1
2017-09-08T17:29:47.110392: step 2369, loss 0.0867704, acc 0.953125
2017-09-08T17:29:47.841489: step 2370, loss 0.0719957, acc 0.96875
2017-09-08T17:29:48.507060: step 2371, loss 0.00303923, acc 1
2017-09-08T17:29:49.248407: step 2372, loss 0.0406421, acc 0.984375
2017-09-08T17:29:50.076754: step 2373, loss 0.0438697, acc 0.984375
2017-09-08T17:29:50.805938: step 2374, loss 0.0677552, acc 0.984375
2017-09-08T17:29:51.608444: step 2375, loss 0.0062483, acc 1
2017-09-08T17:29:52.363441: step 2376, loss 0.0260477, acc 1
2017-09-08T17:29:53.155629: step 2377, loss 0.0364823, acc 0.96875
2017-09-08T17:29:53.954190: step 2378, loss 0.00482773, acc 1
2017-09-08T17:29:54.784509: step 2379, loss 0.00577526, acc 1
2017-09-08T17:29:55.589882: step 2380, loss 0.0426301, acc 0.984375
2017-09-08T17:29:56.354735: step 2381, loss 0.0533307, acc 0.984375
2017-09-08T17:29:57.110970: step 2382, loss 0.0103567, acc 1
2017-09-08T17:29:58.028825: step 2383, loss 0.00318859, acc 1
2017-09-08T17:29:58.885905: step 2384, loss 0.00455672, acc 1
2017-09-08T17:29:59.605345: step 2385, loss 0.113324, acc 0.96875
2017-09-08T17:30:00.397577: step 2386, loss 0.0109229, acc 1
2017-09-08T17:30:01.218445: step 2387, loss 0.00857932, acc 1
2017-09-08T17:30:02.066020: step 2388, loss 0.0101644, acc 1
2017-09-08T17:30:02.787783: step 2389, loss 0.00858257, acc 1
2017-09-08T17:30:03.523619: step 2390, loss 0.00478713, acc 1
2017-09-08T17:30:04.209309: step 2391, loss 0.00901358, acc 1
2017-09-08T17:30:05.050244: step 2392, loss 0.0213693, acc 0.984375
2017-09-08T17:30:05.842001: step 2393, loss 0.0145487, acc 1
2017-09-08T17:30:06.623945: step 2394, loss 0.115505, acc 0.984375
2017-09-08T17:30:07.343065: step 2395, loss 0.00494791, acc 1
2017-09-08T17:30:08.144794: step 2396, loss 0.0494717, acc 0.984375
2017-09-08T17:30:08.871570: step 2397, loss 0.00444011, acc 1
2017-09-08T17:30:09.624828: step 2398, loss 0.0358119, acc 0.96875
2017-09-08T17:30:10.336898: step 2399, loss 0.00405676, acc 1
2017-09-08T17:30:10.990206: step 2400, loss 0.0264998, acc 0.984375

Evaluation:
2017-09-08T17:30:11.688330: step 2400, loss 0.197503, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2400

2017-09-08T17:30:16.020854: step 2401, loss 0.0476868, acc 0.984375
2017-09-08T17:30:16.801567: step 2402, loss 0.0126407, acc 1
2017-09-08T17:30:17.543119: step 2403, loss 0.0114741, acc 1
2017-09-08T17:30:18.347237: step 2404, loss 0.00861643, acc 1
2017-09-08T17:30:19.027882: step 2405, loss 0.00741647, acc 1
2017-09-08T17:30:20.050003: step 2406, loss 0.0291876, acc 0.984375
2017-09-08T17:30:20.814235: step 2407, loss 0.0019097, acc 1
2017-09-08T17:30:21.538686: step 2408, loss 0.0343507, acc 0.984375
2017-09-08T17:30:22.202367: step 2409, loss 0.00259333, acc 1
2017-09-08T17:30:22.838543: step 2410, loss 0.00413607, acc 1
2017-09-08T17:30:23.468541: step 2411, loss 0.0168687, acc 1
2017-09-08T17:30:24.098262: step 2412, loss 0.0297255, acc 0.984375
2017-09-08T17:30:24.809083: step 2413, loss 0.0117371, acc 1
2017-09-08T17:30:25.592363: step 2414, loss 0.0125298, acc 1
2017-09-08T17:30:26.397827: step 2415, loss 0.0439547, acc 0.96875
2017-09-08T17:30:27.138706: step 2416, loss 0.0618975, acc 0.96875
2017-09-08T17:30:27.785472: step 2417, loss 0.0689353, acc 0.984375
2017-09-08T17:30:28.468960: step 2418, loss 0.0308342, acc 0.984375
2017-09-08T17:30:29.106421: step 2419, loss 0.00224577, acc 1
2017-09-08T17:30:29.837725: step 2420, loss 0.00248781, acc 1
2017-09-08T17:30:30.699979: step 2421, loss 0.00255005, acc 1
2017-09-08T17:30:31.529365: step 2422, loss 0.0521846, acc 0.96875
2017-09-08T17:30:32.257847: step 2423, loss 0.030142, acc 0.984375
2017-09-08T17:30:32.969017: step 2424, loss 0.0321334, acc 0.984375
2017-09-08T17:30:33.777450: step 2425, loss 0.0190015, acc 0.984375
2017-09-08T17:30:34.265558: step 2426, loss 0.00324235, acc 1
2017-09-08T17:30:35.027791: step 2427, loss 0.113345, acc 0.984375
2017-09-08T17:30:35.812895: step 2428, loss 0.0554795, acc 0.984375
2017-09-08T17:30:36.574029: step 2429, loss 0.0106417, acc 1
2017-09-08T17:30:37.316844: step 2430, loss 0.0491966, acc 0.96875
2017-09-08T17:30:38.081253: step 2431, loss 0.0243365, acc 0.984375
2017-09-08T17:30:38.943770: step 2432, loss 0.110126, acc 0.96875
2017-09-08T17:30:39.748819: step 2433, loss 0.0890674, acc 0.984375
2017-09-08T17:30:40.559726: step 2434, loss 0.0274641, acc 0.984375
2017-09-08T17:30:41.326353: step 2435, loss 0.0116591, acc 1
2017-09-08T17:30:42.106882: step 2436, loss 0.025217, acc 1
2017-09-08T17:30:42.876575: step 2437, loss 0.0453906, acc 0.984375
2017-09-08T17:30:43.701405: step 2438, loss 0.0841457, acc 0.96875
2017-09-08T17:30:44.452231: step 2439, loss 0.00459278, acc 1
2017-09-08T17:30:45.198008: step 2440, loss 0.00402651, acc 1
2017-09-08T17:30:45.930234: step 2441, loss 0.0192457, acc 0.984375
2017-09-08T17:30:46.660109: step 2442, loss 0.0370892, acc 0.984375
2017-09-08T17:30:47.392214: step 2443, loss 0.0469686, acc 0.984375
2017-09-08T17:30:48.144678: step 2444, loss 0.00570285, acc 1
2017-09-08T17:30:48.926289: step 2445, loss 0.0213793, acc 0.984375
2017-09-08T17:30:49.734694: step 2446, loss 0.0037384, acc 1
2017-09-08T17:30:50.444468: step 2447, loss 0.0369598, acc 0.984375
2017-09-08T17:30:51.144791: step 2448, loss 0.0829328, acc 0.96875
2017-09-08T17:30:51.820361: step 2449, loss 0.114725, acc 0.96875
2017-09-08T17:30:52.431382: step 2450, loss 0.00137958, acc 1

Evaluation:
2017-09-08T17:30:53.138258: step 2450, loss 0.191216, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2450

2017-09-08T17:30:56.242499: step 2451, loss 0.00781524, acc 1
2017-09-08T17:30:57.016648: step 2452, loss 0.0163837, acc 1
2017-09-08T17:30:57.820071: step 2453, loss 0.0190397, acc 1
2017-09-08T17:30:58.513656: step 2454, loss 0.0259566, acc 0.984375
2017-09-08T17:30:59.243458: step 2455, loss 0.0155662, acc 0.984375
2017-09-08T17:30:59.972581: step 2456, loss 0.0744647, acc 0.984375
2017-09-08T17:31:00.796920: step 2457, loss 0.0168637, acc 0.984375
2017-09-08T17:31:01.616652: step 2458, loss 0.0305022, acc 0.984375
2017-09-08T17:31:02.472993: step 2459, loss 0.00326609, acc 1
2017-09-08T17:31:03.189676: step 2460, loss 0.00829008, acc 1
2017-09-08T17:31:03.923878: step 2461, loss 0.038156, acc 0.984375
2017-09-08T17:31:04.616847: step 2462, loss 0.0240944, acc 0.984375
2017-09-08T17:31:05.406414: step 2463, loss 0.0298995, acc 0.984375
2017-09-08T17:31:06.159012: step 2464, loss 0.00688771, acc 1
2017-09-08T17:31:06.989455: step 2465, loss 0.0107204, acc 1
2017-09-08T17:31:07.772762: step 2466, loss 0.0199915, acc 1
2017-09-08T17:31:08.531469: step 2467, loss 0.0541913, acc 0.96875
2017-09-08T17:31:09.304947: step 2468, loss 0.0241988, acc 0.984375
2017-09-08T17:31:10.065744: step 2469, loss 0.0823496, acc 0.96875
2017-09-08T17:31:10.777604: step 2470, loss 0.00816807, acc 1
2017-09-08T17:31:11.407560: step 2471, loss 0.00952386, acc 1
2017-09-08T17:31:12.146894: step 2472, loss 0.0178807, acc 1
2017-09-08T17:31:12.953378: step 2473, loss 0.00620893, acc 1
2017-09-08T17:31:13.636516: step 2474, loss 0.00336222, acc 1
2017-09-08T17:31:14.359202: step 2475, loss 0.00575845, acc 1
2017-09-08T17:31:15.151050: step 2476, loss 0.00510132, acc 1
2017-09-08T17:31:15.815949: step 2477, loss 0.0819844, acc 0.96875
2017-09-08T17:31:16.508967: step 2478, loss 0.00265915, acc 1
2017-09-08T17:31:17.186863: step 2479, loss 0.00412846, acc 1
2017-09-08T17:31:17.909420: step 2480, loss 0.00708226, acc 1
2017-09-08T17:31:18.632808: step 2481, loss 0.0455454, acc 0.984375
2017-09-08T17:31:19.442565: step 2482, loss 0.0189183, acc 0.984375
2017-09-08T17:31:20.188924: step 2483, loss 0.00833276, acc 1
2017-09-08T17:31:20.973820: step 2484, loss 0.0873248, acc 0.984375
2017-09-08T17:31:21.727862: step 2485, loss 0.0511983, acc 0.984375
2017-09-08T17:31:22.463303: step 2486, loss 0.0610664, acc 0.96875
2017-09-08T17:31:23.183852: step 2487, loss 0.00295698, acc 1
2017-09-08T17:31:23.976823: step 2488, loss 0.00532789, acc 1
2017-09-08T17:31:24.739043: step 2489, loss 0.00624812, acc 1
2017-09-08T17:31:25.451850: step 2490, loss 0.0477869, acc 0.984375
2017-09-08T17:31:26.152846: step 2491, loss 0.083177, acc 0.96875
2017-09-08T17:31:26.917388: step 2492, loss 0.095784, acc 0.96875
2017-09-08T17:31:27.619291: step 2493, loss 0.0419869, acc 0.96875
2017-09-08T17:31:28.317673: step 2494, loss 0.0264791, acc 0.984375
2017-09-08T17:31:29.076112: step 2495, loss 0.021887, acc 1
2017-09-08T17:31:29.882720: step 2496, loss 0.0112421, acc 1
2017-09-08T17:31:30.623376: step 2497, loss 0.00901187, acc 1
2017-09-08T17:31:31.272788: step 2498, loss 0.0399288, acc 0.984375
2017-09-08T17:31:31.862130: step 2499, loss 0.0651397, acc 0.984375
2017-09-08T17:31:32.545281: step 2500, loss 0.0369054, acc 0.984375

Evaluation:
2017-09-08T17:31:33.249933: step 2500, loss 0.193336, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2500

2017-09-08T17:31:36.782055: step 2501, loss 0.0675021, acc 0.96875
2017-09-08T17:31:37.577750: step 2502, loss 0.0018128, acc 1
2017-09-08T17:31:38.320260: step 2503, loss 0.0742474, acc 0.984375
2017-09-08T17:31:39.010181: step 2504, loss 0.0243929, acc 0.984375
2017-09-08T17:31:39.686735: step 2505, loss 0.0127018, acc 1
2017-09-08T17:31:40.446047: step 2506, loss 0.00206168, acc 1
2017-09-08T17:31:41.226413: step 2507, loss 0.0114227, acc 1
2017-09-08T17:31:41.964610: step 2508, loss 0.0760082, acc 0.984375
2017-09-08T17:31:42.699244: step 2509, loss 0.00965255, acc 1
2017-09-08T17:31:43.426666: step 2510, loss 0.0107832, acc 1
2017-09-08T17:31:44.187161: step 2511, loss 0.0104932, acc 1
2017-09-08T17:31:44.880731: step 2512, loss 0.0115559, acc 1
2017-09-08T17:31:45.504266: step 2513, loss 0.0370629, acc 0.984375
2017-09-08T17:31:46.230741: step 2514, loss 0.0318738, acc 0.984375
2017-09-08T17:31:46.941354: step 2515, loss 0.0290378, acc 0.984375
2017-09-08T17:31:47.690726: step 2516, loss 0.00817378, acc 1
2017-09-08T17:31:48.469432: step 2517, loss 0.00786473, acc 1
2017-09-08T17:31:49.365623: step 2518, loss 0.00735626, acc 1
2017-09-08T17:31:50.082124: step 2519, loss 0.0138074, acc 1
2017-09-08T17:31:50.779935: step 2520, loss 0.0833104, acc 0.984375
2017-09-08T17:31:51.546074: step 2521, loss 0.0642067, acc 0.96875
2017-09-08T17:31:52.258097: step 2522, loss 0.00214013, acc 1
2017-09-08T17:31:53.057451: step 2523, loss 0.0486979, acc 0.984375
2017-09-08T17:31:53.752595: step 2524, loss 0.0189847, acc 1
2017-09-08T17:31:54.529402: step 2525, loss 0.18591, acc 0.953125
2017-09-08T17:31:55.336304: step 2526, loss 0.0076605, acc 1
2017-09-08T17:31:56.076384: step 2527, loss 0.117305, acc 0.953125
2017-09-08T17:31:56.837375: step 2528, loss 0.0882073, acc 0.96875
2017-09-08T17:31:57.621492: step 2529, loss 0.0228055, acc 0.984375
2017-09-08T17:31:58.392770: step 2530, loss 0.0210781, acc 0.984375
2017-09-08T17:31:59.166158: step 2531, loss 0.0547438, acc 0.984375
2017-09-08T17:31:59.964442: step 2532, loss 0.00256878, acc 1
2017-09-08T17:32:00.630280: step 2533, loss 0.0322367, acc 0.984375
2017-09-08T17:32:01.348983: step 2534, loss 0.013538, acc 1
2017-09-08T17:32:02.084806: step 2535, loss 0.00186786, acc 1
2017-09-08T17:32:02.796585: step 2536, loss 0.0711379, acc 0.96875
2017-09-08T17:32:03.589257: step 2537, loss 0.0750344, acc 0.984375
2017-09-08T17:32:04.442794: step 2538, loss 0.0701328, acc 0.96875
2017-09-08T17:32:05.267744: step 2539, loss 0.00154879, acc 1
2017-09-08T17:32:06.106994: step 2540, loss 0.00123568, acc 1
2017-09-08T17:32:06.906898: step 2541, loss 0.0318518, acc 0.96875
2017-09-08T17:32:07.653470: step 2542, loss 0.101343, acc 0.96875
2017-09-08T17:32:08.343418: step 2543, loss 0.0113922, acc 1
2017-09-08T17:32:09.044958: step 2544, loss 0.0432841, acc 0.984375
2017-09-08T17:32:09.737679: step 2545, loss 0.00286786, acc 1
2017-09-08T17:32:10.350417: step 2546, loss 0.00467603, acc 1
2017-09-08T17:32:11.047140: step 2547, loss 0.00221601, acc 1
2017-09-08T17:32:11.681827: step 2548, loss 0.00808521, acc 1
2017-09-08T17:32:12.461551: step 2549, loss 0.0150839, acc 1
2017-09-08T17:32:13.319138: step 2550, loss 0.0674637, acc 0.96875

Evaluation:
2017-09-08T17:32:13.984967: step 2550, loss 0.196182, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2550

2017-09-08T17:32:18.351294: step 2551, loss 0.0217863, acc 0.984375
2017-09-08T17:32:19.095680: step 2552, loss 0.0399131, acc 0.984375
2017-09-08T17:32:19.881199: step 2553, loss 0.0461119, acc 0.984375
2017-09-08T17:32:20.634641: step 2554, loss 0.00235319, acc 1
2017-09-08T17:32:21.403240: step 2555, loss 0.00570625, acc 1
2017-09-08T17:32:22.144553: step 2556, loss 0.0222335, acc 0.984375
2017-09-08T17:32:23.017811: step 2557, loss 0.00737984, acc 1
2017-09-08T17:32:23.683349: step 2558, loss 0.00259139, acc 1
2017-09-08T17:32:24.404278: step 2559, loss 0.00319592, acc 1
2017-09-08T17:32:25.035139: step 2560, loss 0.0232431, acc 0.984375
2017-09-08T17:32:25.706672: step 2561, loss 0.00284387, acc 1
2017-09-08T17:32:26.309993: step 2562, loss 0.0421736, acc 0.984375
2017-09-08T17:32:26.881769: step 2563, loss 0.0228038, acc 0.984375
2017-09-08T17:32:27.553700: step 2564, loss 0.00517561, acc 1
2017-09-08T17:32:28.315011: step 2565, loss 0.0129655, acc 1
2017-09-08T17:32:29.104743: step 2566, loss 0.0867558, acc 0.96875
2017-09-08T17:32:29.854761: step 2567, loss 0.021199, acc 0.984375
2017-09-08T17:32:30.644214: step 2568, loss 0.0175592, acc 1
2017-09-08T17:32:31.352425: step 2569, loss 0.0293483, acc 0.984375
2017-09-08T17:32:32.013879: step 2570, loss 0.0227723, acc 0.984375
2017-09-08T17:32:32.694269: step 2571, loss 0.0637373, acc 0.984375
2017-09-08T17:32:33.287151: step 2572, loss 0.00301191, acc 1
2017-09-08T17:32:33.870325: step 2573, loss 0.0775146, acc 0.984375
2017-09-08T17:32:34.705541: step 2574, loss 0.00728157, acc 1
2017-09-08T17:32:35.381253: step 2575, loss 0.00154168, acc 1
2017-09-08T17:32:36.390689: step 2576, loss 0.0671884, acc 0.984375
2017-09-08T17:32:37.188125: step 2577, loss 0.00280825, acc 1
2017-09-08T17:32:38.031827: step 2578, loss 0.0155953, acc 1
2017-09-08T17:32:38.822141: step 2579, loss 0.00254855, acc 1
2017-09-08T17:32:39.534720: step 2580, loss 0.0264367, acc 0.984375
2017-09-08T17:32:40.324508: step 2581, loss 0.0673347, acc 0.96875
2017-09-08T17:32:41.008617: step 2582, loss 0.0206482, acc 0.984375
2017-09-08T17:32:41.891718: step 2583, loss 0.00538531, acc 1
2017-09-08T17:32:42.736811: step 2584, loss 0.00200063, acc 1
2017-09-08T17:32:43.527090: step 2585, loss 0.075094, acc 0.984375
2017-09-08T17:32:44.316014: step 2586, loss 0.00402006, acc 1
2017-09-08T17:32:45.072445: step 2587, loss 0.0781335, acc 0.96875
2017-09-08T17:32:45.840992: step 2588, loss 0.0600057, acc 0.984375
2017-09-08T17:32:46.497027: step 2589, loss 0.0344672, acc 0.984375
2017-09-08T17:32:47.345356: step 2590, loss 0.0159543, acc 0.984375
2017-09-08T17:32:48.092985: step 2591, loss 0.00522184, acc 1
2017-09-08T17:32:48.810063: step 2592, loss 0.00856669, acc 1
2017-09-08T17:32:49.601756: step 2593, loss 0.0383411, acc 0.984375
2017-09-08T17:32:50.296094: step 2594, loss 0.00632932, acc 1
2017-09-08T17:32:51.146790: step 2595, loss 0.0177027, acc 1
2017-09-08T17:32:51.948651: step 2596, loss 0.002912, acc 1
2017-09-08T17:32:52.646638: step 2597, loss 0.0114116, acc 1
2017-09-08T17:32:53.356726: step 2598, loss 0.0588703, acc 0.984375
2017-09-08T17:32:54.100064: step 2599, loss 0.0121241, acc 1
2017-09-08T17:32:54.793130: step 2600, loss 0.0475128, acc 0.96875

Evaluation:
2017-09-08T17:32:55.511693: step 2600, loss 0.199072, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2600

2017-09-08T17:32:58.709240: step 2601, loss 0.0539222, acc 0.984375
2017-09-08T17:32:59.427931: step 2602, loss 0.043202, acc 0.984375
2017-09-08T17:33:00.264446: step 2603, loss 0.0785402, acc 0.96875
2017-09-08T17:33:00.916914: step 2604, loss 0.00963422, acc 1
2017-09-08T17:33:01.680165: step 2605, loss 0.102934, acc 0.96875
2017-09-08T17:33:02.473958: step 2606, loss 0.00674516, acc 1
2017-09-08T17:33:03.300355: step 2607, loss 0.0693872, acc 0.96875
2017-09-08T17:33:04.225698: step 2608, loss 0.0474678, acc 0.984375
2017-09-08T17:33:05.025525: step 2609, loss 0.00703417, acc 1
2017-09-08T17:33:05.857857: step 2610, loss 0.0176612, acc 0.984375
2017-09-08T17:33:06.584209: step 2611, loss 0.00589888, acc 1
2017-09-08T17:33:07.269573: step 2612, loss 0.0690423, acc 0.984375
2017-09-08T17:33:08.023540: step 2613, loss 0.041457, acc 0.984375
2017-09-08T17:33:08.775987: step 2614, loss 0.00756418, acc 1
2017-09-08T17:33:09.510292: step 2615, loss 0.00417213, acc 1
2017-09-08T17:33:10.158199: step 2616, loss 0.003189, acc 1
2017-09-08T17:33:10.743553: step 2617, loss 0.083118, acc 0.96875
2017-09-08T17:33:11.517329: step 2618, loss 0.00984665, acc 1
2017-09-08T17:33:12.372991: step 2619, loss 0.00703879, acc 1
2017-09-08T17:33:13.074888: step 2620, loss 0.00942227, acc 1
2017-09-08T17:33:13.741530: step 2621, loss 0.00111958, acc 1
2017-09-08T17:33:14.362328: step 2622, loss 0.00265945, acc 1
2017-09-08T17:33:15.015837: step 2623, loss 0.0947004, acc 0.96875
2017-09-08T17:33:15.674480: step 2624, loss 0.061442, acc 0.984375
2017-09-08T17:33:16.456431: step 2625, loss 0.002679, acc 1
2017-09-08T17:33:17.216873: step 2626, loss 0.0044392, acc 1
2017-09-08T17:33:18.076605: step 2627, loss 0.0627814, acc 0.984375
2017-09-08T17:33:18.862843: step 2628, loss 0.00862402, acc 1
2017-09-08T17:33:19.625087: step 2629, loss 0.0118483, acc 1
2017-09-08T17:33:20.484071: step 2630, loss 0.0225158, acc 1
2017-09-08T17:33:21.305270: step 2631, loss 0.0397217, acc 0.96875
2017-09-08T17:33:22.091125: step 2632, loss 0.0555901, acc 0.96875
2017-09-08T17:33:22.833514: step 2633, loss 0.100641, acc 0.96875
2017-09-08T17:33:23.605732: step 2634, loss 0.0277231, acc 0.984375
2017-09-08T17:33:24.355825: step 2635, loss 0.157674, acc 0.953125
2017-09-08T17:33:25.098216: step 2636, loss 0.00909914, acc 1
2017-09-08T17:33:25.865548: step 2637, loss 0.00466553, acc 1
2017-09-08T17:33:26.697175: step 2638, loss 0.0347083, acc 0.96875
2017-09-08T17:33:27.497369: step 2639, loss 0.00461133, acc 1
2017-09-08T17:33:28.225001: step 2640, loss 0.0020065, acc 1
2017-09-08T17:33:28.925434: step 2641, loss 0.122188, acc 0.984375
2017-09-08T17:33:29.676353: step 2642, loss 0.00966113, acc 1
2017-09-08T17:33:30.519164: step 2643, loss 0.00981966, acc 1
2017-09-08T17:33:31.300591: step 2644, loss 0.0301679, acc 0.984375
2017-09-08T17:33:32.068740: step 2645, loss 0.0752167, acc 0.96875
2017-09-08T17:33:32.713819: step 2646, loss 0.00261358, acc 1
2017-09-08T17:33:33.479562: step 2647, loss 0.00580731, acc 1
2017-09-08T17:33:34.243451: step 2648, loss 0.0172441, acc 0.984375
2017-09-08T17:33:34.870651: step 2649, loss 0.00539741, acc 1
2017-09-08T17:33:35.582795: step 2650, loss 0.00624553, acc 1

Evaluation:
2017-09-08T17:33:36.285001: step 2650, loss 0.197399, acc 0.941007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2650

2017-09-08T17:33:40.100660: step 2651, loss 0.0040643, acc 1
2017-09-08T17:33:40.827925: step 2652, loss 0.0471856, acc 0.984375
2017-09-08T17:33:41.526162: step 2653, loss 0.0241752, acc 0.984375
2017-09-08T17:33:42.304843: step 2654, loss 0.00177237, acc 1
2017-09-08T17:33:43.255880: step 2655, loss 0.06304, acc 0.96875
2017-09-08T17:33:43.906396: step 2656, loss 0.00147717, acc 1
2017-09-08T17:33:44.506178: step 2657, loss 0.0814596, acc 0.984375
2017-09-08T17:33:45.214690: step 2658, loss 0.00454836, acc 1
2017-09-08T17:33:45.993993: step 2659, loss 0.00800919, acc 1
2017-09-08T17:33:46.751204: step 2660, loss 0.00687097, acc 1
2017-09-08T17:33:47.554336: step 2661, loss 0.0621556, acc 0.96875
2017-09-08T17:33:48.210195: step 2662, loss 0.0736871, acc 0.953125
2017-09-08T17:33:48.917999: step 2663, loss 0.00582602, acc 1
2017-09-08T17:33:49.765620: step 2664, loss 0.00206304, acc 1
2017-09-08T17:33:50.567157: step 2665, loss 0.00852431, acc 1
2017-09-08T17:33:51.326393: step 2666, loss 0.0375289, acc 0.984375
2017-09-08T17:33:52.163070: step 2667, loss 0.100507, acc 0.984375
2017-09-08T17:33:53.018447: step 2668, loss 0.0251319, acc 0.984375
2017-09-08T17:33:53.802497: step 2669, loss 0.0125463, acc 1
2017-09-08T17:33:54.599024: step 2670, loss 0.0561359, acc 0.984375
2017-09-08T17:33:55.335817: step 2671, loss 0.00631881, acc 1
2017-09-08T17:33:56.005833: step 2672, loss 0.02899, acc 0.984375
2017-09-08T17:33:56.702160: step 2673, loss 0.0370348, acc 0.984375
2017-09-08T17:33:57.392502: step 2674, loss 0.0769737, acc 0.96875
2017-09-08T17:33:58.097815: step 2675, loss 0.0501117, acc 0.96875
2017-09-08T17:33:58.888650: step 2676, loss 0.0827089, acc 0.984375
2017-09-08T17:33:59.790060: step 2677, loss 0.0757595, acc 0.984375
2017-09-08T17:34:00.628112: step 2678, loss 0.0244215, acc 0.984375
2017-09-08T17:34:01.349237: step 2679, loss 0.00746077, acc 1
2017-09-08T17:34:02.001704: step 2680, loss 0.193605, acc 0.921875
2017-09-08T17:34:02.767716: step 2681, loss 0.00133151, acc 1
2017-09-08T17:34:03.481788: step 2682, loss 0.00184012, acc 1
2017-09-08T17:34:04.234176: step 2683, loss 0.00232527, acc 1
2017-09-08T17:34:04.928303: step 2684, loss 0.0371324, acc 0.984375
2017-09-08T17:34:05.699988: step 2685, loss 0.011611, acc 1
2017-09-08T17:34:06.453117: step 2686, loss 0.00146175, acc 1
2017-09-08T17:34:07.207378: step 2687, loss 0.124096, acc 0.96875
2017-09-08T17:34:07.985480: step 2688, loss 0.0415306, acc 0.984375
2017-09-08T17:34:08.716398: step 2689, loss 0.00301354, acc 1
2017-09-08T17:34:09.450671: step 2690, loss 0.0185427, acc 0.984375
2017-09-08T17:34:10.302128: step 2691, loss 0.0298756, acc 0.984375
2017-09-08T17:34:11.026121: step 2692, loss 0.00367795, acc 1
2017-09-08T17:34:11.828146: step 2693, loss 0.0378823, acc 0.984375
2017-09-08T17:34:12.632577: step 2694, loss 0.0328586, acc 0.984375
2017-09-08T17:34:13.453918: step 2695, loss 0.0156875, acc 0.984375
2017-09-08T17:34:14.230404: step 2696, loss 0.0645473, acc 0.96875
2017-09-08T17:34:14.999255: step 2697, loss 0.0231826, acc 0.984375
2017-09-08T17:34:15.802682: step 2698, loss 0.00450756, acc 1
2017-09-08T17:34:16.548433: step 2699, loss 0.0788291, acc 0.96875
2017-09-08T17:34:17.252876: step 2700, loss 0.0131817, acc 1

Evaluation:
2017-09-08T17:34:17.991136: step 2700, loss 0.191886, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2700

2017-09-08T17:34:20.930277: step 2701, loss 0.00152796, acc 1
2017-09-08T17:34:21.650954: step 2702, loss 0.00286222, acc 1
2017-09-08T17:34:22.417560: step 2703, loss 0.0034434, acc 1
2017-09-08T17:34:23.190396: step 2704, loss 0.0935575, acc 0.984375
2017-09-08T17:34:24.040046: step 2705, loss 0.00697262, acc 1
2017-09-08T17:34:24.869581: step 2706, loss 0.0281072, acc 0.984375
2017-09-08T17:34:25.570563: step 2707, loss 0.0826716, acc 0.96875
2017-09-08T17:34:26.258306: step 2708, loss 0.130311, acc 0.953125
2017-09-08T17:34:26.962326: step 2709, loss 0.0639529, acc 0.984375
2017-09-08T17:34:27.829793: step 2710, loss 0.00967174, acc 1
2017-09-08T17:34:28.423757: step 2711, loss 0.0118514, acc 1
2017-09-08T17:34:29.183954: step 2712, loss 0.00228768, acc 1
2017-09-08T17:34:29.907702: step 2713, loss 0.0894353, acc 0.984375
2017-09-08T17:34:30.709334: step 2714, loss 0.00724074, acc 1
2017-09-08T17:34:31.525980: step 2715, loss 0.0132795, acc 1
2017-09-08T17:34:32.212165: step 2716, loss 0.0368413, acc 0.984375
2017-09-08T17:34:33.029173: step 2717, loss 0.0175935, acc 1
2017-09-08T17:34:33.777337: step 2718, loss 0.0138259, acc 1
2017-09-08T17:34:34.553133: step 2719, loss 0.0174028, acc 1
2017-09-08T17:34:35.361603: step 2720, loss 0.100336, acc 0.953125
2017-09-08T17:34:36.145875: step 2721, loss 0.0570537, acc 0.984375
2017-09-08T17:34:36.990367: step 2722, loss 0.0120683, acc 1
2017-09-08T17:34:37.757317: step 2723, loss 0.00384316, acc 1
2017-09-08T17:34:38.547946: step 2724, loss 0.0430077, acc 0.984375
2017-09-08T17:34:39.245840: step 2725, loss 0.00333265, acc 1
2017-09-08T17:34:40.087819: step 2726, loss 0.107374, acc 0.96875
2017-09-08T17:34:40.918404: step 2727, loss 0.00364643, acc 1
2017-09-08T17:34:41.684830: step 2728, loss 0.0528019, acc 0.984375
2017-09-08T17:34:42.447501: step 2729, loss 0.0199362, acc 0.984375
2017-09-08T17:34:43.218751: step 2730, loss 0.00817627, acc 1
2017-09-08T17:34:43.985664: step 2731, loss 0.0301603, acc 0.984375
2017-09-08T17:34:44.827633: step 2732, loss 0.0603729, acc 0.984375
2017-09-08T17:34:45.613797: step 2733, loss 0.0384623, acc 0.984375
2017-09-08T17:34:46.420241: step 2734, loss 0.016935, acc 1
2017-09-08T17:34:47.178137: step 2735, loss 0.00686477, acc 1
2017-09-08T17:34:48.089500: step 2736, loss 0.0300068, acc 0.984375
2017-09-08T17:34:48.865447: step 2737, loss 0.00159911, acc 1
2017-09-08T17:34:49.611962: step 2738, loss 0.00363106, acc 1
2017-09-08T17:34:50.327364: step 2739, loss 0.0189379, acc 0.984375
2017-09-08T17:34:51.084546: step 2740, loss 0.0346609, acc 0.96875
2017-09-08T17:34:51.851722: step 2741, loss 0.00252826, acc 1
2017-09-08T17:34:52.637707: step 2742, loss 0.0104444, acc 1
2017-09-08T17:34:53.535565: step 2743, loss 0.00166893, acc 1
2017-09-08T17:34:54.255955: step 2744, loss 0.0194489, acc 1
2017-09-08T17:34:55.149863: step 2745, loss 0.00606734, acc 1
2017-09-08T17:34:55.971513: step 2746, loss 0.05185, acc 0.96875
2017-09-08T17:34:56.613197: step 2747, loss 0.0044121, acc 1
2017-09-08T17:34:57.353889: step 2748, loss 0.00846565, acc 1
2017-09-08T17:34:57.960716: step 2749, loss 0.00517301, acc 1
2017-09-08T17:34:58.655687: step 2750, loss 0.0467021, acc 0.984375

Evaluation:
2017-09-08T17:34:59.528829: step 2750, loss 0.195608, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2750

2017-09-08T17:35:02.705399: step 2751, loss 0.00715688, acc 1
2017-09-08T17:35:03.415124: step 2752, loss 0.0312581, acc 1
2017-09-08T17:35:04.161947: step 2753, loss 0.0029033, acc 1
2017-09-08T17:35:04.917757: step 2754, loss 0.0548527, acc 0.984375
2017-09-08T17:35:05.747852: step 2755, loss 0.0275148, acc 0.984375
2017-09-08T17:35:06.601325: step 2756, loss 0.0323912, acc 0.984375
2017-09-08T17:35:07.417301: step 2757, loss 0.0214767, acc 1
2017-09-08T17:35:08.232171: step 2758, loss 0.0122227, acc 1
2017-09-08T17:35:09.198596: step 2759, loss 0.00143995, acc 1
2017-09-08T17:35:09.907341: step 2760, loss 0.0255063, acc 0.984375
2017-09-08T17:35:10.665649: step 2761, loss 0.002969, acc 1
2017-09-08T17:35:11.415028: step 2762, loss 0.0159436, acc 1
2017-09-08T17:35:12.208957: step 2763, loss 0.0172165, acc 0.984375
2017-09-08T17:35:12.988686: step 2764, loss 0.00241095, acc 1
2017-09-08T17:35:13.740281: step 2765, loss 0.00669079, acc 1
2017-09-08T17:35:14.439125: step 2766, loss 0.00184005, acc 1
2017-09-08T17:35:15.124572: step 2767, loss 0.0133151, acc 1
2017-09-08T17:35:15.940022: step 2768, loss 0.000966999, acc 1
2017-09-08T17:35:16.654492: step 2769, loss 0.00592038, acc 1
2017-09-08T17:35:17.404396: step 2770, loss 0.00587212, acc 1
2017-09-08T17:35:18.121649: step 2771, loss 0.0660917, acc 0.984375
2017-09-08T17:35:18.827437: step 2772, loss 0.0609401, acc 0.984375
2017-09-08T17:35:19.565201: step 2773, loss 0.00195368, acc 1
2017-09-08T17:35:20.345928: step 2774, loss 0.00820769, acc 1
2017-09-08T17:35:21.085927: step 2775, loss 0.00562392, acc 1
2017-09-08T17:35:21.866625: step 2776, loss 0.0117095, acc 1
2017-09-08T17:35:22.598311: step 2777, loss 0.00160846, acc 1
2017-09-08T17:35:23.351458: step 2778, loss 0.0119781, acc 1
2017-09-08T17:35:24.154274: step 2779, loss 0.0145995, acc 1
2017-09-08T17:35:24.954349: step 2780, loss 0.0053102, acc 1
2017-09-08T17:35:25.701669: step 2781, loss 0.0710847, acc 0.953125
2017-09-08T17:35:26.449925: step 2782, loss 0.0176988, acc 0.984375
2017-09-08T17:35:27.203591: step 2783, loss 0.00100958, acc 1
2017-09-08T17:35:27.983512: step 2784, loss 0.0072983, acc 1
2017-09-08T17:35:28.739193: step 2785, loss 0.0766331, acc 0.984375
2017-09-08T17:35:29.569872: step 2786, loss 0.0178724, acc 1
2017-09-08T17:35:30.329675: step 2787, loss 0.0371844, acc 0.984375
2017-09-08T17:35:31.089483: step 2788, loss 0.0251118, acc 0.984375
2017-09-08T17:35:31.940315: step 2789, loss 0.00379153, acc 1
2017-09-08T17:35:32.689011: step 2790, loss 0.00407472, acc 1
2017-09-08T17:35:33.483242: step 2791, loss 0.00256732, acc 1
2017-09-08T17:35:34.241938: step 2792, loss 0.00882527, acc 1
2017-09-08T17:35:35.006376: step 2793, loss 0.0439006, acc 0.984375
2017-09-08T17:35:35.805886: step 2794, loss 0.00144424, acc 1
2017-09-08T17:35:36.456723: step 2795, loss 0.00383141, acc 1
2017-09-08T17:35:37.186642: step 2796, loss 0.00348467, acc 1
2017-09-08T17:35:37.863401: step 2797, loss 0.0216704, acc 0.984375
2017-09-08T17:35:38.633603: step 2798, loss 0.00799666, acc 1
2017-09-08T17:35:39.384704: step 2799, loss 0.00376146, acc 1
2017-09-08T17:35:40.142070: step 2800, loss 0.0136898, acc 1

Evaluation:
2017-09-08T17:35:40.963060: step 2800, loss 0.200469, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2800

2017-09-08T17:35:44.829548: step 2801, loss 0.0259907, acc 0.984375
2017-09-08T17:35:45.495278: step 2802, loss 0.0190306, acc 0.984375
2017-09-08T17:35:46.247511: step 2803, loss 0.0847522, acc 0.953125
2017-09-08T17:35:46.947516: step 2804, loss 0.00210323, acc 1
2017-09-08T17:35:47.703277: step 2805, loss 0.00621181, acc 1
2017-09-08T17:35:48.517494: step 2806, loss 0.0638852, acc 0.953125
2017-09-08T17:35:49.420365: step 2807, loss 0.0156519, acc 0.984375
2017-09-08T17:35:50.116048: step 2808, loss 0.157336, acc 0.953125
2017-09-08T17:35:50.890626: step 2809, loss 0.00745047, acc 1
2017-09-08T17:35:51.627615: step 2810, loss 0.0502513, acc 0.984375
2017-09-08T17:35:52.328255: step 2811, loss 0.0166487, acc 1
2017-09-08T17:35:53.228868: step 2812, loss 0.105342, acc 0.96875
2017-09-08T17:35:53.997314: step 2813, loss 0.00190552, acc 1
2017-09-08T17:35:54.808574: step 2814, loss 0.00191824, acc 1
2017-09-08T17:35:55.634629: step 2815, loss 0.0569359, acc 0.984375
2017-09-08T17:35:56.349927: step 2816, loss 0.0398313, acc 0.984375
2017-09-08T17:35:57.070104: step 2817, loss 0.0224349, acc 0.984375
2017-09-08T17:35:57.842056: step 2818, loss 0.00565034, acc 1
2017-09-08T17:35:58.580571: step 2819, loss 0.00556694, acc 1
2017-09-08T17:35:59.330729: step 2820, loss 0.0197367, acc 1
2017-09-08T17:35:59.971383: step 2821, loss 0.00223086, acc 1
2017-09-08T17:36:00.706953: step 2822, loss 0.01005, acc 1
2017-09-08T17:36:01.321633: step 2823, loss 0.0779857, acc 0.984375
2017-09-08T17:36:02.003900: step 2824, loss 0.0315574, acc 0.984375
2017-09-08T17:36:02.699345: step 2825, loss 0.0216647, acc 0.984375
2017-09-08T17:36:03.426240: step 2826, loss 0.142549, acc 0.96875
2017-09-08T17:36:04.190431: step 2827, loss 0.0100636, acc 1
2017-09-08T17:36:04.928188: step 2828, loss 0.00332904, acc 1
2017-09-08T17:36:05.638615: step 2829, loss 0.00543328, acc 1
2017-09-08T17:36:06.433017: step 2830, loss 0.035449, acc 0.984375
2017-09-08T17:36:07.173409: step 2831, loss 0.0101876, acc 1
2017-09-08T17:36:07.984155: step 2832, loss 0.0497292, acc 0.984375
2017-09-08T17:36:08.710589: step 2833, loss 0.00475262, acc 1
2017-09-08T17:36:09.462346: step 2834, loss 0.040975, acc 0.984375
2017-09-08T17:36:10.226087: step 2835, loss 0.0146795, acc 0.984375
2017-09-08T17:36:10.936794: step 2836, loss 0.00740762, acc 1
2017-09-08T17:36:11.688517: step 2837, loss 0.0569993, acc 0.984375
2017-09-08T17:36:12.476550: step 2838, loss 0.0430411, acc 0.984375
2017-09-08T17:36:13.379491: step 2839, loss 0.00193238, acc 1
2017-09-08T17:36:14.141948: step 2840, loss 0.0307762, acc 0.984375
2017-09-08T17:36:14.890754: step 2841, loss 0.00328962, acc 1
2017-09-08T17:36:15.681726: step 2842, loss 0.00532023, acc 1
2017-09-08T17:36:16.435749: step 2843, loss 0.0591447, acc 0.984375
2017-09-08T17:36:17.179432: step 2844, loss 0.0105574, acc 1
2017-09-08T17:36:17.892071: step 2845, loss 0.00119501, acc 1
2017-09-08T17:36:18.671797: step 2846, loss 0.00228971, acc 1
2017-09-08T17:36:19.447671: step 2847, loss 0.0113522, acc 1
2017-09-08T17:36:20.223069: step 2848, loss 0.00382364, acc 1
2017-09-08T17:36:20.948788: step 2849, loss 0.0161999, acc 0.984375
2017-09-08T17:36:21.713692: step 2850, loss 0.05627, acc 0.96875

Evaluation:
2017-09-08T17:36:22.503667: step 2850, loss 0.196624, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2850

2017-09-08T17:36:25.495771: step 2851, loss 0.0234078, acc 0.984375
2017-09-08T17:36:26.151998: step 2852, loss 0.0120147, acc 1
2017-09-08T17:36:26.881206: step 2853, loss 0.0103795, acc 1
2017-09-08T17:36:27.549779: step 2854, loss 0.0079202, acc 1
2017-09-08T17:36:28.305263: step 2855, loss 0.0450135, acc 0.984375
2017-09-08T17:36:29.037515: step 2856, loss 0.0030912, acc 1
2017-09-08T17:36:29.797647: step 2857, loss 0.0131377, acc 1
2017-09-08T17:36:30.656997: step 2858, loss 0.0766393, acc 0.96875
2017-09-08T17:36:31.396636: step 2859, loss 0.13491, acc 0.953125
2017-09-08T17:36:32.043347: step 2860, loss 0.000824406, acc 1
2017-09-08T17:36:32.705675: step 2861, loss 0.00142935, acc 1
2017-09-08T17:36:33.648983: step 2862, loss 0.0518318, acc 0.96875
2017-09-08T17:36:34.388817: step 2863, loss 0.0469355, acc 0.96875
2017-09-08T17:36:35.142593: step 2864, loss 0.0365841, acc 0.984375
2017-09-08T17:36:35.858805: step 2865, loss 0.00219108, acc 1
2017-09-08T17:36:36.687774: step 2866, loss 0.00353905, acc 1
2017-09-08T17:36:37.518511: step 2867, loss 0.0369213, acc 0.984375
2017-09-08T17:36:38.269263: step 2868, loss 0.00308732, acc 1
2017-09-08T17:36:39.055191: step 2869, loss 0.00181882, acc 1
2017-09-08T17:36:39.849474: step 2870, loss 0.0086447, acc 1
2017-09-08T17:36:40.543699: step 2871, loss 0.0220383, acc 0.984375
2017-09-08T17:36:41.233448: step 2872, loss 0.0405891, acc 0.984375
2017-09-08T17:36:41.883094: step 2873, loss 0.0510777, acc 0.984375
2017-09-08T17:36:42.620332: step 2874, loss 0.053075, acc 0.984375
2017-09-08T17:36:43.431392: step 2875, loss 0.0056845, acc 1
2017-09-08T17:36:44.244998: step 2876, loss 0.00420945, acc 1
2017-09-08T17:36:45.064639: step 2877, loss 0.0353979, acc 0.984375
2017-09-08T17:36:45.879770: step 2878, loss 0.00404848, acc 1
2017-09-08T17:36:46.662827: step 2879, loss 0.0326361, acc 0.984375
2017-09-08T17:36:47.422771: step 2880, loss 0.00818857, acc 1
2017-09-08T17:36:48.215303: step 2881, loss 0.00313881, acc 1
2017-09-08T17:36:49.017002: step 2882, loss 0.0148155, acc 1
2017-09-08T17:36:49.832728: step 2883, loss 0.0262724, acc 0.984375
2017-09-08T17:36:50.548675: step 2884, loss 0.0545902, acc 0.96875
2017-09-08T17:36:51.286343: step 2885, loss 0.135336, acc 0.96875
2017-09-08T17:36:51.986154: step 2886, loss 0.107242, acc 0.953125
2017-09-08T17:36:52.813199: step 2887, loss 0.00674994, acc 1
2017-09-08T17:36:53.472338: step 2888, loss 0.00283361, acc 1
2017-09-08T17:36:54.254407: step 2889, loss 0.00311638, acc 1
2017-09-08T17:36:55.029594: step 2890, loss 0.0102131, acc 1
2017-09-08T17:36:55.816448: step 2891, loss 0.0345551, acc 0.984375
2017-09-08T17:36:56.513967: step 2892, loss 0.0021852, acc 1
2017-09-08T17:36:57.367909: step 2893, loss 0.00271262, acc 1
2017-09-08T17:36:58.000502: step 2894, loss 0.0314326, acc 0.984375
2017-09-08T17:36:58.768149: step 2895, loss 0.130222, acc 0.96875
2017-09-08T17:36:59.467867: step 2896, loss 0.0764981, acc 0.984375
2017-09-08T17:37:00.094516: step 2897, loss 0.0722591, acc 0.984375
2017-09-08T17:37:00.816706: step 2898, loss 0.0352162, acc 0.984375
2017-09-08T17:37:01.551883: step 2899, loss 0.0314129, acc 0.984375
2017-09-08T17:37:02.296595: step 2900, loss 0.00308333, acc 1

Evaluation:
2017-09-08T17:37:02.989893: step 2900, loss 0.196497, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2900

2017-09-08T17:37:06.891009: step 2901, loss 0.00472362, acc 1
2017-09-08T17:37:07.617587: step 2902, loss 0.0279867, acc 0.984375
2017-09-08T17:37:08.297634: step 2903, loss 0.00487059, acc 1
2017-09-08T17:37:09.039895: step 2904, loss 0.0177799, acc 1
2017-09-08T17:37:09.786902: step 2905, loss 0.0661893, acc 0.96875
2017-09-08T17:37:10.503314: step 2906, loss 0.0301224, acc 0.984375
2017-09-08T17:37:11.297517: step 2907, loss 0.0189564, acc 1
2017-09-08T17:37:11.894548: step 2908, loss 0.103675, acc 0.984375
2017-09-08T17:37:12.505112: step 2909, loss 0.00533415, acc 1
2017-09-08T17:37:13.067791: step 2910, loss 0.0858975, acc 0.96875
2017-09-08T17:37:13.640109: step 2911, loss 0.00272845, acc 1
2017-09-08T17:37:14.265425: step 2912, loss 0.0682108, acc 0.984375
2017-09-08T17:37:14.980278: step 2913, loss 0.00235636, acc 1
2017-09-08T17:37:15.667652: step 2914, loss 0.00112365, acc 1
2017-09-08T17:37:16.384429: step 2915, loss 0.00928569, acc 1
2017-09-08T17:37:17.100724: step 2916, loss 0.0153346, acc 1
2017-09-08T17:37:17.804950: step 2917, loss 0.0133628, acc 1
2017-09-08T17:37:18.580721: step 2918, loss 0.0468754, acc 0.984375
2017-09-08T17:37:19.397868: step 2919, loss 0.00161521, acc 1
2017-09-08T17:37:20.043868: step 2920, loss 0.0261997, acc 0.984375
2017-09-08T17:37:20.789839: step 2921, loss 0.00215291, acc 1
2017-09-08T17:37:21.427859: step 2922, loss 0.0708098, acc 0.984375
2017-09-08T17:37:22.044643: step 2923, loss 0.0105144, acc 1
2017-09-08T17:37:22.844202: step 2924, loss 0.0208273, acc 0.984375
2017-09-08T17:37:23.603338: step 2925, loss 0.00159378, acc 1
2017-09-08T17:37:24.400584: step 2926, loss 0.0733048, acc 0.96875
2017-09-08T17:37:25.177445: step 2927, loss 0.00196792, acc 1
2017-09-08T17:37:25.912985: step 2928, loss 0.00780675, acc 1
2017-09-08T17:37:26.739567: step 2929, loss 0.185727, acc 0.96875
2017-09-08T17:37:27.545865: step 2930, loss 0.00122254, acc 1
2017-09-08T17:37:28.296607: step 2931, loss 0.0438293, acc 0.984375
2017-09-08T17:37:29.046011: step 2932, loss 0.0251705, acc 0.984375
2017-09-08T17:37:29.845512: step 2933, loss 0.0140272, acc 1
2017-09-08T17:37:30.612551: step 2934, loss 0.00530048, acc 1
2017-09-08T17:37:31.389351: step 2935, loss 0.0207661, acc 0.984375
2017-09-08T17:37:32.203094: step 2936, loss 0.0669822, acc 0.984375
2017-09-08T17:37:32.971779: step 2937, loss 0.0126087, acc 1
2017-09-08T17:37:33.648512: step 2938, loss 0.00604252, acc 1
2017-09-08T17:37:34.344931: step 2939, loss 0.0145539, acc 0.984375
2017-09-08T17:37:34.992684: step 2940, loss 0.0097462, acc 1
2017-09-08T17:37:35.709016: step 2941, loss 0.00358065, acc 1
2017-09-08T17:37:36.374565: step 2942, loss 0.00361411, acc 1
2017-09-08T17:37:37.017434: step 2943, loss 0.0246991, acc 0.984375
2017-09-08T17:37:37.753167: step 2944, loss 0.0309037, acc 0.984375
2017-09-08T17:37:38.404079: step 2945, loss 0.012189, acc 1
2017-09-08T17:37:39.137948: step 2946, loss 0.00171153, acc 1
2017-09-08T17:37:39.897977: step 2947, loss 0.0895113, acc 0.96875
2017-09-08T17:37:40.648885: step 2948, loss 0.0300806, acc 0.984375
2017-09-08T17:37:41.410705: step 2949, loss 0.0235004, acc 0.984375
2017-09-08T17:37:42.211343: step 2950, loss 0.0480124, acc 0.984375

Evaluation:
2017-09-08T17:37:43.019754: step 2950, loss 0.194746, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-2950

2017-09-08T17:37:45.951270: step 2951, loss 0.012116, acc 1
2017-09-08T17:37:46.504465: step 2952, loss 0.00269412, acc 1
2017-09-08T17:37:47.231054: step 2953, loss 0.0257012, acc 0.984375
2017-09-08T17:37:48.000791: step 2954, loss 0.0945204, acc 0.96875
2017-09-08T17:37:48.786563: step 2955, loss 0.00089338, acc 1
2017-09-08T17:37:49.424871: step 2956, loss 0.0394441, acc 0.96875
2017-09-08T17:37:50.165142: step 2957, loss 0.0238789, acc 1
2017-09-08T17:37:50.878665: step 2958, loss 0.0346729, acc 0.984375
2017-09-08T17:37:51.724722: step 2959, loss 0.0156998, acc 1
2017-09-08T17:37:52.365815: step 2960, loss 0.00405444, acc 1
2017-09-08T17:37:53.109140: step 2961, loss 0.00249721, acc 1
2017-09-08T17:37:53.850937: step 2962, loss 0.0504842, acc 0.984375
2017-09-08T17:37:54.576546: step 2963, loss 0.0037546, acc 1
2017-09-08T17:37:55.226796: step 2964, loss 0.00287764, acc 1
2017-09-08T17:37:55.933111: step 2965, loss 0.0172865, acc 1
2017-09-08T17:37:56.670768: step 2966, loss 0.0128658, acc 1
2017-09-08T17:37:57.454756: step 2967, loss 0.00602712, acc 1
2017-09-08T17:37:58.231183: step 2968, loss 0.0574154, acc 0.96875
2017-09-08T17:37:58.899518: step 2969, loss 0.0510626, acc 0.984375
2017-09-08T17:37:59.587570: step 2970, loss 0.0137234, acc 1
2017-09-08T17:38:00.192530: step 2971, loss 0.00222197, acc 1
2017-09-08T17:38:00.869532: step 2972, loss 0.00267759, acc 1
2017-09-08T17:38:01.546935: step 2973, loss 0.00113256, acc 1
2017-09-08T17:38:02.283528: step 2974, loss 0.001408, acc 1
2017-09-08T17:38:03.026154: step 2975, loss 0.00317637, acc 1
2017-09-08T17:38:03.778931: step 2976, loss 0.0436192, acc 0.984375
2017-09-08T17:38:04.503868: step 2977, loss 0.0141265, acc 1
2017-09-08T17:38:05.286936: step 2978, loss 0.00234513, acc 1
2017-09-08T17:38:06.036201: step 2979, loss 0.00347918, acc 1
2017-09-08T17:38:06.806705: step 2980, loss 0.0403043, acc 0.984375
2017-09-08T17:38:07.487687: step 2981, loss 0.0608717, acc 0.96875
2017-09-08T17:38:08.215406: step 2982, loss 0.00491967, acc 1
2017-09-08T17:38:08.973457: step 2983, loss 0.00334992, acc 1
2017-09-08T17:38:09.727675: step 2984, loss 0.00309293, acc 1
2017-09-08T17:38:10.442252: step 2985, loss 0.0523116, acc 0.984375
2017-09-08T17:38:11.202505: step 2986, loss 0.0022011, acc 1
2017-09-08T17:38:11.984499: step 2987, loss 0.0409054, acc 0.984375
2017-09-08T17:38:12.777463: step 2988, loss 0.0487253, acc 0.984375
2017-09-08T17:38:13.693973: step 2989, loss 0.0609252, acc 0.984375
2017-09-08T17:38:14.360423: step 2990, loss 0.00355279, acc 1
2017-09-08T17:38:15.165693: step 2991, loss 0.087, acc 0.984375
2017-09-08T17:38:15.941592: step 2992, loss 0.00322201, acc 1
2017-09-08T17:38:16.629104: step 2993, loss 0.00123299, acc 1
2017-09-08T17:38:17.262431: step 2994, loss 0.00300863, acc 1
2017-09-08T17:38:17.968715: step 2995, loss 0.0023085, acc 1
2017-09-08T17:38:18.667596: step 2996, loss 0.0190331, acc 1
2017-09-08T17:38:19.329080: step 2997, loss 0.050633, acc 0.96875
2017-09-08T17:38:20.052487: step 2998, loss 0.120216, acc 0.9375
2017-09-08T17:38:20.733124: step 2999, loss 0.0511646, acc 0.984375
2017-09-08T17:38:21.434271: step 3000, loss 0.0152994, acc 0.984375

Evaluation:
2017-09-08T17:38:22.151716: step 3000, loss 0.197345, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3000

2017-09-08T17:38:26.044410: step 3001, loss 0.00276945, acc 1
2017-09-08T17:38:26.818529: step 3002, loss 0.0154575, acc 1
2017-09-08T17:38:27.632843: step 3003, loss 0.00402268, acc 1
2017-09-08T17:38:28.301777: step 3004, loss 0.00641654, acc 1
2017-09-08T17:38:29.046444: step 3005, loss 0.00373965, acc 1
2017-09-08T17:38:29.840122: step 3006, loss 0.00501166, acc 1
2017-09-08T17:38:30.546099: step 3007, loss 0.00317809, acc 1
2017-09-08T17:38:31.360345: step 3008, loss 0.0111362, acc 1
2017-09-08T17:38:32.035637: step 3009, loss 0.0282537, acc 0.984375
2017-09-08T17:38:32.736885: step 3010, loss 0.0171615, acc 1
2017-09-08T17:38:33.425781: step 3011, loss 0.00555777, acc 1
2017-09-08T17:38:34.118200: step 3012, loss 0.0664274, acc 0.96875
2017-09-08T17:38:34.868424: step 3013, loss 0.207356, acc 0.9375
2017-09-08T17:38:36.219136: step 3014, loss 0.0147726, acc 1
2017-09-08T17:38:37.017396: step 3015, loss 0.0161086, acc 1
2017-09-08T17:38:37.757477: step 3016, loss 0.0134953, acc 1
2017-09-08T17:38:38.440803: step 3017, loss 0.0509046, acc 0.984375
2017-09-08T17:38:39.130843: step 3018, loss 0.00298574, acc 1
2017-09-08T17:38:39.834141: step 3019, loss 0.0346318, acc 0.984375
2017-09-08T17:38:40.663740: step 3020, loss 0.00134819, acc 1
2017-09-08T17:38:41.435742: step 3021, loss 0.011419, acc 1
2017-09-08T17:38:42.239276: step 3022, loss 0.0129473, acc 1
2017-09-08T17:38:42.944160: step 3023, loss 0.0214523, acc 0.984375
2017-09-08T17:38:43.695188: step 3024, loss 0.0392967, acc 0.984375
2017-09-08T17:38:44.410396: step 3025, loss 0.00525932, acc 1
2017-09-08T17:38:45.131779: step 3026, loss 0.0417308, acc 0.984375
2017-09-08T17:38:45.834621: step 3027, loss 0.0108984, acc 1
2017-09-08T17:38:46.576174: step 3028, loss 0.0309668, acc 0.984375
2017-09-08T17:38:47.323641: step 3029, loss 0.00999977, acc 1
2017-09-08T17:38:47.977851: step 3030, loss 0.120971, acc 0.96875
2017-09-08T17:38:48.804122: step 3031, loss 0.0787037, acc 0.96875
2017-09-08T17:38:49.545299: step 3032, loss 0.00833146, acc 1
2017-09-08T17:38:50.276667: step 3033, loss 0.00337542, acc 1
2017-09-08T17:38:50.969014: step 3034, loss 0.00203542, acc 1
2017-09-08T17:38:51.767874: step 3035, loss 0.092145, acc 0.96875
2017-09-08T17:38:52.442118: step 3036, loss 0.0372461, acc 0.984375
2017-09-08T17:38:53.240565: step 3037, loss 0.00162542, acc 1
2017-09-08T17:38:53.865125: step 3038, loss 0.110113, acc 0.941176
2017-09-08T17:38:54.571217: step 3039, loss 0.073156, acc 0.984375
2017-09-08T17:38:55.363994: step 3040, loss 0.0163105, acc 1
2017-09-08T17:38:56.010902: step 3041, loss 0.0474529, acc 0.96875
2017-09-08T17:38:56.655972: step 3042, loss 0.0128913, acc 1
2017-09-08T17:38:57.475456: step 3043, loss 0.0292553, acc 0.984375
2017-09-08T17:38:58.098221: step 3044, loss 0.00828227, acc 1
2017-09-08T17:38:58.739284: step 3045, loss 0.0139028, acc 1
2017-09-08T17:38:59.412730: step 3046, loss 0.0595645, acc 0.96875
2017-09-08T17:39:00.131636: step 3047, loss 0.012723, acc 1
2017-09-08T17:39:00.885275: step 3048, loss 0.0267356, acc 0.984375
2017-09-08T17:39:01.659296: step 3049, loss 0.00320397, acc 1
2017-09-08T17:39:02.356263: step 3050, loss 0.00815416, acc 1

Evaluation:
2017-09-08T17:39:03.072624: step 3050, loss 0.207765, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3050

2017-09-08T17:39:06.906238: step 3051, loss 0.039155, acc 0.984375
2017-09-08T17:39:07.617876: step 3052, loss 0.0520266, acc 0.96875
2017-09-08T17:39:08.294320: step 3053, loss 0.00281247, acc 1
2017-09-08T17:39:08.923058: step 3054, loss 0.1077, acc 0.96875
2017-09-08T17:39:09.567888: step 3055, loss 0.0538077, acc 0.96875
2017-09-08T17:39:10.164423: step 3056, loss 0.00372594, acc 1
2017-09-08T17:39:10.919260: step 3057, loss 0.0262512, acc 0.984375
2017-09-08T17:39:11.592561: step 3058, loss 0.00610066, acc 1
2017-09-08T17:39:12.207569: step 3059, loss 0.00208907, acc 1
2017-09-08T17:39:12.855968: step 3060, loss 0.00459094, acc 1
2017-09-08T17:39:13.530609: step 3061, loss 0.00381255, acc 1
2017-09-08T17:39:14.236637: step 3062, loss 0.041023, acc 0.984375
2017-09-08T17:39:15.022594: step 3063, loss 0.125179, acc 0.9375
2017-09-08T17:39:15.782792: step 3064, loss 0.0120468, acc 1
2017-09-08T17:39:16.644360: step 3065, loss 0.0157144, acc 0.984375
2017-09-08T17:39:17.317339: step 3066, loss 0.0729195, acc 0.984375
2017-09-08T17:39:18.008829: step 3067, loss 0.00184334, acc 1
2017-09-08T17:39:18.603611: step 3068, loss 0.0285938, acc 0.984375
2017-09-08T17:39:19.335154: step 3069, loss 0.0500121, acc 0.984375
2017-09-08T17:39:20.129016: step 3070, loss 0.00230907, acc 1
2017-09-08T17:39:20.852759: step 3071, loss 0.0360742, acc 0.984375
2017-09-08T17:39:21.582256: step 3072, loss 0.00168296, acc 1
2017-09-08T17:39:22.335020: step 3073, loss 0.00174081, acc 1
2017-09-08T17:39:23.047880: step 3074, loss 0.0069854, acc 1
2017-09-08T17:39:23.890105: step 3075, loss 0.0350678, acc 0.96875
2017-09-08T17:39:24.592522: step 3076, loss 0.0356287, acc 0.984375
2017-09-08T17:39:25.352391: step 3077, loss 0.0456893, acc 0.984375
2017-09-08T17:39:25.967955: step 3078, loss 0.0527787, acc 0.984375
2017-09-08T17:39:26.628669: step 3079, loss 0.0518624, acc 0.96875
2017-09-08T17:39:27.456978: step 3080, loss 0.022149, acc 0.984375
2017-09-08T17:39:28.162380: step 3081, loss 0.0113604, acc 1
2017-09-08T17:39:28.826184: step 3082, loss 0.0790946, acc 0.984375
2017-09-08T17:39:29.613206: step 3083, loss 0.00190563, acc 1
2017-09-08T17:39:30.347705: step 3084, loss 0.00253612, acc 1
2017-09-08T17:39:31.099802: step 3085, loss 0.0170914, acc 1
2017-09-08T17:39:31.754587: step 3086, loss 0.00155933, acc 1
2017-09-08T17:39:32.418831: step 3087, loss 0.00436051, acc 1
2017-09-08T17:39:33.073661: step 3088, loss 0.0384547, acc 0.984375
2017-09-08T17:39:33.738488: step 3089, loss 0.0537018, acc 0.984375
2017-09-08T17:39:34.430131: step 3090, loss 0.00296448, acc 1
2017-09-08T17:39:35.122861: step 3091, loss 0.00387395, acc 1
2017-09-08T17:39:35.873713: step 3092, loss 0.00261319, acc 1
2017-09-08T17:39:36.531894: step 3093, loss 0.0271671, acc 0.984375
2017-09-08T17:39:37.238752: step 3094, loss 0.00274978, acc 1
2017-09-08T17:39:37.972481: step 3095, loss 0.0187454, acc 0.984375
2017-09-08T17:39:38.827516: step 3096, loss 0.0514533, acc 0.984375
2017-09-08T17:39:39.539794: step 3097, loss 0.0768304, acc 0.96875
2017-09-08T17:39:40.352989: step 3098, loss 0.0154172, acc 0.984375
2017-09-08T17:39:41.124895: step 3099, loss 0.0202513, acc 0.984375
2017-09-08T17:39:41.807828: step 3100, loss 0.01385, acc 1

Evaluation:
2017-09-08T17:39:42.570265: step 3100, loss 0.207679, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3100

2017-09-08T17:39:45.697207: step 3101, loss 0.0555346, acc 0.96875
2017-09-08T17:39:46.428661: step 3102, loss 0.00665355, acc 1
2017-09-08T17:39:47.255269: step 3103, loss 0.00126982, acc 1
2017-09-08T17:39:48.050667: step 3104, loss 0.0666789, acc 0.984375
2017-09-08T17:39:48.994980: step 3105, loss 0.0107359, acc 1
2017-09-08T17:39:49.796408: step 3106, loss 0.00419681, acc 1
2017-09-08T17:39:50.580010: step 3107, loss 0.00382849, acc 1
2017-09-08T17:39:51.315302: step 3108, loss 0.0321233, acc 0.984375
2017-09-08T17:39:52.039571: step 3109, loss 0.0410504, acc 0.984375
2017-09-08T17:39:52.667780: step 3110, loss 0.00223183, acc 1
2017-09-08T17:39:53.340528: step 3111, loss 0.028007, acc 0.984375
2017-09-08T17:39:53.934247: step 3112, loss 0.00110429, acc 1
2017-09-08T17:39:54.584381: step 3113, loss 0.018092, acc 0.984375
2017-09-08T17:39:55.210116: step 3114, loss 0.0581645, acc 0.96875
2017-09-08T17:39:55.902940: step 3115, loss 0.0181901, acc 0.984375
2017-09-08T17:39:56.532370: step 3116, loss 0.102888, acc 0.984375
2017-09-08T17:39:57.170773: step 3117, loss 0.0686421, acc 0.984375
2017-09-08T17:39:57.877403: step 3118, loss 0.0301478, acc 0.984375
2017-09-08T17:39:58.646714: step 3119, loss 0.0711151, acc 0.984375
2017-09-08T17:39:59.349387: step 3120, loss 0.0102444, acc 1
2017-09-08T17:40:00.130421: step 3121, loss 0.0207246, acc 0.984375
2017-09-08T17:40:00.885129: step 3122, loss 0.0305722, acc 0.984375
2017-09-08T17:40:01.607235: step 3123, loss 0.0380685, acc 0.984375
2017-09-08T17:40:02.358754: step 3124, loss 0.0609875, acc 0.984375
2017-09-08T17:40:03.082550: step 3125, loss 0.0284291, acc 0.984375
2017-09-08T17:40:03.799788: step 3126, loss 0.00244001, acc 1
2017-09-08T17:40:04.562522: step 3127, loss 0.0250721, acc 0.984375
2017-09-08T17:40:05.405601: step 3128, loss 0.0193889, acc 0.984375
2017-09-08T17:40:06.161291: step 3129, loss 0.0114901, acc 0.984375
2017-09-08T17:40:06.852542: step 3130, loss 0.0132315, acc 1
2017-09-08T17:40:07.656798: step 3131, loss 0.00116865, acc 1
2017-09-08T17:40:08.409943: step 3132, loss 0.0697157, acc 0.96875
2017-09-08T17:40:09.170897: step 3133, loss 0.00511982, acc 1
2017-09-08T17:40:09.896758: step 3134, loss 0.0135452, acc 1
2017-09-08T17:40:10.785057: step 3135, loss 0.0424903, acc 0.96875
2017-09-08T17:40:11.419513: step 3136, loss 0.000909207, acc 1
2017-09-08T17:40:12.856208: step 3137, loss 0.00239959, acc 1
2017-09-08T17:40:13.596636: step 3138, loss 0.0424637, acc 0.984375
2017-09-08T17:40:14.302752: step 3139, loss 0.00323749, acc 1
2017-09-08T17:40:15.064377: step 3140, loss 0.134615, acc 0.953125
2017-09-08T17:40:15.774161: step 3141, loss 0.012069, acc 1
2017-09-08T17:40:16.443349: step 3142, loss 0.0365197, acc 0.984375
2017-09-08T17:40:17.112253: step 3143, loss 0.0466921, acc 0.984375
2017-09-08T17:40:17.733393: step 3144, loss 0.0722816, acc 0.96875
2017-09-08T17:40:18.363679: step 3145, loss 0.0181516, acc 0.984375
2017-09-08T17:40:19.067144: step 3146, loss 0.0197157, acc 1
2017-09-08T17:40:19.687248: step 3147, loss 0.00128395, acc 1
2017-09-08T17:40:20.347657: step 3148, loss 0.013718, acc 1
2017-09-08T17:40:21.032827: step 3149, loss 0.00250231, acc 1
2017-09-08T17:40:21.780772: step 3150, loss 0.0247267, acc 0.984375

Evaluation:
2017-09-08T17:40:22.651763: step 3150, loss 0.208454, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3150

2017-09-08T17:40:26.639373: step 3151, loss 0.00238537, acc 1
2017-09-08T17:40:27.347700: step 3152, loss 0.0282859, acc 0.984375
2017-09-08T17:40:27.920353: step 3153, loss 0.0159859, acc 1
2017-09-08T17:40:28.711594: step 3154, loss 0.00329112, acc 1
2017-09-08T17:40:29.329038: step 3155, loss 0.0422603, acc 0.96875
2017-09-08T17:40:30.080451: step 3156, loss 0.0566089, acc 0.984375
2017-09-08T17:40:30.711039: step 3157, loss 0.00218734, acc 1
2017-09-08T17:40:31.362165: step 3158, loss 0.0201992, acc 0.984375
2017-09-08T17:40:32.044202: step 3159, loss 0.029325, acc 0.984375
2017-09-08T17:40:32.693162: step 3160, loss 0.139672, acc 0.96875
2017-09-08T17:40:33.403403: step 3161, loss 0.0492922, acc 0.984375
2017-09-08T17:40:34.119328: step 3162, loss 0.00242641, acc 1
2017-09-08T17:40:34.917866: step 3163, loss 0.0499881, acc 0.96875
2017-09-08T17:40:35.750969: step 3164, loss 0.00246796, acc 1
2017-09-08T17:40:36.553742: step 3165, loss 0.00139544, acc 1
2017-09-08T17:40:37.319797: step 3166, loss 0.0152204, acc 1
2017-09-08T17:40:37.989228: step 3167, loss 0.00192803, acc 1
2017-09-08T17:40:38.611357: step 3168, loss 0.00182447, acc 1
2017-09-08T17:40:39.246154: step 3169, loss 0.0141319, acc 1
2017-09-08T17:40:39.847616: step 3170, loss 0.0368011, acc 0.984375
2017-09-08T17:40:40.488841: step 3171, loss 0.00456983, acc 1
2017-09-08T17:40:41.218214: step 3172, loss 0.0125283, acc 1
2017-09-08T17:40:41.835845: step 3173, loss 0.00501464, acc 1
2017-09-08T17:40:42.542113: step 3174, loss 0.00768305, acc 1
2017-09-08T17:40:43.258147: step 3175, loss 0.00202096, acc 1
2017-09-08T17:40:43.994755: step 3176, loss 0.0373593, acc 0.984375
2017-09-08T17:40:44.709775: step 3177, loss 0.00363249, acc 1
2017-09-08T17:40:45.409320: step 3178, loss 0.0127451, acc 1
2017-09-08T17:40:46.115324: step 3179, loss 0.0849878, acc 0.953125
2017-09-08T17:40:46.817666: step 3180, loss 0.0251932, acc 0.984375
2017-09-08T17:40:48.128436: step 3181, loss 0.00529725, acc 1
2017-09-08T17:40:48.898442: step 3182, loss 0.0435696, acc 0.984375
2017-09-08T17:40:49.665052: step 3183, loss 0.0707637, acc 0.984375
2017-09-08T17:40:50.382840: step 3184, loss 0.100251, acc 0.96875
2017-09-08T17:40:51.132866: step 3185, loss 0.145331, acc 0.96875
2017-09-08T17:40:51.854153: step 3186, loss 0.00934081, acc 1
2017-09-08T17:40:53.171008: step 3187, loss 0.0900852, acc 0.96875
2017-09-08T17:40:53.916773: step 3188, loss 0.0102911, acc 1
2017-09-08T17:40:54.537240: step 3189, loss 0.00359642, acc 1
2017-09-08T17:40:55.251363: step 3190, loss 0.0029715, acc 1
2017-09-08T17:40:55.895915: step 3191, loss 0.0138671, acc 0.984375
2017-09-08T17:40:56.661009: step 3192, loss 0.00361075, acc 1
2017-09-08T17:40:57.282046: step 3193, loss 0.0417264, acc 0.984375
2017-09-08T17:40:57.989210: step 3194, loss 0.00762011, acc 1
2017-09-08T17:40:58.616548: step 3195, loss 0.100457, acc 0.96875
2017-09-08T17:40:59.350336: step 3196, loss 0.00821355, acc 1
2017-09-08T17:41:00.073727: step 3197, loss 0.0575846, acc 0.984375
2017-09-08T17:41:00.795105: step 3198, loss 0.013737, acc 1
2017-09-08T17:41:01.597695: step 3199, loss 0.00375622, acc 1
2017-09-08T17:41:02.319226: step 3200, loss 0.00791623, acc 1

Evaluation:
2017-09-08T17:41:03.123391: step 3200, loss 0.211409, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3200

2017-09-08T17:41:05.525179: step 3201, loss 0.00204301, acc 1
2017-09-08T17:41:06.177839: step 3202, loss 0.00382438, acc 1
2017-09-08T17:41:06.854704: step 3203, loss 0.00672707, acc 1
2017-09-08T17:41:07.617809: step 3204, loss 0.0102863, acc 1
2017-09-08T17:41:08.290979: step 3205, loss 0.00465341, acc 1
2017-09-08T17:41:09.013193: step 3206, loss 0.00593888, acc 1
2017-09-08T17:41:09.667118: step 3207, loss 0.00476191, acc 1
2017-09-08T17:41:10.305802: step 3208, loss 0.00567962, acc 1
2017-09-08T17:41:10.985961: step 3209, loss 0.0468119, acc 0.984375
2017-09-08T17:41:11.723948: step 3210, loss 0.0160711, acc 1
2017-09-08T17:41:12.419343: step 3211, loss 0.058943, acc 0.984375
2017-09-08T17:41:13.031492: step 3212, loss 0.0134147, acc 1
2017-09-08T17:41:13.773258: step 3213, loss 0.00200092, acc 1
2017-09-08T17:41:14.557177: step 3214, loss 0.114126, acc 0.96875
2017-09-08T17:41:15.346978: step 3215, loss 0.01389, acc 1
2017-09-08T17:41:16.098212: step 3216, loss 0.034744, acc 0.984375
2017-09-08T17:41:16.832604: step 3217, loss 0.0289877, acc 0.984375
2017-09-08T17:41:17.473055: step 3218, loss 0.0581145, acc 0.984375
2017-09-08T17:41:18.151186: step 3219, loss 0.0137115, acc 1
2017-09-08T17:41:18.748455: step 3220, loss 0.00411806, acc 1
2017-09-08T17:41:19.451601: step 3221, loss 0.00338399, acc 1
2017-09-08T17:41:20.153433: step 3222, loss 0.000906472, acc 1
2017-09-08T17:41:20.868190: step 3223, loss 0.00225779, acc 1
2017-09-08T17:41:21.587225: step 3224, loss 0.00153523, acc 1
2017-09-08T17:41:22.346245: step 3225, loss 0.0278545, acc 0.984375
2017-09-08T17:41:23.056294: step 3226, loss 0.0263867, acc 0.984375
2017-09-08T17:41:23.775258: step 3227, loss 0.0433921, acc 0.984375
2017-09-08T17:41:24.518805: step 3228, loss 0.0270469, acc 0.984375
2017-09-08T17:41:25.211617: step 3229, loss 0.0168256, acc 1
2017-09-08T17:41:25.889853: step 3230, loss 0.0299365, acc 0.984375
2017-09-08T17:41:26.580983: step 3231, loss 0.0530991, acc 0.984375
2017-09-08T17:41:27.259270: step 3232, loss 0.0132355, acc 1
2017-09-08T17:41:27.888703: step 3233, loss 0.0638935, acc 0.984375
2017-09-08T17:41:28.496779: step 3234, loss 0.00767682, acc 1
2017-09-08T17:41:29.209885: step 3235, loss 0.05816, acc 0.96875
2017-09-08T17:41:29.966592: step 3236, loss 0.0394225, acc 0.984375
2017-09-08T17:41:30.699611: step 3237, loss 0.0812055, acc 0.96875
2017-09-08T17:41:31.365922: step 3238, loss 0.00216751, acc 1
2017-09-08T17:41:32.120914: step 3239, loss 0.00497606, acc 1
2017-09-08T17:41:32.777571: step 3240, loss 0.0531886, acc 0.984375
2017-09-08T17:41:33.430217: step 3241, loss 0.00487038, acc 1
2017-09-08T17:41:34.142340: step 3242, loss 0.00464041, acc 1
2017-09-08T17:41:34.884467: step 3243, loss 0.0194648, acc 0.984375
2017-09-08T17:41:35.618054: step 3244, loss 0.0807265, acc 0.96875
2017-09-08T17:41:36.296605: step 3245, loss 0.0947515, acc 0.96875
2017-09-08T17:41:37.003650: step 3246, loss 0.00725629, acc 1
2017-09-08T17:41:37.755138: step 3247, loss 0.00250382, acc 1
2017-09-08T17:41:38.444524: step 3248, loss 0.0015993, acc 1
2017-09-08T17:41:39.169771: step 3249, loss 0.0168596, acc 0.984375
2017-09-08T17:41:39.890030: step 3250, loss 0.0602123, acc 0.984375

Evaluation:
2017-09-08T17:41:40.685804: step 3250, loss 0.200755, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3250

2017-09-08T17:41:44.025307: step 3251, loss 0.0651546, acc 0.984375
2017-09-08T17:41:44.759694: step 3252, loss 0.0619859, acc 0.96875
2017-09-08T17:41:45.498910: step 3253, loss 0.00230835, acc 1
2017-09-08T17:41:46.246935: step 3254, loss 0.00178576, acc 1
2017-09-08T17:41:46.837487: step 3255, loss 0.0263999, acc 0.984375
2017-09-08T17:41:47.490199: step 3256, loss 0.0204938, acc 0.984375
2017-09-08T17:41:48.100976: step 3257, loss 0.0349491, acc 0.984375
2017-09-08T17:41:48.702480: step 3258, loss 0.00969071, acc 1
2017-09-08T17:41:49.383673: step 3259, loss 0.041882, acc 0.96875
2017-09-08T17:41:50.059915: step 3260, loss 0.030407, acc 0.984375
2017-09-08T17:41:50.728375: step 3261, loss 0.0037756, acc 1
2017-09-08T17:41:51.456675: step 3262, loss 0.0328623, acc 0.984375
2017-09-08T17:41:52.185529: step 3263, loss 0.018637, acc 0.984375
2017-09-08T17:41:52.954224: step 3264, loss 0.123744, acc 0.984375
2017-09-08T17:41:53.605829: step 3265, loss 0.0848799, acc 0.953125
2017-09-08T17:41:54.310264: step 3266, loss 0.00664135, acc 1
2017-09-08T17:41:54.994491: step 3267, loss 0.00324256, acc 1
2017-09-08T17:41:55.651757: step 3268, loss 0.0156663, acc 1
2017-09-08T17:41:56.319929: step 3269, loss 0.0476615, acc 0.984375
2017-09-08T17:41:57.074037: step 3270, loss 0.00209186, acc 1
2017-09-08T17:41:57.777641: step 3271, loss 0.00790995, acc 1
2017-09-08T17:41:58.503210: step 3272, loss 0.0599622, acc 0.96875
2017-09-08T17:41:59.289072: step 3273, loss 0.00125812, acc 1
2017-09-08T17:42:00.019358: step 3274, loss 0.0354414, acc 0.984375
2017-09-08T17:42:00.703189: step 3275, loss 0.00228225, acc 1
2017-09-08T17:42:01.385409: step 3276, loss 0.00115039, acc 1
2017-09-08T17:42:02.128002: step 3277, loss 0.00328787, acc 1
2017-09-08T17:42:02.872711: step 3278, loss 0.0224437, acc 0.984375
2017-09-08T17:42:03.639996: step 3279, loss 0.00105604, acc 1
2017-09-08T17:42:04.343604: step 3280, loss 0.00338347, acc 1
2017-09-08T17:42:05.002809: step 3281, loss 0.137803, acc 0.984375
2017-09-08T17:42:05.742618: step 3282, loss 0.042917, acc 0.96875
2017-09-08T17:42:06.472180: step 3283, loss 0.0349765, acc 0.984375
2017-09-08T17:42:07.209586: step 3284, loss 0.0486051, acc 0.984375
2017-09-08T17:42:07.969854: step 3285, loss 0.0844669, acc 0.984375
2017-09-08T17:42:08.642399: step 3286, loss 0.0532629, acc 0.984375
2017-09-08T17:42:09.557049: step 3287, loss 0.00558716, acc 1
2017-09-08T17:42:10.162785: step 3288, loss 0.00182471, acc 1
2017-09-08T17:42:10.779559: step 3289, loss 0.0226373, acc 0.984375
2017-09-08T17:42:11.452990: step 3290, loss 0.00431652, acc 1
2017-09-08T17:42:12.085889: step 3291, loss 0.0191608, acc 0.984375
2017-09-08T17:42:12.751801: step 3292, loss 0.00339508, acc 1
2017-09-08T17:42:13.423051: step 3293, loss 0.0769482, acc 0.984375
2017-09-08T17:42:14.097274: step 3294, loss 0.0352676, acc 0.984375
2017-09-08T17:42:14.831909: step 3295, loss 0.0292567, acc 0.984375
2017-09-08T17:42:15.529110: step 3296, loss 0.00241128, acc 1
2017-09-08T17:42:16.227605: step 3297, loss 0.00532147, acc 1
2017-09-08T17:42:16.974244: step 3298, loss 0.00262635, acc 1
2017-09-08T17:42:17.697997: step 3299, loss 0.00256683, acc 1
2017-09-08T17:42:18.418788: step 3300, loss 0.00509956, acc 1

Evaluation:
2017-09-08T17:42:19.177467: step 3300, loss 0.208097, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3300

2017-09-08T17:42:23.053424: step 3301, loss 0.0237813, acc 0.984375
2017-09-08T17:42:23.752308: step 3302, loss 0.061255, acc 0.96875
2017-09-08T17:42:24.400915: step 3303, loss 0.0555532, acc 0.984375
2017-09-08T17:42:25.060506: step 3304, loss 0.0266193, acc 0.984375
2017-09-08T17:42:25.682515: step 3305, loss 0.00992325, acc 1
2017-09-08T17:42:26.301675: step 3306, loss 0.0324247, acc 0.984375
2017-09-08T17:42:26.935974: step 3307, loss 0.0027203, acc 1
2017-09-08T17:42:27.598624: step 3308, loss 0.030801, acc 0.96875
2017-09-08T17:42:28.336364: step 3309, loss 0.00234693, acc 1
2017-09-08T17:42:29.014807: step 3310, loss 0.0084194, acc 1
2017-09-08T17:42:29.757499: step 3311, loss 0.0242679, acc 0.984375
2017-09-08T17:42:30.498394: step 3312, loss 0.0894077, acc 0.984375
2017-09-08T17:42:31.205739: step 3313, loss 0.00210155, acc 1
2017-09-08T17:42:31.890706: step 3314, loss 0.00270006, acc 1
2017-09-08T17:42:32.499592: step 3315, loss 0.0438895, acc 0.984375
2017-09-08T17:42:33.135161: step 3316, loss 0.0355789, acc 0.984375
2017-09-08T17:42:33.760347: step 3317, loss 0.00139641, acc 1
2017-09-08T17:42:34.455599: step 3318, loss 0.0192811, acc 0.984375
2017-09-08T17:42:35.156762: step 3319, loss 0.0254072, acc 0.984375
2017-09-08T17:42:35.834286: step 3320, loss 0.00292081, acc 1
2017-09-08T17:42:36.564073: step 3321, loss 0.0491988, acc 0.984375
2017-09-08T17:42:37.238015: step 3322, loss 0.0370424, acc 0.984375
2017-09-08T17:42:38.046071: step 3323, loss 0.163595, acc 0.953125
2017-09-08T17:42:38.848808: step 3324, loss 0.0932564, acc 0.96875
2017-09-08T17:42:39.767102: step 3325, loss 0.104116, acc 0.984375
2017-09-08T17:42:40.513804: step 3326, loss 0.00503964, acc 1
2017-09-08T17:42:41.198931: step 3327, loss 0.151737, acc 0.953125
2017-09-08T17:42:41.968952: step 3328, loss 0.0591206, acc 0.96875
2017-09-08T17:42:42.712851: step 3329, loss 0.0145277, acc 1
2017-09-08T17:42:43.439615: step 3330, loss 0.0232041, acc 0.984375
2017-09-08T17:42:44.151319: step 3331, loss 0.0492954, acc 0.984375
2017-09-08T17:42:44.805735: step 3332, loss 0.00239156, acc 1
2017-09-08T17:42:45.544182: step 3333, loss 0.00597874, acc 1
2017-09-08T17:42:46.211864: step 3334, loss 0.00375289, acc 1
2017-09-08T17:42:46.862653: step 3335, loss 0.00442855, acc 1
2017-09-08T17:42:47.530603: step 3336, loss 0.00177005, acc 1
2017-09-08T17:42:48.211329: step 3337, loss 0.00306911, acc 1
2017-09-08T17:42:48.950468: step 3338, loss 0.0181217, acc 1
2017-09-08T17:42:49.654837: step 3339, loss 0.00301661, acc 1
2017-09-08T17:42:50.407826: step 3340, loss 0.00781495, acc 1
2017-09-08T17:42:51.134957: step 3341, loss 0.00769472, acc 1
2017-09-08T17:42:51.887179: step 3342, loss 0.0018149, acc 1
2017-09-08T17:42:52.517654: step 3343, loss 0.00422461, acc 1
2017-09-08T17:42:53.229181: step 3344, loss 0.034071, acc 0.96875
2017-09-08T17:42:53.962246: step 3345, loss 0.0224501, acc 0.984375
2017-09-08T17:42:54.694544: step 3346, loss 0.00134058, acc 1
2017-09-08T17:42:55.353549: step 3347, loss 0.00223129, acc 1
2017-09-08T17:42:56.037659: step 3348, loss 0.0633959, acc 0.984375
2017-09-08T17:42:56.785298: step 3349, loss 0.0359627, acc 0.984375
2017-09-08T17:42:57.503599: step 3350, loss 0.00924015, acc 1

Evaluation:
2017-09-08T17:42:58.193191: step 3350, loss 0.199327, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3350

2017-09-08T17:43:00.882629: step 3351, loss 0.0189287, acc 0.984375
2017-09-08T17:43:01.660417: step 3352, loss 0.00223302, acc 1
2017-09-08T17:43:02.352171: step 3353, loss 0.0549108, acc 0.984375
2017-09-08T17:43:03.167259: step 3354, loss 0.05521, acc 0.984375
2017-09-08T17:43:03.829521: step 3355, loss 0.0131365, acc 1
2017-09-08T17:43:04.486031: step 3356, loss 0.00248171, acc 1
2017-09-08T17:43:04.936208: step 3357, loss 0.0140105, acc 0.984375
2017-09-08T17:43:05.542235: step 3358, loss 0.00826291, acc 1
2017-09-08T17:43:06.284784: step 3359, loss 0.0337199, acc 0.984375
2017-09-08T17:43:07.026658: step 3360, loss 0.0380408, acc 0.984375
2017-09-08T17:43:07.733525: step 3361, loss 0.00765793, acc 1
2017-09-08T17:43:08.400539: step 3362, loss 0.026182, acc 0.984375
2017-09-08T17:43:09.107300: step 3363, loss 0.00721179, acc 1
2017-09-08T17:43:10.061152: step 3364, loss 0.0140979, acc 1
2017-09-08T17:43:10.735518: step 3365, loss 0.0450539, acc 0.984375
2017-09-08T17:43:11.379180: step 3366, loss 0.00404041, acc 1
2017-09-08T17:43:12.114298: step 3367, loss 0.00885645, acc 1
2017-09-08T17:43:12.856488: step 3368, loss 0.12708, acc 0.96875
2017-09-08T17:43:13.606557: step 3369, loss 0.0192909, acc 0.984375
2017-09-08T17:43:14.333204: step 3370, loss 0.0302038, acc 0.984375
2017-09-08T17:43:15.040028: step 3371, loss 0.0283253, acc 0.984375
2017-09-08T17:43:15.734036: step 3372, loss 0.0182305, acc 0.984375
2017-09-08T17:43:16.453432: step 3373, loss 0.0527044, acc 0.96875
2017-09-08T17:43:17.206608: step 3374, loss 0.00268447, acc 1
2017-09-08T17:43:17.896353: step 3375, loss 0.103481, acc 0.96875
2017-09-08T17:43:18.584512: step 3376, loss 0.0752575, acc 0.984375
2017-09-08T17:43:19.279110: step 3377, loss 0.0415075, acc 0.984375
2017-09-08T17:43:19.925213: step 3378, loss 0.00428031, acc 1
2017-09-08T17:43:20.604101: step 3379, loss 0.00445825, acc 1
2017-09-08T17:43:21.319877: step 3380, loss 0.00129599, acc 1
2017-09-08T17:43:22.051475: step 3381, loss 0.0028062, acc 1
2017-09-08T17:43:22.811063: step 3382, loss 0.00126892, acc 1
2017-09-08T17:43:23.541404: step 3383, loss 0.0109418, acc 1
2017-09-08T17:43:24.139319: step 3384, loss 0.00542834, acc 1
2017-09-08T17:43:24.808799: step 3385, loss 0.0625494, acc 0.984375
2017-09-08T17:43:25.491443: step 3386, loss 0.0458835, acc 0.984375
2017-09-08T17:43:26.250187: step 3387, loss 0.00442165, acc 1
2017-09-08T17:43:26.883948: step 3388, loss 0.0533954, acc 0.984375
2017-09-08T17:43:27.639717: step 3389, loss 0.0028936, acc 1
2017-09-08T17:43:28.381820: step 3390, loss 0.0159503, acc 0.984375
2017-09-08T17:43:29.100948: step 3391, loss 0.0391983, acc 0.96875
2017-09-08T17:43:29.859800: step 3392, loss 0.00111097, acc 1
2017-09-08T17:43:30.608444: step 3393, loss 0.0223264, acc 0.984375
2017-09-08T17:43:31.360578: step 3394, loss 0.00165004, acc 1
2017-09-08T17:43:32.074841: step 3395, loss 0.0358791, acc 0.984375
2017-09-08T17:43:32.856236: step 3396, loss 0.0257477, acc 0.984375
2017-09-08T17:43:33.561977: step 3397, loss 0.00157959, acc 1
2017-09-08T17:43:34.232815: step 3398, loss 0.0329423, acc 0.984375
2017-09-08T17:43:34.964830: step 3399, loss 0.00221777, acc 1
2017-09-08T17:43:35.708128: step 3400, loss 0.00210024, acc 1

Evaluation:
2017-09-08T17:43:36.520110: step 3400, loss 0.212742, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3400

2017-09-08T17:43:40.311972: step 3401, loss 0.0425279, acc 0.984375
2017-09-08T17:43:40.921982: step 3402, loss 0.123801, acc 0.96875
2017-09-08T17:43:41.433094: step 3403, loss 0.0553532, acc 0.96875
2017-09-08T17:43:41.912670: step 3404, loss 0.00681337, acc 1
2017-09-08T17:43:42.542328: step 3405, loss 0.108265, acc 0.96875
2017-09-08T17:43:43.137230: step 3406, loss 0.0114125, acc 1
2017-09-08T17:43:43.774354: step 3407, loss 0.00134244, acc 1
2017-09-08T17:43:44.562847: step 3408, loss 0.00644017, acc 1
2017-09-08T17:43:45.250864: step 3409, loss 0.0612814, acc 0.984375
2017-09-08T17:43:46.090462: step 3410, loss 0.0391126, acc 0.984375
2017-09-08T17:43:46.844730: step 3411, loss 0.00160154, acc 1
2017-09-08T17:43:47.548571: step 3412, loss 0.071421, acc 0.96875
2017-09-08T17:43:48.133611: step 3413, loss 0.0019491, acc 1
2017-09-08T17:43:48.714858: step 3414, loss 0.00536456, acc 1
2017-09-08T17:43:49.254049: step 3415, loss 0.00436043, acc 1
2017-09-08T17:43:49.778966: step 3416, loss 0.00639917, acc 1
2017-09-08T17:43:50.384148: step 3417, loss 0.0163194, acc 1
2017-09-08T17:43:51.063108: step 3418, loss 0.0394458, acc 0.984375
2017-09-08T17:43:51.726493: step 3419, loss 0.0174481, acc 1
2017-09-08T17:43:52.446711: step 3420, loss 0.00583139, acc 1
2017-09-08T17:43:53.126935: step 3421, loss 0.0498683, acc 0.984375
2017-09-08T17:43:53.827498: step 3422, loss 0.0233957, acc 0.984375
2017-09-08T17:43:54.497020: step 3423, loss 0.00273546, acc 1
2017-09-08T17:43:55.113045: step 3424, loss 0.00782398, acc 1
2017-09-08T17:43:55.661982: step 3425, loss 0.0402147, acc 0.984375
2017-09-08T17:43:56.279692: step 3426, loss 0.00819747, acc 1
2017-09-08T17:43:57.079274: step 3427, loss 0.0776282, acc 0.96875
2017-09-08T17:43:57.832018: step 3428, loss 0.0483185, acc 0.984375
2017-09-08T17:43:58.595670: step 3429, loss 0.0214294, acc 0.984375
2017-09-08T17:43:59.224998: step 3430, loss 0.0590693, acc 0.980392
2017-09-08T17:43:59.914203: step 3431, loss 0.0344895, acc 0.984375
2017-09-08T17:44:00.617236: step 3432, loss 0.0172282, acc 0.984375
2017-09-08T17:44:01.347331: step 3433, loss 0.0488031, acc 0.96875
2017-09-08T17:44:01.960387: step 3434, loss 0.000939975, acc 1
2017-09-08T17:44:02.562962: step 3435, loss 0.0151428, acc 0.984375
2017-09-08T17:44:03.119221: step 3436, loss 0.00293443, acc 1
2017-09-08T17:44:03.805595: step 3437, loss 0.00219926, acc 1
2017-09-08T17:44:04.468408: step 3438, loss 0.00148577, acc 1
2017-09-08T17:44:05.163577: step 3439, loss 0.0421308, acc 0.984375
2017-09-08T17:44:05.852447: step 3440, loss 0.0127106, acc 1
2017-09-08T17:44:06.539626: step 3441, loss 0.0079136, acc 1
2017-09-08T17:44:07.208911: step 3442, loss 0.00234967, acc 1
2017-09-08T17:44:07.922780: step 3443, loss 0.00909643, acc 1
2017-09-08T17:44:08.654794: step 3444, loss 0.00450907, acc 1
2017-09-08T17:44:09.319221: step 3445, loss 0.0028109, acc 1
2017-09-08T17:44:10.089784: step 3446, loss 0.00367906, acc 1
2017-09-08T17:44:10.873797: step 3447, loss 0.0168927, acc 0.984375
2017-09-08T17:44:11.650867: step 3448, loss 0.0168306, acc 1
2017-09-08T17:44:12.334448: step 3449, loss 0.00226846, acc 1
2017-09-08T17:44:13.102802: step 3450, loss 0.0627997, acc 0.984375

Evaluation:
2017-09-08T17:44:13.837800: step 3450, loss 0.203364, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3450

2017-09-08T17:44:16.780737: step 3451, loss 0.0153752, acc 1
2017-09-08T17:44:17.482621: step 3452, loss 0.0156597, acc 0.984375
2017-09-08T17:44:18.247098: step 3453, loss 0.00235007, acc 1
2017-09-08T17:44:19.011540: step 3454, loss 0.0175052, acc 1
2017-09-08T17:44:19.724856: step 3455, loss 0.014289, acc 1
2017-09-08T17:44:20.395140: step 3456, loss 0.00127623, acc 1
2017-09-08T17:44:21.081068: step 3457, loss 0.00120733, acc 1
2017-09-08T17:44:21.737467: step 3458, loss 0.000769184, acc 1
2017-09-08T17:44:22.343544: step 3459, loss 0.0016899, acc 1
2017-09-08T17:44:23.063815: step 3460, loss 0.0234882, acc 0.984375
2017-09-08T17:44:23.760812: step 3461, loss 0.00149643, acc 1
2017-09-08T17:44:24.423355: step 3462, loss 0.0148961, acc 1
2017-09-08T17:44:25.109902: step 3463, loss 0.0150882, acc 1
2017-09-08T17:44:25.868575: step 3464, loss 0.00410428, acc 1
2017-09-08T17:44:26.526320: step 3465, loss 0.0205959, acc 0.984375
2017-09-08T17:44:27.048693: step 3466, loss 0.0146309, acc 0.984375
2017-09-08T17:44:27.549262: step 3467, loss 0.00535902, acc 1
2017-09-08T17:44:28.114923: step 3468, loss 0.0183642, acc 0.984375
2017-09-08T17:44:28.758333: step 3469, loss 0.0860998, acc 0.953125
2017-09-08T17:44:29.495730: step 3470, loss 0.00299222, acc 1
2017-09-08T17:44:30.198970: step 3471, loss 0.0105655, acc 1
2017-09-08T17:44:30.879996: step 3472, loss 0.005826, acc 1
2017-09-08T17:44:31.564698: step 3473, loss 0.0160488, acc 1
2017-09-08T17:44:32.293867: step 3474, loss 0.00752671, acc 1
2017-09-08T17:44:32.984131: step 3475, loss 0.0554351, acc 0.984375
2017-09-08T17:44:33.689463: step 3476, loss 0.043236, acc 0.984375
2017-09-08T17:44:34.360519: step 3477, loss 0.00312147, acc 1
2017-09-08T17:44:35.049757: step 3478, loss 0.00845838, acc 1
2017-09-08T17:44:35.670718: step 3479, loss 0.0283245, acc 0.984375
2017-09-08T17:44:36.265923: step 3480, loss 0.00540857, acc 1
2017-09-08T17:44:36.915600: step 3481, loss 0.0036126, acc 1
2017-09-08T17:44:37.535402: step 3482, loss 0.0527091, acc 0.984375
2017-09-08T17:44:38.240944: step 3483, loss 0.00399823, acc 1
2017-09-08T17:44:39.103499: step 3484, loss 0.0338515, acc 0.984375
2017-09-08T17:44:39.760114: step 3485, loss 0.0011703, acc 1
2017-09-08T17:44:40.415962: step 3486, loss 0.00436125, acc 1
2017-09-08T17:44:41.021230: step 3487, loss 0.0821283, acc 0.96875
2017-09-08T17:44:41.701371: step 3488, loss 0.0294999, acc 0.984375
2017-09-08T17:44:42.421122: step 3489, loss 0.100328, acc 0.96875
2017-09-08T17:44:43.121262: step 3490, loss 0.137474, acc 0.96875
2017-09-08T17:44:43.870750: step 3491, loss 0.00202709, acc 1
2017-09-08T17:44:44.615754: step 3492, loss 0.0194889, acc 1
2017-09-08T17:44:45.280045: step 3493, loss 0.00420746, acc 1
2017-09-08T17:44:45.946956: step 3494, loss 0.00516686, acc 1
2017-09-08T17:44:46.615018: step 3495, loss 0.00203326, acc 1
2017-09-08T17:44:47.333462: step 3496, loss 0.00754352, acc 1
2017-09-08T17:44:47.920433: step 3497, loss 0.0755803, acc 0.984375
2017-09-08T17:44:48.501449: step 3498, loss 0.00307902, acc 1
2017-09-08T17:44:49.070088: step 3499, loss 0.0747365, acc 0.984375
2017-09-08T17:44:49.825470: step 3500, loss 0.0840164, acc 0.984375

Evaluation:
2017-09-08T17:44:50.483578: step 3500, loss 0.210967, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3500

2017-09-08T17:44:53.653946: step 3501, loss 0.0187026, acc 0.984375
2017-09-08T17:44:54.333932: step 3502, loss 0.0417906, acc 0.984375
2017-09-08T17:44:55.040468: step 3503, loss 0.0147143, acc 1
2017-09-08T17:44:55.613805: step 3504, loss 0.00826972, acc 1
2017-09-08T17:44:56.199748: step 3505, loss 0.0048808, acc 1
2017-09-08T17:44:57.145796: step 3506, loss 0.00617929, acc 1
2017-09-08T17:44:57.747280: step 3507, loss 0.00880264, acc 1
2017-09-08T17:44:58.341188: step 3508, loss 0.00234381, acc 1
2017-09-08T17:44:58.959150: step 3509, loss 0.104506, acc 0.984375
2017-09-08T17:44:59.759431: step 3510, loss 0.026365, acc 0.984375
2017-09-08T17:45:00.482733: step 3511, loss 0.00337576, acc 1
2017-09-08T17:45:01.222098: step 3512, loss 0.00102198, acc 1
2017-09-08T17:45:01.907239: step 3513, loss 0.0504903, acc 0.96875
2017-09-08T17:45:02.577869: step 3514, loss 0.0044521, acc 1
2017-09-08T17:45:03.332042: step 3515, loss 0.0221992, acc 0.984375
2017-09-08T17:45:04.033930: step 3516, loss 0.00398353, acc 1
2017-09-08T17:45:04.753529: step 3517, loss 0.00349813, acc 1
2017-09-08T17:45:05.505174: step 3518, loss 0.0164995, acc 0.984375
2017-09-08T17:45:06.164982: step 3519, loss 0.0256795, acc 0.984375
2017-09-08T17:45:06.839845: step 3520, loss 0.0127369, acc 1
2017-09-08T17:45:07.500973: step 3521, loss 0.0496298, acc 0.984375
2017-09-08T17:45:08.110936: step 3522, loss 0.00280423, acc 1
2017-09-08T17:45:08.721980: step 3523, loss 0.0770516, acc 0.984375
2017-09-08T17:45:09.429339: step 3524, loss 0.0130916, acc 1
2017-09-08T17:45:10.134601: step 3525, loss 0.0875783, acc 0.984375
2017-09-08T17:45:10.805767: step 3526, loss 0.00428153, acc 1
2017-09-08T17:45:11.451298: step 3527, loss 0.0267928, acc 0.984375
2017-09-08T17:45:12.064063: step 3528, loss 0.00250661, acc 1
2017-09-08T17:45:12.740070: step 3529, loss 0.0209009, acc 0.984375
2017-09-08T17:45:13.515660: step 3530, loss 0.0545686, acc 0.984375
2017-09-08T17:45:14.239374: step 3531, loss 0.0144419, acc 1
2017-09-08T17:45:14.928200: step 3532, loss 0.00228307, acc 1
2017-09-08T17:45:15.576912: step 3533, loss 0.00475971, acc 1
2017-09-08T17:45:16.289745: step 3534, loss 0.00802809, acc 1
2017-09-08T17:45:16.876744: step 3535, loss 0.0625236, acc 0.984375
2017-09-08T17:45:17.429196: step 3536, loss 0.00211238, acc 1
2017-09-08T17:45:17.977017: step 3537, loss 0.004994, acc 1
2017-09-08T17:45:18.649820: step 3538, loss 0.0292686, acc 0.984375
2017-09-08T17:45:19.278941: step 3539, loss 0.00579141, acc 1
2017-09-08T17:45:19.850355: step 3540, loss 0.00440313, acc 1
2017-09-08T17:45:20.608254: step 3541, loss 0.000693061, acc 1
2017-09-08T17:45:21.345536: step 3542, loss 0.0025752, acc 1
2017-09-08T17:45:22.088187: step 3543, loss 0.010073, acc 1
2017-09-08T17:45:22.814971: step 3544, loss 0.0110716, acc 1
2017-09-08T17:45:23.548536: step 3545, loss 0.026097, acc 0.984375
2017-09-08T17:45:24.295461: step 3546, loss 0.00286322, acc 1
2017-09-08T17:45:25.019444: step 3547, loss 0.0074367, acc 1
2017-09-08T17:45:25.733985: step 3548, loss 0.0251292, acc 0.984375
2017-09-08T17:45:26.474911: step 3549, loss 0.135324, acc 0.953125
2017-09-08T17:45:27.205427: step 3550, loss 0.0093032, acc 1

Evaluation:
2017-09-08T17:45:27.983542: step 3550, loss 0.206403, acc 0.942446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3550

2017-09-08T17:45:31.674844: step 3551, loss 0.00390661, acc 1
2017-09-08T17:45:32.331604: step 3552, loss 0.0735402, acc 0.984375
2017-09-08T17:45:33.010557: step 3553, loss 0.00216957, acc 1
2017-09-08T17:45:33.733505: step 3554, loss 0.00252599, acc 1
2017-09-08T17:45:34.329115: step 3555, loss 0.0435042, acc 0.984375
2017-09-08T17:45:34.951531: step 3556, loss 0.0442799, acc 0.96875
2017-09-08T17:45:35.615563: step 3557, loss 0.0116485, acc 1
2017-09-08T17:45:36.316330: step 3558, loss 0.00094022, acc 1
2017-09-08T17:45:36.914544: step 3559, loss 0.139528, acc 0.953125
2017-09-08T17:45:37.712067: step 3560, loss 0.0686889, acc 0.984375
2017-09-08T17:45:38.483863: step 3561, loss 0.0146906, acc 1
2017-09-08T17:45:39.155265: step 3562, loss 0.0651095, acc 0.96875
2017-09-08T17:45:39.808988: step 3563, loss 0.0018787, acc 1
2017-09-08T17:45:40.549904: step 3564, loss 0.00167346, acc 1
2017-09-08T17:45:41.278007: step 3565, loss 0.00435636, acc 1
2017-09-08T17:45:41.972646: step 3566, loss 0.0483423, acc 0.984375
2017-09-08T17:45:42.650164: step 3567, loss 0.0494291, acc 0.96875
2017-09-08T17:45:43.362673: step 3568, loss 0.00328066, acc 1
2017-09-08T17:45:44.112305: step 3569, loss 0.00166718, acc 1
2017-09-08T17:45:44.722127: step 3570, loss 0.00355703, acc 1
2017-09-08T17:45:45.386572: step 3571, loss 0.0230646, acc 0.984375
2017-09-08T17:45:45.992339: step 3572, loss 0.00473877, acc 1
2017-09-08T17:45:46.725618: step 3573, loss 0.0324381, acc 0.984375
2017-09-08T17:45:47.440510: step 3574, loss 0.0321419, acc 0.984375
2017-09-08T17:45:48.171815: step 3575, loss 0.00415828, acc 1
2017-09-08T17:45:48.900393: step 3576, loss 0.104963, acc 0.96875
2017-09-08T17:45:49.623366: step 3577, loss 0.0197342, acc 0.984375
2017-09-08T17:45:50.383093: step 3578, loss 0.0019206, acc 1
2017-09-08T17:45:51.098027: step 3579, loss 0.00165742, acc 1
2017-09-08T17:45:51.805390: step 3580, loss 0.0218339, acc 0.984375
2017-09-08T17:45:52.455709: step 3581, loss 0.040641, acc 0.984375
2017-09-08T17:45:53.073160: step 3582, loss 0.00406542, acc 1
2017-09-08T17:45:53.715649: step 3583, loss 0.000685431, acc 1
2017-09-08T17:45:54.384286: step 3584, loss 0.0313348, acc 0.984375
2017-09-08T17:45:55.157672: step 3585, loss 0.079417, acc 0.984375
2017-09-08T17:45:55.732070: step 3586, loss 0.0488255, acc 0.984375
2017-09-08T17:45:56.367700: step 3587, loss 0.0265187, acc 0.984375
2017-09-08T17:45:57.066857: step 3588, loss 0.0228688, acc 0.984375
2017-09-08T17:45:57.754771: step 3589, loss 0.0242326, acc 0.984375
2017-09-08T17:45:58.609116: step 3590, loss 0.0119403, acc 1
2017-09-08T17:45:59.380764: step 3591, loss 0.00118092, acc 1
2017-09-08T17:46:00.043870: step 3592, loss 0.0658441, acc 0.96875
2017-09-08T17:46:00.770749: step 3593, loss 0.00251923, acc 1
2017-09-08T17:46:01.466693: step 3594, loss 0.00157299, acc 1
2017-09-08T17:46:02.249650: step 3595, loss 0.0724519, acc 0.96875
2017-09-08T17:46:03.326920: step 3596, loss 0.0693619, acc 0.984375
2017-09-08T17:46:04.063741: step 3597, loss 0.0112378, acc 1
2017-09-08T17:46:04.742453: step 3598, loss 0.10958, acc 0.984375
2017-09-08T17:46:05.417758: step 3599, loss 0.00654151, acc 1
2017-09-08T17:46:06.158910: step 3600, loss 0.00112649, acc 1

Evaluation:
2017-09-08T17:46:06.843997: step 3600, loss 0.219268, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3600

2017-09-08T17:46:09.586732: step 3601, loss 0.000545971, acc 1
2017-09-08T17:46:10.341794: step 3602, loss 0.0862628, acc 0.984375
2017-09-08T17:46:11.112197: step 3603, loss 0.0718564, acc 0.984375
2017-09-08T17:46:11.809493: step 3604, loss 0.122847, acc 0.953125
2017-09-08T17:46:12.585965: step 3605, loss 0.0295932, acc 0.984375
2017-09-08T17:46:13.216952: step 3606, loss 0.0393578, acc 0.96875
2017-09-08T17:46:13.808630: step 3607, loss 0.10181, acc 0.96875
2017-09-08T17:46:14.402251: step 3608, loss 0.0627322, acc 0.984375
2017-09-08T17:46:15.060945: step 3609, loss 0.0291918, acc 0.984375
2017-09-08T17:46:15.750597: step 3610, loss 0.000589473, acc 1
2017-09-08T17:46:16.442087: step 3611, loss 0.012668, acc 1
2017-09-08T17:46:17.130987: step 3612, loss 0.00131355, acc 1
2017-09-08T17:46:17.859126: step 3613, loss 0.087862, acc 0.9375
2017-09-08T17:46:18.613344: step 3614, loss 0.000831224, acc 1
2017-09-08T17:46:19.330317: step 3615, loss 0.00951553, acc 1
2017-09-08T17:46:20.047699: step 3616, loss 0.00530552, acc 1
2017-09-08T17:46:20.790931: step 3617, loss 0.0488088, acc 0.96875
2017-09-08T17:46:21.592749: step 3618, loss 0.00162791, acc 1
2017-09-08T17:46:22.262271: step 3619, loss 0.0241162, acc 1
2017-09-08T17:46:22.913410: step 3620, loss 0.00218447, acc 1
2017-09-08T17:46:23.559830: step 3621, loss 0.0344272, acc 0.984375
2017-09-08T17:46:24.176364: step 3622, loss 0.0278058, acc 0.984375
2017-09-08T17:46:24.868552: step 3623, loss 0.102733, acc 0.984375
2017-09-08T17:46:25.621808: step 3624, loss 0.0021748, acc 1
2017-09-08T17:46:26.347176: step 3625, loss 0.0724395, acc 0.984375
2017-09-08T17:46:27.075061: step 3626, loss 0.0576028, acc 0.980392
2017-09-08T17:46:27.794375: step 3627, loss 0.0211563, acc 0.984375
2017-09-08T17:46:28.577221: step 3628, loss 0.00125548, acc 1
2017-09-08T17:46:29.316243: step 3629, loss 0.0408794, acc 0.984375
2017-09-08T17:46:30.028709: step 3630, loss 0.00816493, acc 1
2017-09-08T17:46:30.748308: step 3631, loss 0.00583665, acc 1
2017-09-08T17:46:31.514228: step 3632, loss 0.0652878, acc 0.96875
2017-09-08T17:46:32.239371: step 3633, loss 0.041656, acc 0.984375
2017-09-08T17:46:33.081471: step 3634, loss 0.00689595, acc 1
2017-09-08T17:46:33.759331: step 3635, loss 0.00342755, acc 1
2017-09-08T17:46:34.381908: step 3636, loss 0.00899104, acc 1
2017-09-08T17:46:35.013459: step 3637, loss 0.0299642, acc 0.984375
2017-09-08T17:46:35.638840: step 3638, loss 0.0171684, acc 0.984375
2017-09-08T17:46:36.275672: step 3639, loss 0.0708093, acc 0.96875
2017-09-08T17:46:36.945776: step 3640, loss 0.0161181, acc 0.984375
2017-09-08T17:46:37.593200: step 3641, loss 0.00314345, acc 1
2017-09-08T17:46:38.259268: step 3642, loss 0.00187875, acc 1
2017-09-08T17:46:38.959531: step 3643, loss 0.0265829, acc 0.984375
2017-09-08T17:46:39.632727: step 3644, loss 0.0234273, acc 0.984375
2017-09-08T17:46:40.302265: step 3645, loss 0.00268271, acc 1
2017-09-08T17:46:41.054642: step 3646, loss 0.0539312, acc 0.984375
2017-09-08T17:46:41.815173: step 3647, loss 0.0205795, acc 0.984375
2017-09-08T17:46:42.531566: step 3648, loss 0.0147725, acc 1
2017-09-08T17:46:43.214507: step 3649, loss 0.0356068, acc 0.96875
2017-09-08T17:46:44.122363: step 3650, loss 0.00166006, acc 1

Evaluation:
2017-09-08T17:46:44.876458: step 3650, loss 0.198734, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3650

2017-09-08T17:46:48.939104: step 3651, loss 0.00583827, acc 1
2017-09-08T17:46:49.605897: step 3652, loss 0.00512318, acc 1
2017-09-08T17:46:50.243689: step 3653, loss 0.0452946, acc 0.984375
2017-09-08T17:46:50.860511: step 3654, loss 0.035802, acc 0.984375
2017-09-08T17:46:51.518609: step 3655, loss 0.042272, acc 0.96875
2017-09-08T17:46:52.170098: step 3656, loss 0.00135181, acc 1
2017-09-08T17:46:52.947067: step 3657, loss 0.00289148, acc 1
2017-09-08T17:46:53.755401: step 3658, loss 0.00295679, acc 1
2017-09-08T17:46:54.538120: step 3659, loss 0.000642725, acc 1
2017-09-08T17:46:55.304460: step 3660, loss 0.00328552, acc 1
2017-09-08T17:46:56.040361: step 3661, loss 0.102877, acc 0.96875
2017-09-08T17:46:56.841641: step 3662, loss 0.00467321, acc 1
2017-09-08T17:46:57.502280: step 3663, loss 0.00282242, acc 1
2017-09-08T17:46:58.209067: step 3664, loss 0.0187106, acc 0.984375
2017-09-08T17:46:58.971756: step 3665, loss 0.00133803, acc 1
2017-09-08T17:46:59.582225: step 3666, loss 0.00565391, acc 1
2017-09-08T17:47:00.280732: step 3667, loss 0.12589, acc 0.96875
2017-09-08T17:47:00.904637: step 3668, loss 0.00723665, acc 1
2017-09-08T17:47:01.511348: step 3669, loss 0.0032119, acc 1
2017-09-08T17:47:02.154809: step 3670, loss 0.00167447, acc 1
2017-09-08T17:47:02.761948: step 3671, loss 0.00783912, acc 1
2017-09-08T17:47:03.510603: step 3672, loss 0.0292992, acc 0.984375
2017-09-08T17:47:04.288338: step 3673, loss 0.0143223, acc 0.984375
2017-09-08T17:47:05.010750: step 3674, loss 0.0577367, acc 0.96875
2017-09-08T17:47:05.750061: step 3675, loss 0.0111832, acc 1
2017-09-08T17:47:06.489964: step 3676, loss 0.0786533, acc 0.984375
2017-09-08T17:47:07.229303: step 3677, loss 0.0258416, acc 0.984375
2017-09-08T17:47:07.987190: step 3678, loss 0.106544, acc 0.984375
2017-09-08T17:47:08.716226: step 3679, loss 0.0416461, acc 0.96875
2017-09-08T17:47:09.455106: step 3680, loss 0.0115218, acc 1
2017-09-08T17:47:10.148374: step 3681, loss 0.00454729, acc 1
2017-09-08T17:47:10.933802: step 3682, loss 0.0198807, acc 0.984375
2017-09-08T17:47:11.586493: step 3683, loss 0.00361597, acc 1
2017-09-08T17:47:12.251010: step 3684, loss 0.00177942, acc 1
2017-09-08T17:47:12.872763: step 3685, loss 0.00339989, acc 1
2017-09-08T17:47:13.636253: step 3686, loss 0.0013945, acc 1
2017-09-08T17:47:14.310685: step 3687, loss 0.00380154, acc 1
2017-09-08T17:47:14.945849: step 3688, loss 0.0729048, acc 0.96875
2017-09-08T17:47:15.616178: step 3689, loss 0.05058, acc 0.96875
2017-09-08T17:47:16.295283: step 3690, loss 0.0168977, acc 0.984375
2017-09-08T17:47:17.016665: step 3691, loss 0.00372906, acc 1
2017-09-08T17:47:17.770223: step 3692, loss 0.00797892, acc 1
2017-09-08T17:47:18.479330: step 3693, loss 0.0566891, acc 0.96875
2017-09-08T17:47:19.193018: step 3694, loss 0.00497741, acc 1
2017-09-08T17:47:19.830564: step 3695, loss 0.00325152, acc 1
2017-09-08T17:47:20.335086: step 3696, loss 0.00194423, acc 1
2017-09-08T17:47:20.935064: step 3697, loss 0.0258482, acc 0.984375
2017-09-08T17:47:21.485081: step 3698, loss 0.0382826, acc 0.96875
2017-09-08T17:47:22.085795: step 3699, loss 0.10815, acc 0.984375
2017-09-08T17:47:22.733420: step 3700, loss 0.044293, acc 0.984375

Evaluation:
2017-09-08T17:47:23.492488: step 3700, loss 0.218158, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3700

2017-09-08T17:47:26.142515: step 3701, loss 0.00330282, acc 1
2017-09-08T17:47:26.842492: step 3702, loss 0.0700983, acc 0.984375
2017-09-08T17:47:27.589090: step 3703, loss 0.110362, acc 0.96875
2017-09-08T17:47:28.293220: step 3704, loss 0.0246049, acc 0.984375
2017-09-08T17:47:28.940612: step 3705, loss 0.085557, acc 0.96875
2017-09-08T17:47:29.584284: step 3706, loss 0.128653, acc 0.96875
2017-09-08T17:47:30.226041: step 3707, loss 0.00216296, acc 1
2017-09-08T17:47:30.933367: step 3708, loss 0.0837533, acc 0.984375
2017-09-08T17:47:31.628059: step 3709, loss 0.00715775, acc 1
2017-09-08T17:47:32.370366: step 3710, loss 0.0882548, acc 0.984375
2017-09-08T17:47:33.079099: step 3711, loss 0.00398144, acc 1
2017-09-08T17:47:33.772764: step 3712, loss 0.00255984, acc 1
2017-09-08T17:47:34.914646: step 3713, loss 0.0302231, acc 0.984375
2017-09-08T17:47:35.674196: step 3714, loss 0.0568116, acc 0.984375
2017-09-08T17:47:36.341933: step 3715, loss 0.00130082, acc 1
2017-09-08T17:47:37.034698: step 3716, loss 0.0144897, acc 1
2017-09-08T17:47:37.753210: step 3717, loss 0.00386251, acc 1
2017-09-08T17:47:38.466638: step 3718, loss 0.0483914, acc 0.984375
2017-09-08T17:47:39.251406: step 3719, loss 0.0488117, acc 0.984375
2017-09-08T17:47:39.920560: step 3720, loss 0.00103641, acc 1
2017-09-08T17:47:40.611922: step 3721, loss 0.0137345, acc 1
2017-09-08T17:47:41.254001: step 3722, loss 0.00210328, acc 1
2017-09-08T17:47:41.968347: step 3723, loss 0.00441115, acc 1
2017-09-08T17:47:42.618462: step 3724, loss 0.001571, acc 1
2017-09-08T17:47:43.275702: step 3725, loss 0.00806236, acc 1
2017-09-08T17:47:43.972997: step 3726, loss 0.000729659, acc 1
2017-09-08T17:47:44.633367: step 3727, loss 0.0470563, acc 0.984375
2017-09-08T17:47:45.327972: step 3728, loss 0.0102007, acc 1
2017-09-08T17:47:45.978211: step 3729, loss 0.0113408, acc 1
2017-09-08T17:47:46.690690: step 3730, loss 0.00213335, acc 1
2017-09-08T17:47:47.378183: step 3731, loss 0.0194105, acc 1
2017-09-08T17:47:48.123117: step 3732, loss 0.0454108, acc 0.984375
2017-09-08T17:47:48.777936: step 3733, loss 0.000975341, acc 1
2017-09-08T17:47:49.451667: step 3734, loss 0.0108462, acc 1
2017-09-08T17:47:50.129287: step 3735, loss 0.0640864, acc 0.984375
2017-09-08T17:47:50.785199: step 3736, loss 0.0023198, acc 1
2017-09-08T17:47:51.443262: step 3737, loss 0.00628161, acc 1
2017-09-08T17:47:52.089197: step 3738, loss 0.0402859, acc 0.984375
2017-09-08T17:47:52.780210: step 3739, loss 0.0296655, acc 0.984375
2017-09-08T17:47:53.471313: step 3740, loss 0.101597, acc 0.96875
2017-09-08T17:47:54.218183: step 3741, loss 0.0598109, acc 0.96875
2017-09-08T17:47:54.935393: step 3742, loss 0.00314615, acc 1
2017-09-08T17:47:55.656714: step 3743, loss 0.0236833, acc 0.984375
2017-09-08T17:47:56.435040: step 3744, loss 0.00435052, acc 1
2017-09-08T17:47:57.175343: step 3745, loss 0.00266558, acc 1
2017-09-08T17:47:57.943213: step 3746, loss 0.00490453, acc 1
2017-09-08T17:47:58.619195: step 3747, loss 0.00809402, acc 1
2017-09-08T17:47:59.336727: step 3748, loss 0.0330171, acc 0.984375
2017-09-08T17:48:00.051683: step 3749, loss 0.00714187, acc 1
2017-09-08T17:48:00.761590: step 3750, loss 0.0339955, acc 0.984375

Evaluation:
2017-09-08T17:48:01.437689: step 3750, loss 0.193321, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3750

2017-09-08T17:48:04.729210: step 3751, loss 0.0479729, acc 0.96875
2017-09-08T17:48:05.501406: step 3752, loss 0.0363772, acc 0.984375
2017-09-08T17:48:06.086968: step 3753, loss 0.00505843, acc 1
2017-09-08T17:48:06.692395: step 3754, loss 0.0541727, acc 0.984375
2017-09-08T17:48:07.241059: step 3755, loss 0.00153699, acc 1
2017-09-08T17:48:07.877582: step 3756, loss 0.00347275, acc 1
2017-09-08T17:48:08.511894: step 3757, loss 0.0659504, acc 0.96875
2017-09-08T17:48:09.219661: step 3758, loss 0.00184404, acc 1
2017-09-08T17:48:09.927460: step 3759, loss 0.117269, acc 0.96875
2017-09-08T17:48:10.606581: step 3760, loss 0.15322, acc 0.96875
2017-09-08T17:48:11.366985: step 3761, loss 0.0886724, acc 0.96875
2017-09-08T17:48:12.101639: step 3762, loss 0.00256426, acc 1
2017-09-08T17:48:12.808692: step 3763, loss 0.0121326, acc 1
2017-09-08T17:48:13.548454: step 3764, loss 0.0166083, acc 1
2017-09-08T17:48:14.219884: step 3765, loss 0.0133191, acc 1
2017-09-08T17:48:14.955931: step 3766, loss 0.0120819, acc 1
2017-09-08T17:48:15.635428: step 3767, loss 0.0209844, acc 0.984375
2017-09-08T17:48:16.371619: step 3768, loss 0.0280821, acc 0.984375
2017-09-08T17:48:17.083600: step 3769, loss 0.0159103, acc 1
2017-09-08T17:48:17.739771: step 3770, loss 0.165154, acc 0.953125
2017-09-08T17:48:18.322148: step 3771, loss 0.00523763, acc 1
2017-09-08T17:48:18.975643: step 3772, loss 0.0237397, acc 0.984375
2017-09-08T17:48:19.633485: step 3773, loss 0.0707448, acc 0.96875
2017-09-08T17:48:20.486998: step 3774, loss 0.0760805, acc 0.96875
2017-09-08T17:48:21.180840: step 3775, loss 0.00142105, acc 1
2017-09-08T17:48:21.850333: step 3776, loss 0.0028165, acc 1
2017-09-08T17:48:22.525578: step 3777, loss 0.00116128, acc 1
2017-09-08T17:48:23.239907: step 3778, loss 0.00198503, acc 1
2017-09-08T17:48:23.946116: step 3779, loss 0.00387045, acc 1
2017-09-08T17:48:24.645220: step 3780, loss 0.00226696, acc 1
2017-09-08T17:48:25.019580: step 3781, loss 0.00394683, acc 1
2017-09-08T17:48:25.764252: step 3782, loss 0.00120627, acc 1
2017-09-08T17:48:26.480299: step 3783, loss 0.0258509, acc 0.984375
2017-09-08T17:48:27.184482: step 3784, loss 0.0584445, acc 0.96875
2017-09-08T17:48:27.800775: step 3785, loss 0.0145202, acc 1
2017-09-08T17:48:28.430828: step 3786, loss 0.00808936, acc 1
2017-09-08T17:48:29.033539: step 3787, loss 0.00607826, acc 1
2017-09-08T17:48:29.687551: step 3788, loss 0.00728792, acc 1
2017-09-08T17:48:30.389528: step 3789, loss 0.00161479, acc 1
2017-09-08T17:48:31.052630: step 3790, loss 0.0128393, acc 1
2017-09-08T17:48:31.730518: step 3791, loss 0.0641644, acc 0.984375
2017-09-08T17:48:32.433045: step 3792, loss 0.0383137, acc 0.984375
2017-09-08T17:48:33.218236: step 3793, loss 0.0276224, acc 0.984375
2017-09-08T17:48:33.951359: step 3794, loss 0.063807, acc 0.984375
2017-09-08T17:48:34.684573: step 3795, loss 0.00510788, acc 1
2017-09-08T17:48:35.392224: step 3796, loss 0.00108048, acc 1
2017-09-08T17:48:36.134402: step 3797, loss 0.0460916, acc 0.984375
2017-09-08T17:48:36.811967: step 3798, loss 0.0186117, acc 0.984375
2017-09-08T17:48:37.547498: step 3799, loss 0.0160757, acc 1
2017-09-08T17:48:38.259148: step 3800, loss 0.00133458, acc 1

Evaluation:
2017-09-08T17:48:38.950484: step 3800, loss 0.200261, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3800

2017-09-08T17:48:42.661602: step 3801, loss 0.0255934, acc 0.984375
2017-09-08T17:48:43.281187: step 3802, loss 0.00328215, acc 1
2017-09-08T17:48:43.930868: step 3803, loss 0.0719802, acc 0.96875
2017-09-08T17:48:44.575682: step 3804, loss 0.00732535, acc 1
2017-09-08T17:48:45.297333: step 3805, loss 0.00202688, acc 1
2017-09-08T17:48:45.937022: step 3806, loss 0.00298113, acc 1
2017-09-08T17:48:46.643132: step 3807, loss 0.00447682, acc 1
2017-09-08T17:48:47.394783: step 3808, loss 0.00396313, acc 1
2017-09-08T17:48:48.166746: step 3809, loss 0.0400743, acc 0.96875
2017-09-08T17:48:48.967088: step 3810, loss 0.0471318, acc 0.984375
2017-09-08T17:48:49.584383: step 3811, loss 0.00211714, acc 1
2017-09-08T17:48:50.276574: step 3812, loss 0.0478753, acc 0.96875
2017-09-08T17:48:50.977874: step 3813, loss 0.00280058, acc 1
2017-09-08T17:48:51.675391: step 3814, loss 0.0238551, acc 0.984375
2017-09-08T17:48:52.347569: step 3815, loss 0.00338204, acc 1
2017-09-08T17:48:53.075728: step 3816, loss 0.0560679, acc 0.984375
2017-09-08T17:48:53.766769: step 3817, loss 0.00245499, acc 1
2017-09-08T17:48:54.487436: step 3818, loss 0.00242591, acc 1
2017-09-08T17:48:55.030095: step 3819, loss 0.0408071, acc 0.984375
2017-09-08T17:48:55.577997: step 3820, loss 0.000688457, acc 1
2017-09-08T17:48:56.079497: step 3821, loss 0.00206662, acc 1
2017-09-08T17:48:56.587382: step 3822, loss 0.00337318, acc 1
2017-09-08T17:48:57.321947: step 3823, loss 0.0011088, acc 1
2017-09-08T17:48:58.110123: step 3824, loss 0.000578753, acc 1
2017-09-08T17:48:58.799166: step 3825, loss 0.0168777, acc 0.984375
2017-09-08T17:48:59.451640: step 3826, loss 0.0134959, acc 1
2017-09-08T17:49:00.170731: step 3827, loss 0.0482645, acc 0.984375
2017-09-08T17:49:00.909601: step 3828, loss 0.00363225, acc 1
2017-09-08T17:49:01.671806: step 3829, loss 0.00670966, acc 1
2017-09-08T17:49:02.404176: step 3830, loss 0.00339828, acc 1
2017-09-08T17:49:03.087194: step 3831, loss 0.0559884, acc 0.984375
2017-09-08T17:49:03.801767: step 3832, loss 0.0168935, acc 0.984375
2017-09-08T17:49:04.503335: step 3833, loss 0.0583074, acc 0.96875
2017-09-08T17:49:05.200754: step 3834, loss 0.0518734, acc 0.984375
2017-09-08T17:49:05.815488: step 3835, loss 0.0528905, acc 0.984375
2017-09-08T17:49:06.459791: step 3836, loss 0.00285329, acc 1
2017-09-08T17:49:07.133379: step 3837, loss 0.0120781, acc 1
2017-09-08T17:49:07.926358: step 3838, loss 0.0438516, acc 0.984375
2017-09-08T17:49:08.683827: step 3839, loss 0.00372736, acc 1
2017-09-08T17:49:09.390895: step 3840, loss 0.00428093, acc 1
2017-09-08T17:49:10.111703: step 3841, loss 0.0387605, acc 0.984375
2017-09-08T17:49:10.799132: step 3842, loss 0.000808756, acc 1
2017-09-08T17:49:11.478305: step 3843, loss 0.0242988, acc 0.984375
2017-09-08T17:49:12.189258: step 3844, loss 0.0145577, acc 1
2017-09-08T17:49:12.867373: step 3845, loss 0.0124271, acc 0.984375
2017-09-08T17:49:13.558406: step 3846, loss 0.0447817, acc 0.984375
2017-09-08T17:49:14.272944: step 3847, loss 0.0305763, acc 0.984375
2017-09-08T17:49:14.950314: step 3848, loss 0.00585446, acc 1
2017-09-08T17:49:15.629884: step 3849, loss 0.0228241, acc 0.984375
2017-09-08T17:49:16.321609: step 3850, loss 0.00310688, acc 1

Evaluation:
2017-09-08T17:49:16.986863: step 3850, loss 0.195783, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3850

2017-09-08T17:49:19.429825: step 3851, loss 0.0449859, acc 0.96875
2017-09-08T17:49:20.086134: step 3852, loss 0.00212475, acc 1
2017-09-08T17:49:20.715817: step 3853, loss 0.00252261, acc 1
2017-09-08T17:49:21.271718: step 3854, loss 0.0193164, acc 1
2017-09-08T17:49:21.790198: step 3855, loss 0.0483627, acc 0.96875
2017-09-08T17:49:22.420740: step 3856, loss 0.00886696, acc 1
2017-09-08T17:49:23.163812: step 3857, loss 0.0515399, acc 0.984375
2017-09-08T17:49:23.929975: step 3858, loss 0.00404489, acc 1
2017-09-08T17:49:24.615458: step 3859, loss 0.0565768, acc 0.96875
2017-09-08T17:49:25.375546: step 3860, loss 0.0491597, acc 0.984375
2017-09-08T17:49:26.108675: step 3861, loss 0.00377807, acc 1
2017-09-08T17:49:26.768559: step 3862, loss 0.0123871, acc 1
2017-09-08T17:49:27.293127: step 3863, loss 0.00340731, acc 1
2017-09-08T17:49:27.916436: step 3864, loss 0.0227202, acc 0.984375
2017-09-08T17:49:28.599323: step 3865, loss 0.00318736, acc 1
2017-09-08T17:49:29.367305: step 3866, loss 0.000549826, acc 1
2017-09-08T17:49:30.015318: step 3867, loss 0.00238222, acc 1
2017-09-08T17:49:30.751158: step 3868, loss 0.00588042, acc 1
2017-09-08T17:49:31.463807: step 3869, loss 0.0195857, acc 0.984375
2017-09-08T17:49:32.075028: step 3870, loss 0.0225585, acc 0.984375
2017-09-08T17:49:32.748332: step 3871, loss 0.00350719, acc 1
2017-09-08T17:49:33.367292: step 3872, loss 0.00578977, acc 1
2017-09-08T17:49:34.046649: step 3873, loss 0.00341319, acc 1
2017-09-08T17:49:34.752642: step 3874, loss 0.00085688, acc 1
2017-09-08T17:49:35.455136: step 3875, loss 0.00151272, acc 1
2017-09-08T17:49:36.199754: step 3876, loss 0.036677, acc 0.984375
2017-09-08T17:49:36.858074: step 3877, loss 0.0652331, acc 0.984375
2017-09-08T17:49:37.602603: step 3878, loss 0.0155404, acc 0.984375
2017-09-08T17:49:38.306952: step 3879, loss 0.0189214, acc 0.984375
2017-09-08T17:49:39.004094: step 3880, loss 0.0170672, acc 0.984375
2017-09-08T17:49:39.786812: step 3881, loss 0.00823822, acc 1
2017-09-08T17:49:40.399496: step 3882, loss 0.0346002, acc 0.984375
2017-09-08T17:49:41.128641: step 3883, loss 0.00256576, acc 1
2017-09-08T17:49:41.892167: step 3884, loss 0.00137847, acc 1
2017-09-08T17:49:42.453361: step 3885, loss 0.00223929, acc 1
2017-09-08T17:49:43.127279: step 3886, loss 0.00234335, acc 1
2017-09-08T17:49:43.752268: step 3887, loss 0.115487, acc 0.953125
2017-09-08T17:49:44.366708: step 3888, loss 0.0132395, acc 1
2017-09-08T17:49:45.009926: step 3889, loss 0.00119851, acc 1
2017-09-08T17:49:45.705732: step 3890, loss 0.125749, acc 0.96875
2017-09-08T17:49:46.382692: step 3891, loss 0.114927, acc 0.984375
2017-09-08T17:49:47.037243: step 3892, loss 0.0130163, acc 1
2017-09-08T17:49:47.710781: step 3893, loss 0.0199068, acc 0.984375
2017-09-08T17:49:48.430501: step 3894, loss 0.0465651, acc 0.984375
2017-09-08T17:49:49.135278: step 3895, loss 0.00153363, acc 1
2017-09-08T17:49:49.851324: step 3896, loss 0.0155294, acc 1
2017-09-08T17:49:50.574545: step 3897, loss 0.105181, acc 0.984375
2017-09-08T17:49:51.264462: step 3898, loss 0.0712711, acc 0.96875
2017-09-08T17:49:51.993942: step 3899, loss 0.00938811, acc 1
2017-09-08T17:49:52.715845: step 3900, loss 0.0253019, acc 1

Evaluation:
2017-09-08T17:49:53.444755: step 3900, loss 0.20553, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3900

2017-09-08T17:49:57.428899: step 3901, loss 0.0131507, acc 0.984375
2017-09-08T17:49:58.119091: step 3902, loss 0.0116191, acc 1
2017-09-08T17:49:58.719135: step 3903, loss 0.130072, acc 0.96875
2017-09-08T17:49:59.365435: step 3904, loss 0.00293958, acc 1
2017-09-08T17:50:00.002783: step 3905, loss 0.13468, acc 0.96875
2017-09-08T17:50:00.631555: step 3906, loss 0.00951454, acc 1
2017-09-08T17:50:01.258559: step 3907, loss 0.0168941, acc 1
2017-09-08T17:50:01.907564: step 3908, loss 0.0432735, acc 0.96875
2017-09-08T17:50:02.596377: step 3909, loss 0.00217232, acc 1
2017-09-08T17:50:03.321932: step 3910, loss 0.0819547, acc 0.96875
2017-09-08T17:50:04.101955: step 3911, loss 0.00751915, acc 1
2017-09-08T17:50:04.805978: step 3912, loss 0.0272075, acc 0.984375
2017-09-08T17:50:05.660770: step 3913, loss 0.00631024, acc 1
2017-09-08T17:50:06.354379: step 3914, loss 0.0476951, acc 0.984375
2017-09-08T17:50:07.133242: step 3915, loss 0.0315363, acc 0.984375
2017-09-08T17:50:08.013246: step 3916, loss 0.00294445, acc 1
2017-09-08T17:50:08.635026: step 3917, loss 0.0136904, acc 1
2017-09-08T17:50:09.324892: step 3918, loss 0.0625922, acc 0.984375
2017-09-08T17:50:09.934232: step 3919, loss 0.00310695, acc 1
2017-09-08T17:50:10.524227: step 3920, loss 0.0024038, acc 1
2017-09-08T17:50:11.259066: step 3921, loss 0.040321, acc 0.984375
2017-09-08T17:50:11.982181: step 3922, loss 0.0499042, acc 0.96875
2017-09-08T17:50:12.594534: step 3923, loss 0.0563088, acc 0.984375
2017-09-08T17:50:13.399848: step 3924, loss 0.017709, acc 0.984375
2017-09-08T17:50:14.108874: step 3925, loss 0.00179819, acc 1
2017-09-08T17:50:14.826504: step 3926, loss 0.0752736, acc 0.984375
2017-09-08T17:50:15.538870: step 3927, loss 0.0483314, acc 0.96875
2017-09-08T17:50:16.231012: step 3928, loss 0.00154267, acc 1
2017-09-08T17:50:16.971978: step 3929, loss 0.0048278, acc 1
2017-09-08T17:50:17.659408: step 3930, loss 0.0835098, acc 0.96875
2017-09-08T17:50:18.526605: step 3931, loss 0.0610572, acc 0.96875
2017-09-08T17:50:19.211404: step 3932, loss 0.00485622, acc 1
2017-09-08T17:50:19.921225: step 3933, loss 0.00160662, acc 1
2017-09-08T17:50:20.651059: step 3934, loss 0.0945588, acc 0.984375
2017-09-08T17:50:21.286125: step 3935, loss 0.00461114, acc 1
2017-09-08T17:50:21.856084: step 3936, loss 0.00110031, acc 1
2017-09-08T17:50:22.487652: step 3937, loss 0.00395001, acc 1
2017-09-08T17:50:23.238301: step 3938, loss 0.0118678, acc 1
2017-09-08T17:50:23.963498: step 3939, loss 0.00246552, acc 1
2017-09-08T17:50:24.650906: step 3940, loss 0.00249477, acc 1
2017-09-08T17:50:25.329730: step 3941, loss 0.00173699, acc 1
2017-09-08T17:50:25.988571: step 3942, loss 0.00140046, acc 1
2017-09-08T17:50:26.683559: step 3943, loss 0.0456386, acc 0.984375
2017-09-08T17:50:27.375766: step 3944, loss 0.00164449, acc 1
2017-09-08T17:50:27.908883: step 3945, loss 0.00176056, acc 1
2017-09-08T17:50:28.877420: step 3946, loss 0.0177753, acc 1
2017-09-08T17:50:29.600129: step 3947, loss 0.0021983, acc 1
2017-09-08T17:50:30.366344: step 3948, loss 0.00432206, acc 1
2017-09-08T17:50:31.111283: step 3949, loss 0.00413163, acc 1
2017-09-08T17:50:31.800847: step 3950, loss 0.0889931, acc 0.953125

Evaluation:
2017-09-08T17:50:32.496102: step 3950, loss 0.218289, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-3950

2017-09-08T17:50:35.315123: step 3951, loss 0.0430958, acc 0.984375
2017-09-08T17:50:36.005991: step 3952, loss 0.0883598, acc 0.984375
2017-09-08T17:50:36.727506: step 3953, loss 0.00378319, acc 1
2017-09-08T17:50:37.491981: step 3954, loss 0.00723754, acc 1
2017-09-08T17:50:38.088333: step 3955, loss 0.015171, acc 0.984375
2017-09-08T17:50:38.700463: step 3956, loss 0.0193693, acc 0.984375
2017-09-08T17:50:39.362547: step 3957, loss 0.00190088, acc 1
2017-09-08T17:50:40.055860: step 3958, loss 0.00194168, acc 1
2017-09-08T17:50:40.791204: step 3959, loss 0.00893256, acc 1
2017-09-08T17:50:41.501563: step 3960, loss 0.0650202, acc 0.984375
2017-09-08T17:50:42.218525: step 3961, loss 0.0345565, acc 0.984375
2017-09-08T17:50:42.964950: step 3962, loss 0.0587771, acc 0.953125
2017-09-08T17:50:43.691236: step 3963, loss 0.00220079, acc 1
2017-09-08T17:50:44.402465: step 3964, loss 0.151429, acc 0.96875
2017-09-08T17:50:45.106376: step 3965, loss 0.0241322, acc 0.984375
2017-09-08T17:50:45.778367: step 3966, loss 0.0627029, acc 0.96875
2017-09-08T17:50:46.398066: step 3967, loss 0.0304027, acc 0.984375
2017-09-08T17:50:47.124123: step 3968, loss 0.0978152, acc 0.984375
2017-09-08T17:50:47.748724: step 3969, loss 0.0391383, acc 0.984375
2017-09-08T17:50:48.348217: step 3970, loss 0.0817259, acc 0.984375
2017-09-08T17:50:48.978246: step 3971, loss 0.00244528, acc 1
2017-09-08T17:50:49.652421: step 3972, loss 0.000874494, acc 1
2017-09-08T17:50:50.300794: step 3973, loss 0.002523, acc 1
2017-09-08T17:50:50.909881: step 3974, loss 0.0854143, acc 0.96875
2017-09-08T17:50:51.578293: step 3975, loss 0.00380469, acc 1
2017-09-08T17:50:52.166495: step 3976, loss 0.00354648, acc 1
2017-09-08T17:50:52.891657: step 3977, loss 0.00214184, acc 1
2017-09-08T17:50:53.541070: step 3978, loss 0.0141964, acc 1
2017-09-08T17:50:54.213792: step 3979, loss 0.00100206, acc 1
2017-09-08T17:50:54.950096: step 3980, loss 0.0529688, acc 0.96875
2017-09-08T17:50:55.645854: step 3981, loss 0.00177436, acc 1
2017-09-08T17:50:56.306950: step 3982, loss 0.00467165, acc 1
2017-09-08T17:50:57.121146: step 3983, loss 0.00278283, acc 1
2017-09-08T17:50:57.807071: step 3984, loss 0.0562609, acc 0.96875
2017-09-08T17:50:58.458730: step 3985, loss 0.0127917, acc 1
2017-09-08T17:50:59.092440: step 3986, loss 0.00980583, acc 1
2017-09-08T17:50:59.678047: step 3987, loss 0.00320975, acc 1
2017-09-08T17:51:00.243284: step 3988, loss 0.067738, acc 0.96875
2017-09-08T17:51:00.735256: step 3989, loss 0.0100997, acc 1
2017-09-08T17:51:01.313056: step 3990, loss 0.0105754, acc 1
2017-09-08T17:51:02.022738: step 3991, loss 0.00962067, acc 1
2017-09-08T17:51:02.717291: step 3992, loss 0.0012218, acc 1
2017-09-08T17:51:03.422005: step 3993, loss 0.0678333, acc 0.96875
2017-09-08T17:51:04.134226: step 3994, loss 0.00794155, acc 1
2017-09-08T17:51:04.845595: step 3995, loss 0.00423849, acc 1
2017-09-08T17:51:05.503204: step 3996, loss 0.00151419, acc 1
2017-09-08T17:51:06.191152: step 3997, loss 0.133887, acc 0.96875
2017-09-08T17:51:06.851966: step 3998, loss 0.0024728, acc 1
2017-09-08T17:51:07.533169: step 3999, loss 0.00161962, acc 1
2017-09-08T17:51:08.166188: step 4000, loss 0.0242527, acc 0.984375

Evaluation:
2017-09-08T17:51:08.844621: step 4000, loss 0.194538, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4000

2017-09-08T17:51:11.750930: step 4001, loss 0.00146832, acc 1
2017-09-08T17:51:12.450612: step 4002, loss 0.0112341, acc 1
2017-09-08T17:51:13.194335: step 4003, loss 0.0689332, acc 0.984375
2017-09-08T17:51:13.866229: step 4004, loss 0.0172726, acc 0.984375
2017-09-08T17:51:14.609526: step 4005, loss 0.0132081, acc 1
2017-09-08T17:51:15.317787: step 4006, loss 0.0587842, acc 0.984375
2017-09-08T17:51:15.987937: step 4007, loss 0.0538978, acc 0.984375
2017-09-08T17:51:16.692653: step 4008, loss 0.00759206, acc 1
2017-09-08T17:51:17.350724: step 4009, loss 0.00139075, acc 1
2017-09-08T17:51:17.984437: step 4010, loss 0.00113982, acc 1
2017-09-08T17:51:18.707503: step 4011, loss 0.168495, acc 0.9375
2017-09-08T17:51:19.389224: step 4012, loss 0.0148601, acc 0.984375
2017-09-08T17:51:20.102471: step 4013, loss 0.00415969, acc 1
2017-09-08T17:51:20.860748: step 4014, loss 0.034326, acc 0.984375
2017-09-08T17:51:21.611722: step 4015, loss 0.00121363, acc 1
2017-09-08T17:51:22.399981: step 4016, loss 0.0039692, acc 1
2017-09-08T17:51:23.134238: step 4017, loss 0.0837172, acc 0.984375
2017-09-08T17:51:23.775961: step 4018, loss 0.0165333, acc 1
2017-09-08T17:51:24.444516: step 4019, loss 0.00221179, acc 1
2017-09-08T17:51:25.097764: step 4020, loss 0.00174716, acc 1
2017-09-08T17:51:25.747555: step 4021, loss 0.00326443, acc 1
2017-09-08T17:51:26.360729: step 4022, loss 0.0528532, acc 0.984375
2017-09-08T17:51:27.080375: step 4023, loss 0.000941476, acc 1
2017-09-08T17:51:27.754045: step 4024, loss 0.00326712, acc 1
2017-09-08T17:51:28.382831: step 4025, loss 0.00117844, acc 1
2017-09-08T17:51:29.087957: step 4026, loss 0.00411001, acc 1
2017-09-08T17:51:29.802922: step 4027, loss 0.035657, acc 0.984375
2017-09-08T17:51:30.516562: step 4028, loss 0.00309397, acc 1
2017-09-08T17:51:31.168106: step 4029, loss 0.00306162, acc 1
2017-09-08T17:51:31.843878: step 4030, loss 0.0814474, acc 0.96875
2017-09-08T17:51:32.508998: step 4031, loss 0.0325603, acc 0.984375
2017-09-08T17:51:33.248813: step 4032, loss 0.0697829, acc 0.96875
2017-09-08T17:51:33.858407: step 4033, loss 0.0100726, acc 1
2017-09-08T17:51:34.584792: step 4034, loss 0.021001, acc 0.984375
2017-09-08T17:51:35.177723: step 4035, loss 0.020046, acc 0.984375
2017-09-08T17:51:35.837349: step 4036, loss 0.105309, acc 0.984375
2017-09-08T17:51:36.532770: step 4037, loss 0.00983515, acc 1
2017-09-08T17:51:37.339378: step 4038, loss 0.00193819, acc 1
2017-09-08T17:51:37.915301: step 4039, loss 0.0511049, acc 0.984375
2017-09-08T17:51:38.468152: step 4040, loss 0.0921147, acc 0.96875
2017-09-08T17:51:39.122593: step 4041, loss 0.00274305, acc 1
2017-09-08T17:51:39.843143: step 4042, loss 0.0137627, acc 1
2017-09-08T17:51:40.538412: step 4043, loss 0.0367352, acc 0.984375
2017-09-08T17:51:41.185347: step 4044, loss 0.00268039, acc 1
2017-09-08T17:51:41.917829: step 4045, loss 0.00899156, acc 1
2017-09-08T17:51:42.630126: step 4046, loss 0.0725759, acc 0.984375
2017-09-08T17:51:43.314633: step 4047, loss 0.00153344, acc 1
2017-09-08T17:51:44.133444: step 4048, loss 0.00054341, acc 1
2017-09-08T17:51:44.849695: step 4049, loss 0.0592334, acc 0.984375
2017-09-08T17:51:45.591180: step 4050, loss 0.00280682, acc 1

Evaluation:
2017-09-08T17:51:46.291921: step 4050, loss 0.220374, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4050

2017-09-08T17:51:50.079222: step 4051, loss 0.06845, acc 0.96875
2017-09-08T17:51:50.666156: step 4052, loss 0.00443764, acc 1
2017-09-08T17:51:51.285997: step 4053, loss 0.00151452, acc 1
2017-09-08T17:51:51.879911: step 4054, loss 0.00263055, acc 1
2017-09-08T17:51:52.536299: step 4055, loss 0.0498198, acc 0.984375
2017-09-08T17:51:53.288870: step 4056, loss 0.040926, acc 0.984375
2017-09-08T17:51:54.166940: step 4057, loss 0.0021458, acc 1
2017-09-08T17:51:54.774579: step 4058, loss 0.00108287, acc 1
2017-09-08T17:51:55.393012: step 4059, loss 0.0037211, acc 1
2017-09-08T17:51:55.990796: step 4060, loss 0.0410703, acc 0.984375
2017-09-08T17:51:56.720316: step 4061, loss 0.00145712, acc 1
2017-09-08T17:51:57.443018: step 4062, loss 0.0170734, acc 0.984375
2017-09-08T17:51:58.189332: step 4063, loss 0.00325823, acc 1
2017-09-08T17:51:58.883165: step 4064, loss 0.062912, acc 0.984375
2017-09-08T17:51:59.592718: step 4065, loss 0.00907226, acc 1
2017-09-08T17:52:00.304284: step 4066, loss 0.00964751, acc 1
2017-09-08T17:52:00.966141: step 4067, loss 0.0771354, acc 0.984375
2017-09-08T17:52:01.756162: step 4068, loss 0.0469816, acc 0.984375
2017-09-08T17:52:02.352438: step 4069, loss 0.00120146, acc 1
2017-09-08T17:52:03.007873: step 4070, loss 0.00681351, acc 1
2017-09-08T17:52:03.659403: step 4071, loss 0.0137497, acc 1
2017-09-08T17:52:04.370897: step 4072, loss 0.00329015, acc 1
2017-09-08T17:52:05.077885: step 4073, loss 0.0173331, acc 1
2017-09-08T17:52:05.788559: step 4074, loss 0.00393954, acc 1
2017-09-08T17:52:06.517914: step 4075, loss 0.0109242, acc 1
2017-09-08T17:52:07.215508: step 4076, loss 0.00313809, acc 1
2017-09-08T17:52:07.880696: step 4077, loss 0.125473, acc 0.96875
2017-09-08T17:52:08.469200: step 4078, loss 0.00219315, acc 1
2017-09-08T17:52:09.101961: step 4079, loss 0.0257195, acc 0.984375
2017-09-08T17:52:09.747378: step 4080, loss 0.0387933, acc 0.984375
2017-09-08T17:52:10.364020: step 4081, loss 0.0563699, acc 0.96875
2017-09-08T17:52:11.038812: step 4082, loss 0.0410055, acc 0.984375
2017-09-08T17:52:11.774182: step 4083, loss 0.0210096, acc 0.984375
2017-09-08T17:52:12.454198: step 4084, loss 0.0335894, acc 0.984375
2017-09-08T17:52:13.141189: step 4085, loss 0.0042502, acc 1
2017-09-08T17:52:13.828999: step 4086, loss 0.0977037, acc 0.953125
2017-09-08T17:52:14.554666: step 4087, loss 0.0309154, acc 0.984375
2017-09-08T17:52:15.267461: step 4088, loss 0.00157206, acc 1
2017-09-08T17:52:15.815203: step 4089, loss 0.027201, acc 0.984375
2017-09-08T17:52:16.467406: step 4090, loss 0.000767976, acc 1
2017-09-08T17:52:17.005370: step 4091, loss 0.060499, acc 0.96875
2017-09-08T17:52:17.520087: step 4092, loss 0.00804715, acc 1
2017-09-08T17:52:17.991602: step 4093, loss 0.0171612, acc 0.984375
2017-09-08T17:52:18.662330: step 4094, loss 0.00229269, acc 1
2017-09-08T17:52:19.386735: step 4095, loss 0.036039, acc 0.984375
2017-09-08T17:52:20.085184: step 4096, loss 0.00146077, acc 1
2017-09-08T17:52:20.768383: step 4097, loss 0.00567835, acc 1
2017-09-08T17:52:21.453617: step 4098, loss 0.00796766, acc 1
2017-09-08T17:52:22.205057: step 4099, loss 0.116223, acc 0.96875
2017-09-08T17:52:22.925231: step 4100, loss 0.0609454, acc 0.984375

Evaluation:
2017-09-08T17:52:23.722689: step 4100, loss 0.218313, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4100

2017-09-08T17:52:26.510658: step 4101, loss 0.0664126, acc 0.96875
2017-09-08T17:52:27.227963: step 4102, loss 0.00213741, acc 1
2017-09-08T17:52:27.936272: step 4103, loss 0.0446034, acc 0.984375
2017-09-08T17:52:28.611388: step 4104, loss 0.00188946, acc 1
2017-09-08T17:52:29.277377: step 4105, loss 0.0292458, acc 0.984375
2017-09-08T17:52:29.975073: step 4106, loss 0.0010627, acc 1
2017-09-08T17:52:30.632636: step 4107, loss 0.0500868, acc 0.984375
2017-09-08T17:52:31.401648: step 4108, loss 0.00619749, acc 1
2017-09-08T17:52:32.130558: step 4109, loss 0.0867083, acc 0.984375
2017-09-08T17:52:32.745571: step 4110, loss 0.0157769, acc 0.984375
2017-09-08T17:52:33.358721: step 4111, loss 0.00123951, acc 1
2017-09-08T17:52:34.024961: step 4112, loss 0.00345729, acc 1
2017-09-08T17:52:34.731058: step 4113, loss 0.00173313, acc 1
2017-09-08T17:52:35.406339: step 4114, loss 0.0042722, acc 1
2017-09-08T17:52:36.103851: step 4115, loss 0.0424707, acc 0.984375
2017-09-08T17:52:36.736162: step 4116, loss 0.0642148, acc 0.960784
2017-09-08T17:52:37.463772: step 4117, loss 0.000661379, acc 1
2017-09-08T17:52:38.142842: step 4118, loss 0.00684825, acc 1
2017-09-08T17:52:38.934237: step 4119, loss 0.0121267, acc 1
2017-09-08T17:52:39.621252: step 4120, loss 0.00179513, acc 1
2017-09-08T17:52:40.226743: step 4121, loss 0.000818488, acc 1
2017-09-08T17:52:40.875492: step 4122, loss 0.0205424, acc 0.984375
2017-09-08T17:52:41.455440: step 4123, loss 0.00166576, acc 1
2017-09-08T17:52:42.145576: step 4124, loss 0.000678058, acc 1
2017-09-08T17:52:42.861864: step 4125, loss 0.0455306, acc 0.96875
2017-09-08T17:52:43.593709: step 4126, loss 0.0484671, acc 0.984375
2017-09-08T17:52:44.328678: step 4127, loss 0.00305291, acc 1
2017-09-08T17:52:45.453668: step 4128, loss 0.0433271, acc 0.984375
2017-09-08T17:52:46.158798: step 4129, loss 0.01686, acc 1
2017-09-08T17:52:46.888673: step 4130, loss 0.063512, acc 0.96875
2017-09-08T17:52:47.594745: step 4131, loss 0.0356314, acc 0.984375
2017-09-08T17:52:48.336822: step 4132, loss 0.0403622, acc 0.96875
2017-09-08T17:52:49.032547: step 4133, loss 0.0395982, acc 0.96875
2017-09-08T17:52:49.734176: step 4134, loss 0.0029484, acc 1
2017-09-08T17:52:50.461275: step 4135, loss 0.00165909, acc 1
2017-09-08T17:52:51.171961: step 4136, loss 0.00123905, acc 1
2017-09-08T17:52:51.896767: step 4137, loss 0.00571833, acc 1
2017-09-08T17:52:52.636787: step 4138, loss 0.0299117, acc 0.96875
2017-09-08T17:52:53.341357: step 4139, loss 0.0255808, acc 0.984375
2017-09-08T17:52:54.185783: step 4140, loss 0.00301063, acc 1
2017-09-08T17:52:54.843471: step 4141, loss 0.0556715, acc 0.953125
2017-09-08T17:52:55.496614: step 4142, loss 0.0354429, acc 0.984375
2017-09-08T17:52:56.074808: step 4143, loss 0.00137552, acc 1
2017-09-08T17:52:56.629956: step 4144, loss 0.00243379, acc 1
2017-09-08T17:52:57.352292: step 4145, loss 0.0231915, acc 0.984375
2017-09-08T17:52:58.067828: step 4146, loss 0.005073, acc 1
2017-09-08T17:52:58.777584: step 4147, loss 0.0424743, acc 0.984375
2017-09-08T17:52:59.460808: step 4148, loss 0.02341, acc 0.984375
2017-09-08T17:53:00.114872: step 4149, loss 0.0552986, acc 0.96875
2017-09-08T17:53:00.760768: step 4150, loss 0.010357, acc 1

Evaluation:
2017-09-08T17:53:01.412527: step 4150, loss 0.209969, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4150

2017-09-08T17:53:05.102311: step 4151, loss 0.00492513, acc 1
2017-09-08T17:53:05.832020: step 4152, loss 0.0247035, acc 0.984375
2017-09-08T17:53:06.623133: step 4153, loss 0.0796288, acc 0.96875
2017-09-08T17:53:07.367271: step 4154, loss 0.0309914, acc 0.96875
2017-09-08T17:53:08.067521: step 4155, loss 0.0442762, acc 0.984375
2017-09-08T17:53:08.849521: step 4156, loss 0.000934708, acc 1
2017-09-08T17:53:09.549447: step 4157, loss 0.0094186, acc 1
2017-09-08T17:53:10.236184: step 4158, loss 0.0029615, acc 1
2017-09-08T17:53:10.882682: step 4159, loss 0.0215187, acc 0.984375
2017-09-08T17:53:11.549598: step 4160, loss 0.0118087, acc 1
2017-09-08T17:53:12.199388: step 4161, loss 0.00083862, acc 1
2017-09-08T17:53:12.891714: step 4162, loss 0.0712983, acc 0.984375
2017-09-08T17:53:13.652381: step 4163, loss 0.0765297, acc 0.96875
2017-09-08T17:53:14.383930: step 4164, loss 0.00183801, acc 1
2017-09-08T17:53:15.144309: step 4165, loss 0.00876142, acc 1
2017-09-08T17:53:15.939401: step 4166, loss 0.00279008, acc 1
2017-09-08T17:53:16.689197: step 4167, loss 0.00127074, acc 1
2017-09-08T17:53:17.260192: step 4168, loss 0.00740385, acc 1
2017-09-08T17:53:17.966639: step 4169, loss 0.00351849, acc 1
2017-09-08T17:53:18.563835: step 4170, loss 0.0566313, acc 0.96875
2017-09-08T17:53:19.144040: step 4171, loss 0.00264316, acc 1
2017-09-08T17:53:19.784833: step 4172, loss 0.0205399, acc 0.984375
2017-09-08T17:53:20.495093: step 4173, loss 0.0615309, acc 0.984375
2017-09-08T17:53:21.171859: step 4174, loss 0.00163972, acc 1
2017-09-08T17:53:21.858420: step 4175, loss 0.000615868, acc 1
2017-09-08T17:53:22.577234: step 4176, loss 0.0677709, acc 0.984375
2017-09-08T17:53:23.291646: step 4177, loss 0.00360869, acc 1
2017-09-08T17:53:24.003982: step 4178, loss 0.160086, acc 0.953125
2017-09-08T17:53:24.667384: step 4179, loss 0.0741837, acc 0.984375
2017-09-08T17:53:25.284044: step 4180, loss 0.0538481, acc 0.96875
2017-09-08T17:53:25.822711: step 4181, loss 0.00521635, acc 1
2017-09-08T17:53:26.406442: step 4182, loss 0.0480272, acc 0.984375
2017-09-08T17:53:27.054930: step 4183, loss 0.00333559, acc 1
2017-09-08T17:53:27.752530: step 4184, loss 0.000885198, acc 1
2017-09-08T17:53:28.483284: step 4185, loss 0.00417085, acc 1
2017-09-08T17:53:29.142080: step 4186, loss 0.0143112, acc 1
2017-09-08T17:53:29.817844: step 4187, loss 0.0175243, acc 0.984375
2017-09-08T17:53:30.493204: step 4188, loss 0.00935004, acc 1
2017-09-08T17:53:31.228400: step 4189, loss 0.00189969, acc 1
2017-09-08T17:53:31.924760: step 4190, loss 0.0883003, acc 0.984375
2017-09-08T17:53:32.710266: step 4191, loss 0.00154236, acc 1
2017-09-08T17:53:33.324989: step 4192, loss 0.00121208, acc 1
2017-09-08T17:53:33.968592: step 4193, loss 0.00922663, acc 1
2017-09-08T17:53:34.605607: step 4194, loss 0.00385945, acc 1
2017-09-08T17:53:35.319401: step 4195, loss 0.0749568, acc 0.96875
2017-09-08T17:53:35.992982: step 4196, loss 0.00470178, acc 1
2017-09-08T17:53:36.683454: step 4197, loss 0.00219433, acc 1
2017-09-08T17:53:37.337400: step 4198, loss 0.0069663, acc 1
2017-09-08T17:53:38.038972: step 4199, loss 0.0148265, acc 0.984375
2017-09-08T17:53:38.717746: step 4200, loss 0.0149769, acc 0.984375

Evaluation:
2017-09-08T17:53:39.401018: step 4200, loss 0.210947, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4200

2017-09-08T17:53:42.347310: step 4201, loss 0.00373641, acc 1
2017-09-08T17:53:43.032602: step 4202, loss 0.00489023, acc 1
2017-09-08T17:53:43.770932: step 4203, loss 0.00560684, acc 1
2017-09-08T17:53:44.547679: step 4204, loss 0.00410682, acc 1
2017-09-08T17:53:45.365523: step 4205, loss 0.09523, acc 0.984375
2017-09-08T17:53:46.071186: step 4206, loss 0.0410186, acc 0.984375
2017-09-08T17:53:46.725509: step 4207, loss 0.140105, acc 0.96875
2017-09-08T17:53:47.460257: step 4208, loss 0.0014676, acc 1
2017-09-08T17:53:48.142839: step 4209, loss 0.0385025, acc 0.984375
2017-09-08T17:53:48.696040: step 4210, loss 0.0214709, acc 0.984375
2017-09-08T17:53:49.286508: step 4211, loss 0.0048869, acc 1
2017-09-08T17:53:49.933177: step 4212, loss 0.00262413, acc 1
2017-09-08T17:53:50.587965: step 4213, loss 0.000845222, acc 1
2017-09-08T17:53:51.193418: step 4214, loss 0.00522141, acc 1
2017-09-08T17:53:51.891616: step 4215, loss 0.00161485, acc 1
2017-09-08T17:53:52.555608: step 4216, loss 0.0019431, acc 1
2017-09-08T17:53:53.247870: step 4217, loss 0.00329615, acc 1
2017-09-08T17:53:53.941141: step 4218, loss 0.00274706, acc 1
2017-09-08T17:53:54.645567: step 4219, loss 0.0630087, acc 0.96875
2017-09-08T17:53:55.299940: step 4220, loss 0.00182631, acc 1
2017-09-08T17:53:56.028077: step 4221, loss 0.0267841, acc 1
2017-09-08T17:53:56.756746: step 4222, loss 0.00350712, acc 1
2017-09-08T17:53:57.526463: step 4223, loss 0.00265755, acc 1
2017-09-08T17:53:58.270530: step 4224, loss 0.00322502, acc 1
2017-09-08T17:53:59.006410: step 4225, loss 0.0293801, acc 0.984375
2017-09-08T17:53:59.754390: step 4226, loss 0.0174136, acc 1
2017-09-08T17:54:00.499455: step 4227, loss 0.00644641, acc 1
2017-09-08T17:54:01.295366: step 4228, loss 0.0501471, acc 0.984375
2017-09-08T17:54:02.013463: step 4229, loss 0.0524551, acc 0.984375
2017-09-08T17:54:02.714126: step 4230, loss 0.0199922, acc 0.984375
2017-09-08T17:54:03.443381: step 4231, loss 0.03225, acc 0.984375
2017-09-08T17:54:04.101991: step 4232, loss 0.00250936, acc 1
2017-09-08T17:54:04.769871: step 4233, loss 0.00103065, acc 1
2017-09-08T17:54:05.414021: step 4234, loss 0.0176801, acc 0.984375
2017-09-08T17:54:06.077977: step 4235, loss 0.0264892, acc 0.984375
2017-09-08T17:54:06.742869: step 4236, loss 0.0259552, acc 0.984375
2017-09-08T17:54:07.393288: step 4237, loss 0.0235176, acc 0.984375
2017-09-08T17:54:08.094771: step 4238, loss 0.00532822, acc 1
2017-09-08T17:54:08.784056: step 4239, loss 0.05098, acc 0.984375
2017-09-08T17:54:09.459564: step 4240, loss 0.00560326, acc 1
2017-09-08T17:54:09.977031: step 4241, loss 0.00214099, acc 1
2017-09-08T17:54:10.784554: step 4242, loss 0.0892105, acc 0.984375
2017-09-08T17:54:11.328432: step 4243, loss 0.00120029, acc 1
2017-09-08T17:54:11.859871: step 4244, loss 0.017258, acc 1
2017-09-08T17:54:12.505566: step 4245, loss 0.00983989, acc 1
2017-09-08T17:54:13.140571: step 4246, loss 0.000974724, acc 1
2017-09-08T17:54:13.885527: step 4247, loss 0.0373039, acc 0.96875
2017-09-08T17:54:14.617174: step 4248, loss 0.0536605, acc 0.984375
2017-09-08T17:54:15.405832: step 4249, loss 0.0325244, acc 0.984375
2017-09-08T17:54:16.094358: step 4250, loss 0.00272582, acc 1

Evaluation:
2017-09-08T17:54:16.808371: step 4250, loss 0.217138, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4250

2017-09-08T17:54:19.998904: step 4251, loss 0.0322629, acc 0.984375
2017-09-08T17:54:20.737075: step 4252, loss 0.060537, acc 0.984375
2017-09-08T17:54:21.425548: step 4253, loss 0.0624656, acc 0.96875
2017-09-08T17:54:22.093957: step 4254, loss 0.0193316, acc 0.984375
2017-09-08T17:54:22.835524: step 4255, loss 0.00126661, acc 1
2017-09-08T17:54:23.615187: step 4256, loss 0.000840544, acc 1
2017-09-08T17:54:24.306449: step 4257, loss 0.00198762, acc 1
2017-09-08T17:54:24.972539: step 4258, loss 0.0571778, acc 0.984375
2017-09-08T17:54:25.622137: step 4259, loss 0.031025, acc 0.984375
2017-09-08T17:54:26.286078: step 4260, loss 0.0847976, acc 0.96875
2017-09-08T17:54:26.870366: step 4261, loss 0.01138, acc 1
2017-09-08T17:54:27.556278: step 4262, loss 0.00921761, acc 1
2017-09-08T17:54:28.243572: step 4263, loss 0.00089076, acc 1
2017-09-08T17:54:28.951400: step 4264, loss 0.0334297, acc 0.984375
2017-09-08T17:54:29.688689: step 4265, loss 0.00645375, acc 1
2017-09-08T17:54:30.368648: step 4266, loss 0.0280319, acc 0.984375
2017-09-08T17:54:31.137258: step 4267, loss 0.017795, acc 0.984375
2017-09-08T17:54:31.823329: step 4268, loss 0.00242204, acc 1
2017-09-08T17:54:32.436240: step 4269, loss 0.00380821, acc 1
2017-09-08T17:54:33.040472: step 4270, loss 0.00154995, acc 1
2017-09-08T17:54:33.635358: step 4271, loss 0.0227987, acc 1
2017-09-08T17:54:34.232071: step 4272, loss 0.000987325, acc 1
2017-09-08T17:54:34.960048: step 4273, loss 0.00560198, acc 1
2017-09-08T17:54:35.649689: step 4274, loss 0.0127071, acc 0.984375
2017-09-08T17:54:36.302473: step 4275, loss 0.011202, acc 1
2017-09-08T17:54:37.033690: step 4276, loss 0.00126304, acc 1
2017-09-08T17:54:37.748307: step 4277, loss 0.0677546, acc 0.984375
2017-09-08T17:54:38.483650: step 4278, loss 0.0130741, acc 1
2017-09-08T17:54:39.192837: step 4279, loss 0.0525303, acc 0.984375
2017-09-08T17:54:39.884502: step 4280, loss 0.0399762, acc 0.984375
2017-09-08T17:54:40.576564: step 4281, loss 0.0536023, acc 0.984375
2017-09-08T17:54:41.260878: step 4282, loss 0.00313185, acc 1
2017-09-08T17:54:41.973461: step 4283, loss 0.00325467, acc 1
2017-09-08T17:54:42.771834: step 4284, loss 0.00555435, acc 1
2017-09-08T17:54:43.461250: step 4285, loss 0.0118122, acc 1
2017-09-08T17:54:44.106602: step 4286, loss 0.00145635, acc 1
2017-09-08T17:54:44.806875: step 4287, loss 0.0315699, acc 0.96875
2017-09-08T17:54:45.482193: step 4288, loss 0.00353882, acc 1
2017-09-08T17:54:46.176754: step 4289, loss 0.00311825, acc 1
2017-09-08T17:54:46.800471: step 4290, loss 0.00325703, acc 1
2017-09-08T17:54:47.423560: step 4291, loss 0.0081579, acc 1
2017-09-08T17:54:48.096027: step 4292, loss 0.0117454, acc 1
2017-09-08T17:54:48.804421: step 4293, loss 0.00193439, acc 1
2017-09-08T17:54:49.434138: step 4294, loss 0.00347366, acc 1
2017-09-08T17:54:49.991257: step 4295, loss 0.00694698, acc 1
2017-09-08T17:54:50.590065: step 4296, loss 0.0261889, acc 0.984375
2017-09-08T17:54:51.271974: step 4297, loss 0.00244859, acc 1
2017-09-08T17:54:51.901758: step 4298, loss 0.0441564, acc 0.984375
2017-09-08T17:54:52.588243: step 4299, loss 0.03342, acc 0.984375
2017-09-08T17:54:53.305833: step 4300, loss 0.00149207, acc 1

Evaluation:
2017-09-08T17:54:53.932204: step 4300, loss 0.200173, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4300

2017-09-08T17:54:57.423001: step 4301, loss 0.028144, acc 0.984375
2017-09-08T17:54:58.109159: step 4302, loss 0.0463167, acc 0.96875
2017-09-08T17:54:58.774891: step 4303, loss 0.0571432, acc 0.984375
2017-09-08T17:54:59.425089: step 4304, loss 0.0151325, acc 1
2017-09-08T17:55:00.161443: step 4305, loss 0.00264845, acc 1
2017-09-08T17:55:00.920504: step 4306, loss 0.00955998, acc 1
2017-09-08T17:55:01.663201: step 4307, loss 0.0557665, acc 0.984375
2017-09-08T17:55:02.365297: step 4308, loss 0.000845931, acc 1
2017-09-08T17:55:03.005127: step 4309, loss 0.0548838, acc 0.96875
2017-09-08T17:55:03.668104: step 4310, loss 0.112961, acc 0.96875
2017-09-08T17:55:04.367851: step 4311, loss 0.0665158, acc 0.984375
2017-09-08T17:55:04.975247: step 4312, loss 0.00267838, acc 1
2017-09-08T17:55:05.664984: step 4313, loss 0.0324489, acc 0.984375
2017-09-08T17:55:06.495664: step 4314, loss 0.00119507, acc 1
2017-09-08T17:55:07.147460: step 4315, loss 0.00235533, acc 1
2017-09-08T17:55:07.843006: step 4316, loss 0.0599849, acc 0.96875
2017-09-08T17:55:08.566768: step 4317, loss 0.0113096, acc 1
2017-09-08T17:55:09.311080: step 4318, loss 0.00126071, acc 1
2017-09-08T17:55:09.953916: step 4319, loss 0.0629648, acc 0.96875
2017-09-08T17:55:10.585228: step 4320, loss 0.0043679, acc 1
2017-09-08T17:55:11.184508: step 4321, loss 0.0826924, acc 0.96875
2017-09-08T17:55:11.858197: step 4322, loss 0.0499921, acc 0.984375
2017-09-08T17:55:12.535598: step 4323, loss 0.00207424, acc 1
2017-09-08T17:55:13.208973: step 4324, loss 0.0204108, acc 1
2017-09-08T17:55:13.926490: step 4325, loss 0.0375838, acc 0.984375
2017-09-08T17:55:14.666369: step 4326, loss 0.0140453, acc 0.984375
2017-09-08T17:55:15.357986: step 4327, loss 0.00777833, acc 1
2017-09-08T17:55:16.073425: step 4328, loss 0.0184704, acc 0.984375
2017-09-08T17:55:16.770234: step 4329, loss 0.126615, acc 0.96875
2017-09-08T17:55:17.478596: step 4330, loss 0.0324885, acc 0.96875
2017-09-08T17:55:18.175547: step 4331, loss 0.000811378, acc 1
2017-09-08T17:55:18.923527: step 4332, loss 0.00179534, acc 1
2017-09-08T17:55:19.646676: step 4333, loss 0.0014507, acc 1
2017-09-08T17:55:20.420261: step 4334, loss 0.0127203, acc 1
2017-09-08T17:55:21.192121: step 4335, loss 0.00729652, acc 1
2017-09-08T17:55:21.914866: step 4336, loss 0.111374, acc 0.96875
2017-09-08T17:55:22.611813: step 4337, loss 0.113095, acc 0.984375
2017-09-08T17:55:23.222269: step 4338, loss 0.00211151, acc 1
2017-09-08T17:55:23.828069: step 4339, loss 0.0402814, acc 0.984375
2017-09-08T17:55:24.479011: step 4340, loss 0.0317997, acc 0.984375
2017-09-08T17:55:25.105102: step 4341, loss 0.011959, acc 1
2017-09-08T17:55:25.838987: step 4342, loss 0.000753096, acc 1
2017-09-08T17:55:26.552317: step 4343, loss 0.0719302, acc 0.96875
2017-09-08T17:55:27.260929: step 4344, loss 0.0119829, acc 1
2017-09-08T17:55:27.896734: step 4345, loss 0.00154937, acc 1
2017-09-08T17:55:28.536481: step 4346, loss 0.0749333, acc 0.984375
2017-09-08T17:55:29.231341: step 4347, loss 0.0245109, acc 0.984375
2017-09-08T17:55:29.963993: step 4348, loss 0.00187253, acc 1
2017-09-08T17:55:30.716184: step 4349, loss 0.0183827, acc 0.984375
2017-09-08T17:55:31.459147: step 4350, loss 0.00204754, acc 1

Evaluation:
2017-09-08T17:55:32.190936: step 4350, loss 0.205439, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4350

2017-09-08T17:55:34.610128: step 4351, loss 0.0148177, acc 1
2017-09-08T17:55:35.357050: step 4352, loss 0.00665477, acc 1
2017-09-08T17:55:36.060970: step 4353, loss 0.00133101, acc 1
2017-09-08T17:55:36.774022: step 4354, loss 0.00879885, acc 1
2017-09-08T17:55:37.469748: step 4355, loss 0.0203587, acc 0.984375
2017-09-08T17:55:38.202935: step 4356, loss 0.000672801, acc 1
2017-09-08T17:55:38.915797: step 4357, loss 0.0507844, acc 0.984375
2017-09-08T17:55:39.537275: step 4358, loss 0.0211147, acc 0.984375
2017-09-08T17:55:40.224016: step 4359, loss 0.0130574, acc 1
2017-09-08T17:55:40.816899: step 4360, loss 0.0674208, acc 0.96875
2017-09-08T17:55:41.480945: step 4361, loss 0.0706539, acc 0.984375
2017-09-08T17:55:42.109346: step 4362, loss 0.0355034, acc 0.984375
2017-09-08T17:55:42.903382: step 4363, loss 0.000824695, acc 1
2017-09-08T17:55:44.214174: step 4364, loss 0.0296799, acc 0.984375
2017-09-08T17:55:44.913089: step 4365, loss 0.00242718, acc 1
2017-09-08T17:55:45.675427: step 4366, loss 0.0316291, acc 0.984375
2017-09-08T17:55:46.404249: step 4367, loss 0.0513434, acc 0.984375
2017-09-08T17:55:47.207068: step 4368, loss 0.0390381, acc 0.984375
2017-09-08T17:55:47.860233: step 4369, loss 0.00373914, acc 1
2017-09-08T17:55:48.519143: step 4370, loss 0.0292196, acc 0.984375
2017-09-08T17:55:49.153329: step 4371, loss 0.00151651, acc 1
2017-09-08T17:55:49.761573: step 4372, loss 0.0260048, acc 0.984375
2017-09-08T17:55:50.436621: step 4373, loss 0.00281525, acc 1
2017-09-08T17:55:51.155166: step 4374, loss 0.00175139, acc 1
2017-09-08T17:55:51.897126: step 4375, loss 0.0495108, acc 0.96875
2017-09-08T17:55:52.659643: step 4376, loss 0.0318012, acc 0.984375
2017-09-08T17:55:53.376837: step 4377, loss 0.0252228, acc 0.984375
2017-09-08T17:55:54.129823: step 4378, loss 0.00278668, acc 1
2017-09-08T17:55:54.868467: step 4379, loss 0.00102174, acc 1
2017-09-08T17:55:55.502904: step 4380, loss 0.00148569, acc 1
2017-09-08T17:55:56.174857: step 4381, loss 0.00159256, acc 1
2017-09-08T17:55:56.863264: step 4382, loss 0.0306375, acc 0.984375
2017-09-08T17:55:57.612271: step 4383, loss 0.011621, acc 1
2017-09-08T17:55:58.253609: step 4384, loss 0.0655037, acc 0.96875
2017-09-08T17:55:58.977943: step 4385, loss 0.0129312, acc 0.984375
2017-09-08T17:55:59.636884: step 4386, loss 0.00244056, acc 1
2017-09-08T17:56:00.334906: step 4387, loss 0.161779, acc 0.953125
2017-09-08T17:56:01.080194: step 4388, loss 0.0022191, acc 1
2017-09-08T17:56:01.720148: step 4389, loss 0.00258968, acc 1
2017-09-08T17:56:02.353860: step 4390, loss 0.00133755, acc 1
2017-09-08T17:56:02.921748: step 4391, loss 0.018166, acc 0.984375
2017-09-08T17:56:03.450572: step 4392, loss 0.0435622, acc 0.984375
2017-09-08T17:56:04.134126: step 4393, loss 0.0538698, acc 0.984375
2017-09-08T17:56:04.776818: step 4394, loss 0.0431367, acc 0.984375
2017-09-08T17:56:05.570442: step 4395, loss 0.00290499, acc 1
2017-09-08T17:56:06.208863: step 4396, loss 0.0414074, acc 0.96875
2017-09-08T17:56:06.845049: step 4397, loss 0.0116712, acc 1
2017-09-08T17:56:07.454859: step 4398, loss 0.00133146, acc 1
2017-09-08T17:56:08.121106: step 4399, loss 0.0154766, acc 1
2017-09-08T17:56:08.880968: step 4400, loss 0.0630961, acc 0.96875

Evaluation:
2017-09-08T17:56:09.592073: step 4400, loss 0.201484, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4400

2017-09-08T17:56:13.708995: step 4401, loss 0.090999, acc 0.96875
2017-09-08T17:56:14.396512: step 4402, loss 0.0427837, acc 0.984375
2017-09-08T17:56:15.069381: step 4403, loss 0.0011484, acc 1
2017-09-08T17:56:15.710464: step 4404, loss 0.0606397, acc 0.96875
2017-09-08T17:56:16.361109: step 4405, loss 0.00136513, acc 1
2017-09-08T17:56:17.038712: step 4406, loss 0.019481, acc 0.984375
2017-09-08T17:56:17.675298: step 4407, loss 0.0219232, acc 0.984375
2017-09-08T17:56:18.350033: step 4408, loss 0.045888, acc 0.984375
2017-09-08T17:56:18.933827: step 4409, loss 0.00337985, acc 1
2017-09-08T17:56:19.474665: step 4410, loss 0.0089435, acc 1
2017-09-08T17:56:20.079058: step 4411, loss 0.00325507, acc 1
2017-09-08T17:56:20.671544: step 4412, loss 0.00276369, acc 1
2017-09-08T17:56:21.396838: step 4413, loss 0.00537428, acc 1
2017-09-08T17:56:22.104809: step 4414, loss 0.0507379, acc 0.984375
2017-09-08T17:56:22.814189: step 4415, loss 0.0384749, acc 0.984375
2017-09-08T17:56:23.523277: step 4416, loss 0.131367, acc 0.984375
2017-09-08T17:56:24.243633: step 4417, loss 0.0240431, acc 0.984375
2017-09-08T17:56:25.023849: step 4418, loss 0.0540593, acc 0.984375
2017-09-08T17:56:25.636921: step 4419, loss 0.00387969, acc 1
2017-09-08T17:56:26.225027: step 4420, loss 0.0136494, acc 1
2017-09-08T17:56:26.887474: step 4421, loss 0.106058, acc 0.984375
2017-09-08T17:56:27.556992: step 4422, loss 0.0100725, acc 1
2017-09-08T17:56:28.183745: step 4423, loss 0.08593, acc 0.96875
2017-09-08T17:56:28.981506: step 4424, loss 0.000950148, acc 1
2017-09-08T17:56:29.849802: step 4425, loss 0.00157878, acc 1
2017-09-08T17:56:30.571613: step 4426, loss 0.00278384, acc 1
2017-09-08T17:56:31.295790: step 4427, loss 0.0121114, acc 1
2017-09-08T17:56:31.943665: step 4428, loss 0.00172442, acc 1
2017-09-08T17:56:32.649972: step 4429, loss 0.000639254, acc 1
2017-09-08T17:56:33.399637: step 4430, loss 0.0262819, acc 0.984375
2017-09-08T17:56:34.177400: step 4431, loss 0.0177806, acc 0.984375
2017-09-08T17:56:34.851806: step 4432, loss 0.000857668, acc 1
2017-09-08T17:56:35.623240: step 4433, loss 0.00518165, acc 1
2017-09-08T17:56:36.367975: step 4434, loss 0.0249507, acc 0.984375
2017-09-08T17:56:37.122379: step 4435, loss 0.0295307, acc 0.984375
2017-09-08T17:56:37.799453: step 4436, loss 0.00132953, acc 1
2017-09-08T17:56:38.427716: step 4437, loss 0.137556, acc 0.953125
2017-09-08T17:56:39.098844: step 4438, loss 0.0314239, acc 0.984375
2017-09-08T17:56:39.786005: step 4439, loss 0.00145149, acc 1
2017-09-08T17:56:40.416761: step 4440, loss 0.00272026, acc 1
2017-09-08T17:56:41.091533: step 4441, loss 0.00898958, acc 1
2017-09-08T17:56:41.781133: step 4442, loss 0.1028, acc 0.984375
2017-09-08T17:56:42.451827: step 4443, loss 0.000992281, acc 1
2017-09-08T17:56:43.186015: step 4444, loss 0.0231287, acc 0.984375
2017-09-08T17:56:43.941800: step 4445, loss 0.0866067, acc 0.984375
2017-09-08T17:56:44.689312: step 4446, loss 0.00663848, acc 1
2017-09-08T17:56:45.370058: step 4447, loss 0.058422, acc 0.96875
2017-09-08T17:56:46.024233: step 4448, loss 0.00128696, acc 1
2017-09-08T17:56:46.674958: step 4449, loss 0.0223502, acc 0.984375
2017-09-08T17:56:47.385440: step 4450, loss 0.0100313, acc 1

Evaluation:
2017-09-08T17:56:48.187672: step 4450, loss 0.213947, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4450

2017-09-08T17:56:51.289106: step 4451, loss 0.0413499, acc 0.984375
2017-09-08T17:56:52.003059: step 4452, loss 0.138742, acc 0.984375
2017-09-08T17:56:52.740856: step 4453, loss 0.041956, acc 0.984375
2017-09-08T17:56:53.456643: step 4454, loss 0.0157405, acc 0.984375
2017-09-08T17:56:54.198473: step 4455, loss 0.0102158, acc 1
2017-09-08T17:56:54.908366: step 4456, loss 0.00114004, acc 1
2017-09-08T17:56:55.575714: step 4457, loss 0.000696021, acc 1
2017-09-08T17:56:56.313737: step 4458, loss 0.00103892, acc 1
2017-09-08T17:56:56.921608: step 4459, loss 0.0522946, acc 0.984375
2017-09-08T17:56:57.591494: step 4460, loss 0.00259365, acc 1
2017-09-08T17:56:58.291928: step 4461, loss 0.000811062, acc 1
2017-09-08T17:56:59.105794: step 4462, loss 0.119407, acc 0.984375
2017-09-08T17:56:59.838040: step 4463, loss 0.0517388, acc 0.984375
2017-09-08T17:57:00.556462: step 4464, loss 0.0525518, acc 0.96875
2017-09-08T17:57:01.265362: step 4465, loss 0.0131482, acc 0.984375
2017-09-08T17:57:01.972978: step 4466, loss 0.0108286, acc 1
2017-09-08T17:57:02.661393: step 4467, loss 0.00709606, acc 1
2017-09-08T17:57:03.388231: step 4468, loss 0.0288324, acc 0.984375
2017-09-08T17:57:04.080597: step 4469, loss 0.00162122, acc 1
2017-09-08T17:57:04.768260: step 4470, loss 0.000623734, acc 1
2017-09-08T17:57:05.391473: step 4471, loss 0.000972567, acc 1
2017-09-08T17:57:06.097666: step 4472, loss 0.0501131, acc 0.984375
2017-09-08T17:57:06.905349: step 4473, loss 0.00646985, acc 1
2017-09-08T17:57:07.637246: step 4474, loss 0.00320687, acc 1
2017-09-08T17:57:08.335898: step 4475, loss 0.00237343, acc 1
2017-09-08T17:57:09.039245: step 4476, loss 0.102143, acc 0.984375
2017-09-08T17:57:09.839538: step 4477, loss 0.00262711, acc 1
2017-09-08T17:57:10.532264: step 4478, loss 0.00138934, acc 1
2017-09-08T17:57:11.176687: step 4479, loss 0.00228093, acc 1
2017-09-08T17:57:11.860659: step 4480, loss 0.0199583, acc 0.984375
2017-09-08T17:57:12.594194: step 4481, loss 0.00157023, acc 1
2017-09-08T17:57:13.348975: step 4482, loss 0.0828571, acc 0.984375
2017-09-08T17:57:14.119058: step 4483, loss 0.0801533, acc 0.984375
2017-09-08T17:57:14.822329: step 4484, loss 0.0214154, acc 0.984375
2017-09-08T17:57:15.582234: step 4485, loss 0.00139549, acc 1
2017-09-08T17:57:16.272783: step 4486, loss 0.00184068, acc 1
2017-09-08T17:57:17.014765: step 4487, loss 0.00120444, acc 1
2017-09-08T17:57:17.793528: step 4488, loss 0.0049207, acc 1
2017-09-08T17:57:18.527430: step 4489, loss 0.0578666, acc 0.984375
2017-09-08T17:57:19.268037: step 4490, loss 0.00597765, acc 1
2017-09-08T17:57:19.996216: step 4491, loss 0.00615527, acc 1
2017-09-08T17:57:20.720499: step 4492, loss 0.000960338, acc 1
2017-09-08T17:57:21.547098: step 4493, loss 0.106812, acc 0.96875
2017-09-08T17:57:22.251101: step 4494, loss 0.0736278, acc 0.984375
2017-09-08T17:57:22.979203: step 4495, loss 0.0274779, acc 0.984375
2017-09-08T17:57:23.609719: step 4496, loss 0.045472, acc 0.984375
2017-09-08T17:57:24.132359: step 4497, loss 0.00123497, acc 1
2017-09-08T17:57:24.737943: step 4498, loss 0.0403441, acc 0.96875
2017-09-08T17:57:25.356762: step 4499, loss 0.0157537, acc 0.984375
2017-09-08T17:57:26.077929: step 4500, loss 0.0647525, acc 0.984375

Evaluation:
2017-09-08T17:57:26.755641: step 4500, loss 0.198113, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4500

2017-09-08T17:57:30.248791: step 4501, loss 0.000610932, acc 1
2017-09-08T17:57:30.900711: step 4502, loss 0.00209931, acc 1
2017-09-08T17:57:31.642920: step 4503, loss 0.184839, acc 0.921875
2017-09-08T17:57:32.301959: step 4504, loss 0.0744908, acc 0.984375
2017-09-08T17:57:33.025718: step 4505, loss 0.0240018, acc 0.984375
2017-09-08T17:57:33.611459: step 4506, loss 0.000924953, acc 1
2017-09-08T17:57:34.202430: step 4507, loss 0.0235183, acc 1
2017-09-08T17:57:34.791049: step 4508, loss 0.00104445, acc 1
2017-09-08T17:57:35.465946: step 4509, loss 0.00335762, acc 1
2017-09-08T17:57:36.172000: step 4510, loss 0.00208004, acc 1
2017-09-08T17:57:36.950757: step 4511, loss 0.0225187, acc 0.984375
2017-09-08T17:57:37.693728: step 4512, loss 0.0042382, acc 1
2017-09-08T17:57:38.480287: step 4513, loss 0.00236893, acc 1
2017-09-08T17:57:39.120872: step 4514, loss 0.00265334, acc 1
2017-09-08T17:57:39.773038: step 4515, loss 0.00400002, acc 1
2017-09-08T17:57:40.274537: step 4516, loss 0.0316449, acc 0.984375
2017-09-08T17:57:40.981666: step 4517, loss 0.0302265, acc 0.984375
2017-09-08T17:57:41.609245: step 4518, loss 0.00317059, acc 1
2017-09-08T17:57:42.237245: step 4519, loss 0.0433491, acc 0.984375
2017-09-08T17:57:42.873162: step 4520, loss 0.0106038, acc 1
2017-09-08T17:57:43.480907: step 4521, loss 0.0127889, acc 1
2017-09-08T17:57:44.178992: step 4522, loss 0.00417784, acc 1
2017-09-08T17:57:44.939376: step 4523, loss 0.0034862, acc 1
2017-09-08T17:57:45.621459: step 4524, loss 0.0157312, acc 0.984375
2017-09-08T17:57:46.250706: step 4525, loss 0.0139975, acc 1
2017-09-08T17:57:46.916259: step 4526, loss 0.00550971, acc 1
2017-09-08T17:57:47.606980: step 4527, loss 0.0102072, acc 1
2017-09-08T17:57:48.221213: step 4528, loss 0.0108957, acc 1
2017-09-08T17:57:48.773368: step 4529, loss 0.0228495, acc 1
2017-09-08T17:57:49.280467: step 4530, loss 0.00278409, acc 1
2017-09-08T17:57:49.896324: step 4531, loss 0.00351597, acc 1
2017-09-08T17:57:51.005420: step 4532, loss 0.0612117, acc 0.984375
2017-09-08T17:57:51.689060: step 4533, loss 0.0319251, acc 0.984375
2017-09-08T17:57:52.420830: step 4534, loss 0.0018752, acc 1
2017-09-08T17:57:53.095543: step 4535, loss 0.00173573, acc 1
2017-09-08T17:57:53.827439: step 4536, loss 0.0851232, acc 0.96875
2017-09-08T17:57:54.522356: step 4537, loss 0.0777132, acc 0.953125
2017-09-08T17:57:55.247739: step 4538, loss 0.00094912, acc 1
2017-09-08T17:57:55.908619: step 4539, loss 0.00260083, acc 1
2017-09-08T17:57:56.585369: step 4540, loss 0.0411592, acc 0.984375
2017-09-08T17:57:57.285276: step 4541, loss 0.00293649, acc 1
2017-09-08T17:57:57.925389: step 4542, loss 0.114676, acc 0.984375
2017-09-08T17:57:58.663569: step 4543, loss 0.0410975, acc 0.984375
2017-09-08T17:57:59.362601: step 4544, loss 0.00429115, acc 1
2017-09-08T17:58:00.102014: step 4545, loss 0.000813676, acc 1
2017-09-08T17:58:00.787517: step 4546, loss 0.000458001, acc 1
2017-09-08T17:58:01.427686: step 4547, loss 0.0068346, acc 1
2017-09-08T17:58:02.061105: step 4548, loss 0.00194488, acc 1
2017-09-08T17:58:02.813077: step 4549, loss 0.00102428, acc 1
2017-09-08T17:58:03.518724: step 4550, loss 0.00379542, acc 1

Evaluation:
2017-09-08T17:58:04.216340: step 4550, loss 0.200309, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4550

2017-09-08T17:58:07.808891: step 4551, loss 0.001591, acc 1
2017-09-08T17:58:08.499563: step 4552, loss 0.024068, acc 0.984375
2017-09-08T17:58:09.196416: step 4553, loss 0.00089761, acc 1
2017-09-08T17:58:09.853386: step 4554, loss 0.00783303, acc 1
2017-09-08T17:58:10.547090: step 4555, loss 0.0627971, acc 0.984375
2017-09-08T17:58:11.268659: step 4556, loss 0.0164901, acc 0.984375
2017-09-08T17:58:11.914128: step 4557, loss 0.0626759, acc 0.984375
2017-09-08T17:58:12.567403: step 4558, loss 0.0290215, acc 1
2017-09-08T17:58:13.210041: step 4559, loss 0.0553165, acc 0.984375
2017-09-08T17:58:13.893282: step 4560, loss 0.0039298, acc 1
2017-09-08T17:58:14.551975: step 4561, loss 0.148769, acc 0.953125
2017-09-08T17:58:15.232362: step 4562, loss 0.003991, acc 1
2017-09-08T17:58:15.972096: step 4563, loss 0.0487563, acc 0.984375
2017-09-08T17:58:16.734166: step 4564, loss 0.0493093, acc 0.984375
2017-09-08T17:58:17.435291: step 4565, loss 0.0633793, acc 0.984375
2017-09-08T17:58:18.109461: step 4566, loss 0.00228667, acc 1
2017-09-08T17:58:18.822047: step 4567, loss 0.00300918, acc 1
2017-09-08T17:58:19.505688: step 4568, loss 0.0303263, acc 0.984375
2017-09-08T17:58:20.125137: step 4569, loss 0.0130087, acc 1
2017-09-08T17:58:20.798921: step 4570, loss 0.00156301, acc 1
2017-09-08T17:58:21.451814: step 4571, loss 0.0613666, acc 0.984375
2017-09-08T17:58:22.157456: step 4572, loss 0.00181258, acc 1
2017-09-08T17:58:22.903836: step 4573, loss 0.0444121, acc 0.984375
2017-09-08T17:58:23.639237: step 4574, loss 0.00481153, acc 1
2017-09-08T17:58:24.370668: step 4575, loss 0.00996417, acc 1
2017-09-08T17:58:25.171263: step 4576, loss 0.00759559, acc 1
2017-09-08T17:58:25.820292: step 4577, loss 0.0906154, acc 0.984375
2017-09-08T17:58:26.621223: step 4578, loss 0.000796763, acc 1
2017-09-08T17:58:27.366510: step 4579, loss 0.00444447, acc 1
2017-09-08T17:58:28.096823: step 4580, loss 0.0720278, acc 0.96875
2017-09-08T17:58:28.863039: step 4581, loss 0.0378938, acc 0.984375
2017-09-08T17:58:29.579789: step 4582, loss 0.0329595, acc 0.96875
2017-09-08T17:58:30.258217: step 4583, loss 0.00359494, acc 1
2017-09-08T17:58:30.982896: step 4584, loss 0.00163832, acc 1
2017-09-08T17:58:31.687568: step 4585, loss 0.0269037, acc 0.984375
2017-09-08T17:58:32.382080: step 4586, loss 0.0674998, acc 0.984375
2017-09-08T17:58:33.039429: step 4587, loss 0.0187358, acc 0.984375
2017-09-08T17:58:33.748610: step 4588, loss 0.0288744, acc 0.984375
2017-09-08T17:58:34.439129: step 4589, loss 0.0142809, acc 0.984375
2017-09-08T17:58:35.160399: step 4590, loss 0.00159816, acc 1
2017-09-08T17:58:35.838895: step 4591, loss 0.0176819, acc 1
2017-09-08T17:58:36.583651: step 4592, loss 0.00131356, acc 1
2017-09-08T17:58:37.212308: step 4593, loss 0.000803325, acc 1
2017-09-08T17:58:37.867398: step 4594, loss 0.188339, acc 0.953125
2017-09-08T17:58:38.525115: step 4595, loss 0.00494716, acc 1
2017-09-08T17:58:39.180631: step 4596, loss 0.00947317, acc 1
2017-09-08T17:58:39.854223: step 4597, loss 0.00173164, acc 1
2017-09-08T17:58:40.489005: step 4598, loss 0.0743009, acc 0.984375
2017-09-08T17:58:41.369006: step 4599, loss 0.0325579, acc 0.984375
2017-09-08T17:58:42.019655: step 4600, loss 0.0470371, acc 0.984375

Evaluation:
2017-09-08T17:58:42.687907: step 4600, loss 0.21124, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4600

2017-09-08T17:58:45.621670: step 4601, loss 0.00041674, acc 1
2017-09-08T17:58:46.306215: step 4602, loss 0.0192349, acc 0.984375
2017-09-08T17:58:46.999953: step 4603, loss 0.00434186, acc 1
2017-09-08T17:58:47.599580: step 4604, loss 0.000549854, acc 1
2017-09-08T17:58:48.273404: step 4605, loss 0.00106924, acc 1
2017-09-08T17:58:48.914676: step 4606, loss 0.0308397, acc 1
2017-09-08T17:58:49.622285: step 4607, loss 0.0335644, acc 0.984375
2017-09-08T17:58:50.379009: step 4608, loss 0.00183515, acc 1
2017-09-08T17:58:50.942891: step 4609, loss 0.0012927, acc 1
2017-09-08T17:58:51.572341: step 4610, loss 0.000713849, acc 1
2017-09-08T17:58:52.081724: step 4611, loss 0.00528891, acc 1
2017-09-08T17:58:52.584010: step 4612, loss 0.00129227, acc 1
2017-09-08T17:58:53.257671: step 4613, loss 0.00586288, acc 1
2017-09-08T17:58:53.958960: step 4614, loss 0.00426902, acc 1
2017-09-08T17:58:54.560226: step 4615, loss 0.00159607, acc 1
2017-09-08T17:58:55.228725: step 4616, loss 0.0657831, acc 0.96875
2017-09-08T17:58:55.939537: step 4617, loss 0.051506, acc 0.984375
2017-09-08T17:58:56.646894: step 4618, loss 0.00456583, acc 1
2017-09-08T17:58:57.250681: step 4619, loss 0.000747234, acc 1
2017-09-08T17:58:57.882771: step 4620, loss 0.00252005, acc 1
2017-09-08T17:58:58.507731: step 4621, loss 0.09127, acc 0.953125
2017-09-08T17:58:59.112296: step 4622, loss 0.000629967, acc 1
2017-09-08T17:58:59.799664: step 4623, loss 0.000852377, acc 1
2017-09-08T17:59:00.531645: step 4624, loss 0.0324221, acc 0.984375
2017-09-08T17:59:01.194037: step 4625, loss 0.0129205, acc 0.984375
2017-09-08T17:59:01.942069: step 4626, loss 0.00136776, acc 1
2017-09-08T17:59:02.604215: step 4627, loss 0.0130226, acc 0.984375
2017-09-08T17:59:03.198046: step 4628, loss 0.000879288, acc 1
2017-09-08T17:59:03.779227: step 4629, loss 0.0192479, acc 0.984375
2017-09-08T17:59:04.394085: step 4630, loss 0.00123707, acc 1
2017-09-08T17:59:04.980896: step 4631, loss 0.00144531, acc 1
2017-09-08T17:59:05.692621: step 4632, loss 0.0603044, acc 0.984375
2017-09-08T17:59:06.429598: step 4633, loss 0.000877159, acc 1
2017-09-08T17:59:07.218531: step 4634, loss 0.00218326, acc 1
2017-09-08T17:59:07.947485: step 4635, loss 0.0575629, acc 0.96875
2017-09-08T17:59:08.659483: step 4636, loss 0.0012025, acc 1
2017-09-08T17:59:09.470995: step 4637, loss 0.0215108, acc 0.984375
2017-09-08T17:59:10.140706: step 4638, loss 0.0021501, acc 1
2017-09-08T17:59:10.829196: step 4639, loss 0.0603565, acc 0.984375
2017-09-08T17:59:11.585957: step 4640, loss 0.00142291, acc 1
2017-09-08T17:59:12.290555: step 4641, loss 0.000566034, acc 1
2017-09-08T17:59:12.958463: step 4642, loss 0.0170578, acc 0.984375
2017-09-08T17:59:13.709726: step 4643, loss 0.0070353, acc 1
2017-09-08T17:59:14.516441: step 4644, loss 0.0414017, acc 0.984375
2017-09-08T17:59:15.100384: step 4645, loss 0.00172557, acc 1
2017-09-08T17:59:15.716022: step 4646, loss 0.0140983, acc 1
2017-09-08T17:59:16.411053: step 4647, loss 0.019671, acc 0.984375
2017-09-08T17:59:17.086396: step 4648, loss 0.0111566, acc 1
2017-09-08T17:59:17.742797: step 4649, loss 0.0317928, acc 0.984375
2017-09-08T17:59:18.454603: step 4650, loss 0.00483213, acc 1

Evaluation:
2017-09-08T17:59:19.270980: step 4650, loss 0.209624, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4650

2017-09-08T17:59:22.829811: step 4651, loss 0.0353164, acc 0.984375
2017-09-08T17:59:23.516201: step 4652, loss 0.0010192, acc 1
2017-09-08T17:59:24.258788: step 4653, loss 0.001955, acc 1
2017-09-08T17:59:24.998994: step 4654, loss 0.00218161, acc 1
2017-09-08T17:59:25.755089: step 4655, loss 0.0326268, acc 0.984375
2017-09-08T17:59:26.429412: step 4656, loss 0.00129342, acc 1
2017-09-08T17:59:27.124817: step 4657, loss 0.0140018, acc 1
2017-09-08T17:59:27.813490: step 4658, loss 0.0983109, acc 0.984375
2017-09-08T17:59:28.518228: step 4659, loss 0.0699957, acc 0.96875
2017-09-08T17:59:29.163050: step 4660, loss 0.0601948, acc 0.953125
2017-09-08T17:59:29.765627: step 4661, loss 0.0164495, acc 0.984375
2017-09-08T17:59:30.530576: step 4662, loss 0.0268696, acc 0.984375
2017-09-08T17:59:31.291353: step 4663, loss 0.0567505, acc 0.984375
2017-09-08T17:59:32.043834: step 4664, loss 0.0796984, acc 0.96875
2017-09-08T17:59:32.895040: step 4665, loss 0.0108339, acc 1
2017-09-08T17:59:33.606409: step 4666, loss 0.0090284, acc 1
2017-09-08T17:59:34.339558: step 4667, loss 0.00354357, acc 1
2017-09-08T17:59:35.093437: step 4668, loss 0.0031518, acc 1
2017-09-08T17:59:35.690190: step 4669, loss 0.0112872, acc 1
2017-09-08T17:59:36.355532: step 4670, loss 0.00894188, acc 1
2017-09-08T17:59:36.989206: step 4671, loss 0.00875161, acc 1
2017-09-08T17:59:37.633030: step 4672, loss 0.0237624, acc 0.984375
2017-09-08T17:59:38.266798: step 4673, loss 0.00386795, acc 1
2017-09-08T17:59:39.013610: step 4674, loss 0.0272823, acc 0.984375
2017-09-08T17:59:39.692430: step 4675, loss 0.00210226, acc 1
2017-09-08T17:59:40.376317: step 4676, loss 0.0015137, acc 1
2017-09-08T17:59:41.093106: step 4677, loss 0.00315302, acc 1
2017-09-08T17:59:41.848560: step 4678, loss 0.00929487, acc 1
2017-09-08T17:59:42.533729: step 4679, loss 0.0038511, acc 1
2017-09-08T17:59:43.207855: step 4680, loss 0.00118026, acc 1
2017-09-08T17:59:43.934434: step 4681, loss 0.0473648, acc 0.984375
2017-09-08T17:59:44.653542: step 4682, loss 0.22684, acc 0.953125
2017-09-08T17:59:45.350266: step 4683, loss 0.00329347, acc 1
2017-09-08T17:59:46.125996: step 4684, loss 0.00900456, acc 1
2017-09-08T17:59:46.848146: step 4685, loss 0.0513932, acc 0.96875
2017-09-08T17:59:47.555011: step 4686, loss 0.052245, acc 0.984375
2017-09-08T17:59:48.193640: step 4687, loss 0.00237633, acc 1
2017-09-08T17:59:48.915846: step 4688, loss 0.030227, acc 0.984375
2017-09-08T17:59:49.597042: step 4689, loss 0.0258352, acc 0.984375
2017-09-08T17:59:50.281810: step 4690, loss 0.000600172, acc 1
2017-09-08T17:59:50.955369: step 4691, loss 0.0219765, acc 0.984375
2017-09-08T17:59:51.765635: step 4692, loss 0.0607913, acc 0.96875
2017-09-08T17:59:52.543509: step 4693, loss 0.0509928, acc 0.96875
2017-09-08T17:59:53.184662: step 4694, loss 0.00234522, acc 1
2017-09-08T17:59:53.885232: step 4695, loss 0.0434839, acc 0.984375
2017-09-08T17:59:54.538266: step 4696, loss 0.000405714, acc 1
2017-09-08T17:59:55.261330: step 4697, loss 0.0192157, acc 0.984375
2017-09-08T17:59:56.032573: step 4698, loss 0.00165859, acc 1
2017-09-08T17:59:56.731221: step 4699, loss 0.0167359, acc 0.984375
2017-09-08T17:59:57.456078: step 4700, loss 0.000951638, acc 1

Evaluation:
2017-09-08T17:59:58.156653: step 4700, loss 0.225849, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4700

2017-09-08T18:00:00.890714: step 4701, loss 0.21493, acc 0.953125
2017-09-08T18:00:01.532140: step 4702, loss 0.0366156, acc 0.984375
2017-09-08T18:00:02.204529: step 4703, loss 0.0336032, acc 0.984375
2017-09-08T18:00:02.839885: step 4704, loss 0.000966333, acc 1
2017-09-08T18:00:03.552554: step 4705, loss 0.00199652, acc 1
2017-09-08T18:00:04.282513: step 4706, loss 0.00958268, acc 1
2017-09-08T18:00:04.962095: step 4707, loss 0.000945289, acc 1
2017-09-08T18:00:05.567860: step 4708, loss 0.0537621, acc 0.984375
2017-09-08T18:00:06.192211: step 4709, loss 0.00085119, acc 1
2017-09-08T18:00:06.888073: step 4710, loss 0.00123811, acc 1
2017-09-08T18:00:08.106093: step 4711, loss 0.0119434, acc 1
2017-09-08T18:00:08.799604: step 4712, loss 0.023197, acc 0.984375
2017-09-08T18:00:09.491538: step 4713, loss 0.00103309, acc 1
2017-09-08T18:00:10.216758: step 4714, loss 0.0165065, acc 0.984375
2017-09-08T18:00:10.928500: step 4715, loss 0.00260345, acc 1
2017-09-08T18:00:11.621108: step 4716, loss 0.0229594, acc 1
2017-09-08T18:00:12.362087: step 4717, loss 0.00130888, acc 1
2017-09-08T18:00:13.061428: step 4718, loss 0.00107048, acc 1
2017-09-08T18:00:13.800161: step 4719, loss 0.0143571, acc 1
2017-09-08T18:00:14.498691: step 4720, loss 0.0244228, acc 1
2017-09-08T18:00:15.175471: step 4721, loss 0.0133854, acc 0.984375
2017-09-08T18:00:15.842753: step 4722, loss 0.00071664, acc 1
2017-09-08T18:00:16.535681: step 4723, loss 0.00298453, acc 1
2017-09-08T18:00:17.384759: step 4724, loss 0.00735816, acc 1
2017-09-08T18:00:18.158967: step 4725, loss 0.038066, acc 0.96875
2017-09-08T18:00:18.842089: step 4726, loss 0.000588356, acc 1
2017-09-08T18:00:19.498950: step 4727, loss 0.0208109, acc 0.984375
2017-09-08T18:00:20.196052: step 4728, loss 0.00188833, acc 1
2017-09-08T18:00:20.979939: step 4729, loss 0.048285, acc 0.984375
2017-09-08T18:00:21.673085: step 4730, loss 0.00320712, acc 1
2017-09-08T18:00:22.373551: step 4731, loss 0.0285759, acc 0.984375
2017-09-08T18:00:23.105949: step 4732, loss 0.0169178, acc 0.984375
2017-09-08T18:00:23.836394: step 4733, loss 0.0333707, acc 0.984375
2017-09-08T18:00:24.564883: step 4734, loss 0.000779888, acc 1
2017-09-08T18:00:25.333286: step 4735, loss 0.0179713, acc 0.984375
2017-09-08T18:00:26.080102: step 4736, loss 0.0147283, acc 0.984375
2017-09-08T18:00:26.880366: step 4737, loss 0.00118763, acc 1
2017-09-08T18:00:27.633235: step 4738, loss 0.0146805, acc 1
2017-09-08T18:00:28.448640: step 4739, loss 0.0251031, acc 0.984375
2017-09-08T18:00:29.245728: step 4740, loss 0.0314889, acc 0.984375
2017-09-08T18:00:29.927100: step 4741, loss 0.0750636, acc 0.96875
2017-09-08T18:00:30.718516: step 4742, loss 0.0148741, acc 0.984375
2017-09-08T18:00:31.373236: step 4743, loss 0.00154739, acc 1
2017-09-08T18:00:31.992390: step 4744, loss 0.000528189, acc 1
2017-09-08T18:00:32.627526: step 4745, loss 0.0435927, acc 0.984375
2017-09-08T18:00:33.301101: step 4746, loss 0.0301228, acc 0.984375
2017-09-08T18:00:34.048733: step 4747, loss 0.0958565, acc 0.984375
2017-09-08T18:00:34.785433: step 4748, loss 0.0806637, acc 0.984375
2017-09-08T18:00:35.514303: step 4749, loss 0.00508444, acc 1
2017-09-08T18:00:36.261610: step 4750, loss 0.015477, acc 0.984375

Evaluation:
2017-09-08T18:00:36.984318: step 4750, loss 0.207305, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4750

2017-09-08T18:00:40.085192: step 4751, loss 0.00496749, acc 1
2017-09-08T18:00:40.733818: step 4752, loss 0.0314832, acc 0.984375
2017-09-08T18:00:41.462420: step 4753, loss 0.0235917, acc 0.984375
2017-09-08T18:00:42.166766: step 4754, loss 0.00395174, acc 1
2017-09-08T18:00:42.896903: step 4755, loss 0.00120854, acc 1
2017-09-08T18:00:43.560952: step 4756, loss 0.054289, acc 0.96875
2017-09-08T18:00:44.156596: step 4757, loss 0.0965387, acc 0.96875
2017-09-08T18:00:44.755642: step 4758, loss 0.0053187, acc 1
2017-09-08T18:00:45.344169: step 4759, loss 0.00431455, acc 1
2017-09-08T18:00:46.010318: step 4760, loss 0.00145862, acc 1
2017-09-08T18:00:46.708111: step 4761, loss 0.00642859, acc 1
2017-09-08T18:00:47.257510: step 4762, loss 0.00230149, acc 1
2017-09-08T18:00:47.870128: step 4763, loss 0.0075102, acc 1
2017-09-08T18:00:48.567583: step 4764, loss 0.0620434, acc 0.984375
2017-09-08T18:00:49.204840: step 4765, loss 0.0927553, acc 0.96875
2017-09-08T18:00:49.908684: step 4766, loss 0.00947588, acc 1
2017-09-08T18:00:50.631383: step 4767, loss 0.00783633, acc 1
2017-09-08T18:00:51.495854: step 4768, loss 0.0357014, acc 0.984375
2017-09-08T18:00:52.067549: step 4769, loss 0.000735039, acc 1
2017-09-08T18:00:52.734046: step 4770, loss 0.0148753, acc 0.984375
2017-09-08T18:00:53.420769: step 4771, loss 0.0197075, acc 0.984375
2017-09-08T18:00:54.087234: step 4772, loss 0.0035776, acc 1
2017-09-08T18:00:54.756626: step 4773, loss 0.038463, acc 0.96875
2017-09-08T18:00:55.491001: step 4774, loss 0.048758, acc 0.96875
2017-09-08T18:00:56.209590: step 4775, loss 0.0104048, acc 1
2017-09-08T18:00:56.900595: step 4776, loss 0.0224929, acc 0.984375
2017-09-08T18:00:57.597355: step 4777, loss 0.0122585, acc 1
2017-09-08T18:00:58.330654: step 4778, loss 0.00120805, acc 1
2017-09-08T18:00:59.058538: step 4779, loss 0.00239029, acc 1
2017-09-08T18:00:59.758371: step 4780, loss 0.00234292, acc 1
2017-09-08T18:01:00.498966: step 4781, loss 0.0109149, acc 1
2017-09-08T18:01:01.201913: step 4782, loss 0.00740408, acc 1
2017-09-08T18:01:01.934937: step 4783, loss 0.0123368, acc 1
2017-09-08T18:01:02.638009: step 4784, loss 0.000546058, acc 1
2017-09-08T18:01:03.393861: step 4785, loss 0.000873938, acc 1
2017-09-08T18:01:04.091783: step 4786, loss 0.00119254, acc 1
2017-09-08T18:01:04.797208: step 4787, loss 0.0593682, acc 0.96875
2017-09-08T18:01:05.331791: step 4788, loss 0.0730137, acc 0.984375
2017-09-08T18:01:05.969498: step 4789, loss 0.0404201, acc 0.984375
2017-09-08T18:01:06.609112: step 4790, loss 0.0273996, acc 0.984375
2017-09-08T18:01:07.261074: step 4791, loss 0.0426605, acc 0.984375
2017-09-08T18:01:07.922051: step 4792, loss 0.000655878, acc 1
2017-09-08T18:01:08.598685: step 4793, loss 0.0443942, acc 0.984375
2017-09-08T18:01:09.338951: step 4794, loss 0.160663, acc 0.953125
2017-09-08T18:01:09.981431: step 4795, loss 0.0184097, acc 1
2017-09-08T18:01:10.697351: step 4796, loss 0.00223566, acc 1
2017-09-08T18:01:11.335100: step 4797, loss 0.00378799, acc 1
2017-09-08T18:01:12.033749: step 4798, loss 0.0499865, acc 0.96875
2017-09-08T18:01:12.747766: step 4799, loss 0.000768055, acc 1
2017-09-08T18:01:13.557975: step 4800, loss 0.00963968, acc 1

Evaluation:
2017-09-08T18:01:14.244795: step 4800, loss 0.210881, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4800

2017-09-08T18:01:17.861942: step 4801, loss 0.00158244, acc 1
2017-09-08T18:01:18.535014: step 4802, loss 0.00110302, acc 1
2017-09-08T18:01:19.277859: step 4803, loss 0.00607579, acc 1
2017-09-08T18:01:20.053412: step 4804, loss 0.0296909, acc 0.96875
2017-09-08T18:01:20.858895: step 4805, loss 0.0348466, acc 0.984375
2017-09-08T18:01:21.500683: step 4806, loss 0.000813571, acc 1
2017-09-08T18:01:22.123626: step 4807, loss 0.0108635, acc 1
2017-09-08T18:01:22.770381: step 4808, loss 0.00103568, acc 1
2017-09-08T18:01:23.423564: step 4809, loss 0.0195643, acc 0.984375
2017-09-08T18:01:24.212420: step 4810, loss 0.00189213, acc 1
2017-09-08T18:01:24.884890: step 4811, loss 0.0357111, acc 0.96875
2017-09-08T18:01:25.634978: step 4812, loss 0.00343059, acc 1
2017-09-08T18:01:26.379261: step 4813, loss 0.00424036, acc 1
2017-09-08T18:01:27.077493: step 4814, loss 0.010278, acc 1
2017-09-08T18:01:27.826508: step 4815, loss 0.00428, acc 1
2017-09-08T18:01:28.506162: step 4816, loss 0.00996134, acc 1
2017-09-08T18:01:29.191012: step 4817, loss 0.0341011, acc 0.984375
2017-09-08T18:01:29.786004: step 4818, loss 0.016107, acc 0.984375
2017-09-08T18:01:30.434358: step 4819, loss 0.00329289, acc 1
2017-09-08T18:01:31.058264: step 4820, loss 0.0578394, acc 0.96875
2017-09-08T18:01:31.778802: step 4821, loss 0.00105462, acc 1
2017-09-08T18:01:32.478688: step 4822, loss 0.0216463, acc 0.984375
2017-09-08T18:01:33.248757: step 4823, loss 0.000800332, acc 1
2017-09-08T18:01:33.952276: step 4824, loss 0.000766042, acc 1
2017-09-08T18:01:34.635723: step 4825, loss 0.00435882, acc 1
2017-09-08T18:01:35.350995: step 4826, loss 0.0218209, acc 0.984375
2017-09-08T18:01:36.078792: step 4827, loss 0.0153571, acc 1
2017-09-08T18:01:36.770643: step 4828, loss 0.00433951, acc 1
2017-09-08T18:01:37.534137: step 4829, loss 0.000553317, acc 1
2017-09-08T18:01:38.242328: step 4830, loss 0.0181708, acc 0.984375
2017-09-08T18:01:39.049672: step 4831, loss 0.0270035, acc 0.984375
2017-09-08T18:01:39.863603: step 4832, loss 0.0107797, acc 1
2017-09-08T18:01:40.604779: step 4833, loss 0.00750881, acc 1
2017-09-08T18:01:41.279868: step 4834, loss 0.000587183, acc 1
2017-09-08T18:01:41.996029: step 4835, loss 0.00174719, acc 1
2017-09-08T18:01:42.673638: step 4836, loss 0.009012, acc 1
2017-09-08T18:01:43.404488: step 4837, loss 0.0399981, acc 0.984375
2017-09-08T18:01:44.224808: step 4838, loss 0.00604483, acc 1
2017-09-08T18:01:44.884114: step 4839, loss 0.00862915, acc 1
2017-09-08T18:01:45.561656: step 4840, loss 0.017465, acc 0.984375
2017-09-08T18:01:46.326621: step 4841, loss 0.0147271, acc 1
2017-09-08T18:01:46.918326: step 4842, loss 0.000602791, acc 1
2017-09-08T18:01:47.570298: step 4843, loss 0.0912702, acc 0.984375
2017-09-08T18:01:48.177881: step 4844, loss 0.000352376, acc 1
2017-09-08T18:01:48.831673: step 4845, loss 0.0136347, acc 1
2017-09-08T18:01:49.620842: step 4846, loss 0.00633569, acc 1
2017-09-08T18:01:50.342550: step 4847, loss 0.0287765, acc 0.984375
2017-09-08T18:01:51.075539: step 4848, loss 0.0380843, acc 0.984375
2017-09-08T18:01:51.771013: step 4849, loss 0.00170314, acc 1
2017-09-08T18:01:52.501350: step 4850, loss 0.0006126, acc 1

Evaluation:
2017-09-08T18:01:53.317209: step 4850, loss 0.2113, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4850

2017-09-08T18:01:56.256071: step 4851, loss 0.000686196, acc 1
2017-09-08T18:01:56.914391: step 4852, loss 0.0217168, acc 0.984375
2017-09-08T18:01:57.493036: step 4853, loss 0.000612774, acc 1
2017-09-08T18:01:58.168996: step 4854, loss 0.0472503, acc 0.984375
2017-09-08T18:01:58.834610: step 4855, loss 0.0485393, acc 0.96875
2017-09-08T18:01:59.627358: step 4856, loss 0.0223852, acc 0.984375
2017-09-08T18:02:00.277981: step 4857, loss 0.066829, acc 0.984375
2017-09-08T18:02:00.930225: step 4858, loss 0.0612642, acc 0.96875
2017-09-08T18:02:01.641924: step 4859, loss 0.00102123, acc 1
2017-09-08T18:02:02.313503: step 4860, loss 0.0135141, acc 1
2017-09-08T18:02:03.021964: step 4861, loss 0.103087, acc 0.984375
2017-09-08T18:02:03.773119: step 4862, loss 0.00187567, acc 1
2017-09-08T18:02:04.472397: step 4863, loss 0.0127109, acc 0.984375
2017-09-08T18:02:05.172283: step 4864, loss 0.00659139, acc 1
2017-09-08T18:02:05.886077: step 4865, loss 0.0183917, acc 0.984375
2017-09-08T18:02:06.620612: step 4866, loss 0.0076919, acc 1
2017-09-08T18:02:07.399099: step 4867, loss 0.0463309, acc 0.984375
2017-09-08T18:02:08.104645: step 4868, loss 0.0577199, acc 0.984375
2017-09-08T18:02:08.772847: step 4869, loss 0.00127605, acc 1
2017-09-08T18:02:09.413158: step 4870, loss 0.00335569, acc 1
2017-09-08T18:02:10.045895: step 4871, loss 0.0106693, acc 1
2017-09-08T18:02:10.789287: step 4872, loss 0.0723627, acc 0.984375
2017-09-08T18:02:11.483098: step 4873, loss 0.00206571, acc 1
2017-09-08T18:02:12.205363: step 4874, loss 0.00780031, acc 1
2017-09-08T18:02:12.883387: step 4875, loss 0.00190936, acc 1
2017-09-08T18:02:13.623875: step 4876, loss 0.0108248, acc 1
2017-09-08T18:02:14.275244: step 4877, loss 0.00910022, acc 1
2017-09-08T18:02:14.945133: step 4878, loss 0.0114997, acc 1
2017-09-08T18:02:15.686290: step 4879, loss 0.0380707, acc 0.984375
2017-09-08T18:02:16.307789: step 4880, loss 0.012857, acc 1
2017-09-08T18:02:16.871354: step 4881, loss 0.0906587, acc 0.96875
2017-09-08T18:02:17.511367: step 4882, loss 0.129579, acc 0.96875
2017-09-08T18:02:18.293530: step 4883, loss 0.00827696, acc 1
2017-09-08T18:02:19.032703: step 4884, loss 0.00105542, acc 1
2017-09-08T18:02:19.719135: step 4885, loss 0.0225592, acc 0.984375
2017-09-08T18:02:20.462775: step 4886, loss 0.00118794, acc 1
2017-09-08T18:02:21.188540: step 4887, loss 0.00555356, acc 1
2017-09-08T18:02:21.885268: step 4888, loss 0.00341064, acc 1
2017-09-08T18:02:22.676140: step 4889, loss 0.00356715, acc 1
2017-09-08T18:02:23.372222: step 4890, loss 0.00307883, acc 1
2017-09-08T18:02:24.105530: step 4891, loss 0.00575321, acc 1
2017-09-08T18:02:24.722565: step 4892, loss 0.00215457, acc 1
2017-09-08T18:02:25.346224: step 4893, loss 0.0491637, acc 0.96875
2017-09-08T18:02:25.938672: step 4894, loss 0.0264769, acc 0.984375
2017-09-08T18:02:26.474852: step 4895, loss 0.120052, acc 0.984375
2017-09-08T18:02:26.985159: step 4896, loss 0.0861488, acc 0.984375
2017-09-08T18:02:27.599655: step 4897, loss 0.0451226, acc 0.984375
2017-09-08T18:02:28.307258: step 4898, loss 0.0443289, acc 0.984375
2017-09-08T18:02:29.068335: step 4899, loss 0.000725201, acc 1
2017-09-08T18:02:29.711010: step 4900, loss 0.00679095, acc 1

Evaluation:
2017-09-08T18:02:30.361433: step 4900, loss 0.212963, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4900

2017-09-08T18:02:34.195945: step 4901, loss 0.00329958, acc 1
2017-09-08T18:02:34.960612: step 4902, loss 0.00555167, acc 1
2017-09-08T18:02:35.640786: step 4903, loss 0.0120168, acc 0.984375
2017-09-08T18:02:36.272515: step 4904, loss 0.043524, acc 0.953125
2017-09-08T18:02:36.997709: step 4905, loss 0.000860218, acc 1
2017-09-08T18:02:37.601547: step 4906, loss 0.0439125, acc 0.984375
2017-09-08T18:02:38.293270: step 4907, loss 0.0039761, acc 1
2017-09-08T18:02:38.897888: step 4908, loss 0.00445281, acc 1
2017-09-08T18:02:39.586517: step 4909, loss 0.00100549, acc 1
2017-09-08T18:02:40.253186: step 4910, loss 0.00120062, acc 1
2017-09-08T18:02:40.943151: step 4911, loss 0.00305568, acc 1
2017-09-08T18:02:41.650107: step 4912, loss 0.00728555, acc 1
2017-09-08T18:02:42.296575: step 4913, loss 0.000739542, acc 1
2017-09-08T18:02:42.996608: step 4914, loss 0.000933392, acc 1
2017-09-08T18:02:43.707421: step 4915, loss 0.00153741, acc 1
2017-09-08T18:02:44.463416: step 4916, loss 0.00137081, acc 1
2017-09-08T18:02:45.066923: step 4917, loss 0.00211304, acc 1
2017-09-08T18:02:45.724420: step 4918, loss 0.00460654, acc 1
2017-09-08T18:02:46.409451: step 4919, loss 0.0665801, acc 0.96875
2017-09-08T18:02:47.062013: step 4920, loss 0.00123309, acc 1
2017-09-08T18:02:47.706229: step 4921, loss 0.0146759, acc 0.984375
2017-09-08T18:02:48.415588: step 4922, loss 0.0634196, acc 0.96875
2017-09-08T18:02:49.119614: step 4923, loss 0.0643935, acc 0.984375
2017-09-08T18:02:49.828720: step 4924, loss 0.00690236, acc 1
2017-09-08T18:02:50.549404: step 4925, loss 0.000892242, acc 1
2017-09-08T18:02:51.215214: step 4926, loss 0.003649, acc 1
2017-09-08T18:02:51.933565: step 4927, loss 0.00345779, acc 1
2017-09-08T18:02:52.702311: step 4928, loss 0.063008, acc 0.984375
2017-09-08T18:02:53.429434: step 4929, loss 0.00315298, acc 1
2017-09-08T18:02:54.122720: step 4930, loss 0.0126958, acc 0.984375
2017-09-08T18:02:54.783376: step 4931, loss 0.000745502, acc 1
2017-09-08T18:02:55.535837: step 4932, loss 0.0489027, acc 0.96875
2017-09-08T18:02:56.147928: step 4933, loss 0.00964472, acc 1
2017-09-08T18:02:56.705934: step 4934, loss 0.0670461, acc 0.984375
2017-09-08T18:02:57.271825: step 4935, loss 0.0511648, acc 0.984375
2017-09-08T18:02:57.968594: step 4936, loss 0.00820116, acc 1
2017-09-08T18:02:58.669334: step 4937, loss 0.0474881, acc 0.984375
2017-09-08T18:02:59.415059: step 4938, loss 0.00113643, acc 1
2017-09-08T18:03:00.159199: step 4939, loss 0.00157917, acc 1
2017-09-08T18:03:00.909389: step 4940, loss 0.0112215, acc 1
2017-09-08T18:03:01.627473: step 4941, loss 0.0419695, acc 0.984375
2017-09-08T18:03:02.344591: step 4942, loss 0.00563876, acc 1
2017-09-08T18:03:03.104550: step 4943, loss 0.00255688, acc 1
2017-09-08T18:03:03.793630: step 4944, loss 0.0388093, acc 0.984375
2017-09-08T18:03:04.429657: step 4945, loss 0.0489992, acc 0.96875
2017-09-08T18:03:05.046971: step 4946, loss 0.00423951, acc 1
2017-09-08T18:03:05.748229: step 4947, loss 0.00165972, acc 1
2017-09-08T18:03:06.391483: step 4948, loss 0.0348975, acc 0.984375
2017-09-08T18:03:07.090364: step 4949, loss 0.020041, acc 0.984375
2017-09-08T18:03:07.851851: step 4950, loss 0.00229315, acc 1

Evaluation:
2017-09-08T18:03:08.598438: step 4950, loss 0.21376, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-4950

2017-09-08T18:03:11.461423: step 4951, loss 0.0108363, acc 1
2017-09-08T18:03:12.141619: step 4952, loss 0.0317276, acc 0.984375
2017-09-08T18:03:12.885053: step 4953, loss 0.00287624, acc 1
2017-09-08T18:03:13.614683: step 4954, loss 0.0251201, acc 0.984375
2017-09-08T18:03:14.316100: step 4955, loss 0.00771648, acc 1
2017-09-08T18:03:15.040734: step 4956, loss 0.0311011, acc 0.984375
2017-09-08T18:03:15.757399: step 4957, loss 0.000380325, acc 1
2017-09-08T18:03:16.414764: step 4958, loss 0.0620954, acc 0.984375
2017-09-08T18:03:17.096227: step 4959, loss 0.00252493, acc 1
2017-09-08T18:03:17.760482: step 4960, loss 0.00544417, acc 1
2017-09-08T18:03:18.464128: step 4961, loss 0.00200435, acc 1
2017-09-08T18:03:19.149707: step 4962, loss 0.0135872, acc 0.984375
2017-09-08T18:03:19.817070: step 4963, loss 0.0516709, acc 0.984375
2017-09-08T18:03:20.489639: step 4964, loss 0.00835661, acc 1
2017-09-08T18:03:21.175118: step 4965, loss 0.00180687, acc 1
2017-09-08T18:03:21.792704: step 4966, loss 0.00440292, acc 1
2017-09-08T18:03:22.509848: step 4967, loss 0.0830559, acc 0.984375
2017-09-08T18:03:23.143249: step 4968, loss 0.00336098, acc 1
2017-09-08T18:03:23.794217: step 4969, loss 0.0620354, acc 0.96875
2017-09-08T18:03:24.461818: step 4970, loss 0.0943229, acc 0.953125
2017-09-08T18:03:25.187346: step 4971, loss 0.000575847, acc 1
2017-09-08T18:03:25.903879: step 4972, loss 0.00206652, acc 1
2017-09-08T18:03:26.606831: step 4973, loss 0.0472116, acc 0.984375
2017-09-08T18:03:27.349391: step 4974, loss 0.0493715, acc 0.96875
2017-09-08T18:03:28.028963: step 4975, loss 0.0178651, acc 0.984375
2017-09-08T18:03:28.761772: step 4976, loss 0.000607973, acc 1
2017-09-08T18:03:29.510990: step 4977, loss 0.000663697, acc 1
2017-09-08T18:03:30.259758: step 4978, loss 0.00267391, acc 1
2017-09-08T18:03:31.028227: step 4979, loss 0.0714014, acc 0.96875
2017-09-08T18:03:31.741044: step 4980, loss 0.0105106, acc 1
2017-09-08T18:03:32.432830: step 4981, loss 0.0857554, acc 0.984375
2017-09-08T18:03:33.183871: step 4982, loss 0.0620482, acc 0.984375
2017-09-08T18:03:33.852714: step 4983, loss 0.0299434, acc 0.984375
2017-09-08T18:03:34.489902: step 4984, loss 0.00629205, acc 1
2017-09-08T18:03:35.021578: step 4985, loss 0.0341085, acc 0.984375
2017-09-08T18:03:35.596124: step 4986, loss 0.0772186, acc 0.984375
2017-09-08T18:03:36.288865: step 4987, loss 0.001746, acc 1
2017-09-08T18:03:36.999989: step 4988, loss 0.00145703, acc 1
2017-09-08T18:03:37.660404: step 4989, loss 0.0039646, acc 1
2017-09-08T18:03:38.317645: step 4990, loss 0.0386353, acc 0.96875
2017-09-08T18:03:39.000602: step 4991, loss 0.0142423, acc 0.984375
2017-09-08T18:03:39.733147: step 4992, loss 0.00518092, acc 1
2017-09-08T18:03:40.424717: step 4993, loss 0.159312, acc 0.96875
2017-09-08T18:03:41.172583: step 4994, loss 0.00279682, acc 1
2017-09-08T18:03:41.972260: step 4995, loss 0.0113533, acc 1
2017-09-08T18:03:42.630522: step 4996, loss 0.00387876, acc 1
2017-09-08T18:03:43.264474: step 4997, loss 0.103473, acc 0.96875
2017-09-08T18:03:43.851720: step 4998, loss 0.0050349, acc 1
2017-09-08T18:03:44.581489: step 4999, loss 0.0279009, acc 0.984375
2017-09-08T18:03:45.356867: step 5000, loss 0.000990702, acc 1

Evaluation:
2017-09-08T18:03:46.042236: step 5000, loss 0.21459, acc 0.939568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5000

2017-09-08T18:03:49.282961: step 5001, loss 0.000550391, acc 1
2017-09-08T18:03:49.981566: step 5002, loss 0.037192, acc 0.984375
2017-09-08T18:03:50.695897: step 5003, loss 0.00136952, acc 1
2017-09-08T18:03:51.372968: step 5004, loss 0.00420642, acc 1
2017-09-08T18:03:51.943888: step 5005, loss 0.0389285, acc 0.984375
2017-09-08T18:03:52.579761: step 5006, loss 0.00553946, acc 1
2017-09-08T18:03:53.252069: step 5007, loss 0.00102614, acc 1
2017-09-08T18:03:54.024017: step 5008, loss 0.000241751, acc 1
2017-09-08T18:03:54.642364: step 5009, loss 0.0451378, acc 0.984375
2017-09-08T18:03:55.263811: step 5010, loss 0.00173417, acc 1
2017-09-08T18:03:55.917518: step 5011, loss 0.0558024, acc 0.96875
2017-09-08T18:03:56.553975: step 5012, loss 0.00247384, acc 1
2017-09-08T18:03:57.222977: step 5013, loss 0.0128676, acc 1
2017-09-08T18:03:57.979848: step 5014, loss 0.00527626, acc 1
2017-09-08T18:03:58.602567: step 5015, loss 0.0117806, acc 1
2017-09-08T18:03:59.313050: step 5016, loss 0.00334659, acc 1
2017-09-08T18:04:00.107839: step 5017, loss 0.00121254, acc 1
2017-09-08T18:04:00.712559: step 5018, loss 0.00147462, acc 1
2017-09-08T18:04:01.356097: step 5019, loss 0.000762872, acc 1
2017-09-08T18:04:01.957787: step 5020, loss 0.00313102, acc 1
2017-09-08T18:04:02.651538: step 5021, loss 0.000497424, acc 1
2017-09-08T18:04:03.268323: step 5022, loss 0.00760106, acc 1
2017-09-08T18:04:03.951868: step 5023, loss 0.0765175, acc 0.96875
2017-09-08T18:04:04.690718: step 5024, loss 0.000709752, acc 1
2017-09-08T18:04:05.372703: step 5025, loss 0.00339158, acc 1
2017-09-08T18:04:06.059684: step 5026, loss 0.0187602, acc 0.984375
2017-09-08T18:04:06.696539: step 5027, loss 0.0282515, acc 0.984375
2017-09-08T18:04:07.364191: step 5028, loss 0.00825021, acc 1
2017-09-08T18:04:08.112718: step 5029, loss 0.0128915, acc 0.984375
2017-09-08T18:04:08.875058: step 5030, loss 0.0272388, acc 0.984375
2017-09-08T18:04:09.589112: step 5031, loss 0.0873633, acc 0.984375
2017-09-08T18:04:10.323549: step 5032, loss 0.0034118, acc 1
2017-09-08T18:04:11.021994: step 5033, loss 0.0231935, acc 0.984375
2017-09-08T18:04:11.741625: step 5034, loss 0.0678325, acc 0.984375
2017-09-08T18:04:12.517809: step 5035, loss 0.0342952, acc 0.984375
2017-09-08T18:04:13.277094: step 5036, loss 0.0214956, acc 0.984375
2017-09-08T18:04:13.993839: step 5037, loss 0.00186918, acc 1
2017-09-08T18:04:14.700441: step 5038, loss 0.000461384, acc 1
2017-09-08T18:04:15.414487: step 5039, loss 0.0558155, acc 0.984375
2017-09-08T18:04:16.120169: step 5040, loss 0.0597194, acc 0.984375
2017-09-08T18:04:16.805673: step 5041, loss 0.0277852, acc 0.984375
2017-09-08T18:04:17.645574: step 5042, loss 0.00417843, acc 1
2017-09-08T18:04:18.324954: step 5043, loss 0.000688078, acc 1
2017-09-08T18:04:19.014083: step 5044, loss 0.00192977, acc 1
2017-09-08T18:04:19.759249: step 5045, loss 0.0187343, acc 0.984375
2017-09-08T18:04:20.376130: step 5046, loss 0.022428, acc 0.984375
2017-09-08T18:04:21.001442: step 5047, loss 0.0038804, acc 1
2017-09-08T18:04:21.679984: step 5048, loss 0.0513875, acc 0.984375
2017-09-08T18:04:22.384644: step 5049, loss 0.00595927, acc 1
2017-09-08T18:04:23.096519: step 5050, loss 0.0134506, acc 1

Evaluation:
2017-09-08T18:04:23.853882: step 5050, loss 0.206126, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5050

2017-09-08T18:04:28.262892: step 5051, loss 0.0383253, acc 0.96875
2017-09-08T18:04:29.032881: step 5052, loss 0.00177183, acc 1
2017-09-08T18:04:29.791122: step 5053, loss 0.126679, acc 0.96875
2017-09-08T18:04:30.535288: step 5054, loss 0.0129678, acc 1
2017-09-08T18:04:31.260196: step 5055, loss 0.0172737, acc 1
2017-09-08T18:04:31.898551: step 5056, loss 0.0372077, acc 0.984375
2017-09-08T18:04:32.589278: step 5057, loss 0.000492944, acc 1
2017-09-08T18:04:33.235620: step 5058, loss 0.122985, acc 0.96875
2017-09-08T18:04:33.929090: step 5059, loss 0.000835057, acc 1
2017-09-08T18:04:34.688024: step 5060, loss 0.0175266, acc 0.984375
2017-09-08T18:04:35.483801: step 5061, loss 0.0887152, acc 0.984375
2017-09-08T18:04:36.306212: step 5062, loss 0.0372609, acc 0.984375
2017-09-08T18:04:37.055605: step 5063, loss 0.0187943, acc 0.984375
2017-09-08T18:04:37.790003: step 5064, loss 0.0695543, acc 0.96875
2017-09-08T18:04:38.561946: step 5065, loss 0.0136957, acc 1
2017-09-08T18:04:39.193097: step 5066, loss 0.012648, acc 1
2017-09-08T18:04:39.826792: step 5067, loss 0.00283354, acc 1
2017-09-08T18:04:40.441214: step 5068, loss 0.0154943, acc 1
2017-09-08T18:04:41.121891: step 5069, loss 0.00146275, acc 1
2017-09-08T18:04:41.885261: step 5070, loss 0.00110989, acc 1
2017-09-08T18:04:42.582920: step 5071, loss 0.00113345, acc 1
2017-09-08T18:04:43.294145: step 5072, loss 0.042072, acc 0.984375
2017-09-08T18:04:44.005758: step 5073, loss 0.00161999, acc 1
2017-09-08T18:04:44.755761: step 5074, loss 0.000981262, acc 1
2017-09-08T18:04:45.465298: step 5075, loss 0.0968358, acc 0.96875
2017-09-08T18:04:46.161724: step 5076, loss 0.00321854, acc 1
2017-09-08T18:04:46.911750: step 5077, loss 0.0491845, acc 0.984375
2017-09-08T18:04:47.609763: step 5078, loss 0.000641551, acc 1
2017-09-08T18:04:48.325782: step 5079, loss 0.0683777, acc 0.96875
2017-09-08T18:04:48.964349: step 5080, loss 0.00170645, acc 1
2017-09-08T18:04:49.668359: step 5081, loss 0.0111361, acc 1
2017-09-08T18:04:50.349985: step 5082, loss 0.0428824, acc 0.96875
2017-09-08T18:04:51.037876: step 5083, loss 0.000928979, acc 1
2017-09-08T18:04:51.707396: step 5084, loss 0.00205878, acc 1
2017-09-08T18:04:52.423881: step 5085, loss 0.00145959, acc 1
2017-09-08T18:04:53.126212: step 5086, loss 0.00181485, acc 1
2017-09-08T18:04:53.790872: step 5087, loss 0.0375818, acc 0.984375
2017-09-08T18:04:54.530815: step 5088, loss 0.00166921, acc 1
2017-09-08T18:04:55.243103: step 5089, loss 0.100142, acc 0.96875
2017-09-08T18:04:55.939853: step 5090, loss 0.133565, acc 0.96875
2017-09-08T18:04:56.659608: step 5091, loss 0.056083, acc 0.984375
2017-09-08T18:04:57.356245: step 5092, loss 0.00682443, acc 1
2017-09-08T18:04:57.907966: step 5093, loss 0.06149, acc 0.96875
2017-09-08T18:04:58.480978: step 5094, loss 0.0055051, acc 1
2017-09-08T18:04:59.081958: step 5095, loss 0.0014183, acc 1
2017-09-08T18:04:59.610954: step 5096, loss 0.0017662, acc 1
2017-09-08T18:05:00.264903: step 5097, loss 0.00469556, acc 1
2017-09-08T18:05:00.957107: step 5098, loss 0.00167613, acc 1
2017-09-08T18:05:01.742298: step 5099, loss 0.102867, acc 0.96875
2017-09-08T18:05:02.457076: step 5100, loss 0.00122879, acc 1

Evaluation:
2017-09-08T18:05:03.247678: step 5100, loss 0.205174, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5100

2017-09-08T18:05:05.859369: step 5101, loss 0.00130432, acc 1
2017-09-08T18:05:06.516519: step 5102, loss 0.0012923, acc 1
2017-09-08T18:05:07.226257: step 5103, loss 0.0158136, acc 1
2017-09-08T18:05:08.040788: step 5104, loss 0.00122698, acc 1
2017-09-08T18:05:08.792864: step 5105, loss 0.015153, acc 1
2017-09-08T18:05:09.478433: step 5106, loss 0.000924215, acc 1
2017-09-08T18:05:10.259570: step 5107, loss 0.00133557, acc 1
2017-09-08T18:05:10.842027: step 5108, loss 0.02903, acc 1
2017-09-08T18:05:11.429492: step 5109, loss 0.00700881, acc 1
2017-09-08T18:05:12.065733: step 5110, loss 0.0129692, acc 0.984375
2017-09-08T18:05:12.790782: step 5111, loss 0.00825234, acc 1
2017-09-08T18:05:13.516699: step 5112, loss 0.0258338, acc 0.984375
2017-09-08T18:05:14.264175: step 5113, loss 0.0625688, acc 0.96875
2017-09-08T18:05:14.993924: step 5114, loss 0.000778243, acc 1
2017-09-08T18:05:15.738845: step 5115, loss 0.0202027, acc 0.984375
2017-09-08T18:05:16.495753: step 5116, loss 0.0427164, acc 0.984375
2017-09-08T18:05:17.151140: step 5117, loss 0.0233053, acc 0.984375
2017-09-08T18:05:17.854400: step 5118, loss 0.00671497, acc 1
2017-09-08T18:05:19.037161: step 5119, loss 0.00107322, acc 1
2017-09-08T18:05:19.722321: step 5120, loss 0.000471601, acc 1
2017-09-08T18:05:20.434438: step 5121, loss 0.00254973, acc 1
2017-09-08T18:05:21.145324: step 5122, loss 0.0341839, acc 0.984375
2017-09-08T18:05:21.845472: step 5123, loss 0.0473804, acc 0.984375
2017-09-08T18:05:22.570020: step 5124, loss 0.000447037, acc 1
2017-09-08T18:05:23.266415: step 5125, loss 0.000769555, acc 1
2017-09-08T18:05:23.969835: step 5126, loss 0.00151337, acc 1
2017-09-08T18:05:24.654220: step 5127, loss 0.0408195, acc 0.984375
2017-09-08T18:05:25.343923: step 5128, loss 0.00103458, acc 1
2017-09-08T18:05:25.977713: step 5129, loss 0.00278272, acc 1
2017-09-08T18:05:26.707090: step 5130, loss 0.0032671, acc 1
2017-09-08T18:05:27.399062: step 5131, loss 0.00513058, acc 1
2017-09-08T18:05:28.088405: step 5132, loss 0.00837756, acc 1
2017-09-08T18:05:28.859951: step 5133, loss 0.0309813, acc 0.984375
2017-09-08T18:05:29.561191: step 5134, loss 0.00816715, acc 1
2017-09-08T18:05:30.315538: step 5135, loss 0.00026752, acc 1
2017-09-08T18:05:31.008335: step 5136, loss 0.000894215, acc 1
2017-09-08T18:05:31.730638: step 5137, loss 0.0201752, acc 0.984375
2017-09-08T18:05:32.435649: step 5138, loss 0.0384004, acc 0.984375
2017-09-08T18:05:33.229056: step 5139, loss 0.00422525, acc 1
2017-09-08T18:05:33.907829: step 5140, loss 0.000707629, acc 1
2017-09-08T18:05:34.730484: step 5141, loss 0.0810108, acc 0.984375
2017-09-08T18:05:35.331321: step 5142, loss 0.0758207, acc 0.984375
2017-09-08T18:05:35.933463: step 5143, loss 0.011259, acc 1
2017-09-08T18:05:36.629599: step 5144, loss 0.0150788, acc 1
2017-09-08T18:05:37.330402: step 5145, loss 0.00081306, acc 1
2017-09-08T18:05:37.988364: step 5146, loss 0.0650732, acc 0.96875
2017-09-08T18:05:38.559857: step 5147, loss 0.0439548, acc 0.96875
2017-09-08T18:05:39.478530: step 5148, loss 0.000312857, acc 1
2017-09-08T18:05:40.160256: step 5149, loss 0.0563124, acc 0.984375
2017-09-08T18:05:40.884746: step 5150, loss 0.00116844, acc 1

Evaluation:
2017-09-08T18:05:41.618956: step 5150, loss 0.21547, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5150

2017-09-08T18:05:46.103750: step 5151, loss 0.0400221, acc 0.96875
2017-09-08T18:05:46.645137: step 5152, loss 0.0489426, acc 0.96875
2017-09-08T18:05:47.327483: step 5153, loss 0.000934315, acc 1
2017-09-08T18:05:48.069721: step 5154, loss 0.00129156, acc 1
2017-09-08T18:05:48.720062: step 5155, loss 0.0241928, acc 0.984375
2017-09-08T18:05:49.392785: step 5156, loss 0.0431179, acc 0.984375
2017-09-08T18:05:50.029317: step 5157, loss 0.000441773, acc 1
2017-09-08T18:05:50.667176: step 5158, loss 0.00336352, acc 1
2017-09-08T18:05:51.388714: step 5159, loss 0.00375876, acc 1
2017-09-08T18:05:52.082850: step 5160, loss 0.00175613, acc 1
2017-09-08T18:05:52.871842: step 5161, loss 0.000831203, acc 1
2017-09-08T18:05:53.587934: step 5162, loss 0.0051553, acc 1
2017-09-08T18:05:54.216341: step 5163, loss 0.00656876, acc 1
2017-09-08T18:05:54.889567: step 5164, loss 0.000256992, acc 1
2017-09-08T18:05:55.476178: step 5165, loss 0.000768263, acc 1
2017-09-08T18:05:56.101033: step 5166, loss 0.0302577, acc 0.984375
2017-09-08T18:05:56.803289: step 5167, loss 0.0112532, acc 1
2017-09-08T18:05:57.453705: step 5168, loss 0.0525986, acc 0.984375
2017-09-08T18:05:58.141278: step 5169, loss 0.05883, acc 0.984375
2017-09-08T18:05:58.891633: step 5170, loss 0.0619073, acc 0.984375
2017-09-08T18:05:59.587788: step 5171, loss 0.0105526, acc 1
2017-09-08T18:06:00.208799: step 5172, loss 0.0246965, acc 0.984375
2017-09-08T18:06:00.779286: step 5173, loss 0.000848972, acc 1
2017-09-08T18:06:01.495361: step 5174, loss 0.0260371, acc 0.984375
2017-09-08T18:06:02.213588: step 5175, loss 0.0347619, acc 0.984375
2017-09-08T18:06:03.017140: step 5176, loss 0.0254306, acc 0.984375
2017-09-08T18:06:03.755360: step 5177, loss 0.11789, acc 0.984375
2017-09-08T18:06:04.539478: step 5178, loss 0.0378179, acc 0.984375
2017-09-08T18:06:05.283056: step 5179, loss 0.0110805, acc 1
2017-09-08T18:06:05.926913: step 5180, loss 0.00832365, acc 1
2017-09-08T18:06:06.623056: step 5181, loss 0.00110552, acc 1
2017-09-08T18:06:07.356520: step 5182, loss 0.0181798, acc 0.984375
2017-09-08T18:06:08.081972: step 5183, loss 0.00846711, acc 1
2017-09-08T18:06:08.832985: step 5184, loss 0.0948144, acc 0.96875
2017-09-08T18:06:09.571656: step 5185, loss 0.0288006, acc 0.984375
2017-09-08T18:06:10.274354: step 5186, loss 0.0141697, acc 1
2017-09-08T18:06:10.950489: step 5187, loss 0.0616233, acc 0.96875
2017-09-08T18:06:11.610799: step 5188, loss 0.0144862, acc 0.984375
2017-09-08T18:06:12.365042: step 5189, loss 0.00430405, acc 1
2017-09-08T18:06:13.073566: step 5190, loss 0.000741612, acc 1
2017-09-08T18:06:13.820966: step 5191, loss 0.0071323, acc 1
2017-09-08T18:06:14.465639: step 5192, loss 0.0354121, acc 0.984375
2017-09-08T18:06:15.136054: step 5193, loss 0.00113277, acc 1
2017-09-08T18:06:15.748719: step 5194, loss 0.0387426, acc 0.980392
2017-09-08T18:06:16.494475: step 5195, loss 0.04941, acc 0.984375
2017-09-08T18:06:17.175863: step 5196, loss 0.0809949, acc 0.96875
2017-09-08T18:06:17.853673: step 5197, loss 0.0121252, acc 0.984375
2017-09-08T18:06:18.584182: step 5198, loss 0.00147423, acc 1
2017-09-08T18:06:19.226364: step 5199, loss 0.00233018, acc 1
2017-09-08T18:06:19.913035: step 5200, loss 0.00184799, acc 1

Evaluation:
2017-09-08T18:06:20.627742: step 5200, loss 0.206528, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5200

2017-09-08T18:06:23.275747: step 5201, loss 0.022133, acc 0.984375
2017-09-08T18:06:24.030396: step 5202, loss 0.01491, acc 0.984375
2017-09-08T18:06:24.742544: step 5203, loss 0.0638043, acc 0.984375
2017-09-08T18:06:25.463047: step 5204, loss 0.000385737, acc 1
2017-09-08T18:06:26.134865: step 5205, loss 0.00170157, acc 1
2017-09-08T18:06:26.898011: step 5206, loss 0.000634558, acc 1
2017-09-08T18:06:27.544730: step 5207, loss 0.000675335, acc 1
2017-09-08T18:06:28.230027: step 5208, loss 0.0213737, acc 0.984375
2017-09-08T18:06:28.942411: step 5209, loss 0.011295, acc 1
2017-09-08T18:06:29.638175: step 5210, loss 0.0032459, acc 1
2017-09-08T18:06:30.322678: step 5211, loss 0.000334809, acc 1
2017-09-08T18:06:31.088403: step 5212, loss 0.0425636, acc 0.984375
2017-09-08T18:06:31.785926: step 5213, loss 0.0115145, acc 1
2017-09-08T18:06:32.491239: step 5214, loss 0.00778824, acc 1
2017-09-08T18:06:33.076195: step 5215, loss 0.0737963, acc 0.96875
2017-09-08T18:06:33.699001: step 5216, loss 0.00120942, acc 1
2017-09-08T18:06:34.337928: step 5217, loss 0.0595115, acc 0.96875
2017-09-08T18:06:35.124697: step 5218, loss 0.000720428, acc 1
2017-09-08T18:06:35.808109: step 5219, loss 0.0235202, acc 0.984375
2017-09-08T18:06:36.545026: step 5220, loss 0.00272903, acc 1
2017-09-08T18:06:37.284943: step 5221, loss 0.00121261, acc 1
2017-09-08T18:06:37.955717: step 5222, loss 0.00142453, acc 1
2017-09-08T18:06:38.634941: step 5223, loss 0.00146047, acc 1
2017-09-08T18:06:39.330900: step 5224, loss 0.00106719, acc 1
2017-09-08T18:06:40.008203: step 5225, loss 0.00216722, acc 1
2017-09-08T18:06:40.661425: step 5226, loss 0.0187358, acc 0.984375
2017-09-08T18:06:41.149673: step 5227, loss 0.00982761, acc 1
2017-09-08T18:06:41.775382: step 5228, loss 0.0527373, acc 0.984375
2017-09-08T18:06:42.455883: step 5229, loss 0.000837178, acc 1
2017-09-08T18:06:43.135440: step 5230, loss 0.0285521, acc 0.984375
2017-09-08T18:06:43.865778: step 5231, loss 0.00810254, acc 1
2017-09-08T18:06:44.602693: step 5232, loss 0.00260084, acc 1
2017-09-08T18:06:45.360632: step 5233, loss 0.000727829, acc 1
2017-09-08T18:06:46.091617: step 5234, loss 0.0523732, acc 0.96875
2017-09-08T18:06:46.837649: step 5235, loss 0.000988342, acc 1
2017-09-08T18:06:47.489067: step 5236, loss 0.0141645, acc 0.984375
2017-09-08T18:06:48.278708: step 5237, loss 0.000537481, acc 1
2017-09-08T18:06:48.947700: step 5238, loss 8.72079e-05, acc 1
2017-09-08T18:06:49.623685: step 5239, loss 0.00744494, acc 1
2017-09-08T18:06:50.212562: step 5240, loss 0.000407199, acc 1
2017-09-08T18:06:50.930630: step 5241, loss 0.00947048, acc 1
2017-09-08T18:06:51.665272: step 5242, loss 0.0162688, acc 0.984375
2017-09-08T18:06:52.259095: step 5243, loss 0.0298913, acc 0.984375
2017-09-08T18:06:52.913877: step 5244, loss 0.0373445, acc 0.984375
2017-09-08T18:06:53.534097: step 5245, loss 0.0280496, acc 0.984375
2017-09-08T18:06:54.173332: step 5246, loss 0.0427129, acc 0.96875
2017-09-08T18:06:54.832236: step 5247, loss 0.000745521, acc 1
2017-09-08T18:06:55.491668: step 5248, loss 0.00985333, acc 1
2017-09-08T18:06:56.129859: step 5249, loss 0.0597241, acc 0.984375
2017-09-08T18:06:56.844906: step 5250, loss 0.00232214, acc 1

Evaluation:
2017-09-08T18:06:57.551973: step 5250, loss 0.206173, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5250

2017-09-08T18:07:00.427753: step 5251, loss 0.013734, acc 0.984375
2017-09-08T18:07:01.032456: step 5252, loss 0.000397594, acc 1
2017-09-08T18:07:01.644257: step 5253, loss 0.00411279, acc 1
2017-09-08T18:07:02.372169: step 5254, loss 0.0106708, acc 1
2017-09-08T18:07:03.041654: step 5255, loss 0.0591464, acc 0.984375
2017-09-08T18:07:03.727650: step 5256, loss 0.00114547, acc 1
2017-09-08T18:07:04.420672: step 5257, loss 0.00142498, acc 1
2017-09-08T18:07:05.013535: step 5258, loss 0.0278864, acc 0.984375
2017-09-08T18:07:05.698240: step 5259, loss 0.0364198, acc 0.984375
2017-09-08T18:07:06.363785: step 5260, loss 0.000618534, acc 1
2017-09-08T18:07:06.990194: step 5261, loss 0.00683896, acc 1
2017-09-08T18:07:07.697286: step 5262, loss 0.00443971, acc 1
2017-09-08T18:07:08.426113: step 5263, loss 0.0074709, acc 1
2017-09-08T18:07:09.132661: step 5264, loss 0.0601181, acc 0.96875
2017-09-08T18:07:09.776785: step 5265, loss 0.00429981, acc 1
2017-09-08T18:07:10.464664: step 5266, loss 0.00798274, acc 1
2017-09-08T18:07:11.052851: step 5267, loss 0.124332, acc 0.984375
2017-09-08T18:07:11.686372: step 5268, loss 0.0490094, acc 0.984375
2017-09-08T18:07:12.311093: step 5269, loss 0.000470594, acc 1
2017-09-08T18:07:12.945496: step 5270, loss 0.00531962, acc 1
2017-09-08T18:07:13.548948: step 5271, loss 0.0145605, acc 1
2017-09-08T18:07:14.342420: step 5272, loss 0.0720878, acc 0.984375
2017-09-08T18:07:15.131264: step 5273, loss 0.00862583, acc 1
2017-09-08T18:07:15.859007: step 5274, loss 0.00149341, acc 1
2017-09-08T18:07:16.602953: step 5275, loss 0.0997883, acc 0.953125
2017-09-08T18:07:17.272577: step 5276, loss 0.000790894, acc 1
2017-09-08T18:07:17.962565: step 5277, loss 0.0344597, acc 0.984375
2017-09-08T18:07:18.634114: step 5278, loss 0.000679909, acc 1
2017-09-08T18:07:19.396083: step 5279, loss 0.00148853, acc 1
2017-09-08T18:07:20.090419: step 5280, loss 0.00271952, acc 1
2017-09-08T18:07:21.113131: step 5281, loss 0.0387488, acc 0.984375
2017-09-08T18:07:21.854276: step 5282, loss 0.00127413, acc 1
2017-09-08T18:07:22.566852: step 5283, loss 0.00719158, acc 1
2017-09-08T18:07:23.321794: step 5284, loss 0.0145031, acc 1
2017-09-08T18:07:24.003209: step 5285, loss 0.0443216, acc 0.96875
2017-09-08T18:07:24.647596: step 5286, loss 0.0605557, acc 0.984375
2017-09-08T18:07:25.345786: step 5287, loss 0.0130615, acc 1
2017-09-08T18:07:26.131300: step 5288, loss 0.0279692, acc 0.984375
2017-09-08T18:07:26.846578: step 5289, loss 0.0307156, acc 0.984375
2017-09-08T18:07:27.570902: step 5290, loss 0.00120155, acc 1
2017-09-08T18:07:28.312446: step 5291, loss 0.00342362, acc 1
2017-09-08T18:07:28.921723: step 5292, loss 0.147502, acc 0.941176
2017-09-08T18:07:29.663766: step 5293, loss 0.0416117, acc 0.96875
2017-09-08T18:07:30.316192: step 5294, loss 0.000768509, acc 1
2017-09-08T18:07:30.940153: step 5295, loss 0.00318615, acc 1
2017-09-08T18:07:31.575010: step 5296, loss 0.0185788, acc 0.984375
2017-09-08T18:07:32.269054: step 5297, loss 0.00066472, acc 1
2017-09-08T18:07:32.951930: step 5298, loss 0.00136714, acc 1
2017-09-08T18:07:33.710252: step 5299, loss 0.000356945, acc 1
2017-09-08T18:07:34.374941: step 5300, loss 0.0451006, acc 0.984375

Evaluation:
2017-09-08T18:07:35.119287: step 5300, loss 0.220781, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5300

2017-09-08T18:07:38.691950: step 5301, loss 0.00102326, acc 1
2017-09-08T18:07:39.402082: step 5302, loss 0.00678427, acc 1
2017-09-08T18:07:40.067752: step 5303, loss 0.0250662, acc 0.984375
2017-09-08T18:07:40.634480: step 5304, loss 0.00954729, acc 1
2017-09-08T18:07:41.197342: step 5305, loss 0.000624719, acc 1
2017-09-08T18:07:41.755556: step 5306, loss 0.0012706, acc 1
2017-09-08T18:07:42.445276: step 5307, loss 0.0605165, acc 0.984375
2017-09-08T18:07:43.196716: step 5308, loss 0.00198414, acc 1
2017-09-08T18:07:43.835900: step 5309, loss 0.000974324, acc 1
2017-09-08T18:07:44.444911: step 5310, loss 0.00129137, acc 1
2017-09-08T18:07:45.037843: step 5311, loss 0.0560362, acc 0.984375
2017-09-08T18:07:45.641384: step 5312, loss 0.0218114, acc 0.984375
2017-09-08T18:07:46.391804: step 5313, loss 0.000899032, acc 1
2017-09-08T18:07:47.095764: step 5314, loss 0.000847229, acc 1
2017-09-08T18:07:47.746507: step 5315, loss 0.00262171, acc 1
2017-09-08T18:07:48.483980: step 5316, loss 0.105881, acc 0.96875
2017-09-08T18:07:49.208271: step 5317, loss 0.100341, acc 0.984375
2017-09-08T18:07:49.886812: step 5318, loss 0.00919991, acc 1
2017-09-08T18:07:50.601944: step 5319, loss 0.00988895, acc 1
2017-09-08T18:07:51.226747: step 5320, loss 0.00065407, acc 1
2017-09-08T18:07:51.885568: step 5321, loss 0.0219361, acc 0.984375
2017-09-08T18:07:52.615430: step 5322, loss 0.00192254, acc 1
2017-09-08T18:07:53.384473: step 5323, loss 0.00370138, acc 1
2017-09-08T18:07:54.070820: step 5324, loss 0.00104244, acc 1
2017-09-08T18:07:54.811247: step 5325, loss 0.0389115, acc 0.984375
2017-09-08T18:07:55.519855: step 5326, loss 0.00517645, acc 1
2017-09-08T18:07:56.279711: step 5327, loss 0.105252, acc 0.984375
2017-09-08T18:07:57.021141: step 5328, loss 0.00136092, acc 1
2017-09-08T18:07:57.758350: step 5329, loss 0.000997528, acc 1
2017-09-08T18:07:58.441899: step 5330, loss 0.0888884, acc 0.984375
2017-09-08T18:07:59.105654: step 5331, loss 0.000812283, acc 1
2017-09-08T18:07:59.662265: step 5332, loss 0.0796052, acc 0.984375
2017-09-08T18:08:00.316092: step 5333, loss 0.00125148, acc 1
2017-09-08T18:08:00.979121: step 5334, loss 0.0629534, acc 0.984375
2017-09-08T18:08:01.634288: step 5335, loss 0.000670248, acc 1
2017-09-08T18:08:02.388000: step 5336, loss 0.000559485, acc 1
2017-09-08T18:08:03.059044: step 5337, loss 0.0867235, acc 0.96875
2017-09-08T18:08:03.766425: step 5338, loss 0.000460827, acc 1
2017-09-08T18:08:04.430343: step 5339, loss 0.109941, acc 0.953125
2017-09-08T18:08:05.239297: step 5340, loss 0.00293296, acc 1
2017-09-08T18:08:05.991416: step 5341, loss 0.0211053, acc 1
2017-09-08T18:08:06.811692: step 5342, loss 0.0923165, acc 0.984375
2017-09-08T18:08:07.467047: step 5343, loss 0.0192132, acc 0.984375
2017-09-08T18:08:08.076555: step 5344, loss 0.0126444, acc 1
2017-09-08T18:08:08.676249: step 5345, loss 0.0173547, acc 0.984375
2017-09-08T18:08:09.304635: step 5346, loss 0.0276485, acc 0.984375
2017-09-08T18:08:09.991732: step 5347, loss 0.000620583, acc 1
2017-09-08T18:08:10.670032: step 5348, loss 0.0545387, acc 0.984375
2017-09-08T18:08:11.380948: step 5349, loss 0.00149757, acc 1
2017-09-08T18:08:11.998862: step 5350, loss 0.0148467, acc 0.984375

Evaluation:
2017-09-08T18:08:12.783867: step 5350, loss 0.20426, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5350

2017-09-08T18:08:15.301330: step 5351, loss 0.00141673, acc 1
2017-09-08T18:08:15.852747: step 5352, loss 0.151853, acc 0.953125
2017-09-08T18:08:16.488661: step 5353, loss 0.01097, acc 1
2017-09-08T18:08:17.338823: step 5354, loss 0.0705532, acc 0.984375
2017-09-08T18:08:18.049962: step 5355, loss 0.00514774, acc 1
2017-09-08T18:08:18.797861: step 5356, loss 0.0404036, acc 0.984375
2017-09-08T18:08:19.566953: step 5357, loss 0.00122712, acc 1
2017-09-08T18:08:20.255652: step 5358, loss 0.00368605, acc 1
2017-09-08T18:08:20.909071: step 5359, loss 0.0793268, acc 0.984375
2017-09-08T18:08:21.620049: step 5360, loss 0.0210985, acc 0.984375
2017-09-08T18:08:22.263594: step 5361, loss 0.00206802, acc 1
2017-09-08T18:08:22.934798: step 5362, loss 0.000706013, acc 1
2017-09-08T18:08:23.615914: step 5363, loss 0.0205831, acc 0.984375
2017-09-08T18:08:24.318964: step 5364, loss 0.0315703, acc 0.984375
2017-09-08T18:08:25.024560: step 5365, loss 0.00546955, acc 1
2017-09-08T18:08:25.720210: step 5366, loss 0.0196478, acc 0.984375
2017-09-08T18:08:26.476737: step 5367, loss 0.0998748, acc 0.984375
2017-09-08T18:08:27.263941: step 5368, loss 0.00166291, acc 1
2017-09-08T18:08:27.887371: step 5369, loss 0.0118416, acc 1
2017-09-08T18:08:28.634657: step 5370, loss 0.0258204, acc 0.984375
2017-09-08T18:08:29.321474: step 5371, loss 0.00684534, acc 1
2017-09-08T18:08:29.931140: step 5372, loss 0.00049506, acc 1
2017-09-08T18:08:30.574910: step 5373, loss 0.00619685, acc 1
2017-09-08T18:08:31.173162: step 5374, loss 0.0837061, acc 0.96875
2017-09-08T18:08:31.847150: step 5375, loss 0.03111, acc 0.984375
2017-09-08T18:08:32.576931: step 5376, loss 0.000440308, acc 1
2017-09-08T18:08:33.229213: step 5377, loss 0.0063489, acc 1
2017-09-08T18:08:33.923749: step 5378, loss 0.0520604, acc 0.96875
2017-09-08T18:08:34.621404: step 5379, loss 0.0249773, acc 0.984375
2017-09-08T18:08:35.261095: step 5380, loss 0.0603521, acc 0.984375
2017-09-08T18:08:36.005446: step 5381, loss 0.0013037, acc 1
2017-09-08T18:08:36.651335: step 5382, loss 0.000339919, acc 1
2017-09-08T18:08:37.272470: step 5383, loss 0.0341128, acc 0.984375
2017-09-08T18:08:37.884595: step 5384, loss 0.0015107, acc 1
2017-09-08T18:08:38.452376: step 5385, loss 0.0765515, acc 0.96875
2017-09-08T18:08:39.077589: step 5386, loss 0.00126676, acc 1
2017-09-08T18:08:39.739242: step 5387, loss 0.0440303, acc 0.984375
2017-09-08T18:08:40.425967: step 5388, loss 0.00930057, acc 1
2017-09-08T18:08:41.104889: step 5389, loss 0.0013124, acc 1
2017-09-08T18:08:41.743504: step 5390, loss 0.0014933, acc 1
2017-09-08T18:08:42.411410: step 5391, loss 0.00149446, acc 1
2017-09-08T18:08:43.116315: step 5392, loss 0.053667, acc 0.984375
2017-09-08T18:08:43.839508: step 5393, loss 0.0166774, acc 1
2017-09-08T18:08:44.472988: step 5394, loss 0.0573263, acc 0.984375
2017-09-08T18:08:45.115309: step 5395, loss 0.000346316, acc 1
2017-09-08T18:08:45.744087: step 5396, loss 0.00267752, acc 1
2017-09-08T18:08:46.370905: step 5397, loss 0.0407386, acc 0.96875
2017-09-08T18:08:47.001232: step 5398, loss 0.000427886, acc 1
2017-09-08T18:08:47.729650: step 5399, loss 0.0147144, acc 1
2017-09-08T18:08:48.439816: step 5400, loss 0.00332149, acc 1

Evaluation:
2017-09-08T18:08:49.177580: step 5400, loss 0.207642, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5400

2017-09-08T18:08:53.189776: step 5401, loss 0.00205909, acc 1
2017-09-08T18:08:53.862511: step 5402, loss 0.000779329, acc 1
2017-09-08T18:08:54.546841: step 5403, loss 0.00324516, acc 1
2017-09-08T18:08:55.265214: step 5404, loss 0.00128672, acc 1
2017-09-08T18:08:55.905704: step 5405, loss 0.00485255, acc 1
2017-09-08T18:08:56.565811: step 5406, loss 0.00572737, acc 1
2017-09-08T18:08:57.263611: step 5407, loss 0.0615087, acc 0.96875
2017-09-08T18:08:58.019009: step 5408, loss 0.031078, acc 0.984375
2017-09-08T18:08:58.621090: step 5409, loss 0.0705263, acc 0.96875
2017-09-08T18:08:59.204715: step 5410, loss 0.000939982, acc 1
2017-09-08T18:08:59.866555: step 5411, loss 0.000927311, acc 1
2017-09-08T18:09:00.472378: step 5412, loss 0.0234728, acc 1
2017-09-08T18:09:01.088375: step 5413, loss 0.00479337, acc 1
2017-09-08T18:09:01.763044: step 5414, loss 0.00151121, acc 1
2017-09-08T18:09:02.568609: step 5415, loss 0.00614435, acc 1
2017-09-08T18:09:03.244084: step 5416, loss 0.00388523, acc 1
2017-09-08T18:09:03.927120: step 5417, loss 0.00550553, acc 1
2017-09-08T18:09:04.611201: step 5418, loss 0.0318811, acc 0.984375
2017-09-08T18:09:05.365585: step 5419, loss 0.0006521, acc 1
2017-09-08T18:09:05.960428: step 5420, loss 0.0325119, acc 0.984375
2017-09-08T18:09:06.574214: step 5421, loss 0.023598, acc 0.984375
2017-09-08T18:09:07.142109: step 5422, loss 0.000558302, acc 1
2017-09-08T18:09:07.781336: step 5423, loss 0.00173278, acc 1
2017-09-08T18:09:08.366064: step 5424, loss 0.000962173, acc 1
2017-09-08T18:09:09.026950: step 5425, loss 0.0731112, acc 0.953125
2017-09-08T18:09:09.717463: step 5426, loss 0.0763314, acc 0.96875
2017-09-08T18:09:10.404005: step 5427, loss 0.0368812, acc 0.984375
2017-09-08T18:09:11.105648: step 5428, loss 0.0400006, acc 0.984375
2017-09-08T18:09:11.837888: step 5429, loss 0.00352864, acc 1
2017-09-08T18:09:12.572249: step 5430, loss 0.000751667, acc 1
2017-09-08T18:09:13.340665: step 5431, loss 0.00175349, acc 1
2017-09-08T18:09:14.056119: step 5432, loss 0.0304256, acc 0.984375
2017-09-08T18:09:14.816665: step 5433, loss 0.0167266, acc 0.984375
2017-09-08T18:09:15.554203: step 5434, loss 0.0122545, acc 0.984375
2017-09-08T18:09:16.270404: step 5435, loss 0.0280492, acc 0.984375
2017-09-08T18:09:17.006153: step 5436, loss 0.0300288, acc 0.96875
2017-09-08T18:09:17.766127: step 5437, loss 0.00276866, acc 1
2017-09-08T18:09:18.441677: step 5438, loss 0.00139817, acc 1
2017-09-08T18:09:19.163199: step 5439, loss 0.00107214, acc 1
2017-09-08T18:09:19.774043: step 5440, loss 0.0360649, acc 0.984375
2017-09-08T18:09:20.401802: step 5441, loss 0.00265278, acc 1
2017-09-08T18:09:20.979413: step 5442, loss 0.0327511, acc 0.984375
2017-09-08T18:09:21.689166: step 5443, loss 0.00333627, acc 1
2017-09-08T18:09:22.368010: step 5444, loss 0.112433, acc 0.984375
2017-09-08T18:09:23.129575: step 5445, loss 0.00143338, acc 1
2017-09-08T18:09:23.817463: step 5446, loss 0.013878, acc 1
2017-09-08T18:09:24.486375: step 5447, loss 0.0638896, acc 0.984375
2017-09-08T18:09:25.140984: step 5448, loss 0.0599061, acc 0.984375
2017-09-08T18:09:25.843373: step 5449, loss 0.001519, acc 1
2017-09-08T18:09:26.527101: step 5450, loss 0.00161604, acc 1

Evaluation:
2017-09-08T18:09:27.232453: step 5450, loss 0.214584, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5450

2017-09-08T18:09:29.904777: step 5451, loss 0.0559575, acc 0.984375
2017-09-08T18:09:30.505343: step 5452, loss 0.0424661, acc 0.984375
2017-09-08T18:09:31.183916: step 5453, loss 0.0560088, acc 0.984375
2017-09-08T18:09:31.849494: step 5454, loss 0.0790182, acc 0.96875
2017-09-08T18:09:32.536974: step 5455, loss 0.0024616, acc 1
2017-09-08T18:09:33.224133: step 5456, loss 0.0243602, acc 0.984375
2017-09-08T18:09:33.909858: step 5457, loss 0.0602007, acc 0.984375
2017-09-08T18:09:34.574690: step 5458, loss 0.0570155, acc 0.984375
2017-09-08T18:09:35.195178: step 5459, loss 0.000582479, acc 1
2017-09-08T18:09:35.932553: step 5460, loss 0.00256494, acc 1
2017-09-08T18:09:36.560838: step 5461, loss 0.00162183, acc 1
2017-09-08T18:09:37.155033: step 5462, loss 0.0660955, acc 0.96875
2017-09-08T18:09:37.794678: step 5463, loss 0.0504014, acc 0.96875
2017-09-08T18:09:38.413603: step 5464, loss 0.00581592, acc 1
2017-09-08T18:09:39.153034: step 5465, loss 0.0675436, acc 0.984375
2017-09-08T18:09:39.825118: step 5466, loss 0.000976908, acc 1
2017-09-08T18:09:40.400506: step 5467, loss 0.00399367, acc 1
2017-09-08T18:09:40.986508: step 5468, loss 0.0179517, acc 0.984375
2017-09-08T18:09:41.698372: step 5469, loss 0.0033212, acc 1
2017-09-08T18:09:42.360861: step 5470, loss 0.0136675, acc 0.984375
2017-09-08T18:09:43.438978: step 5471, loss 0.000704428, acc 1
2017-09-08T18:09:44.047266: step 5472, loss 0.0189982, acc 0.984375
2017-09-08T18:09:44.641188: step 5473, loss 0.000641946, acc 1
2017-09-08T18:09:45.340420: step 5474, loss 0.00127728, acc 1
2017-09-08T18:09:46.072162: step 5475, loss 0.00130557, acc 1
2017-09-08T18:09:46.732733: step 5476, loss 0.0330487, acc 0.984375
2017-09-08T18:09:47.431399: step 5477, loss 0.0812456, acc 0.96875
2017-09-08T18:09:48.140663: step 5478, loss 0.000380607, acc 1
2017-09-08T18:09:48.826482: step 5479, loss 0.00629589, acc 1
2017-09-08T18:09:49.499653: step 5480, loss 0.0390938, acc 0.984375
2017-09-08T18:09:50.200666: step 5481, loss 0.0763037, acc 0.984375
2017-09-08T18:09:50.839225: step 5482, loss 0.016353, acc 0.984375
2017-09-08T18:09:51.454036: step 5483, loss 0.00115033, acc 1
2017-09-08T18:09:52.033717: step 5484, loss 0.0646975, acc 0.96875
2017-09-08T18:09:52.554907: step 5485, loss 0.000411418, acc 1
2017-09-08T18:09:53.127721: step 5486, loss 0.00147101, acc 1
2017-09-08T18:09:53.787106: step 5487, loss 0.0302903, acc 0.984375
2017-09-08T18:09:54.341957: step 5488, loss 0.0262272, acc 0.980392
2017-09-08T18:09:55.088821: step 5489, loss 0.000495742, acc 1
2017-09-08T18:09:55.732101: step 5490, loss 0.0014728, acc 1
2017-09-08T18:09:56.359895: step 5491, loss 0.00149994, acc 1
2017-09-08T18:09:56.994202: step 5492, loss 0.00677592, acc 1
2017-09-08T18:09:57.542633: step 5493, loss 0.00339889, acc 1
2017-09-08T18:09:58.085712: step 5494, loss 0.0766479, acc 0.96875
2017-09-08T18:09:58.647656: step 5495, loss 0.00486678, acc 1
2017-09-08T18:09:59.263461: step 5496, loss 0.000562304, acc 1
2017-09-08T18:09:59.861540: step 5497, loss 0.00421297, acc 1
2017-09-08T18:10:00.500733: step 5498, loss 0.00250415, acc 1
2017-09-08T18:10:01.165774: step 5499, loss 0.0107921, acc 1
2017-09-08T18:10:01.906331: step 5500, loss 0.00657803, acc 1

Evaluation:
2017-09-08T18:10:02.585760: step 5500, loss 0.206701, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5500

2017-09-08T18:10:05.803310: step 5501, loss 0.00661392, acc 1
2017-09-08T18:10:06.557217: step 5502, loss 0.0281581, acc 0.984375
2017-09-08T18:10:07.179111: step 5503, loss 0.127433, acc 0.96875
2017-09-08T18:10:07.893987: step 5504, loss 0.000363274, acc 1
2017-09-08T18:10:08.559060: step 5505, loss 0.0333747, acc 0.984375
2017-09-08T18:10:09.286261: step 5506, loss 0.00278693, acc 1
2017-09-08T18:10:09.966283: step 5507, loss 0.016455, acc 0.984375
2017-09-08T18:10:10.741313: step 5508, loss 0.0010907, acc 1
2017-09-08T18:10:11.462797: step 5509, loss 0.000598988, acc 1
2017-09-08T18:10:12.055468: step 5510, loss 0.0436025, acc 0.984375
2017-09-08T18:10:12.731300: step 5511, loss 0.0460498, acc 0.984375
2017-09-08T18:10:13.339758: step 5512, loss 0.00658542, acc 1
2017-09-08T18:10:14.007798: step 5513, loss 0.0351529, acc 0.984375
2017-09-08T18:10:14.622737: step 5514, loss 0.0093855, acc 1
2017-09-08T18:10:15.277792: step 5515, loss 0.000402265, acc 1
2017-09-08T18:10:15.907216: step 5516, loss 0.0151083, acc 0.984375
2017-09-08T18:10:16.567924: step 5517, loss 0.0199606, acc 0.984375
2017-09-08T18:10:17.248251: step 5518, loss 0.0227075, acc 0.984375
2017-09-08T18:10:18.010053: step 5519, loss 0.00961252, acc 1
2017-09-08T18:10:18.592688: step 5520, loss 0.00700243, acc 1
2017-09-08T18:10:19.158830: step 5521, loss 0.0112575, acc 1
2017-09-08T18:10:19.787486: step 5522, loss 0.0522469, acc 0.96875
2017-09-08T18:10:20.394458: step 5523, loss 0.0155098, acc 0.984375
2017-09-08T18:10:21.025324: step 5524, loss 0.000469946, acc 1
2017-09-08T18:10:21.713057: step 5525, loss 0.0233829, acc 0.984375
2017-09-08T18:10:22.398987: step 5526, loss 0.0969903, acc 0.96875
2017-09-08T18:10:23.105879: step 5527, loss 0.000509318, acc 1
2017-09-08T18:10:23.852718: step 5528, loss 0.00156012, acc 1
2017-09-08T18:10:24.545386: step 5529, loss 0.0933268, acc 0.984375
2017-09-08T18:10:25.233936: step 5530, loss 0.00620185, acc 1
2017-09-08T18:10:25.888774: step 5531, loss 0.00106388, acc 1
2017-09-08T18:10:26.591288: step 5532, loss 0.000937346, acc 1
2017-09-08T18:10:27.356216: step 5533, loss 0.0207829, acc 0.984375
2017-09-08T18:10:28.064803: step 5534, loss 0.0660071, acc 0.96875
2017-09-08T18:10:28.733308: step 5535, loss 0.00462694, acc 1
2017-09-08T18:10:29.475765: step 5536, loss 0.0700232, acc 0.984375
2017-09-08T18:10:30.174294: step 5537, loss 0.0155747, acc 1
2017-09-08T18:10:30.887826: step 5538, loss 0.0658738, acc 0.96875
2017-09-08T18:10:31.598400: step 5539, loss 0.0405943, acc 0.96875
2017-09-08T18:10:32.339346: step 5540, loss 0.115072, acc 0.953125
2017-09-08T18:10:32.948646: step 5541, loss 0.00485089, acc 1
2017-09-08T18:10:33.551659: step 5542, loss 0.00117598, acc 1
2017-09-08T18:10:34.258112: step 5543, loss 0.00199273, acc 1
2017-09-08T18:10:34.945677: step 5544, loss 0.035102, acc 0.984375
2017-09-08T18:10:35.585227: step 5545, loss 0.00114046, acc 1
2017-09-08T18:10:36.192069: step 5546, loss 0.00187322, acc 1
2017-09-08T18:10:36.839563: step 5547, loss 0.00120843, acc 1
2017-09-08T18:10:37.484667: step 5548, loss 0.00475785, acc 1
2017-09-08T18:10:38.162721: step 5549, loss 0.0243174, acc 0.984375
2017-09-08T18:10:38.834570: step 5550, loss 0.0274156, acc 0.984375

Evaluation:
2017-09-08T18:10:39.493930: step 5550, loss 0.231049, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5550

2017-09-08T18:10:42.727754: step 5551, loss 0.00118355, acc 1
2017-09-08T18:10:43.197634: step 5552, loss 0.0194265, acc 0.984375
2017-09-08T18:10:43.834237: step 5553, loss 0.07355, acc 0.984375
2017-09-08T18:10:44.461611: step 5554, loss 0.0011677, acc 1
2017-09-08T18:10:45.061743: step 5555, loss 0.00309965, acc 1
2017-09-08T18:10:45.722830: step 5556, loss 0.000674437, acc 1
2017-09-08T18:10:46.321792: step 5557, loss 0.000402043, acc 1
2017-09-08T18:10:47.017501: step 5558, loss 0.0194966, acc 1
2017-09-08T18:10:47.602245: step 5559, loss 0.0391168, acc 0.984375
2017-09-08T18:10:48.162475: step 5560, loss 0.0883865, acc 0.96875
2017-09-08T18:10:48.745406: step 5561, loss 0.00105355, acc 1
2017-09-08T18:10:49.320327: step 5562, loss 0.00278735, acc 1
2017-09-08T18:10:49.710275: step 5563, loss 0.071694, acc 0.984375
2017-09-08T18:10:50.354840: step 5564, loss 0.0337087, acc 0.984375
2017-09-08T18:10:51.001899: step 5565, loss 0.0189701, acc 1
2017-09-08T18:10:51.670816: step 5566, loss 0.00106573, acc 1
2017-09-08T18:10:52.284620: step 5567, loss 0.00170771, acc 1
2017-09-08T18:10:52.892589: step 5568, loss 0.00895545, acc 1
2017-09-08T18:10:53.552343: step 5569, loss 0.00377995, acc 1
2017-09-08T18:10:54.281658: step 5570, loss 0.000498239, acc 1
2017-09-08T18:10:54.801277: step 5571, loss 0.0126997, acc 1
2017-09-08T18:10:55.376185: step 5572, loss 0.0547394, acc 0.984375
2017-09-08T18:10:55.975931: step 5573, loss 0.0338874, acc 0.984375
2017-09-08T18:10:56.611527: step 5574, loss 0.00300752, acc 1
2017-09-08T18:10:57.227934: step 5575, loss 0.0623991, acc 0.96875
2017-09-08T18:10:57.874176: step 5576, loss 0.0286279, acc 0.984375
2017-09-08T18:10:58.534030: step 5577, loss 0.00968024, acc 1
2017-09-08T18:10:59.153131: step 5578, loss 0.00381984, acc 1
2017-09-08T18:10:59.779933: step 5579, loss 0.0208169, acc 0.984375
2017-09-08T18:11:00.421164: step 5580, loss 0.00997859, acc 1
2017-09-08T18:11:01.051112: step 5581, loss 0.0112639, acc 1
2017-09-08T18:11:01.657339: step 5582, loss 0.00816797, acc 1
2017-09-08T18:11:02.170793: step 5583, loss 0.00965345, acc 1
2017-09-08T18:11:02.687157: step 5584, loss 0.00143602, acc 1
2017-09-08T18:11:03.313564: step 5585, loss 0.000491841, acc 1
2017-09-08T18:11:03.882688: step 5586, loss 0.00294796, acc 1
2017-09-08T18:11:04.487242: step 5587, loss 0.000878694, acc 1
2017-09-08T18:11:05.135457: step 5588, loss 0.000577699, acc 1
2017-09-08T18:11:05.788786: step 5589, loss 0.00134391, acc 1
2017-09-08T18:11:06.464349: step 5590, loss 0.000490047, acc 1
2017-09-08T18:11:07.061983: step 5591, loss 0.0037677, acc 1
2017-09-08T18:11:07.619957: step 5592, loss 0.00630836, acc 1
2017-09-08T18:11:08.198008: step 5593, loss 0.0384679, acc 0.96875
2017-09-08T18:11:08.766512: step 5594, loss 0.00144593, acc 1
2017-09-08T18:11:09.404948: step 5595, loss 0.000848117, acc 1
2017-09-08T18:11:10.016885: step 5596, loss 0.0167778, acc 0.984375
2017-09-08T18:11:10.652474: step 5597, loss 0.0162887, acc 1
2017-09-08T18:11:11.268454: step 5598, loss 0.0120831, acc 1
2017-09-08T18:11:11.813727: step 5599, loss 0.00505654, acc 1
2017-09-08T18:11:12.299046: step 5600, loss 0.0500935, acc 0.984375

Evaluation:
2017-09-08T18:11:13.000809: step 5600, loss 0.210907, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5600

2017-09-08T18:11:15.466126: step 5601, loss 0.010836, acc 1
2017-09-08T18:11:16.310841: step 5602, loss 0.0159619, acc 0.984375
2017-09-08T18:11:16.960934: step 5603, loss 0.0166755, acc 1
2017-09-08T18:11:17.636880: step 5604, loss 0.00261381, acc 1
2017-09-08T18:11:18.303217: step 5605, loss 0.00205773, acc 1
2017-09-08T18:11:18.920540: step 5606, loss 0.000337756, acc 1
2017-09-08T18:11:19.592935: step 5607, loss 0.00141786, acc 1
2017-09-08T18:11:20.234891: step 5608, loss 0.000670345, acc 1
2017-09-08T18:11:20.896847: step 5609, loss 0.00242606, acc 1
2017-09-08T18:11:21.577919: step 5610, loss 0.00112027, acc 1
2017-09-08T18:11:22.220779: step 5611, loss 0.000497077, acc 1
2017-09-08T18:11:22.830557: step 5612, loss 0.000837991, acc 1
2017-09-08T18:11:23.403886: step 5613, loss 0.0272363, acc 1
2017-09-08T18:11:24.028492: step 5614, loss 0.000355086, acc 1
2017-09-08T18:11:24.697091: step 5615, loss 0.0211143, acc 0.984375
2017-09-08T18:11:25.329068: step 5616, loss 0.000862903, acc 1
2017-09-08T18:11:26.008732: step 5617, loss 0.000966613, acc 1
2017-09-08T18:11:26.741656: step 5618, loss 0.0260176, acc 0.984375
2017-09-08T18:11:27.407950: step 5619, loss 0.00248942, acc 1
2017-09-08T18:11:28.051353: step 5620, loss 0.00068212, acc 1
2017-09-08T18:11:28.742508: step 5621, loss 0.00248467, acc 1
2017-09-08T18:11:29.293511: step 5622, loss 0.01782, acc 0.984375
2017-09-08T18:11:29.862601: step 5623, loss 0.118387, acc 0.96875
2017-09-08T18:11:30.438689: step 5624, loss 0.0138747, acc 1
2017-09-08T18:11:31.088031: step 5625, loss 0.00426762, acc 1
2017-09-08T18:11:31.739152: step 5626, loss 0.00690597, acc 1
2017-09-08T18:11:32.437327: step 5627, loss 0.0107583, acc 1
2017-09-08T18:11:33.055567: step 5628, loss 0.000636968, acc 1
2017-09-08T18:11:33.666394: step 5629, loss 0.000960637, acc 1
2017-09-08T18:11:34.280613: step 5630, loss 0.00892246, acc 1
2017-09-08T18:11:34.976032: step 5631, loss 0.00140766, acc 1
2017-09-08T18:11:35.642338: step 5632, loss 0.0462929, acc 0.984375
2017-09-08T18:11:36.287538: step 5633, loss 0.000917769, acc 1
2017-09-08T18:11:36.935783: step 5634, loss 0.0110597, acc 1
2017-09-08T18:11:37.580409: step 5635, loss 0.0104151, acc 1
2017-09-08T18:11:38.202360: step 5636, loss 0.00816731, acc 1
2017-09-08T18:11:38.842249: step 5637, loss 0.0461016, acc 0.984375
2017-09-08T18:11:39.508452: step 5638, loss 0.0759169, acc 0.984375
2017-09-08T18:11:40.183221: step 5639, loss 0.00128446, acc 1
2017-09-08T18:11:40.885117: step 5640, loss 0.00813499, acc 1
2017-09-08T18:11:41.467221: step 5641, loss 0.00158346, acc 1
2017-09-08T18:11:42.058564: step 5642, loss 0.0131198, acc 0.984375
2017-09-08T18:11:42.649446: step 5643, loss 0.159849, acc 0.953125
2017-09-08T18:11:43.221767: step 5644, loss 0.0154983, acc 1
2017-09-08T18:11:43.876154: step 5645, loss 0.00115163, acc 1
2017-09-08T18:11:44.550761: step 5646, loss 0.0181056, acc 0.984375
2017-09-08T18:11:45.208042: step 5647, loss 0.00680838, acc 1
2017-09-08T18:11:45.901911: step 5648, loss 0.0281013, acc 0.984375
2017-09-08T18:11:46.569392: step 5649, loss 0.00830872, acc 1
2017-09-08T18:11:47.237934: step 5650, loss 0.059756, acc 0.984375

Evaluation:
2017-09-08T18:11:47.884060: step 5650, loss 0.204351, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5650

2017-09-08T18:11:51.136226: step 5651, loss 0.0602478, acc 0.96875
2017-09-08T18:11:52.078074: step 5652, loss 0.00723688, acc 1
2017-09-08T18:11:52.748013: step 5653, loss 0.0214262, acc 1
2017-09-08T18:11:53.400293: step 5654, loss 0.0562526, acc 0.984375
2017-09-08T18:11:54.005257: step 5655, loss 0.00578789, acc 1
2017-09-08T18:11:54.646043: step 5656, loss 0.0406277, acc 0.984375
2017-09-08T18:11:55.256879: step 5657, loss 0.000879433, acc 1
2017-09-08T18:11:55.930146: step 5658, loss 0.0665444, acc 0.96875
2017-09-08T18:11:56.730319: step 5659, loss 0.00754436, acc 1
2017-09-08T18:11:57.290384: step 5660, loss 0.0016257, acc 1
2017-09-08T18:11:57.899534: step 5661, loss 0.00120182, acc 1
2017-09-08T18:11:58.477883: step 5662, loss 0.0033315, acc 1
2017-09-08T18:11:59.075858: step 5663, loss 0.130907, acc 0.96875
2017-09-08T18:11:59.644282: step 5664, loss 0.00055211, acc 1
2017-09-08T18:12:00.222193: step 5665, loss 0.0012369, acc 1
2017-09-08T18:12:00.868179: step 5666, loss 0.0210208, acc 0.984375
2017-09-08T18:12:01.599809: step 5667, loss 0.0215141, acc 0.984375
2017-09-08T18:12:02.834756: step 5668, loss 0.0140936, acc 1
2017-09-08T18:12:03.408803: step 5669, loss 0.0478436, acc 0.96875
2017-09-08T18:12:03.974905: step 5670, loss 0.0293348, acc 0.984375
2017-09-08T18:12:04.561674: step 5671, loss 0.00133241, acc 1
2017-09-08T18:12:05.251360: step 5672, loss 0.045063, acc 0.96875
2017-09-08T18:12:05.913736: step 5673, loss 0.0413346, acc 0.984375
2017-09-08T18:12:06.568690: step 5674, loss 0.0274136, acc 0.984375
2017-09-08T18:12:07.202534: step 5675, loss 0.0185007, acc 0.984375
2017-09-08T18:12:07.891709: step 5676, loss 0.00349999, acc 1
2017-09-08T18:12:08.532044: step 5677, loss 0.0106768, acc 1
2017-09-08T18:12:09.210762: step 5678, loss 0.0783309, acc 0.96875
2017-09-08T18:12:09.922145: step 5679, loss 0.00158324, acc 1
2017-09-08T18:12:10.562635: step 5680, loss 0.000321271, acc 1
2017-09-08T18:12:11.256437: step 5681, loss 0.037449, acc 0.984375
2017-09-08T18:12:11.955487: step 5682, loss 0.00250144, acc 1
2017-09-08T18:12:12.653762: step 5683, loss 0.000701131, acc 1
2017-09-08T18:12:13.281967: step 5684, loss 0.0262925, acc 0.980392
2017-09-08T18:12:13.965330: step 5685, loss 0.00143888, acc 1
2017-09-08T18:12:14.658791: step 5686, loss 0.0014836, acc 1
2017-09-08T18:12:15.414208: step 5687, loss 0.0251709, acc 0.984375
2017-09-08T18:12:16.026604: step 5688, loss 0.0075236, acc 1
2017-09-08T18:12:16.597661: step 5689, loss 0.00576952, acc 1
2017-09-08T18:12:17.164389: step 5690, loss 0.00129444, acc 1
2017-09-08T18:12:17.884032: step 5691, loss 0.00134607, acc 1
2017-09-08T18:12:18.495141: step 5692, loss 0.00473923, acc 1
2017-09-08T18:12:19.159079: step 5693, loss 0.0325422, acc 0.984375
2017-09-08T18:12:19.818397: step 5694, loss 0.00311892, acc 1
2017-09-08T18:12:20.498783: step 5695, loss 0.0701733, acc 0.984375
2017-09-08T18:12:21.130385: step 5696, loss 0.0006996, acc 1
2017-09-08T18:12:21.803850: step 5697, loss 0.000977891, acc 1
2017-09-08T18:12:22.432552: step 5698, loss 0.00844858, acc 1
2017-09-08T18:12:23.091836: step 5699, loss 0.000446724, acc 1
2017-09-08T18:12:23.749358: step 5700, loss 0.000743073, acc 1

Evaluation:
2017-09-08T18:12:24.421566: step 5700, loss 0.201671, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5700

2017-09-08T18:12:27.053504: step 5701, loss 0.0242304, acc 0.984375
2017-09-08T18:12:27.617932: step 5702, loss 0.00732499, acc 1
2017-09-08T18:12:28.247304: step 5703, loss 0.000862651, acc 1
2017-09-08T18:12:28.913449: step 5704, loss 0.0454435, acc 0.984375
2017-09-08T18:12:29.512640: step 5705, loss 0.000311198, acc 1
2017-09-08T18:12:30.134653: step 5706, loss 0.00667537, acc 1
2017-09-08T18:12:30.784793: step 5707, loss 0.00264453, acc 1
2017-09-08T18:12:31.429064: step 5708, loss 0.00115497, acc 1
2017-09-08T18:12:32.141330: step 5709, loss 0.00332669, acc 1
2017-09-08T18:12:32.748836: step 5710, loss 0.0437646, acc 0.96875
2017-09-08T18:12:33.341813: step 5711, loss 0.066298, acc 0.96875
2017-09-08T18:12:33.903870: step 5712, loss 0.0118002, acc 1
2017-09-08T18:12:34.633796: step 5713, loss 0.00083833, acc 1
2017-09-08T18:12:35.283818: step 5714, loss 0.0178395, acc 0.984375
2017-09-08T18:12:35.979387: step 5715, loss 0.00986212, acc 1
2017-09-08T18:12:36.649587: step 5716, loss 0.000443789, acc 1
2017-09-08T18:12:37.267601: step 5717, loss 0.0164909, acc 0.984375
2017-09-08T18:12:37.844059: step 5718, loss 0.00435155, acc 1
2017-09-08T18:12:38.451575: step 5719, loss 0.0747241, acc 0.984375
2017-09-08T18:12:38.997163: step 5720, loss 0.000591656, acc 1
2017-09-08T18:12:39.664971: step 5721, loss 0.0414706, acc 0.984375
2017-09-08T18:12:40.245916: step 5722, loss 0.00142941, acc 1
2017-09-08T18:12:40.864544: step 5723, loss 0.000272921, acc 1
2017-09-08T18:12:41.496545: step 5724, loss 0.0366704, acc 0.984375
2017-09-08T18:12:42.150616: step 5725, loss 0.00100509, acc 1
2017-09-08T18:12:42.757261: step 5726, loss 0.000542058, acc 1
2017-09-08T18:12:43.351802: step 5727, loss 0.000800845, acc 1
2017-09-08T18:12:44.035675: step 5728, loss 0.000960195, acc 1
2017-09-08T18:12:44.683150: step 5729, loss 0.0407861, acc 0.984375
2017-09-08T18:12:45.348506: step 5730, loss 0.000558558, acc 1
2017-09-08T18:12:46.010442: step 5731, loss 0.0938593, acc 0.984375
2017-09-08T18:12:46.707683: step 5732, loss 0.0126893, acc 1
2017-09-08T18:12:47.323735: step 5733, loss 0.00259723, acc 1
2017-09-08T18:12:48.001292: step 5734, loss 0.0718781, acc 0.984375
2017-09-08T18:12:48.745136: step 5735, loss 0.00754977, acc 1
2017-09-08T18:12:49.475814: step 5736, loss 0.00210724, acc 1
2017-09-08T18:12:50.056295: step 5737, loss 0.00173881, acc 1
2017-09-08T18:12:50.655948: step 5738, loss 0.0893001, acc 0.984375
2017-09-08T18:12:51.239342: step 5739, loss 0.0939543, acc 0.96875
2017-09-08T18:12:51.867679: step 5740, loss 0.110309, acc 0.96875
2017-09-08T18:12:52.427444: step 5741, loss 0.000805171, acc 1
2017-09-08T18:12:53.090355: step 5742, loss 0.0260635, acc 0.984375
2017-09-08T18:12:53.732302: step 5743, loss 0.000826074, acc 1
2017-09-08T18:12:54.416220: step 5744, loss 0.0716504, acc 0.96875
2017-09-08T18:12:55.114839: step 5745, loss 0.00815895, acc 1
2017-09-08T18:12:55.696031: step 5746, loss 0.00694231, acc 1
2017-09-08T18:12:56.359108: step 5747, loss 0.00529683, acc 1
2017-09-08T18:12:56.980020: step 5748, loss 0.0176167, acc 0.984375
2017-09-08T18:12:57.643268: step 5749, loss 0.000733157, acc 1
2017-09-08T18:12:58.253375: step 5750, loss 0.00138272, acc 1

Evaluation:
2017-09-08T18:12:59.327947: step 5750, loss 0.196121, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5750

2017-09-08T18:13:02.277621: step 5751, loss 0.00162109, acc 1
2017-09-08T18:13:02.922287: step 5752, loss 0.0666036, acc 0.96875
2017-09-08T18:13:03.494284: step 5753, loss 0.00292836, acc 1
2017-09-08T18:13:03.969010: step 5754, loss 0.0273925, acc 0.984375
2017-09-08T18:13:04.528033: step 5755, loss 0.00051775, acc 1
2017-09-08T18:13:05.166835: step 5756, loss 0.00318542, acc 1
2017-09-08T18:13:05.787610: step 5757, loss 0.0642105, acc 0.984375
2017-09-08T18:13:06.476543: step 5758, loss 0.0985148, acc 0.984375
2017-09-08T18:13:07.062482: step 5759, loss 0.00145364, acc 1
2017-09-08T18:13:07.662125: step 5760, loss 0.0139862, acc 0.984375
2017-09-08T18:13:08.264747: step 5761, loss 0.0375398, acc 0.984375
2017-09-08T18:13:08.951676: step 5762, loss 0.0564228, acc 0.984375
2017-09-08T18:13:09.618508: step 5763, loss 0.0467712, acc 0.984375
2017-09-08T18:13:10.336825: step 5764, loss 0.00112343, acc 1
2017-09-08T18:13:11.008135: step 5765, loss 0.0318761, acc 0.984375
2017-09-08T18:13:11.665731: step 5766, loss 0.0830266, acc 0.984375
2017-09-08T18:13:12.279218: step 5767, loss 0.00217631, acc 1
2017-09-08T18:13:12.866731: step 5768, loss 0.00947532, acc 1
2017-09-08T18:13:13.389632: step 5769, loss 0.0270209, acc 0.984375
2017-09-08T18:13:14.003230: step 5770, loss 0.0265058, acc 0.984375
2017-09-08T18:13:14.604420: step 5771, loss 0.00497365, acc 1
2017-09-08T18:13:15.175953: step 5772, loss 0.00952564, acc 1
2017-09-08T18:13:15.764377: step 5773, loss 0.0849184, acc 0.984375
2017-09-08T18:13:16.406046: step 5774, loss 0.000698633, acc 1
2017-09-08T18:13:16.972733: step 5775, loss 0.0510679, acc 0.96875
2017-09-08T18:13:17.508393: step 5776, loss 0.034947, acc 0.984375
2017-09-08T18:13:18.082540: step 5777, loss 0.0218538, acc 0.984375
2017-09-08T18:13:18.684456: step 5778, loss 0.00066457, acc 1
2017-09-08T18:13:19.288740: step 5779, loss 0.00051244, acc 1
2017-09-08T18:13:19.927832: step 5780, loss 0.000566441, acc 1
2017-09-08T18:13:20.595285: step 5781, loss 0.0313288, acc 0.984375
2017-09-08T18:13:21.222432: step 5782, loss 0.000488861, acc 1
2017-09-08T18:13:21.853869: step 5783, loss 0.0634146, acc 0.953125
2017-09-08T18:13:22.466887: step 5784, loss 0.00897256, acc 1
2017-09-08T18:13:23.130747: step 5785, loss 0.00132404, acc 1
2017-09-08T18:13:23.838951: step 5786, loss 0.00107741, acc 1
2017-09-08T18:13:24.464993: step 5787, loss 0.00129592, acc 1
2017-09-08T18:13:24.975125: step 5788, loss 0.0162852, acc 0.984375
2017-09-08T18:13:25.527420: step 5789, loss 0.00109004, acc 1
2017-09-08T18:13:26.180333: step 5790, loss 0.00895065, acc 1
2017-09-08T18:13:26.835187: step 5791, loss 0.0169134, acc 0.984375
2017-09-08T18:13:27.449663: step 5792, loss 0.00446398, acc 1
2017-09-08T18:13:28.051514: step 5793, loss 0.0740808, acc 0.984375
2017-09-08T18:13:28.746779: step 5794, loss 0.000767514, acc 1
2017-09-08T18:13:29.356791: step 5795, loss 0.000340761, acc 1
2017-09-08T18:13:30.049890: step 5796, loss 0.000433068, acc 1
2017-09-08T18:13:30.718487: step 5797, loss 0.00150646, acc 1
2017-09-08T18:13:31.408877: step 5798, loss 0.00488961, acc 1
2017-09-08T18:13:32.079007: step 5799, loss 0.0333809, acc 0.984375
2017-09-08T18:13:32.755851: step 5800, loss 0.00299558, acc 1

Evaluation:
2017-09-08T18:13:33.412198: step 5800, loss 0.232099, acc 0.928058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5800

2017-09-08T18:13:36.808500: step 5801, loss 0.0693214, acc 0.984375
2017-09-08T18:13:37.328730: step 5802, loss 0.00078028, acc 1
2017-09-08T18:13:37.859414: step 5803, loss 0.0356879, acc 0.984375
2017-09-08T18:13:38.498679: step 5804, loss 0.00195099, acc 1
2017-09-08T18:13:39.132997: step 5805, loss 0.000779815, acc 1
2017-09-08T18:13:39.760624: step 5806, loss 0.0540287, acc 0.96875
2017-09-08T18:13:41.246086: step 5807, loss 0.100283, acc 0.953125
2017-09-08T18:13:41.796288: step 5808, loss 0.00132906, acc 1
2017-09-08T18:13:42.347165: step 5809, loss 0.00272068, acc 1
2017-09-08T18:13:42.908956: step 5810, loss 0.000501812, acc 1
2017-09-08T18:13:43.487188: step 5811, loss 0.00398907, acc 1
2017-09-08T18:13:44.053205: step 5812, loss 0.00117747, acc 1
2017-09-08T18:13:44.690780: step 5813, loss 0.0246168, acc 0.984375
2017-09-08T18:13:45.368129: step 5814, loss 0.00236457, acc 1
2017-09-08T18:13:46.066033: step 5815, loss 0.000520582, acc 1
2017-09-08T18:13:46.693510: step 5816, loss 0.0132733, acc 1
2017-09-08T18:13:47.333406: step 5817, loss 0.0821865, acc 0.96875
2017-09-08T18:13:47.995536: step 5818, loss 0.000988348, acc 1
2017-09-08T18:13:48.715241: step 5819, loss 0.000848548, acc 1
2017-09-08T18:13:49.207244: step 5820, loss 0.00716186, acc 1
2017-09-08T18:13:49.804190: step 5821, loss 0.00365946, acc 1
2017-09-08T18:13:50.360937: step 5822, loss 0.0142513, acc 0.984375
2017-09-08T18:13:51.008418: step 5823, loss 0.00148671, acc 1
2017-09-08T18:13:51.675639: step 5824, loss 0.0573005, acc 0.984375
2017-09-08T18:13:52.332016: step 5825, loss 0.00347645, acc 1
2017-09-08T18:13:52.963817: step 5826, loss 0.00940907, acc 1
2017-09-08T18:13:53.571838: step 5827, loss 0.0328369, acc 0.984375
2017-09-08T18:13:54.244499: step 5828, loss 0.021743, acc 0.984375
2017-09-08T18:13:54.880011: step 5829, loss 0.0101528, acc 1
2017-09-08T18:13:55.533747: step 5830, loss 0.000523677, acc 1
2017-09-08T18:13:56.156350: step 5831, loss 0.000859343, acc 1
2017-09-08T18:13:56.809654: step 5832, loss 0.0205241, acc 0.984375
2017-09-08T18:13:57.400847: step 5833, loss 0.00196537, acc 1
2017-09-08T18:13:58.032950: step 5834, loss 0.000632581, acc 1
2017-09-08T18:13:58.710697: step 5835, loss 0.0413446, acc 0.96875
2017-09-08T18:13:59.307662: step 5836, loss 0.0143254, acc 0.984375
2017-09-08T18:13:59.887379: step 5837, loss 0.00907301, acc 1
2017-09-08T18:14:00.443232: step 5838, loss 0.044531, acc 0.984375
2017-09-08T18:14:01.100455: step 5839, loss 0.00192281, acc 1
2017-09-08T18:14:01.781224: step 5840, loss 0.0441621, acc 0.984375
2017-09-08T18:14:02.483050: step 5841, loss 0.000933119, acc 1
2017-09-08T18:14:03.180934: step 5842, loss 0.00159599, acc 1
2017-09-08T18:14:03.839690: step 5843, loss 0.0125872, acc 1
2017-09-08T18:14:04.529849: step 5844, loss 0.00236093, acc 1
2017-09-08T18:14:05.138593: step 5845, loss 0.0654523, acc 0.984375
2017-09-08T18:14:06.261844: step 5846, loss 0.0061565, acc 1
2017-09-08T18:14:06.916062: step 5847, loss 0.0639776, acc 0.96875
2017-09-08T18:14:07.604339: step 5848, loss 0.00262825, acc 1
2017-09-08T18:14:08.262594: step 5849, loss 0.00423271, acc 1
2017-09-08T18:14:08.894587: step 5850, loss 0.000293617, acc 1

Evaluation:
2017-09-08T18:14:09.557358: step 5850, loss 0.218577, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5850

2017-09-08T18:14:12.097454: step 5851, loss 0.00308573, acc 1
2017-09-08T18:14:12.756122: step 5852, loss 0.00164858, acc 1
2017-09-08T18:14:13.366253: step 5853, loss 0.00970747, acc 1
2017-09-08T18:14:14.074396: step 5854, loss 0.0496654, acc 0.984375
2017-09-08T18:14:14.707393: step 5855, loss 0.0558679, acc 0.96875
2017-09-08T18:14:15.377275: step 5856, loss 0.0467069, acc 0.984375
2017-09-08T18:14:16.005892: step 5857, loss 0.00801714, acc 1
2017-09-08T18:14:16.657459: step 5858, loss 0.00108326, acc 1
2017-09-08T18:14:17.234799: step 5859, loss 0.0198328, acc 0.984375
2017-09-08T18:14:17.809373: step 5860, loss 0.00680053, acc 1
2017-09-08T18:14:18.424156: step 5861, loss 0.00109006, acc 1
2017-09-08T18:14:19.033192: step 5862, loss 0.0189653, acc 0.984375
2017-09-08T18:14:19.745338: step 5863, loss 0.0335855, acc 0.984375
2017-09-08T18:14:20.388077: step 5864, loss 0.00337328, acc 1
2017-09-08T18:14:21.057388: step 5865, loss 0.0390689, acc 0.984375
2017-09-08T18:14:21.772535: step 5866, loss 0.0434824, acc 0.984375
2017-09-08T18:14:22.372162: step 5867, loss 0.0242414, acc 0.984375
2017-09-08T18:14:23.074873: step 5868, loss 0.00314103, acc 1
2017-09-08T18:14:23.626577: step 5869, loss 0.0559374, acc 0.984375
2017-09-08T18:14:24.200833: step 5870, loss 0.00169663, acc 1
2017-09-08T18:14:24.760329: step 5871, loss 0.0616893, acc 0.96875
2017-09-08T18:14:25.417491: step 5872, loss 0.00143758, acc 1
2017-09-08T18:14:26.035748: step 5873, loss 0.0460171, acc 0.984375
2017-09-08T18:14:26.714780: step 5874, loss 0.00798803, acc 1
2017-09-08T18:14:27.371610: step 5875, loss 0.000403519, acc 1
2017-09-08T18:14:28.043399: step 5876, loss 0.0260141, acc 0.984375
2017-09-08T18:14:28.701100: step 5877, loss 0.0592235, acc 0.96875
2017-09-08T18:14:29.335328: step 5878, loss 0.0162037, acc 0.984375
2017-09-08T18:14:29.985640: step 5879, loss 0.000968653, acc 1
2017-09-08T18:14:30.536713: step 5880, loss 0.00191536, acc 1
2017-09-08T18:14:31.202188: step 5881, loss 0.000995862, acc 1
2017-09-08T18:14:31.885026: step 5882, loss 0.00306055, acc 1
2017-09-08T18:14:32.553391: step 5883, loss 0.0486415, acc 0.984375
2017-09-08T18:14:33.257283: step 5884, loss 0.000287811, acc 1
2017-09-08T18:14:33.791957: step 5885, loss 0.00479811, acc 1
2017-09-08T18:14:34.374958: step 5886, loss 0.0155423, acc 1
2017-09-08T18:14:34.959628: step 5887, loss 0.0208802, acc 0.984375
2017-09-08T18:14:35.540486: step 5888, loss 0.0424403, acc 0.984375
2017-09-08T18:14:36.138087: step 5889, loss 0.0593271, acc 0.96875
2017-09-08T18:14:36.757784: step 5890, loss 0.00181565, acc 1
2017-09-08T18:14:37.394361: step 5891, loss 0.00023506, acc 1
2017-09-08T18:14:38.062998: step 5892, loss 0.0195043, acc 0.984375
2017-09-08T18:14:38.709588: step 5893, loss 0.00580861, acc 1
2017-09-08T18:14:39.328741: step 5894, loss 0.0116597, acc 1
2017-09-08T18:14:39.977292: step 5895, loss 0.000812405, acc 1
2017-09-08T18:14:40.630810: step 5896, loss 0.00088175, acc 1
2017-09-08T18:14:41.252533: step 5897, loss 0.0385916, acc 0.984375
2017-09-08T18:14:41.909394: step 5898, loss 0.013559, acc 0.984375
2017-09-08T18:14:42.604943: step 5899, loss 0.0195645, acc 0.984375
2017-09-08T18:14:43.276298: step 5900, loss 0.0817073, acc 0.953125

Evaluation:
2017-09-08T18:14:43.916649: step 5900, loss 0.212871, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5900

2017-09-08T18:14:47.698572: step 5901, loss 0.0011098, acc 1
2017-09-08T18:14:48.348460: step 5902, loss 0.0613803, acc 0.953125
2017-09-08T18:14:49.018141: step 5903, loss 0.00270917, acc 1
2017-09-08T18:14:49.706290: step 5904, loss 0.00807619, acc 1
2017-09-08T18:14:50.392302: step 5905, loss 0.00811078, acc 1
2017-09-08T18:14:50.891352: step 5906, loss 0.000802689, acc 1
2017-09-08T18:14:51.440865: step 5907, loss 0.00750843, acc 1
2017-09-08T18:14:51.957689: step 5908, loss 0.000364781, acc 1
2017-09-08T18:14:52.554067: step 5909, loss 0.000595941, acc 1
2017-09-08T18:14:53.145525: step 5910, loss 0.0455579, acc 0.984375
2017-09-08T18:14:53.787072: step 5911, loss 0.00116453, acc 1
2017-09-08T18:14:54.417166: step 5912, loss 0.0295352, acc 0.984375
2017-09-08T18:14:55.051772: step 5913, loss 0.00310334, acc 1
2017-09-08T18:14:55.742242: step 5914, loss 0.0005293, acc 1
2017-09-08T18:14:56.411574: step 5915, loss 0.0365372, acc 0.96875
2017-09-08T18:14:57.187212: step 5916, loss 0.0159694, acc 0.984375
2017-09-08T18:14:57.792030: step 5917, loss 0.00305466, acc 1
2017-09-08T18:14:58.365592: step 5918, loss 0.00594618, acc 1
2017-09-08T18:14:58.963439: step 5919, loss 0.041846, acc 0.984375
2017-09-08T18:14:59.537797: step 5920, loss 0.000771903, acc 1
2017-09-08T18:15:00.171219: step 5921, loss 0.000694765, acc 1
2017-09-08T18:15:00.780498: step 5922, loss 0.0945624, acc 0.984375
2017-09-08T18:15:01.368540: step 5923, loss 0.00142499, acc 1
2017-09-08T18:15:02.052426: step 5924, loss 0.00745835, acc 1
2017-09-08T18:15:02.684232: step 5925, loss 0.0351248, acc 0.984375
2017-09-08T18:15:03.303158: step 5926, loss 0.00100259, acc 1
2017-09-08T18:15:03.915059: step 5927, loss 0.0084713, acc 1
2017-09-08T18:15:04.557498: step 5928, loss 0.010646, acc 1
2017-09-08T18:15:05.187614: step 5929, loss 0.000675749, acc 1
2017-09-08T18:15:05.838437: step 5930, loss 0.0921304, acc 0.984375
2017-09-08T18:15:06.499851: step 5931, loss 0.000829042, acc 1
2017-09-08T18:15:07.225192: step 5932, loss 0.000167038, acc 1
2017-09-08T18:15:07.776369: step 5933, loss 0.000431058, acc 1
2017-09-08T18:15:08.329179: step 5934, loss 0.000482056, acc 1
2017-09-08T18:15:08.917559: step 5935, loss 0.000222972, acc 1
2017-09-08T18:15:09.537322: step 5936, loss 0.0966695, acc 0.96875
2017-09-08T18:15:10.214694: step 5937, loss 0.00908627, acc 1
2017-09-08T18:15:10.858301: step 5938, loss 0.0191785, acc 0.984375
2017-09-08T18:15:11.476365: step 5939, loss 0.00110453, acc 1
2017-09-08T18:15:12.108232: step 5940, loss 0.0315499, acc 0.96875
2017-09-08T18:15:12.719238: step 5941, loss 0.0143401, acc 0.984375
2017-09-08T18:15:13.379953: step 5942, loss 0.0104141, acc 1
2017-09-08T18:15:14.098819: step 5943, loss 0.000851415, acc 1
2017-09-08T18:15:14.736728: step 5944, loss 0.0193446, acc 0.984375
2017-09-08T18:15:15.400046: step 5945, loss 0.000554656, acc 1
2017-09-08T18:15:16.054009: step 5946, loss 0.116964, acc 0.984375
2017-09-08T18:15:16.715897: step 5947, loss 0.000264594, acc 1
2017-09-08T18:15:17.336210: step 5948, loss 0.00655232, acc 1
2017-09-08T18:15:17.991153: step 5949, loss 0.00913505, acc 1
2017-09-08T18:15:18.654510: step 5950, loss 0.00428136, acc 1

Evaluation:
2017-09-08T18:15:19.328500: step 5950, loss 0.216976, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-5950

2017-09-08T18:15:21.998437: step 5951, loss 0.00134806, acc 1
2017-09-08T18:15:22.654274: step 5952, loss 0.0771903, acc 0.953125
2017-09-08T18:15:23.371199: step 5953, loss 0.0245973, acc 0.984375
2017-09-08T18:15:24.102005: step 5954, loss 0.00422226, acc 1
2017-09-08T18:15:24.703263: step 5955, loss 0.0061923, acc 1
2017-09-08T18:15:25.337429: step 5956, loss 0.000507226, acc 1
2017-09-08T18:15:26.003868: step 5957, loss 0.000360787, acc 1
2017-09-08T18:15:26.586577: step 5958, loss 0.040495, acc 0.96875
2017-09-08T18:15:27.139054: step 5959, loss 0.0566341, acc 0.984375
2017-09-08T18:15:27.689938: step 5960, loss 0.0420713, acc 0.96875
2017-09-08T18:15:28.221560: step 5961, loss 0.0720229, acc 0.984375
2017-09-08T18:15:28.895984: step 5962, loss 0.0686795, acc 0.953125
2017-09-08T18:15:29.539296: step 5963, loss 0.000749429, acc 1
2017-09-08T18:15:30.212759: step 5964, loss 0.00128762, acc 1
2017-09-08T18:15:30.871616: step 5965, loss 0.000816732, acc 1
2017-09-08T18:15:31.510094: step 5966, loss 0.00354816, acc 1
2017-09-08T18:15:32.154082: step 5967, loss 0.0782182, acc 0.96875
2017-09-08T18:15:32.792063: step 5968, loss 0.0848133, acc 0.984375
2017-09-08T18:15:33.379935: step 5969, loss 0.0222237, acc 0.984375
2017-09-08T18:15:33.956642: step 5970, loss 0.0583192, acc 0.984375
2017-09-08T18:15:34.626685: step 5971, loss 0.000695135, acc 1
2017-09-08T18:15:35.288308: step 5972, loss 0.0128271, acc 1
2017-09-08T18:15:35.937001: step 5973, loss 0.0277126, acc 0.984375
2017-09-08T18:15:36.590805: step 5974, loss 0.0185445, acc 0.984375
2017-09-08T18:15:37.275055: step 5975, loss 0.0879661, acc 0.984375
2017-09-08T18:15:37.964549: step 5976, loss 0.000302729, acc 1
2017-09-08T18:15:38.617182: step 5977, loss 0.000860403, acc 1
2017-09-08T18:15:39.202792: step 5978, loss 0.00681634, acc 1
2017-09-08T18:15:39.840954: step 5979, loss 0.116936, acc 0.953125
2017-09-08T18:15:40.496829: step 5980, loss 0.000306476, acc 1
2017-09-08T18:15:41.147808: step 5981, loss 0.00259822, acc 1
2017-09-08T18:15:41.825033: step 5982, loss 0.00950282, acc 1
2017-09-08T18:15:42.423901: step 5983, loss 0.0448735, acc 0.984375
2017-09-08T18:15:42.976247: step 5984, loss 0.0237068, acc 0.984375
2017-09-08T18:15:43.558760: step 5985, loss 0.00040626, acc 1
2017-09-08T18:15:44.147115: step 5986, loss 0.00543555, acc 1
2017-09-08T18:15:44.733355: step 5987, loss 0.037951, acc 0.984375
2017-09-08T18:15:45.409294: step 5988, loss 0.00703678, acc 1
2017-09-08T18:15:46.093681: step 5989, loss 0.000891248, acc 1
2017-09-08T18:15:46.736739: step 5990, loss 0.0277483, acc 0.984375
2017-09-08T18:15:47.421335: step 5991, loss 0.0018147, acc 1
2017-09-08T18:15:48.033476: step 5992, loss 0.00148866, acc 1
2017-09-08T18:15:48.707934: step 5993, loss 0.0563411, acc 0.96875
2017-09-08T18:15:49.367118: step 5994, loss 0.000505779, acc 1
2017-09-08T18:15:49.997976: step 5995, loss 0.0138354, acc 0.984375
2017-09-08T18:15:50.641134: step 5996, loss 0.00114151, acc 1
2017-09-08T18:15:51.263924: step 5997, loss 0.0506672, acc 0.984375
2017-09-08T18:15:51.944122: step 5998, loss 0.00809032, acc 1
2017-09-08T18:15:52.581113: step 5999, loss 0.000765014, acc 1
2017-09-08T18:15:53.213594: step 6000, loss 0.0217148, acc 0.984375

Evaluation:
2017-09-08T18:15:53.889896: step 6000, loss 0.211735, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6000

2017-09-08T18:15:57.116245: step 6001, loss 0.0296245, acc 0.984375
2017-09-08T18:15:57.776298: step 6002, loss 0.00513468, acc 1
2017-09-08T18:15:58.373232: step 6003, loss 0.00191129, acc 1
2017-09-08T18:15:59.012109: step 6004, loss 0.0219858, acc 0.984375
2017-09-08T18:15:59.681436: step 6005, loss 0.00920964, acc 1
2017-09-08T18:16:00.342356: step 6006, loss 0.0376409, acc 0.96875
2017-09-08T18:16:01.032650: step 6007, loss 0.115029, acc 0.984375
2017-09-08T18:16:01.611875: step 6008, loss 0.0984553, acc 0.953125
2017-09-08T18:16:02.196338: step 6009, loss 0.0292991, acc 0.984375
2017-09-08T18:16:02.789043: step 6010, loss 0.00789485, acc 1
2017-09-08T18:16:03.379857: step 6011, loss 0.00165412, acc 1
2017-09-08T18:16:04.002210: step 6012, loss 0.0170346, acc 0.984375
2017-09-08T18:16:04.667441: step 6013, loss 0.0292043, acc 0.984375
2017-09-08T18:16:05.357522: step 6014, loss 0.0235334, acc 0.984375
2017-09-08T18:16:06.038737: step 6015, loss 0.0157336, acc 0.984375
2017-09-08T18:16:06.555792: step 6016, loss 0.000838948, acc 1
2017-09-08T18:16:07.150901: step 6017, loss 0.074795, acc 0.96875
2017-09-08T18:16:07.733316: step 6018, loss 0.000305615, acc 1
2017-09-08T18:16:08.296354: step 6019, loss 0.000931979, acc 1
2017-09-08T18:16:08.883536: step 6020, loss 0.000782339, acc 1
2017-09-08T18:16:09.581951: step 6021, loss 0.000475289, acc 1
2017-09-08T18:16:10.251806: step 6022, loss 0.0154601, acc 1
2017-09-08T18:16:10.928266: step 6023, loss 0.00485378, acc 1
2017-09-08T18:16:11.591745: step 6024, loss 0.00160159, acc 1
2017-09-08T18:16:12.256347: step 6025, loss 0.00043898, acc 1
2017-09-08T18:16:12.931979: step 6026, loss 0.000583516, acc 1
2017-09-08T18:16:13.613281: step 6027, loss 0.000416162, acc 1
2017-09-08T18:16:14.282593: step 6028, loss 0.000726352, acc 1
2017-09-08T18:16:14.915104: step 6029, loss 0.01344, acc 1
2017-09-08T18:16:15.523513: step 6030, loss 0.000384835, acc 1
2017-09-08T18:16:16.130855: step 6031, loss 0.00277239, acc 1
2017-09-08T18:16:16.731128: step 6032, loss 0.0142966, acc 0.984375
2017-09-08T18:16:17.443810: step 6033, loss 0.0125958, acc 0.984375
2017-09-08T18:16:18.039384: step 6034, loss 0.00131888, acc 1
2017-09-08T18:16:18.644278: step 6035, loss 0.00250876, acc 1
2017-09-08T18:16:19.278485: step 6036, loss 0.0254776, acc 0.984375
2017-09-08T18:16:19.989118: step 6037, loss 0.00672073, acc 1
2017-09-08T18:16:20.655520: step 6038, loss 0.0235116, acc 0.984375
2017-09-08T18:16:21.246306: step 6039, loss 0.00337658, acc 1
2017-09-08T18:16:21.748166: step 6040, loss 0.0563325, acc 0.96875
2017-09-08T18:16:22.331985: step 6041, loss 0.0708458, acc 0.984375
2017-09-08T18:16:22.953524: step 6042, loss 0.0186743, acc 0.984375
2017-09-08T18:16:23.618961: step 6043, loss 0.00083116, acc 1
2017-09-08T18:16:24.288179: step 6044, loss 0.000181044, acc 1
2017-09-08T18:16:24.859769: step 6045, loss 0.02983, acc 0.984375
2017-09-08T18:16:25.350673: step 6046, loss 0.0183657, acc 0.984375
2017-09-08T18:16:25.910933: step 6047, loss 0.00114974, acc 1
2017-09-08T18:16:26.506346: step 6048, loss 0.0134197, acc 0.984375
2017-09-08T18:16:27.181828: step 6049, loss 0.000921149, acc 1
2017-09-08T18:16:27.888067: step 6050, loss 0.0131083, acc 1

Evaluation:
2017-09-08T18:16:28.533140: step 6050, loss 0.221368, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6050

2017-09-08T18:16:32.187846: step 6051, loss 0.014611, acc 0.984375
2017-09-08T18:16:32.827653: step 6052, loss 0.000748135, acc 1
2017-09-08T18:16:33.462303: step 6053, loss 0.0346195, acc 0.984375
2017-09-08T18:16:34.117587: step 6054, loss 0.00129146, acc 1
2017-09-08T18:16:34.799698: step 6055, loss 0.0525625, acc 0.96875
2017-09-08T18:16:35.455656: step 6056, loss 0.0017764, acc 1
2017-09-08T18:16:36.200448: step 6057, loss 0.0296563, acc 0.984375
2017-09-08T18:16:36.743294: step 6058, loss 0.0241953, acc 0.984375
2017-09-08T18:16:37.319465: step 6059, loss 0.049559, acc 0.984375
2017-09-08T18:16:37.909117: step 6060, loss 0.0652418, acc 0.96875
2017-09-08T18:16:38.469390: step 6061, loss 0.000948901, acc 1
2017-09-08T18:16:39.042553: step 6062, loss 0.00602243, acc 1
2017-09-08T18:16:39.741549: step 6063, loss 0.000846734, acc 1
2017-09-08T18:16:40.403389: step 6064, loss 0.00136114, acc 1
2017-09-08T18:16:40.997741: step 6065, loss 0.0158001, acc 1
2017-09-08T18:16:41.623218: step 6066, loss 0.0120977, acc 1
2017-09-08T18:16:42.226106: step 6067, loss 0.0430832, acc 0.984375
2017-09-08T18:16:42.984180: step 6068, loss 0.0160644, acc 0.984375
2017-09-08T18:16:43.652648: step 6069, loss 0.00107037, acc 1
2017-09-08T18:16:44.345025: step 6070, loss 0.0624449, acc 0.984375
2017-09-08T18:16:45.016473: step 6071, loss 0.000507403, acc 1
2017-09-08T18:16:45.633139: step 6072, loss 0.0456047, acc 0.984375
2017-09-08T18:16:46.314423: step 6073, loss 0.00173047, acc 1
2017-09-08T18:16:46.975873: step 6074, loss 0.0197026, acc 0.984375
2017-09-08T18:16:47.628987: step 6075, loss 0.00309282, acc 1
2017-09-08T18:16:48.284240: step 6076, loss 0.0767083, acc 0.980392
2017-09-08T18:16:48.941386: step 6077, loss 0.0014832, acc 1
2017-09-08T18:16:49.610897: step 6078, loss 0.000501301, acc 1
2017-09-08T18:16:50.232397: step 6079, loss 0.000594614, acc 1
2017-09-08T18:16:50.903589: step 6080, loss 0.00308911, acc 1
2017-09-08T18:16:51.591713: step 6081, loss 0.0170538, acc 1
2017-09-08T18:16:52.209102: step 6082, loss 0.0354991, acc 0.984375
2017-09-08T18:16:52.773989: step 6083, loss 0.00305056, acc 1
2017-09-08T18:16:53.359115: step 6084, loss 0.0142297, acc 1
2017-09-08T18:16:53.938962: step 6085, loss 0.00328087, acc 1
2017-09-08T18:16:54.605942: step 6086, loss 0.0322527, acc 0.984375
2017-09-08T18:16:55.248384: step 6087, loss 0.00177363, acc 1
2017-09-08T18:16:55.879498: step 6088, loss 0.0312466, acc 0.984375
2017-09-08T18:16:56.507410: step 6089, loss 0.0102172, acc 1
2017-09-08T18:16:57.174672: step 6090, loss 0.00724433, acc 1
2017-09-08T18:16:57.814722: step 6091, loss 0.00696543, acc 1
2017-09-08T18:16:58.446471: step 6092, loss 0.000526625, acc 1
2017-09-08T18:16:59.096747: step 6093, loss 0.0209467, acc 0.984375
2017-09-08T18:16:59.717479: step 6094, loss 0.00100077, acc 1
2017-09-08T18:17:00.333375: step 6095, loss 0.0583574, acc 0.96875
2017-09-08T18:17:00.899579: step 6096, loss 0.000791077, acc 1
2017-09-08T18:17:01.463370: step 6097, loss 0.00475556, acc 1
2017-09-08T18:17:01.975432: step 6098, loss 0.0403085, acc 0.984375
2017-09-08T18:17:02.526083: step 6099, loss 0.00623994, acc 1
2017-09-08T18:17:03.158479: step 6100, loss 0.000452554, acc 1

Evaluation:
2017-09-08T18:17:03.773696: step 6100, loss 0.222159, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6100

2017-09-08T18:17:06.199136: step 6101, loss 0.000461742, acc 1
2017-09-08T18:17:06.866199: step 6102, loss 0.00333033, acc 1
2017-09-08T18:17:07.523383: step 6103, loss 0.0114741, acc 1
2017-09-08T18:17:08.249108: step 6104, loss 0.0109464, acc 1
2017-09-08T18:17:08.851784: step 6105, loss 0.00438421, acc 1
2017-09-08T18:17:09.468998: step 6106, loss 0.00336224, acc 1
2017-09-08T18:17:10.088917: step 6107, loss 0.000592611, acc 1
2017-09-08T18:17:10.612484: step 6108, loss 0.0308037, acc 0.984375
2017-09-08T18:17:11.122939: step 6109, loss 0.00285897, acc 1
2017-09-08T18:17:11.818857: step 6110, loss 0.0122063, acc 0.984375
2017-09-08T18:17:12.360440: step 6111, loss 0.00310115, acc 1
2017-09-08T18:17:12.946281: step 6112, loss 0.0824793, acc 0.96875
2017-09-08T18:17:13.578438: step 6113, loss 0.0335417, acc 0.984375
2017-09-08T18:17:14.198566: step 6114, loss 0.00112771, acc 1
2017-09-08T18:17:14.839295: step 6115, loss 0.0134713, acc 1
2017-09-08T18:17:15.506030: step 6116, loss 0.00657965, acc 1
2017-09-08T18:17:16.049615: step 6117, loss 0.0195449, acc 0.984375
2017-09-08T18:17:16.682864: step 6118, loss 0.000843794, acc 1
2017-09-08T18:17:17.262831: step 6119, loss 0.0517607, acc 0.984375
2017-09-08T18:17:17.818889: step 6120, loss 0.0384325, acc 0.984375
2017-09-08T18:17:18.440640: step 6121, loss 0.0221914, acc 0.984375
2017-09-08T18:17:19.116869: step 6122, loss 0.00297893, acc 1
2017-09-08T18:17:19.783559: step 6123, loss 0.00470538, acc 1
2017-09-08T18:17:20.479173: step 6124, loss 0.100848, acc 0.96875
2017-09-08T18:17:21.165959: step 6125, loss 0.0142929, acc 1
2017-09-08T18:17:21.814121: step 6126, loss 0.0502964, acc 0.96875
2017-09-08T18:17:22.456191: step 6127, loss 0.0279718, acc 0.984375
2017-09-08T18:17:23.168858: step 6128, loss 0.00106782, acc 1
2017-09-08T18:17:23.839983: step 6129, loss 0.0302511, acc 0.984375
2017-09-08T18:17:24.533848: step 6130, loss 0.0390886, acc 0.984375
2017-09-08T18:17:25.185622: step 6131, loss 0.00207193, acc 1
2017-09-08T18:17:25.944773: step 6132, loss 0.000911879, acc 1
2017-09-08T18:17:26.508957: step 6133, loss 0.00127847, acc 1
2017-09-08T18:17:27.083304: step 6134, loss 0.0316209, acc 0.984375
2017-09-08T18:17:27.674650: step 6135, loss 0.0645478, acc 0.984375
2017-09-08T18:17:28.232978: step 6136, loss 0.00306785, acc 1
2017-09-08T18:17:28.807583: step 6137, loss 0.00547249, acc 1
2017-09-08T18:17:29.467167: step 6138, loss 0.00630604, acc 1
2017-09-08T18:17:30.094469: step 6139, loss 0.00187433, acc 1
2017-09-08T18:17:30.735667: step 6140, loss 0.120525, acc 0.921875
2017-09-08T18:17:31.386235: step 6141, loss 0.0411144, acc 0.984375
2017-09-08T18:17:32.069987: step 6142, loss 0.00428733, acc 1
2017-09-08T18:17:32.718201: step 6143, loss 0.000704292, acc 1
2017-09-08T18:17:33.352623: step 6144, loss 0.0446903, acc 0.96875
2017-09-08T18:17:33.957116: step 6145, loss 0.0507696, acc 0.96875
2017-09-08T18:17:34.596979: step 6146, loss 0.0247102, acc 0.984375
2017-09-08T18:17:35.254937: step 6147, loss 0.000974269, acc 1
2017-09-08T18:17:35.914106: step 6148, loss 0.0396949, acc 0.984375
2017-09-08T18:17:36.592063: step 6149, loss 0.00052165, acc 1
2017-09-08T18:17:37.248683: step 6150, loss 0.00377594, acc 1

Evaluation:
2017-09-08T18:17:37.920866: step 6150, loss 0.222287, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6150

2017-09-08T18:17:41.642658: step 6151, loss 0.0626904, acc 0.984375
2017-09-08T18:17:42.287957: step 6152, loss 0.00231985, acc 1
2017-09-08T18:17:42.948209: step 6153, loss 0.0421295, acc 0.984375
2017-09-08T18:17:43.644783: step 6154, loss 0.0247032, acc 0.984375
2017-09-08T18:17:44.339675: step 6155, loss 0.00160772, acc 1
2017-09-08T18:17:44.973077: step 6156, loss 0.0126336, acc 1
2017-09-08T18:17:45.613662: step 6157, loss 0.00516782, acc 1
2017-09-08T18:17:46.155681: step 6158, loss 0.0193754, acc 0.984375
2017-09-08T18:17:46.767562: step 6159, loss 0.0746359, acc 0.984375
2017-09-08T18:17:47.347755: step 6160, loss 0.0458542, acc 0.984375
2017-09-08T18:17:47.973025: step 6161, loss 0.051696, acc 0.984375
2017-09-08T18:17:48.646630: step 6162, loss 0.170028, acc 0.9375
2017-09-08T18:17:49.314824: step 6163, loss 0.000404859, acc 1
2017-09-08T18:17:50.012410: step 6164, loss 0.00637738, acc 1
2017-09-08T18:17:50.758015: step 6165, loss 0.00132374, acc 1
2017-09-08T18:17:51.317646: step 6166, loss 0.0352773, acc 0.984375
2017-09-08T18:17:51.880943: step 6167, loss 0.0287131, acc 0.984375
2017-09-08T18:17:52.424620: step 6168, loss 0.0109952, acc 1
2017-09-08T18:17:53.029758: step 6169, loss 0.00187275, acc 1
2017-09-08T18:17:53.698497: step 6170, loss 0.00114553, acc 1
2017-09-08T18:17:54.417573: step 6171, loss 0.000449093, acc 1
2017-09-08T18:17:55.087291: step 6172, loss 0.000224101, acc 1
2017-09-08T18:17:55.705837: step 6173, loss 0.00024996, acc 1
2017-09-08T18:17:56.269028: step 6174, loss 0.0173926, acc 0.980392
2017-09-08T18:17:56.851092: step 6175, loss 0.000609641, acc 1
2017-09-08T18:17:57.349970: step 6176, loss 0.00103428, acc 1
2017-09-08T18:17:57.971048: step 6177, loss 0.00190355, acc 1
2017-09-08T18:17:58.626187: step 6178, loss 0.00128939, acc 1
2017-09-08T18:17:59.302225: step 6179, loss 0.00053645, acc 1
2017-09-08T18:17:59.949637: step 6180, loss 0.00229489, acc 1
2017-09-08T18:18:00.659409: step 6181, loss 0.00367745, acc 1
2017-09-08T18:18:01.289380: step 6182, loss 0.0296118, acc 0.984375
2017-09-08T18:18:01.849702: step 6183, loss 0.00108286, acc 1
2017-09-08T18:18:02.478626: step 6184, loss 0.0006411, acc 1
2017-09-08T18:18:03.165211: step 6185, loss 0.00103683, acc 1
2017-09-08T18:18:03.798112: step 6186, loss 0.00135814, acc 1
2017-09-08T18:18:04.434016: step 6187, loss 0.0419346, acc 0.984375
2017-09-08T18:18:05.098843: step 6188, loss 0.000862886, acc 1
2017-09-08T18:18:05.774146: step 6189, loss 0.000253469, acc 1
2017-09-08T18:18:06.405112: step 6190, loss 0.0325129, acc 0.984375
2017-09-08T18:18:07.066556: step 6191, loss 0.00919802, acc 1
2017-09-08T18:18:07.721161: step 6192, loss 0.0397889, acc 0.984375
2017-09-08T18:18:08.402939: step 6193, loss 0.00930888, acc 1
2017-09-08T18:18:09.062382: step 6194, loss 0.0551516, acc 0.96875
2017-09-08T18:18:09.757378: step 6195, loss 0.0139008, acc 0.984375
2017-09-08T18:18:10.491253: step 6196, loss 0.0367275, acc 0.96875
2017-09-08T18:18:11.244421: step 6197, loss 0.0078882, acc 1
2017-09-08T18:18:11.911889: step 6198, loss 0.00052171, acc 1
2017-09-08T18:18:12.562818: step 6199, loss 0.000283839, acc 1
2017-09-08T18:18:13.213642: step 6200, loss 0.000381546, acc 1

Evaluation:
2017-09-08T18:18:13.930994: step 6200, loss 0.208132, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6200

2017-09-08T18:18:16.909203: step 6201, loss 0.00056924, acc 1
2017-09-08T18:18:17.552467: step 6202, loss 0.000435139, acc 1
2017-09-08T18:18:18.191841: step 6203, loss 0.0434519, acc 0.96875
2017-09-08T18:18:18.811515: step 6204, loss 0.0200511, acc 0.984375
2017-09-08T18:18:19.469615: step 6205, loss 0.0350462, acc 0.984375
2017-09-08T18:18:20.143653: step 6206, loss 0.000751691, acc 1
2017-09-08T18:18:20.700171: step 6207, loss 0.000532638, acc 1
2017-09-08T18:18:21.224501: step 6208, loss 0.00101716, acc 1
2017-09-08T18:18:21.722621: step 6209, loss 0.00128346, acc 1
2017-09-08T18:18:22.334308: step 6210, loss 0.0303418, acc 0.984375
2017-09-08T18:18:22.828045: step 6211, loss 0.107007, acc 0.96875
2017-09-08T18:18:23.432900: step 6212, loss 0.00490019, acc 1
2017-09-08T18:18:23.972452: step 6213, loss 0.00182324, acc 1
2017-09-08T18:18:24.665790: step 6214, loss 0.017135, acc 0.984375
2017-09-08T18:18:25.323315: step 6215, loss 0.000515254, acc 1
2017-09-08T18:18:26.007224: step 6216, loss 0.0556781, acc 0.984375
2017-09-08T18:18:26.839150: step 6217, loss 0.00130334, acc 1
2017-09-08T18:18:27.385186: step 6218, loss 0.000830726, acc 1
2017-09-08T18:18:27.958516: step 6219, loss 0.00188971, acc 1
2017-09-08T18:18:28.601098: step 6220, loss 0.00630592, acc 1
2017-09-08T18:18:29.251779: step 6221, loss 0.0152813, acc 1
2017-09-08T18:18:29.892883: step 6222, loss 0.0486892, acc 0.984375
2017-09-08T18:18:30.512236: step 6223, loss 0.00739315, acc 1
2017-09-08T18:18:31.134450: step 6224, loss 0.044435, acc 0.984375
2017-09-08T18:18:31.766861: step 6225, loss 0.000283296, acc 1
2017-09-08T18:18:32.461968: step 6226, loss 0.000786258, acc 1
2017-09-08T18:18:33.089334: step 6227, loss 0.00816695, acc 1
2017-09-08T18:18:33.759746: step 6228, loss 0.00971046, acc 1
2017-09-08T18:18:34.464121: step 6229, loss 0.000280808, acc 1
2017-09-08T18:18:34.994235: step 6230, loss 0.00873208, acc 1
2017-09-08T18:18:35.584974: step 6231, loss 0.0808839, acc 0.984375
2017-09-08T18:18:36.145026: step 6232, loss 0.00317246, acc 1
2017-09-08T18:18:36.731272: step 6233, loss 0.00112721, acc 1
2017-09-08T18:18:37.296450: step 6234, loss 0.0558919, acc 0.984375
2017-09-08T18:18:37.916273: step 6235, loss 0.00186546, acc 1
2017-09-08T18:18:38.543000: step 6236, loss 0.110419, acc 0.9375
2017-09-08T18:18:39.180703: step 6237, loss 0.00554187, acc 1
2017-09-08T18:18:39.862301: step 6238, loss 0.00110878, acc 1
2017-09-08T18:18:40.543654: step 6239, loss 0.0567788, acc 0.984375
2017-09-08T18:18:41.193696: step 6240, loss 0.00051546, acc 1
2017-09-08T18:18:41.836398: step 6241, loss 0.00124456, acc 1
2017-09-08T18:18:42.493562: step 6242, loss 0.00122213, acc 1
2017-09-08T18:18:43.345889: step 6243, loss 0.0589827, acc 0.984375
2017-09-08T18:18:44.051602: step 6244, loss 0.000250218, acc 1
2017-09-08T18:18:44.749008: step 6245, loss 0.00655449, acc 1
2017-09-08T18:18:45.401472: step 6246, loss 0.168858, acc 0.96875
2017-09-08T18:18:46.091916: step 6247, loss 0.0924513, acc 0.96875
2017-09-08T18:18:46.771235: step 6248, loss 0.00119812, acc 1
2017-09-08T18:18:47.439149: step 6249, loss 0.00299328, acc 1
2017-09-08T18:18:48.117031: step 6250, loss 0.00664771, acc 1

Evaluation:
2017-09-08T18:18:48.776444: step 6250, loss 0.2253, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6250

2017-09-08T18:18:51.813467: step 6251, loss 0.00124285, acc 1
2017-09-08T18:18:52.469302: step 6252, loss 0.00904053, acc 1
2017-09-08T18:18:53.117808: step 6253, loss 0.0246681, acc 0.984375
2017-09-08T18:18:53.746040: step 6254, loss 0.00343231, acc 1
2017-09-08T18:18:54.435392: step 6255, loss 0.000983756, acc 1
2017-09-08T18:18:55.030951: step 6256, loss 0.0571063, acc 0.984375
2017-09-08T18:18:55.605153: step 6257, loss 0.0637702, acc 0.96875
2017-09-08T18:18:56.166873: step 6258, loss 0.000557281, acc 1
2017-09-08T18:18:56.764446: step 6259, loss 0.019576, acc 1
2017-09-08T18:18:57.431292: step 6260, loss 0.0220645, acc 0.984375
2017-09-08T18:18:58.068720: step 6261, loss 0.000295869, acc 1
2017-09-08T18:18:58.686571: step 6262, loss 0.0313443, acc 0.984375
2017-09-08T18:18:59.393904: step 6263, loss 0.0145045, acc 0.984375
2017-09-08T18:19:00.003587: step 6264, loss 0.0413825, acc 0.96875
2017-09-08T18:19:00.639571: step 6265, loss 0.0270784, acc 0.984375
2017-09-08T18:19:01.245824: step 6266, loss 0.00142812, acc 1
2017-09-08T18:19:01.825735: step 6267, loss 0.0142517, acc 1
2017-09-08T18:19:02.504067: step 6268, loss 0.00364189, acc 1
2017-09-08T18:19:03.171331: step 6269, loss 0.0506672, acc 0.984375
2017-09-08T18:19:03.834784: step 6270, loss 0.0242527, acc 0.984375
2017-09-08T18:19:04.490875: step 6271, loss 0.0173886, acc 0.984375
2017-09-08T18:19:05.082985: step 6272, loss 0.000306958, acc 1
2017-09-08T18:19:05.758123: step 6273, loss 0.00637664, acc 1
2017-09-08T18:19:06.431292: step 6274, loss 0.00413452, acc 1
2017-09-08T18:19:07.124790: step 6275, loss 0.0132371, acc 1
2017-09-08T18:19:07.843250: step 6276, loss 0.00702346, acc 1
2017-09-08T18:19:08.491089: step 6277, loss 0.0184712, acc 0.984375
2017-09-08T18:19:09.155745: step 6278, loss 0.00261587, acc 1
2017-09-08T18:19:09.818193: step 6279, loss 0.0231391, acc 0.984375
2017-09-08T18:19:10.373369: step 6280, loss 0.0112511, acc 1
2017-09-08T18:19:10.952275: step 6281, loss 0.00132289, acc 1
2017-09-08T18:19:11.537199: step 6282, loss 0.0163887, acc 0.984375
2017-09-08T18:19:12.199709: step 6283, loss 0.0234394, acc 0.984375
2017-09-08T18:19:12.841349: step 6284, loss 0.0390083, acc 0.984375
2017-09-08T18:19:13.496822: step 6285, loss 0.0111851, acc 1
2017-09-08T18:19:14.161176: step 6286, loss 0.0104248, acc 1
2017-09-08T18:19:14.843941: step 6287, loss 0.00168288, acc 1
2017-09-08T18:19:15.458065: step 6288, loss 0.00633399, acc 1
2017-09-08T18:19:16.063188: step 6289, loss 0.0149504, acc 0.984375
2017-09-08T18:19:16.533877: step 6290, loss 0.000641382, acc 1
2017-09-08T18:19:17.098720: step 6291, loss 0.00185905, acc 1
2017-09-08T18:19:17.705409: step 6292, loss 0.000286836, acc 1
2017-09-08T18:19:18.326827: step 6293, loss 0.000764105, acc 1
2017-09-08T18:19:18.962642: step 6294, loss 0.00665086, acc 1
2017-09-08T18:19:19.625852: step 6295, loss 0.00871553, acc 1
2017-09-08T18:19:20.230898: step 6296, loss 0.00893963, acc 1
2017-09-08T18:19:20.909684: step 6297, loss 0.0551726, acc 0.984375
2017-09-08T18:19:21.593067: step 6298, loss 0.0126256, acc 0.984375
2017-09-08T18:19:22.259945: step 6299, loss 0.00110185, acc 1
2017-09-08T18:19:22.930455: step 6300, loss 0.0723536, acc 0.984375

Evaluation:
2017-09-08T18:19:23.604178: step 6300, loss 0.214353, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6300

2017-09-08T18:19:27.131311: step 6301, loss 0.0288387, acc 0.984375
2017-09-08T18:19:27.820388: step 6302, loss 0.0693408, acc 0.96875
2017-09-08T18:19:28.480243: step 6303, loss 0.00057373, acc 1
2017-09-08T18:19:29.079733: step 6304, loss 0.00214145, acc 1
2017-09-08T18:19:29.658198: step 6305, loss 0.0238716, acc 0.984375
2017-09-08T18:19:30.269051: step 6306, loss 0.000602706, acc 1
2017-09-08T18:19:30.855135: step 6307, loss 0.0742409, acc 0.984375
2017-09-08T18:19:31.459194: step 6308, loss 0.00163976, acc 1
2017-09-08T18:19:32.115825: step 6309, loss 0.00945303, acc 1
2017-09-08T18:19:32.798577: step 6310, loss 0.00997467, acc 1
2017-09-08T18:19:33.453299: step 6311, loss 0.000542171, acc 1
2017-09-08T18:19:34.080909: step 6312, loss 0.0565037, acc 0.984375
2017-09-08T18:19:34.606735: step 6313, loss 0.0343646, acc 0.984375
2017-09-08T18:19:35.109539: step 6314, loss 0.00392574, acc 1
2017-09-08T18:19:35.599959: step 6315, loss 0.0211951, acc 0.984375
2017-09-08T18:19:36.040055: step 6316, loss 0.0417018, acc 0.984375
2017-09-08T18:19:36.730988: step 6317, loss 0.000492957, acc 1
2017-09-08T18:19:37.404157: step 6318, loss 0.0019945, acc 1
2017-09-08T18:19:37.991461: step 6319, loss 0.0176404, acc 0.984375
2017-09-08T18:19:38.703442: step 6320, loss 0.000622449, acc 1
2017-09-08T18:19:39.305040: step 6321, loss 0.0990168, acc 0.953125
2017-09-08T18:19:40.073845: step 6322, loss 0.0575731, acc 0.96875
2017-09-08T18:19:40.745097: step 6323, loss 0.000726147, acc 1
2017-09-08T18:19:41.361953: step 6324, loss 0.0444146, acc 0.984375
2017-09-08T18:19:42.053332: step 6325, loss 0.0741272, acc 0.96875
2017-09-08T18:19:42.737263: step 6326, loss 0.00648989, acc 1
2017-09-08T18:19:43.454127: step 6327, loss 0.00275264, acc 1
2017-09-08T18:19:44.004816: step 6328, loss 0.00113632, acc 1
2017-09-08T18:19:44.568628: step 6329, loss 0.0601804, acc 0.96875
2017-09-08T18:19:45.160134: step 6330, loss 0.000355682, acc 1
2017-09-08T18:19:45.751206: step 6331, loss 0.000376082, acc 1
2017-09-08T18:19:46.445140: step 6332, loss 0.000307102, acc 1
2017-09-08T18:19:47.083643: step 6333, loss 0.00105644, acc 1
2017-09-08T18:19:47.758486: step 6334, loss 0.00047867, acc 1
2017-09-08T18:19:48.400163: step 6335, loss 0.0114037, acc 1
2017-09-08T18:19:49.044983: step 6336, loss 0.0300426, acc 0.96875
2017-09-08T18:19:49.799046: step 6337, loss 0.000909287, acc 1
2017-09-08T18:19:50.402692: step 6338, loss 0.0131734, acc 1
2017-09-08T18:19:51.093748: step 6339, loss 0.00031789, acc 1
2017-09-08T18:19:51.743682: step 6340, loss 0.00128015, acc 1
2017-09-08T18:19:52.405049: step 6341, loss 0.000773982, acc 1
2017-09-08T18:19:53.059122: step 6342, loss 0.0109638, acc 1
2017-09-08T18:19:53.705447: step 6343, loss 0.000524781, acc 1
2017-09-08T18:19:54.339459: step 6344, loss 0.0109399, acc 1
2017-09-08T18:19:54.955436: step 6345, loss 0.000931737, acc 1
2017-09-08T18:19:55.460480: step 6346, loss 0.018412, acc 0.984375
2017-09-08T18:19:56.013693: step 6347, loss 0.00610139, acc 1
2017-09-08T18:19:56.608047: step 6348, loss 0.00299897, acc 1
2017-09-08T18:19:57.246189: step 6349, loss 0.0337816, acc 0.984375
2017-09-08T18:19:57.924435: step 6350, loss 0.0289474, acc 0.984375

Evaluation:
2017-09-08T18:19:58.628386: step 6350, loss 0.226383, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6350

2017-09-08T18:20:01.039983: step 6351, loss 0.000329481, acc 1
2017-09-08T18:20:01.665755: step 6352, loss 0.0943277, acc 0.953125
2017-09-08T18:20:02.279511: step 6353, loss 0.0396626, acc 0.984375
2017-09-08T18:20:02.913593: step 6354, loss 0.000284221, acc 1
2017-09-08T18:20:03.640694: step 6355, loss 0.000283457, acc 1
2017-09-08T18:20:04.181192: step 6356, loss 0.00495661, acc 1
2017-09-08T18:20:04.758003: step 6357, loss 0.0861161, acc 0.953125
2017-09-08T18:20:05.374709: step 6358, loss 0.00092578, acc 1
2017-09-08T18:20:05.995142: step 6359, loss 0.00129363, acc 1
2017-09-08T18:20:06.662565: step 6360, loss 0.00178105, acc 1
2017-09-08T18:20:07.374013: step 6361, loss 0.00136232, acc 1
2017-09-08T18:20:08.053226: step 6362, loss 0.0549549, acc 0.984375
2017-09-08T18:20:08.749469: step 6363, loss 0.0359515, acc 0.984375
2017-09-08T18:20:09.437211: step 6364, loss 0.0725063, acc 0.984375
2017-09-08T18:20:10.064691: step 6365, loss 0.0386665, acc 0.984375
2017-09-08T18:20:10.665195: step 6366, loss 0.0337903, acc 0.96875
2017-09-08T18:20:11.256778: step 6367, loss 0.0556851, acc 0.96875
2017-09-08T18:20:11.909061: step 6368, loss 0.024056, acc 0.984375
2017-09-08T18:20:12.561860: step 6369, loss 0.000522632, acc 1
2017-09-08T18:20:13.163023: step 6370, loss 0.0544692, acc 0.980392
2017-09-08T18:20:13.809299: step 6371, loss 0.00330111, acc 1
2017-09-08T18:20:14.420456: step 6372, loss 0.000754197, acc 1
2017-09-08T18:20:15.046977: step 6373, loss 0.00744996, acc 1
2017-09-08T18:20:15.707624: step 6374, loss 0.00170152, acc 1
2017-09-08T18:20:16.368208: step 6375, loss 0.00035922, acc 1
2017-09-08T18:20:16.998308: step 6376, loss 0.024369, acc 0.984375
2017-09-08T18:20:17.580882: step 6377, loss 0.0110638, acc 1
2017-09-08T18:20:18.268409: step 6378, loss 0.00127182, acc 1
2017-09-08T18:20:18.839450: step 6379, loss 0.0128637, acc 1
2017-09-08T18:20:19.431897: step 6380, loss 0.0127988, acc 0.984375
2017-09-08T18:20:20.043118: step 6381, loss 0.038763, acc 0.984375
2017-09-08T18:20:20.661078: step 6382, loss 0.00166274, acc 1
2017-09-08T18:20:21.298458: step 6383, loss 0.0540961, acc 0.984375
2017-09-08T18:20:21.963438: step 6384, loss 0.00473946, acc 1
2017-09-08T18:20:22.610853: step 6385, loss 0.000595133, acc 1
2017-09-08T18:20:23.254801: step 6386, loss 0.0271235, acc 0.984375
2017-09-08T18:20:23.917333: step 6387, loss 0.0459935, acc 0.96875
2017-09-08T18:20:24.538791: step 6388, loss 0.00541261, acc 1
2017-09-08T18:20:25.177581: step 6389, loss 0.00381299, acc 1
2017-09-08T18:20:25.786657: step 6390, loss 0.0410491, acc 0.984375
2017-09-08T18:20:26.413918: step 6391, loss 0.0549185, acc 0.96875
2017-09-08T18:20:27.002534: step 6392, loss 0.000295539, acc 1
2017-09-08T18:20:27.561076: step 6393, loss 0.000196732, acc 1
2017-09-08T18:20:28.082612: step 6394, loss 0.000362194, acc 1
2017-09-08T18:20:28.674764: step 6395, loss 0.000616402, acc 1
2017-09-08T18:20:29.304663: step 6396, loss 0.000562756, acc 1
2017-09-08T18:20:29.926913: step 6397, loss 0.00418046, acc 1
2017-09-08T18:20:30.574040: step 6398, loss 0.0138973, acc 1
2017-09-08T18:20:31.248109: step 6399, loss 0.0300354, acc 0.984375
2017-09-08T18:20:31.920948: step 6400, loss 0.0527135, acc 0.96875

Evaluation:
2017-09-08T18:20:32.668076: step 6400, loss 0.220576, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6400

2017-09-08T18:20:36.335195: step 6401, loss 0.0376883, acc 0.96875
2017-09-08T18:20:36.990610: step 6402, loss 0.0165483, acc 0.984375
2017-09-08T18:20:37.646706: step 6403, loss 0.0276056, acc 1
2017-09-08T18:20:38.378934: step 6404, loss 0.0353304, acc 0.984375
2017-09-08T18:20:39.000528: step 6405, loss 0.00121516, acc 1
2017-09-08T18:20:39.598087: step 6406, loss 0.0212368, acc 0.984375
2017-09-08T18:20:40.164855: step 6407, loss 0.0474913, acc 0.96875
2017-09-08T18:20:40.780538: step 6408, loss 0.00562795, acc 1
2017-09-08T18:20:41.471461: step 6409, loss 0.0317507, acc 0.984375
2017-09-08T18:20:42.149730: step 6410, loss 0.00952689, acc 1
2017-09-08T18:20:42.807610: step 6411, loss 0.000353021, acc 1
2017-09-08T18:20:43.505378: step 6412, loss 0.000672881, acc 1
2017-09-08T18:20:44.200543: step 6413, loss 0.0316424, acc 0.984375
2017-09-08T18:20:44.792496: step 6414, loss 0.0444279, acc 0.984375
2017-09-08T18:20:45.395057: step 6415, loss 0.00434644, acc 1
2017-09-08T18:20:45.955861: step 6416, loss 0.0338109, acc 0.984375
2017-09-08T18:20:46.558674: step 6417, loss 0.000289007, acc 1
2017-09-08T18:20:47.175109: step 6418, loss 0.000849752, acc 1
2017-09-08T18:20:47.886714: step 6419, loss 0.0119372, acc 1
2017-09-08T18:20:48.500600: step 6420, loss 0.00123598, acc 1
2017-09-08T18:20:49.165970: step 6421, loss 0.00616937, acc 1
2017-09-08T18:20:49.869107: step 6422, loss 0.00736537, acc 1
2017-09-08T18:20:50.558252: step 6423, loss 0.00955358, acc 1
2017-09-08T18:20:51.235708: step 6424, loss 0.000314276, acc 1
2017-09-08T18:20:51.929811: step 6425, loss 0.00107224, acc 1
2017-09-08T18:20:52.624145: step 6426, loss 0.000378104, acc 1
2017-09-08T18:20:53.222062: step 6427, loss 0.00204357, acc 1
2017-09-08T18:20:53.813873: step 6428, loss 0.144998, acc 0.953125
2017-09-08T18:20:54.408193: step 6429, loss 0.0921403, acc 0.96875
2017-09-08T18:20:55.059294: step 6430, loss 0.00221879, acc 1
2017-09-08T18:20:55.718670: step 6431, loss 0.034626, acc 0.984375
2017-09-08T18:20:56.368796: step 6432, loss 0.0273781, acc 0.984375
2017-09-08T18:20:57.021030: step 6433, loss 0.0102929, acc 1
2017-09-08T18:20:57.723495: step 6434, loss 0.000219929, acc 1
2017-09-08T18:20:58.365375: step 6435, loss 0.000808076, acc 1
2017-09-08T18:20:59.036539: step 6436, loss 0.0254163, acc 0.984375
2017-09-08T18:20:59.684834: step 6437, loss 0.0255507, acc 0.984375
2017-09-08T18:21:00.360002: step 6438, loss 0.000902463, acc 1
2017-09-08T18:21:00.987653: step 6439, loss 0.000699536, acc 1
2017-09-08T18:21:01.604729: step 6440, loss 0.000740379, acc 1
2017-09-08T18:21:02.281114: step 6441, loss 0.000564044, acc 1
2017-09-08T18:21:03.318650: step 6442, loss 0.014207, acc 1
2017-09-08T18:21:03.986515: step 6443, loss 0.00694936, acc 1
2017-09-08T18:21:04.667700: step 6444, loss 0.00756719, acc 1
2017-09-08T18:21:05.337618: step 6445, loss 0.000703066, acc 1
2017-09-08T18:21:05.955228: step 6446, loss 0.000689226, acc 1
2017-09-08T18:21:06.588395: step 6447, loss 0.000362086, acc 1
2017-09-08T18:21:07.218613: step 6448, loss 0.000791315, acc 1
2017-09-08T18:21:07.838774: step 6449, loss 0.00801863, acc 1
2017-09-08T18:21:08.496219: step 6450, loss 0.00456341, acc 1

Evaluation:
2017-09-08T18:21:09.145223: step 6450, loss 0.230928, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6450

2017-09-08T18:21:11.569399: step 6451, loss 0.00206, acc 1
2017-09-08T18:21:12.209583: step 6452, loss 0.00149201, acc 1
2017-09-08T18:21:12.912511: step 6453, loss 0.00309233, acc 1
2017-09-08T18:21:13.619892: step 6454, loss 0.00209071, acc 1
2017-09-08T18:21:14.331823: step 6455, loss 0.0274887, acc 1
2017-09-08T18:21:14.929707: step 6456, loss 0.000548859, acc 1
2017-09-08T18:21:15.498057: step 6457, loss 0.000684631, acc 1
2017-09-08T18:21:16.141511: step 6458, loss 0.0231697, acc 0.984375
2017-09-08T18:21:16.826120: step 6459, loss 0.0838753, acc 0.984375
2017-09-08T18:21:17.560519: step 6460, loss 0.0585285, acc 0.984375
2017-09-08T18:21:18.324854: step 6461, loss 0.0192772, acc 0.984375
2017-09-08T18:21:19.015206: step 6462, loss 0.000934268, acc 1
2017-09-08T18:21:19.638608: step 6463, loss 0.00113441, acc 1
2017-09-08T18:21:20.196873: step 6464, loss 0.0238244, acc 0.984375
2017-09-08T18:21:20.794259: step 6465, loss 0.0357755, acc 0.984375
2017-09-08T18:21:21.440870: step 6466, loss 0.0843312, acc 0.96875
2017-09-08T18:21:22.101318: step 6467, loss 0.0556484, acc 0.984375
2017-09-08T18:21:22.719849: step 6468, loss 0.0944004, acc 0.980392
2017-09-08T18:21:23.453805: step 6469, loss 0.000359031, acc 1
2017-09-08T18:21:24.105274: step 6470, loss 0.00330059, acc 1
2017-09-08T18:21:24.758736: step 6471, loss 0.0181033, acc 0.984375
2017-09-08T18:21:25.435623: step 6472, loss 0.0475734, acc 0.984375
2017-09-08T18:21:26.133288: step 6473, loss 0.000768124, acc 1
2017-09-08T18:21:26.779991: step 6474, loss 0.0926188, acc 0.96875
2017-09-08T18:21:27.404840: step 6475, loss 0.00217004, acc 1
2017-09-08T18:21:28.030329: step 6476, loss 0.0216821, acc 0.984375
2017-09-08T18:21:28.608561: step 6477, loss 0.000804527, acc 1
2017-09-08T18:21:29.184720: step 6478, loss 0.0424978, acc 0.984375
2017-09-08T18:21:29.744719: step 6479, loss 0.0238491, acc 0.984375
2017-09-08T18:21:30.318534: step 6480, loss 0.0743615, acc 0.984375
2017-09-08T18:21:31.009316: step 6481, loss 0.00525769, acc 1
2017-09-08T18:21:31.649808: step 6482, loss 0.0831202, acc 0.96875
2017-09-08T18:21:32.317099: step 6483, loss 0.0224258, acc 0.984375
2017-09-08T18:21:32.964186: step 6484, loss 0.0650091, acc 0.96875
2017-09-08T18:21:33.603948: step 6485, loss 0.00261261, acc 1
2017-09-08T18:21:34.253693: step 6486, loss 0.00194699, acc 1
2017-09-08T18:21:34.890590: step 6487, loss 0.00364147, acc 1
2017-09-08T18:21:35.544577: step 6488, loss 0.0818854, acc 0.984375
2017-09-08T18:21:36.189799: step 6489, loss 0.000862207, acc 1
2017-09-08T18:21:36.882506: step 6490, loss 0.0427033, acc 0.984375
2017-09-08T18:21:37.571516: step 6491, loss 0.00446465, acc 1
2017-09-08T18:21:38.303502: step 6492, loss 0.00827035, acc 1
2017-09-08T18:21:39.131486: step 6493, loss 0.000866048, acc 1
2017-09-08T18:21:39.754061: step 6494, loss 0.000847163, acc 1
2017-09-08T18:21:40.404483: step 6495, loss 0.00163081, acc 1
2017-09-08T18:21:41.047594: step 6496, loss 0.0166886, acc 1
2017-09-08T18:21:41.744673: step 6497, loss 0.0014005, acc 1
2017-09-08T18:21:42.361242: step 6498, loss 0.0410336, acc 0.96875
2017-09-08T18:21:42.994302: step 6499, loss 0.00182535, acc 1
2017-09-08T18:21:43.618328: step 6500, loss 0.0348667, acc 0.984375

Evaluation:
2017-09-08T18:21:44.038639: step 6500, loss 0.216685, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6500

2017-09-08T18:21:47.162416: step 6501, loss 0.00424099, acc 1
2017-09-08T18:21:47.810427: step 6502, loss 0.0119975, acc 1
2017-09-08T18:21:48.604180: step 6503, loss 0.0162449, acc 1
2017-09-08T18:21:49.176650: step 6504, loss 0.00104616, acc 1
2017-09-08T18:21:49.763909: step 6505, loss 0.00047193, acc 1
2017-09-08T18:21:50.320748: step 6506, loss 0.00036551, acc 1
2017-09-08T18:21:50.909433: step 6507, loss 0.0811257, acc 0.96875
2017-09-08T18:21:51.628839: step 6508, loss 0.00420023, acc 1
2017-09-08T18:21:52.287007: step 6509, loss 0.0630732, acc 0.96875
2017-09-08T18:21:52.969432: step 6510, loss 0.0426919, acc 0.984375
2017-09-08T18:21:53.624860: step 6511, loss 0.0351368, acc 0.984375
2017-09-08T18:21:54.351455: step 6512, loss 0.000746666, acc 1
2017-09-08T18:21:54.871169: step 6513, loss 0.000968361, acc 1
2017-09-08T18:21:55.490138: step 6514, loss 0.0011099, acc 1
2017-09-08T18:21:56.094105: step 6515, loss 0.00089587, acc 1
2017-09-08T18:21:56.715278: step 6516, loss 0.00064751, acc 1
2017-09-08T18:21:57.370747: step 6517, loss 0.0653861, acc 0.984375
2017-09-08T18:21:58.067283: step 6518, loss 0.00134839, acc 1
2017-09-08T18:21:58.725689: step 6519, loss 0.00882115, acc 1
2017-09-08T18:21:59.440279: step 6520, loss 0.000268191, acc 1
2017-09-08T18:22:00.107997: step 6521, loss 0.00270158, acc 1
2017-09-08T18:22:00.788217: step 6522, loss 0.0438477, acc 0.984375
2017-09-08T18:22:01.504932: step 6523, loss 0.00103622, acc 1
2017-09-08T18:22:02.132055: step 6524, loss 0.00320535, acc 1
2017-09-08T18:22:02.814960: step 6525, loss 0.00224224, acc 1
2017-09-08T18:22:03.395663: step 6526, loss 0.000996511, acc 1
2017-09-08T18:22:03.986418: step 6527, loss 0.00121523, acc 1
2017-09-08T18:22:04.578790: step 6528, loss 0.00105021, acc 1
2017-09-08T18:22:05.202884: step 6529, loss 0.0307349, acc 0.984375
2017-09-08T18:22:05.931791: step 6530, loss 0.0150819, acc 1
2017-09-08T18:22:06.624726: step 6531, loss 0.000230138, acc 1
2017-09-08T18:22:07.317823: step 6532, loss 0.0651306, acc 0.96875
2017-09-08T18:22:07.940167: step 6533, loss 0.000613854, acc 1
2017-09-08T18:22:08.569106: step 6534, loss 0.0521631, acc 0.984375
2017-09-08T18:22:09.221048: step 6535, loss 0.0420513, acc 0.984375
2017-09-08T18:22:09.869217: step 6536, loss 0.0508494, acc 0.984375
2017-09-08T18:22:10.489538: step 6537, loss 0.000794935, acc 1
2017-09-08T18:22:11.085863: step 6538, loss 0.0160395, acc 1
2017-09-08T18:22:11.748615: step 6539, loss 0.00836728, acc 1
2017-09-08T18:22:12.399837: step 6540, loss 0.00092165, acc 1
2017-09-08T18:22:13.038445: step 6541, loss 0.0595212, acc 0.984375
2017-09-08T18:22:13.687134: step 6542, loss 0.000364032, acc 1
2017-09-08T18:22:14.298715: step 6543, loss 0.00589497, acc 1
2017-09-08T18:22:15.037583: step 6544, loss 0.025875, acc 0.984375
2017-09-08T18:22:15.674353: step 6545, loss 0.00380218, acc 1
2017-09-08T18:22:16.362465: step 6546, loss 0.0113497, acc 1
2017-09-08T18:22:17.008413: step 6547, loss 0.00100705, acc 1
2017-09-08T18:22:17.629256: step 6548, loss 0.070158, acc 0.984375
2017-09-08T18:22:18.312634: step 6549, loss 0.0002347, acc 1
2017-09-08T18:22:18.922898: step 6550, loss 0.000497541, acc 1

Evaluation:
2017-09-08T18:22:19.565884: step 6550, loss 0.223049, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6550

2017-09-08T18:22:22.812930: step 6551, loss 0.107518, acc 0.984375
2017-09-08T18:22:23.548265: step 6552, loss 0.00707025, acc 1
2017-09-08T18:22:24.104317: step 6553, loss 0.0397798, acc 0.984375
2017-09-08T18:22:24.690311: step 6554, loss 0.000567038, acc 1
2017-09-08T18:22:25.288908: step 6555, loss 0.0151412, acc 0.984375
2017-09-08T18:22:25.899372: step 6556, loss 0.0345531, acc 0.984375
2017-09-08T18:22:26.488133: step 6557, loss 0.0669538, acc 0.984375
2017-09-08T18:22:27.269315: step 6558, loss 0.0325114, acc 0.984375
2017-09-08T18:22:27.948267: step 6559, loss 0.000177209, acc 1
2017-09-08T18:22:28.517033: step 6560, loss 0.100459, acc 0.96875
2017-09-08T18:22:29.230187: step 6561, loss 0.0434077, acc 0.96875
2017-09-08T18:22:29.815798: step 6562, loss 0.00509115, acc 1
2017-09-08T18:22:30.391405: step 6563, loss 0.000424868, acc 1
2017-09-08T18:22:30.966453: step 6564, loss 0.00536327, acc 1
2017-09-08T18:22:31.606186: step 6565, loss 0.0369215, acc 0.984375
2017-09-08T18:22:32.194891: step 6566, loss 0.000292679, acc 1
2017-09-08T18:22:32.828346: step 6567, loss 0.0906134, acc 0.984375
2017-09-08T18:22:33.476882: step 6568, loss 0.000552087, acc 1
2017-09-08T18:22:34.143330: step 6569, loss 0.000961573, acc 1
2017-09-08T18:22:34.893432: step 6570, loss 0.026769, acc 0.984375
2017-09-08T18:22:35.601740: step 6571, loss 0.0158009, acc 1
2017-09-08T18:22:36.333249: step 6572, loss 0.00547401, acc 1
2017-09-08T18:22:36.926449: step 6573, loss 0.0280357, acc 0.984375
2017-09-08T18:22:37.513601: step 6574, loss 0.00331619, acc 1
2017-09-08T18:22:38.082870: step 6575, loss 0.00151071, acc 1
2017-09-08T18:22:38.653186: step 6576, loss 0.00138011, acc 1
2017-09-08T18:22:39.290391: step 6577, loss 0.0047683, acc 1
2017-09-08T18:22:39.928958: step 6578, loss 0.0039433, acc 1
2017-09-08T18:22:40.623559: step 6579, loss 0.071381, acc 0.984375
2017-09-08T18:22:41.328478: step 6580, loss 0.0021597, acc 1
2017-09-08T18:22:42.008731: step 6581, loss 0.0450547, acc 0.984375
2017-09-08T18:22:42.727881: step 6582, loss 0.000530131, acc 1
2017-09-08T18:22:43.390594: step 6583, loss 0.0366411, acc 0.984375
2017-09-08T18:22:44.034534: step 6584, loss 0.0145866, acc 0.984375
2017-09-08T18:22:44.650700: step 6585, loss 0.037014, acc 0.984375
2017-09-08T18:22:45.378650: step 6586, loss 0.0624206, acc 0.984375
2017-09-08T18:22:46.006179: step 6587, loss 0.0134744, acc 1
2017-09-08T18:22:46.682484: step 6588, loss 0.0451444, acc 0.984375
2017-09-08T18:22:47.277882: step 6589, loss 0.000497709, acc 1
2017-09-08T18:22:47.976562: step 6590, loss 0.000696674, acc 1
2017-09-08T18:22:48.642850: step 6591, loss 0.00323231, acc 1
2017-09-08T18:22:49.324847: step 6592, loss 0.0107871, acc 1
2017-09-08T18:22:50.005772: step 6593, loss 0.0322792, acc 0.984375
2017-09-08T18:22:50.659609: step 6594, loss 0.0854212, acc 0.984375
2017-09-08T18:22:51.334376: step 6595, loss 0.000703451, acc 1
2017-09-08T18:22:51.968602: step 6596, loss 0.00456697, acc 1
2017-09-08T18:22:52.555129: step 6597, loss 0.00483763, acc 1
2017-09-08T18:22:53.102752: step 6598, loss 0.0188178, acc 0.984375
2017-09-08T18:22:53.593346: step 6599, loss 0.000763235, acc 1
2017-09-08T18:22:54.217884: step 6600, loss 0.00817981, acc 1

Evaluation:
2017-09-08T18:22:54.865707: step 6600, loss 0.212468, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6600

2017-09-08T18:22:57.309747: step 6601, loss 0.00082972, acc 1
2017-09-08T18:22:57.943295: step 6602, loss 0.00839437, acc 1
2017-09-08T18:22:58.615335: step 6603, loss 0.00356118, acc 1
2017-09-08T18:22:59.284044: step 6604, loss 0.000532056, acc 1
2017-09-08T18:22:59.837757: step 6605, loss 0.00145601, acc 1
2017-09-08T18:23:00.425469: step 6606, loss 0.000294858, acc 1
2017-09-08T18:23:01.024694: step 6607, loss 0.000320723, acc 1
2017-09-08T18:23:01.684677: step 6608, loss 0.00533776, acc 1
2017-09-08T18:23:02.362840: step 6609, loss 0.043213, acc 0.984375
2017-09-08T18:23:03.080552: step 6610, loss 0.00107148, acc 1
2017-09-08T18:23:03.645922: step 6611, loss 0.000472327, acc 1
2017-09-08T18:23:04.224879: step 6612, loss 0.0511341, acc 0.96875
2017-09-08T18:23:04.799235: step 6613, loss 0.000505506, acc 1
2017-09-08T18:23:05.382275: step 6614, loss 0.000482283, acc 1
2017-09-08T18:23:06.004089: step 6615, loss 0.00219411, acc 1
2017-09-08T18:23:06.684374: step 6616, loss 0.000695405, acc 1
2017-09-08T18:23:07.322302: step 6617, loss 0.0327191, acc 0.984375
2017-09-08T18:23:07.955252: step 6618, loss 0.0220174, acc 0.984375
2017-09-08T18:23:08.599954: step 6619, loss 0.00117066, acc 1
2017-09-08T18:23:09.257248: step 6620, loss 0.000670438, acc 1
2017-09-08T18:23:09.903136: step 6621, loss 0.0160112, acc 0.984375
2017-09-08T18:23:10.556690: step 6622, loss 0.0014217, acc 1
2017-09-08T18:23:11.211217: step 6623, loss 0.030794, acc 0.984375
2017-09-08T18:23:11.932214: step 6624, loss 0.00700351, acc 1
2017-09-08T18:23:12.547342: step 6625, loss 0.0120163, acc 1
2017-09-08T18:23:13.179948: step 6626, loss 0.0148659, acc 1
2017-09-08T18:23:13.710692: step 6627, loss 0.00270375, acc 1
2017-09-08T18:23:14.270765: step 6628, loss 0.101061, acc 0.96875
2017-09-08T18:23:14.850548: step 6629, loss 0.0417607, acc 0.984375
2017-09-08T18:23:15.423294: step 6630, loss 0.00061579, acc 1
2017-09-08T18:23:16.051548: step 6631, loss 0.00945608, acc 1
2017-09-08T18:23:16.681813: step 6632, loss 0.033096, acc 0.984375
2017-09-08T18:23:17.822357: step 6633, loss 0.0302362, acc 0.984375
2017-09-08T18:23:18.467143: step 6634, loss 0.00138318, acc 1
2017-09-08T18:23:19.065656: step 6635, loss 0.000255743, acc 1
2017-09-08T18:23:19.701926: step 6636, loss 0.0368192, acc 0.96875
2017-09-08T18:23:20.328040: step 6637, loss 0.000443405, acc 1
2017-09-08T18:23:20.944532: step 6638, loss 0.000367165, acc 1
2017-09-08T18:23:21.603722: step 6639, loss 0.0633184, acc 0.984375
2017-09-08T18:23:22.232924: step 6640, loss 0.000263075, acc 1
2017-09-08T18:23:22.906045: step 6641, loss 0.0579628, acc 0.96875
2017-09-08T18:23:23.550459: step 6642, loss 0.0493609, acc 0.96875
2017-09-08T18:23:24.217277: step 6643, loss 0.0420823, acc 0.984375
2017-09-08T18:23:24.858611: step 6644, loss 0.0239494, acc 0.984375
2017-09-08T18:23:25.588242: step 6645, loss 0.0490687, acc 0.984375
2017-09-08T18:23:26.228349: step 6646, loss 0.00822064, acc 1
2017-09-08T18:23:26.894090: step 6647, loss 0.00194718, acc 1
2017-09-08T18:23:27.548688: step 6648, loss 0.0202864, acc 0.984375
2017-09-08T18:23:28.126276: step 6649, loss 0.000372323, acc 1
2017-09-08T18:23:28.656491: step 6650, loss 0.0366385, acc 0.984375

Evaluation:
2017-09-08T18:23:29.292461: step 6650, loss 0.226154, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6650

2017-09-08T18:23:32.497730: step 6651, loss 0.0111674, acc 1
2017-09-08T18:23:33.068571: step 6652, loss 0.0037647, acc 1
2017-09-08T18:23:33.679670: step 6653, loss 0.00125295, acc 1
2017-09-08T18:23:34.147549: step 6654, loss 0.0238038, acc 0.984375
2017-09-08T18:23:34.675185: step 6655, loss 0.00279076, acc 1
2017-09-08T18:23:35.225551: step 6656, loss 0.000729791, acc 1
2017-09-08T18:23:35.846724: step 6657, loss 0.0322976, acc 0.984375
2017-09-08T18:23:36.442956: step 6658, loss 0.000964931, acc 1
2017-09-08T18:23:37.064056: step 6659, loss 0.000636547, acc 1
2017-09-08T18:23:37.784216: step 6660, loss 0.000858209, acc 1
2017-09-08T18:23:38.281188: step 6661, loss 0.000413894, acc 1
2017-09-08T18:23:38.825492: step 6662, loss 0.0552299, acc 0.984375
2017-09-08T18:23:39.388154: step 6663, loss 0.0876979, acc 0.96875
2017-09-08T18:23:39.886216: step 6664, loss 0.00910855, acc 1
2017-09-08T18:23:40.512500: step 6665, loss 0.000450511, acc 1
2017-09-08T18:23:41.145697: step 6666, loss 0.0404534, acc 0.984375
2017-09-08T18:23:41.821952: step 6667, loss 0.000933629, acc 1
2017-09-08T18:23:42.504669: step 6668, loss 0.00760171, acc 1
2017-09-08T18:23:43.162057: step 6669, loss 0.0242188, acc 0.984375
2017-09-08T18:23:43.807639: step 6670, loss 0.00828452, acc 1
2017-09-08T18:23:44.513432: step 6671, loss 0.0105696, acc 1
2017-09-08T18:23:45.175562: step 6672, loss 0.0209779, acc 0.984375
2017-09-08T18:23:45.881026: step 6673, loss 0.000736708, acc 1
2017-09-08T18:23:46.587549: step 6674, loss 0.00439765, acc 1
2017-09-08T18:23:47.236072: step 6675, loss 0.000587296, acc 1
2017-09-08T18:23:47.898577: step 6676, loss 0.000287384, acc 1
2017-09-08T18:23:48.502546: step 6677, loss 0.00225947, acc 1
2017-09-08T18:23:49.069947: step 6678, loss 0.0126575, acc 0.984375
2017-09-08T18:23:49.657989: step 6679, loss 0.0302596, acc 0.984375
2017-09-08T18:23:50.325061: step 6680, loss 0.0495092, acc 0.984375
2017-09-08T18:23:51.015060: step 6681, loss 0.000688748, acc 1
2017-09-08T18:23:51.749650: step 6682, loss 0.00423308, acc 1
2017-09-08T18:23:52.394107: step 6683, loss 0.00374542, acc 1
2017-09-08T18:23:53.092891: step 6684, loss 0.018298, acc 0.984375
2017-09-08T18:23:53.801618: step 6685, loss 0.0249754, acc 0.984375
2017-09-08T18:23:54.452473: step 6686, loss 0.00577718, acc 1
2017-09-08T18:23:55.125307: step 6687, loss 0.00336437, acc 1
2017-09-08T18:23:55.750063: step 6688, loss 0.047061, acc 0.96875
2017-09-08T18:23:56.467813: step 6689, loss 0.0839732, acc 0.984375
2017-09-08T18:23:57.158994: step 6690, loss 0.00135746, acc 1
2017-09-08T18:23:57.881383: step 6691, loss 0.000641412, acc 1
2017-09-08T18:23:58.528254: step 6692, loss 0.00254694, acc 1
2017-09-08T18:23:59.185459: step 6693, loss 0.0462276, acc 0.984375
2017-09-08T18:23:59.802716: step 6694, loss 0.0125819, acc 1
2017-09-08T18:24:00.443717: step 6695, loss 0.0013809, acc 1
2017-09-08T18:24:01.139038: step 6696, loss 0.0678954, acc 0.96875
2017-09-08T18:24:01.793468: step 6697, loss 0.000485059, acc 1
2017-09-08T18:24:02.398982: step 6698, loss 0.000181149, acc 1
2017-09-08T18:24:03.054625: step 6699, loss 0.000362565, acc 1
2017-09-08T18:24:03.703145: step 6700, loss 0.000181713, acc 1

Evaluation:
2017-09-08T18:24:04.429350: step 6700, loss 0.225041, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6700

2017-09-08T18:24:06.942792: step 6701, loss 0.0262877, acc 0.984375
2017-09-08T18:24:07.602073: step 6702, loss 0.0717717, acc 0.96875
2017-09-08T18:24:08.213847: step 6703, loss 0.0464141, acc 0.984375
2017-09-08T18:24:08.854356: step 6704, loss 0.0196828, acc 0.984375
2017-09-08T18:24:09.532896: step 6705, loss 0.116498, acc 0.953125
2017-09-08T18:24:10.099398: step 6706, loss 0.000911217, acc 1
2017-09-08T18:24:10.687522: step 6707, loss 0.000747322, acc 1
2017-09-08T18:24:11.260293: step 6708, loss 0.000518581, acc 1
2017-09-08T18:24:11.932607: step 6709, loss 0.000557361, acc 1
2017-09-08T18:24:12.591487: step 6710, loss 0.0280615, acc 0.984375
2017-09-08T18:24:13.200318: step 6711, loss 0.0210225, acc 0.984375
2017-09-08T18:24:13.780424: step 6712, loss 0.0177939, acc 1
2017-09-08T18:24:14.337143: step 6713, loss 0.000909744, acc 1
2017-09-08T18:24:14.962723: step 6714, loss 0.00601532, acc 1
2017-09-08T18:24:15.629399: step 6715, loss 0.000123608, acc 1
2017-09-08T18:24:16.299212: step 6716, loss 0.0373514, acc 0.984375
2017-09-08T18:24:16.958193: step 6717, loss 0.000640845, acc 1
2017-09-08T18:24:17.596811: step 6718, loss 0.00245574, acc 1
2017-09-08T18:24:18.400303: step 6719, loss 0.000522066, acc 1
2017-09-08T18:24:19.033957: step 6720, loss 0.0017016, acc 1
2017-09-08T18:24:19.753952: step 6721, loss 0.0104254, acc 1
2017-09-08T18:24:20.426247: step 6722, loss 0.000852849, acc 1
2017-09-08T18:24:21.102719: step 6723, loss 0.00325455, acc 1
2017-09-08T18:24:21.682492: step 6724, loss 0.00293794, acc 1
2017-09-08T18:24:22.271393: step 6725, loss 0.0365364, acc 0.984375
2017-09-08T18:24:22.822109: step 6726, loss 0.000477888, acc 1
2017-09-08T18:24:23.376454: step 6727, loss 0.00625492, acc 1
2017-09-08T18:24:23.949275: step 6728, loss 0.00417922, acc 1
2017-09-08T18:24:24.671617: step 6729, loss 0.00110415, acc 1
2017-09-08T18:24:25.346851: step 6730, loss 0.0285545, acc 0.984375
2017-09-08T18:24:25.983758: step 6731, loss 0.0258976, acc 1
2017-09-08T18:24:26.625484: step 6732, loss 0.00169815, acc 1
2017-09-08T18:24:27.278255: step 6733, loss 0.0206825, acc 0.984375
2017-09-08T18:24:27.892591: step 6734, loss 0.0158838, acc 0.984375
2017-09-08T18:24:28.526963: step 6735, loss 0.0219052, acc 0.984375
2017-09-08T18:24:29.141962: step 6736, loss 0.0530566, acc 0.984375
2017-09-08T18:24:29.742445: step 6737, loss 0.0366012, acc 0.96875
2017-09-08T18:24:30.395092: step 6738, loss 0.00163244, acc 1
2017-09-08T18:24:30.983826: step 6739, loss 0.0028299, acc 1
2017-09-08T18:24:31.565920: step 6740, loss 0.0396607, acc 0.984375
2017-09-08T18:24:32.055715: step 6741, loss 0.0410826, acc 0.984375
2017-09-08T18:24:32.680369: step 6742, loss 0.020688, acc 0.984375
2017-09-08T18:24:33.320610: step 6743, loss 0.00822651, acc 1
2017-09-08T18:24:33.968451: step 6744, loss 0.000194662, acc 1
2017-09-08T18:24:34.613705: step 6745, loss 0.00167123, acc 1
2017-09-08T18:24:35.275727: step 6746, loss 0.00176654, acc 1
2017-09-08T18:24:35.877049: step 6747, loss 0.000983315, acc 1
2017-09-08T18:24:36.543098: step 6748, loss 0.00318752, acc 1
2017-09-08T18:24:37.195133: step 6749, loss 0.0847589, acc 0.96875
2017-09-08T18:24:37.878925: step 6750, loss 0.00791882, acc 1

Evaluation:
2017-09-08T18:24:38.533412: step 6750, loss 0.240669, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6750

2017-09-08T18:24:41.524654: step 6751, loss 0.00123433, acc 1
2017-09-08T18:24:42.188809: step 6752, loss 0.0168528, acc 1
2017-09-08T18:24:42.818078: step 6753, loss 0.000517712, acc 1
2017-09-08T18:24:43.491988: step 6754, loss 0.000362177, acc 1
2017-09-08T18:24:44.252662: step 6755, loss 0.0746864, acc 0.984375
2017-09-08T18:24:44.831834: step 6756, loss 0.0132868, acc 1
2017-09-08T18:24:45.492569: step 6757, loss 0.00870789, acc 1
2017-09-08T18:24:45.929078: step 6758, loss 0.00228713, acc 1
2017-09-08T18:24:46.420700: step 6759, loss 0.00134916, acc 1
2017-09-08T18:24:47.023379: step 6760, loss 0.0125317, acc 1
2017-09-08T18:24:47.608216: step 6761, loss 0.0279679, acc 0.984375
2017-09-08T18:24:48.057647: step 6762, loss 0.000541895, acc 1
2017-09-08T18:24:48.674371: step 6763, loss 0.00202752, acc 1
2017-09-08T18:24:49.269666: step 6764, loss 0.00104193, acc 1
2017-09-08T18:24:49.847154: step 6765, loss 0.0302256, acc 0.984375
2017-09-08T18:24:50.584603: step 6766, loss 0.000209214, acc 1
2017-09-08T18:24:51.274729: step 6767, loss 0.0464714, acc 0.96875
2017-09-08T18:24:51.956343: step 6768, loss 0.000241096, acc 1
2017-09-08T18:24:52.611017: step 6769, loss 0.00282661, acc 1
2017-09-08T18:24:53.214327: step 6770, loss 0.0210383, acc 0.984375
2017-09-08T18:24:53.856466: step 6771, loss 0.000913536, acc 1
2017-09-08T18:24:54.497592: step 6772, loss 0.0284815, acc 0.984375
2017-09-08T18:24:55.181993: step 6773, loss 0.00133553, acc 1
2017-09-08T18:24:55.894117: step 6774, loss 0.00027203, acc 1
2017-09-08T18:24:56.467540: step 6775, loss 0.0159457, acc 0.984375
2017-09-08T18:24:57.036712: step 6776, loss 0.011556, acc 1
2017-09-08T18:24:57.683410: step 6777, loss 0.00242672, acc 1
2017-09-08T18:24:58.382773: step 6778, loss 0.00124597, acc 1
2017-09-08T18:24:59.071454: step 6779, loss 0.0569276, acc 0.984375
2017-09-08T18:24:59.799141: step 6780, loss 0.00736893, acc 1
2017-09-08T18:25:00.517701: step 6781, loss 0.0270552, acc 0.984375
2017-09-08T18:25:01.245441: step 6782, loss 0.0393044, acc 0.984375
2017-09-08T18:25:01.913515: step 6783, loss 0.0149697, acc 0.984375
2017-09-08T18:25:02.534235: step 6784, loss 0.000581498, acc 1
2017-09-08T18:25:03.169292: step 6785, loss 0.00467965, acc 1
2017-09-08T18:25:03.841579: step 6786, loss 0.0181504, acc 0.984375
2017-09-08T18:25:04.531359: step 6787, loss 0.00451564, acc 1
2017-09-08T18:25:05.240563: step 6788, loss 0.00515049, acc 1
2017-09-08T18:25:05.914988: step 6789, loss 0.000272961, acc 1
2017-09-08T18:25:06.577993: step 6790, loss 0.015921, acc 0.984375
2017-09-08T18:25:07.254429: step 6791, loss 0.000262823, acc 1
2017-09-08T18:25:07.908866: step 6792, loss 0.00078037, acc 1
2017-09-08T18:25:08.520958: step 6793, loss 0.0164291, acc 0.984375
2017-09-08T18:25:09.167813: step 6794, loss 0.0725828, acc 0.96875
2017-09-08T18:25:09.793049: step 6795, loss 0.000213182, acc 1
2017-09-08T18:25:10.441579: step 6796, loss 0.0308345, acc 0.96875
2017-09-08T18:25:11.104629: step 6797, loss 0.000398456, acc 1
2017-09-08T18:25:11.795859: step 6798, loss 0.000799406, acc 1
2017-09-08T18:25:12.436251: step 6799, loss 0.112008, acc 0.96875
2017-09-08T18:25:13.072035: step 6800, loss 0.000655546, acc 1

Evaluation:
2017-09-08T18:25:13.738287: step 6800, loss 0.246646, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6800

2017-09-08T18:25:17.292705: step 6801, loss 0.000331275, acc 1
2017-09-08T18:25:17.989781: step 6802, loss 0.0676082, acc 0.984375
2017-09-08T18:25:18.709334: step 6803, loss 0.000648936, acc 1
2017-09-08T18:25:19.283404: step 6804, loss 0.00969737, acc 1
2017-09-08T18:25:19.960869: step 6805, loss 0.0177472, acc 0.984375
2017-09-08T18:25:20.457902: step 6806, loss 0.00302959, acc 1
2017-09-08T18:25:20.942239: step 6807, loss 0.00250103, acc 1
2017-09-08T18:25:21.429194: step 6808, loss 0.0160999, acc 0.984375
2017-09-08T18:25:22.077417: step 6809, loss 0.035712, acc 0.984375
2017-09-08T18:25:22.693737: step 6810, loss 0.0561292, acc 0.984375
2017-09-08T18:25:23.353098: step 6811, loss 0.00910405, acc 1
2017-09-08T18:25:24.022962: step 6812, loss 0.000371235, acc 1
2017-09-08T18:25:24.661731: step 6813, loss 0.00302673, acc 1
2017-09-08T18:25:25.327886: step 6814, loss 0.000905384, acc 1
2017-09-08T18:25:25.940218: step 6815, loss 0.0118543, acc 1
2017-09-08T18:25:26.564992: step 6816, loss 0.00802049, acc 1
2017-09-08T18:25:27.230360: step 6817, loss 0.0554291, acc 0.984375
2017-09-08T18:25:27.900531: step 6818, loss 0.000576202, acc 1
2017-09-08T18:25:28.587935: step 6819, loss 0.00141341, acc 1
2017-09-08T18:25:29.224629: step 6820, loss 0.0155008, acc 0.984375
2017-09-08T18:25:29.894066: step 6821, loss 0.032953, acc 0.984375
2017-09-08T18:25:30.472378: step 6822, loss 0.000214314, acc 1
2017-09-08T18:25:31.019984: step 6823, loss 0.00185698, acc 1
2017-09-08T18:25:31.602567: step 6824, loss 0.0633215, acc 0.96875
2017-09-08T18:25:32.297048: step 6825, loss 0.0184403, acc 0.984375
2017-09-08T18:25:32.981476: step 6826, loss 0.0639903, acc 0.984375
2017-09-08T18:25:33.632595: step 6827, loss 0.0449845, acc 0.984375
2017-09-08T18:25:34.289523: step 6828, loss 0.0670408, acc 0.984375
2017-09-08T18:25:34.884343: step 6829, loss 0.035327, acc 0.984375
2017-09-08T18:25:35.545347: step 6830, loss 0.000678459, acc 1
2017-09-08T18:25:36.213481: step 6831, loss 0.00589627, acc 1
2017-09-08T18:25:36.919264: step 6832, loss 0.00149276, acc 1
2017-09-08T18:25:37.635657: step 6833, loss 0.00975974, acc 1
2017-09-08T18:25:38.286695: step 6834, loss 0.000776607, acc 1
2017-09-08T18:25:38.947653: step 6835, loss 0.000869839, acc 1
2017-09-08T18:25:39.596820: step 6836, loss 0.0208969, acc 0.984375
2017-09-08T18:25:40.202129: step 6837, loss 0.02978, acc 0.984375
2017-09-08T18:25:40.894623: step 6838, loss 0.000799547, acc 1
2017-09-08T18:25:41.576198: step 6839, loss 0.0605298, acc 0.984375
2017-09-08T18:25:42.254229: step 6840, loss 0.0601783, acc 0.984375
2017-09-08T18:25:42.952827: step 6841, loss 0.00863954, acc 1
2017-09-08T18:25:43.571485: step 6842, loss 0.0493957, acc 0.984375
2017-09-08T18:25:44.282281: step 6843, loss 0.000651238, acc 1
2017-09-08T18:25:44.955937: step 6844, loss 0.0103603, acc 1
2017-09-08T18:25:45.581938: step 6845, loss 0.0299902, acc 0.984375
2017-09-08T18:25:46.235732: step 6846, loss 0.0441813, acc 0.984375
2017-09-08T18:25:46.899135: step 6847, loss 0.0241787, acc 0.984375
2017-09-08T18:25:47.567183: step 6848, loss 0.000214142, acc 1
2017-09-08T18:25:48.233032: step 6849, loss 0.0079397, acc 1
2017-09-08T18:25:48.843193: step 6850, loss 0.000795304, acc 1

Evaluation:
2017-09-08T18:25:49.524278: step 6850, loss 0.243167, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6850

2017-09-08T18:25:51.884208: step 6851, loss 0.000299843, acc 1
2017-09-08T18:25:52.440922: step 6852, loss 0.0344897, acc 0.984375
2017-09-08T18:25:53.012936: step 6853, loss 0.000536297, acc 1
2017-09-08T18:25:53.652279: step 6854, loss 0.0423095, acc 0.96875
2017-09-08T18:25:54.395520: step 6855, loss 0.00838978, acc 1
2017-09-08T18:25:54.850542: step 6856, loss 0.0010001, acc 1
2017-09-08T18:25:55.354136: step 6857, loss 0.0022725, acc 1
2017-09-08T18:25:55.892350: step 6858, loss 0.018001, acc 0.984375
2017-09-08T18:25:56.501233: step 6859, loss 0.0322316, acc 0.984375
2017-09-08T18:25:57.109909: step 6860, loss 0.000384099, acc 1
2017-09-08T18:25:57.716577: step 6861, loss 0.00260048, acc 1
2017-09-08T18:25:58.323017: step 6862, loss 0.10153, acc 0.96875
2017-09-08T18:25:58.891121: step 6863, loss 0.0306517, acc 0.984375
2017-09-08T18:25:59.527418: step 6864, loss 0.00154232, acc 1
2017-09-08T18:26:00.074269: step 6865, loss 0.000862392, acc 1
2017-09-08T18:26:00.732972: step 6866, loss 0.00243876, acc 1
2017-09-08T18:26:01.374677: step 6867, loss 0.00072781, acc 1
2017-09-08T18:26:01.982560: step 6868, loss 0.000447162, acc 1
2017-09-08T18:26:02.618469: step 6869, loss 0.00102714, acc 1
2017-09-08T18:26:03.228783: step 6870, loss 0.000358274, acc 1
2017-09-08T18:26:03.887942: step 6871, loss 0.00160762, acc 1
2017-09-08T18:26:04.461602: step 6872, loss 0.0137709, acc 0.984375
2017-09-08T18:26:05.029782: step 6873, loss 0.00145442, acc 1
2017-09-08T18:26:05.611618: step 6874, loss 0.00485027, acc 1
2017-09-08T18:26:06.181504: step 6875, loss 0.000342209, acc 1
2017-09-08T18:26:06.794925: step 6876, loss 0.00058132, acc 1
2017-09-08T18:26:07.451682: step 6877, loss 0.00174348, acc 1
2017-09-08T18:26:08.159913: step 6878, loss 0.000645878, acc 1
2017-09-08T18:26:08.816548: step 6879, loss 0.00931204, acc 1
2017-09-08T18:26:09.487495: step 6880, loss 0.00209214, acc 1
2017-09-08T18:26:10.135340: step 6881, loss 0.00982458, acc 1
2017-09-08T18:26:10.799601: step 6882, loss 0.0782741, acc 0.953125
2017-09-08T18:26:11.424756: step 6883, loss 0.00054607, acc 1
2017-09-08T18:26:12.044922: step 6884, loss 0.0992981, acc 0.96875
2017-09-08T18:26:12.657755: step 6885, loss 0.000653785, acc 1
2017-09-08T18:26:13.328010: step 6886, loss 0.0318788, acc 0.984375
2017-09-08T18:26:14.020415: step 6887, loss 0.00438484, acc 1
2017-09-08T18:26:14.680032: step 6888, loss 0.055101, acc 0.96875
2017-09-08T18:26:15.313297: step 6889, loss 0.00150699, acc 1
2017-09-08T18:26:15.989571: step 6890, loss 0.00417472, acc 1
2017-09-08T18:26:16.641702: step 6891, loss 0.00110128, acc 1
2017-09-08T18:26:17.266725: step 6892, loss 0.0452743, acc 0.984375
2017-09-08T18:26:17.922290: step 6893, loss 0.00658591, acc 1
2017-09-08T18:26:18.612475: step 6894, loss 0.00477913, acc 1
2017-09-08T18:26:19.254743: step 6895, loss 0.000403761, acc 1
2017-09-08T18:26:19.973917: step 6896, loss 0.000176194, acc 1
2017-09-08T18:26:20.629537: step 6897, loss 0.00082602, acc 1
2017-09-08T18:26:21.289219: step 6898, loss 0.0430802, acc 0.984375
2017-09-08T18:26:21.942557: step 6899, loss 0.00446545, acc 1
2017-09-08T18:26:22.614491: step 6900, loss 0.0113194, acc 1

Evaluation:
2017-09-08T18:26:23.256682: step 6900, loss 0.233611, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6900

2017-09-08T18:26:26.312078: step 6901, loss 0.00065226, acc 1
2017-09-08T18:26:26.976375: step 6902, loss 0.00677487, acc 1
2017-09-08T18:26:27.672532: step 6903, loss 0.0504639, acc 0.984375
2017-09-08T18:26:28.282016: step 6904, loss 0.0528925, acc 0.984375
2017-09-08T18:26:28.895947: step 6905, loss 0.0925999, acc 0.984375
2017-09-08T18:26:29.483524: step 6906, loss 0.000408811, acc 1
2017-09-08T18:26:29.986345: step 6907, loss 0.0222675, acc 0.984375
2017-09-08T18:26:30.486887: step 6908, loss 0.000533246, acc 1
2017-09-08T18:26:31.044841: step 6909, loss 0.0150671, acc 0.984375
2017-09-08T18:26:31.607860: step 6910, loss 0.00947264, acc 1
2017-09-08T18:26:32.242421: step 6911, loss 0.0461619, acc 0.984375
2017-09-08T18:26:32.856526: step 6912, loss 0.00376239, acc 1
2017-09-08T18:26:33.479598: step 6913, loss 0.0247137, acc 0.984375
2017-09-08T18:26:34.174185: step 6914, loss 0.0102957, acc 1
2017-09-08T18:26:34.838221: step 6915, loss 0.000334652, acc 1
2017-09-08T18:26:35.504250: step 6916, loss 0.00206877, acc 1
2017-09-08T18:26:36.129422: step 6917, loss 0.0003629, acc 1
2017-09-08T18:26:36.791699: step 6918, loss 0.000260869, acc 1
2017-09-08T18:26:37.441097: step 6919, loss 0.000945039, acc 1
2017-09-08T18:26:38.242819: step 6920, loss 0.000962763, acc 1
2017-09-08T18:26:38.889586: step 6921, loss 0.00218741, acc 1
2017-09-08T18:26:39.447575: step 6922, loss 0.00498798, acc 1
2017-09-08T18:26:40.010549: step 6923, loss 0.00030436, acc 1
2017-09-08T18:26:40.705713: step 6924, loss 0.0823369, acc 0.984375
2017-09-08T18:26:41.389900: step 6925, loss 0.0023192, acc 1
2017-09-08T18:26:42.006097: step 6926, loss 0.0199264, acc 0.984375
2017-09-08T18:26:42.645812: step 6927, loss 0.0230708, acc 0.984375
2017-09-08T18:26:43.370343: step 6928, loss 0.0145029, acc 0.984375
2017-09-08T18:26:44.025095: step 6929, loss 0.00611553, acc 1
2017-09-08T18:26:44.722306: step 6930, loss 0.0295552, acc 0.984375
2017-09-08T18:26:45.398580: step 6931, loss 0.00108169, acc 1
2017-09-08T18:26:46.063121: step 6932, loss 0.0224294, acc 0.984375
2017-09-08T18:26:46.717203: step 6933, loss 0.000724289, acc 1
2017-09-08T18:26:47.395724: step 6934, loss 0.0260823, acc 0.984375
2017-09-08T18:26:48.013671: step 6935, loss 0.0479049, acc 0.984375
2017-09-08T18:26:48.635440: step 6936, loss 0.0131115, acc 0.984375
2017-09-08T18:26:49.334167: step 6937, loss 0.0642253, acc 0.96875
2017-09-08T18:26:49.992022: step 6938, loss 0.0981437, acc 0.9375
2017-09-08T18:26:50.710634: step 6939, loss 0.0119677, acc 1
2017-09-08T18:26:51.384965: step 6940, loss 0.00327817, acc 1
2017-09-08T18:26:51.998845: step 6941, loss 0.0212256, acc 0.984375
2017-09-08T18:26:52.650537: step 6942, loss 0.00876569, acc 1
2017-09-08T18:26:53.330506: step 6943, loss 0.000392347, acc 1
2017-09-08T18:26:54.000711: step 6944, loss 0.0323976, acc 0.984375
2017-09-08T18:26:54.648481: step 6945, loss 0.0244829, acc 0.984375
2017-09-08T18:26:55.298724: step 6946, loss 0.0283817, acc 0.984375
2017-09-08T18:26:55.941843: step 6947, loss 0.00102253, acc 1
2017-09-08T18:26:56.592006: step 6948, loss 0.0388938, acc 0.984375
2017-09-08T18:26:57.196383: step 6949, loss 0.0211714, acc 0.984375
2017-09-08T18:26:57.847520: step 6950, loss 0.0100022, acc 1

Evaluation:
2017-09-08T18:26:58.495602: step 6950, loss 0.246588, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-6950

2017-09-08T18:27:00.903599: step 6951, loss 0.0385925, acc 0.984375
2017-09-08T18:27:01.540056: step 6952, loss 0.0167124, acc 1
2017-09-08T18:27:02.184645: step 6953, loss 0.107228, acc 0.953125
2017-09-08T18:27:02.950035: step 6954, loss 0.0495861, acc 0.984375
2017-09-08T18:27:03.510525: step 6955, loss 0.000373846, acc 1
2017-09-08T18:27:04.127367: step 6956, loss 0.0120228, acc 1
2017-09-08T18:27:04.605570: step 6957, loss 0.0259842, acc 0.984375
2017-09-08T18:27:05.141878: step 6958, loss 0.0194524, acc 1
2017-09-08T18:27:05.730376: step 6959, loss 0.000446015, acc 1
2017-09-08T18:27:06.398256: step 6960, loss 0.000168135, acc 1
2017-09-08T18:27:07.035486: step 6961, loss 0.0281276, acc 0.984375
2017-09-08T18:27:07.714872: step 6962, loss 0.000474157, acc 1
2017-09-08T18:27:08.422833: step 6963, loss 0.00922657, acc 1
2017-09-08T18:27:09.084399: step 6964, loss 0.0265754, acc 0.984375
2017-09-08T18:27:09.730183: step 6965, loss 0.000428689, acc 1
2017-09-08T18:27:10.357883: step 6966, loss 0.000763852, acc 1
2017-09-08T18:27:10.910786: step 6967, loss 0.0221472, acc 0.984375
2017-09-08T18:27:11.453915: step 6968, loss 0.000252562, acc 1
2017-09-08T18:27:12.008867: step 6969, loss 0.0497894, acc 0.984375
2017-09-08T18:27:12.671013: step 6970, loss 0.0441202, acc 0.984375
2017-09-08T18:27:13.417585: step 6971, loss 0.0468145, acc 0.953125
2017-09-08T18:27:14.012550: step 6972, loss 0.0198511, acc 1
2017-09-08T18:27:14.611663: step 6973, loss 0.00715499, acc 1
2017-09-08T18:27:15.252173: step 6974, loss 0.00199626, acc 1
2017-09-08T18:27:15.828530: step 6975, loss 0.0526098, acc 0.984375
2017-09-08T18:27:16.570035: step 6976, loss 0.0856142, acc 0.984375
2017-09-08T18:27:17.197933: step 6977, loss 0.00109844, acc 1
2017-09-08T18:27:17.875287: step 6978, loss 0.0246538, acc 0.984375
2017-09-08T18:27:18.514692: step 6979, loss 0.000384401, acc 1
2017-09-08T18:27:19.206656: step 6980, loss 0.0144407, acc 1
2017-09-08T18:27:19.817223: step 6981, loss 0.000346725, acc 1
2017-09-08T18:27:20.462149: step 6982, loss 0.0729791, acc 0.984375
2017-09-08T18:27:21.126755: step 6983, loss 0.0419732, acc 0.984375
2017-09-08T18:27:21.824676: step 6984, loss 0.000298645, acc 1
2017-09-08T18:27:22.486394: step 6985, loss 0.00268538, acc 1
2017-09-08T18:27:23.157567: step 6986, loss 0.000794073, acc 1
2017-09-08T18:27:23.796071: step 6987, loss 0.00227572, acc 1
2017-09-08T18:27:24.498932: step 6988, loss 0.00016288, acc 1
2017-09-08T18:27:25.202800: step 6989, loss 0.0427218, acc 0.96875
2017-09-08T18:27:25.881516: step 6990, loss 0.017881, acc 0.984375
2017-09-08T18:27:26.532490: step 6991, loss 0.0490638, acc 0.984375
2017-09-08T18:27:27.200947: step 6992, loss 0.00363535, acc 1
2017-09-08T18:27:27.843232: step 6993, loss 0.00039331, acc 1
2017-09-08T18:27:28.547032: step 6994, loss 0.0018432, acc 1
2017-09-08T18:27:29.181551: step 6995, loss 0.00110645, acc 1
2017-09-08T18:27:29.838523: step 6996, loss 0.0147278, acc 1
2017-09-08T18:27:30.485671: step 6997, loss 0.000647789, acc 1
2017-09-08T18:27:31.141493: step 6998, loss 0.0335941, acc 0.984375
2017-09-08T18:27:31.846993: step 6999, loss 0.0231795, acc 0.984375
2017-09-08T18:27:32.550581: step 7000, loss 0.0102407, acc 1

Evaluation:
2017-09-08T18:27:33.176999: step 7000, loss 0.2431, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7000

2017-09-08T18:27:35.881215: step 7001, loss 0.0164389, acc 0.984375
2017-09-08T18:27:36.546697: step 7002, loss 0.0432641, acc 0.984375
2017-09-08T18:27:37.196341: step 7003, loss 0.0529861, acc 0.984375
2017-09-08T18:27:37.780181: step 7004, loss 0.000702029, acc 1
2017-09-08T18:27:38.385075: step 7005, loss 0.0100345, acc 1
2017-09-08T18:27:38.877263: step 7006, loss 0.0657132, acc 0.96875
2017-09-08T18:27:39.392573: step 7007, loss 0.000238281, acc 1
2017-09-08T18:27:40.000270: step 7008, loss 0.0585092, acc 0.984375
2017-09-08T18:27:40.600812: step 7009, loss 0.00961975, acc 1
2017-09-08T18:27:41.270489: step 7010, loss 0.0507394, acc 0.984375
2017-09-08T18:27:41.927633: step 7011, loss 0.0498362, acc 0.96875
2017-09-08T18:27:42.591782: step 7012, loss 0.0509919, acc 0.96875
2017-09-08T18:27:43.315298: step 7013, loss 0.0119863, acc 0.984375
2017-09-08T18:27:43.987981: step 7014, loss 0.0699985, acc 0.984375
2017-09-08T18:27:44.682351: step 7015, loss 0.0253682, acc 0.984375
2017-09-08T18:27:45.328088: step 7016, loss 0.000234438, acc 1
2017-09-08T18:27:45.945679: step 7017, loss 0.000323872, acc 1
2017-09-08T18:27:46.575072: step 7018, loss 0.00111927, acc 1
2017-09-08T18:27:47.296520: step 7019, loss 0.0290736, acc 0.984375
2017-09-08T18:27:47.833006: step 7020, loss 0.0184779, acc 0.984375
2017-09-08T18:27:48.411043: step 7021, loss 0.069107, acc 0.96875
2017-09-08T18:27:48.996252: step 7022, loss 0.000528672, acc 1
2017-09-08T18:27:49.681988: step 7023, loss 0.0517228, acc 0.984375
2017-09-08T18:27:50.287182: step 7024, loss 0.00715321, acc 1
2017-09-08T18:27:51.060513: step 7025, loss 0.0690129, acc 0.96875
2017-09-08T18:27:51.673355: step 7026, loss 0.0761877, acc 0.984375
2017-09-08T18:27:52.343606: step 7027, loss 0.00847497, acc 1
2017-09-08T18:27:53.015591: step 7028, loss 0.00116394, acc 1
2017-09-08T18:27:53.630115: step 7029, loss 0.0739391, acc 0.96875
2017-09-08T18:27:54.274997: step 7030, loss 0.0162104, acc 0.984375
2017-09-08T18:27:54.968444: step 7031, loss 0.0647058, acc 0.984375
2017-09-08T18:27:55.580030: step 7032, loss 0.00935185, acc 1
2017-09-08T18:27:56.251329: step 7033, loss 0.0380365, acc 0.984375
2017-09-08T18:27:56.888362: step 7034, loss 0.00266483, acc 1
2017-09-08T18:27:57.500855: step 7035, loss 0.000839047, acc 1
2017-09-08T18:27:58.177190: step 7036, loss 0.0042569, acc 1
2017-09-08T18:27:58.812211: step 7037, loss 0.0320025, acc 0.984375
2017-09-08T18:27:59.530699: step 7038, loss 0.000519405, acc 1
2017-09-08T18:28:00.167402: step 7039, loss 0.000264648, acc 1
2017-09-08T18:28:00.750238: step 7040, loss 0.0110022, acc 1
2017-09-08T18:28:01.383134: step 7041, loss 0.000301533, acc 1
2017-09-08T18:28:02.037276: step 7042, loss 0.02107, acc 0.984375
2017-09-08T18:28:02.679279: step 7043, loss 0.000225742, acc 1
2017-09-08T18:28:03.317320: step 7044, loss 0.000722951, acc 1
2017-09-08T18:28:03.939674: step 7045, loss 0.0538146, acc 0.984375
2017-09-08T18:28:04.581605: step 7046, loss 0.000694139, acc 1
2017-09-08T18:28:05.203355: step 7047, loss 0.00438502, acc 1
2017-09-08T18:28:05.840781: step 7048, loss 0.0538491, acc 0.984375
2017-09-08T18:28:06.474070: step 7049, loss 0.0151875, acc 1
2017-09-08T18:28:07.127999: step 7050, loss 0.00148301, acc 1

Evaluation:
2017-09-08T18:28:07.843123: step 7050, loss 0.24144, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7050

2017-09-08T18:28:11.470766: step 7051, loss 0.0322472, acc 0.984375
2017-09-08T18:28:12.183913: step 7052, loss 0.000879519, acc 1
2017-09-08T18:28:12.735508: step 7053, loss 0.000322646, acc 1
2017-09-08T18:28:13.463967: step 7054, loss 0.0155952, acc 0.984375
2017-09-08T18:28:13.936967: step 7055, loss 0.00599819, acc 1
2017-09-08T18:28:14.369451: step 7056, loss 0.00292236, acc 1
2017-09-08T18:28:14.870355: step 7057, loss 0.00127694, acc 1
2017-09-08T18:28:15.349762: step 7058, loss 0.00754944, acc 1
2017-09-08T18:28:15.947106: step 7059, loss 0.0602231, acc 0.984375
2017-09-08T18:28:16.484128: step 7060, loss 0.0214665, acc 0.984375
2017-09-08T18:28:17.044778: step 7061, loss 0.000314739, acc 1
2017-09-08T18:28:17.584017: step 7062, loss 0.00145477, acc 1
2017-09-08T18:28:18.022397: step 7063, loss 0.0045943, acc 1
2017-09-08T18:28:18.531184: step 7064, loss 0.0195193, acc 0.984375
2017-09-08T18:28:19.157241: step 7065, loss 0.000210412, acc 1
2017-09-08T18:28:19.720649: step 7066, loss 0.0663313, acc 0.96875
2017-09-08T18:28:20.293815: step 7067, loss 0.00046336, acc 1
2017-09-08T18:28:20.947191: step 7068, loss 0.000341604, acc 1
2017-09-08T18:28:21.386639: step 7069, loss 0.0291315, acc 0.984375
2017-09-08T18:28:21.863005: step 7070, loss 0.0764221, acc 0.96875
2017-09-08T18:28:22.342858: step 7071, loss 0.00967565, acc 1
2017-09-08T18:28:22.813739: step 7072, loss 0.00216654, acc 1
2017-09-08T18:28:23.318059: step 7073, loss 0.00176918, acc 1
2017-09-08T18:28:23.900155: step 7074, loss 0.00080093, acc 1
2017-09-08T18:28:24.518135: step 7075, loss 0.000398343, acc 1
2017-09-08T18:28:25.125295: step 7076, loss 0.00360913, acc 1
2017-09-08T18:28:25.717805: step 7077, loss 0.00231828, acc 1
2017-09-08T18:28:26.294494: step 7078, loss 0.00605577, acc 1
2017-09-08T18:28:26.923241: step 7079, loss 0.007564, acc 1
2017-09-08T18:28:27.475015: step 7080, loss 0.00306609, acc 1
2017-09-08T18:28:28.023489: step 7081, loss 0.00114703, acc 1
2017-09-08T18:28:28.607589: step 7082, loss 0.00102366, acc 1
2017-09-08T18:28:29.146216: step 7083, loss 0.00124934, acc 1
2017-09-08T18:28:29.700387: step 7084, loss 0.00135941, acc 1
2017-09-08T18:28:30.270056: step 7085, loss 0.0203525, acc 0.984375
2017-09-08T18:28:30.855872: step 7086, loss 0.00435098, acc 1
2017-09-08T18:28:31.426118: step 7087, loss 0.0782666, acc 0.984375
2017-09-08T18:28:32.046259: step 7088, loss 0.00018579, acc 1
2017-09-08T18:28:32.630737: step 7089, loss 0.0864691, acc 0.96875
2017-09-08T18:28:33.206893: step 7090, loss 0.000310349, acc 1
2017-09-08T18:28:33.798311: step 7091, loss 0.00500894, acc 1
2017-09-08T18:28:34.361532: step 7092, loss 0.000281977, acc 1
2017-09-08T18:28:34.918569: step 7093, loss 0.000715963, acc 1
2017-09-08T18:28:35.564789: step 7094, loss 0.000865634, acc 1
2017-09-08T18:28:36.148565: step 7095, loss 0.000304853, acc 1
2017-09-08T18:28:36.708570: step 7096, loss 0.00573688, acc 1
2017-09-08T18:28:37.308294: step 7097, loss 0.0457132, acc 0.96875
2017-09-08T18:28:37.877241: step 7098, loss 0.0115391, acc 0.984375
2017-09-08T18:28:38.461318: step 7099, loss 0.0516602, acc 0.96875
2017-09-08T18:28:39.038593: step 7100, loss 0.0134063, acc 0.984375

Evaluation:
2017-09-08T18:28:39.648513: step 7100, loss 0.233959, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7100

2017-09-08T18:28:41.501983: step 7101, loss 0.000253754, acc 1
2017-09-08T18:28:42.008057: step 7102, loss 0.0637939, acc 0.96875
2017-09-08T18:28:42.505815: step 7103, loss 0.000176893, acc 1
2017-09-08T18:28:43.034728: step 7104, loss 0.000652962, acc 1
2017-09-08T18:28:43.590493: step 7105, loss 0.00355438, acc 1
2017-09-08T18:28:44.202363: step 7106, loss 0.0624224, acc 0.984375
2017-09-08T18:28:44.691801: step 7107, loss 0.0686932, acc 0.984375
2017-09-08T18:28:45.181036: step 7108, loss 0.000368665, acc 1
2017-09-08T18:28:45.683877: step 7109, loss 0.036208, acc 0.984375
2017-09-08T18:28:46.152702: step 7110, loss 0.0490904, acc 0.984375
2017-09-08T18:28:46.746827: step 7111, loss 0.00078445, acc 1
2017-09-08T18:28:47.281953: step 7112, loss 0.0285987, acc 0.984375
2017-09-08T18:28:47.835963: step 7113, loss 0.0215123, acc 0.984375
2017-09-08T18:28:48.432351: step 7114, loss 0.00140966, acc 1
2017-09-08T18:28:49.029967: step 7115, loss 0.0416798, acc 0.984375
2017-09-08T18:28:49.616100: step 7116, loss 0.0416749, acc 0.984375
2017-09-08T18:28:50.177003: step 7117, loss 0.00943915, acc 1
2017-09-08T18:28:50.761881: step 7118, loss 0.00182754, acc 1
2017-09-08T18:28:51.338544: step 7119, loss 0.000549779, acc 1
2017-09-08T18:28:51.819861: step 7120, loss 0.0211937, acc 0.984375
2017-09-08T18:28:52.317522: step 7121, loss 0.00203195, acc 1
2017-09-08T18:28:52.796648: step 7122, loss 0.0501204, acc 0.984375
2017-09-08T18:28:53.288214: step 7123, loss 0.0239318, acc 0.984375
2017-09-08T18:28:53.803898: step 7124, loss 0.00144862, acc 1
2017-09-08T18:28:54.390979: step 7125, loss 0.053665, acc 0.953125
2017-09-08T18:28:54.955888: step 7126, loss 0.0204132, acc 1
2017-09-08T18:28:55.551742: step 7127, loss 0.040793, acc 0.984375
2017-09-08T18:28:56.124922: step 7128, loss 0.00959623, acc 1
2017-09-08T18:28:56.722586: step 7129, loss 0.000514368, acc 1
2017-09-08T18:28:57.305797: step 7130, loss 0.000203713, acc 1
2017-09-08T18:28:57.875520: step 7131, loss 0.000646492, acc 1
2017-09-08T18:28:58.454148: step 7132, loss 0.00588856, acc 1
2017-09-08T18:28:59.003860: step 7133, loss 0.0071062, acc 1
2017-09-08T18:28:59.568508: step 7134, loss 0.0318808, acc 0.984375
2017-09-08T18:29:00.135651: step 7135, loss 0.000619723, acc 1
2017-09-08T18:29:00.712433: step 7136, loss 0.0147841, acc 0.984375
2017-09-08T18:29:01.315747: step 7137, loss 0.0354155, acc 0.984375
2017-09-08T18:29:01.901638: step 7138, loss 0.0384989, acc 0.984375
2017-09-08T18:29:02.479731: step 7139, loss 0.000195128, acc 1
2017-09-08T18:29:02.950049: step 7140, loss 0.000865985, acc 1
2017-09-08T18:29:03.452765: step 7141, loss 0.0302348, acc 0.984375
2017-09-08T18:29:04.007629: step 7142, loss 0.041481, acc 0.96875
2017-09-08T18:29:04.557230: step 7143, loss 0.0599619, acc 0.96875
2017-09-08T18:29:05.087354: step 7144, loss 0.00349641, acc 1
2017-09-08T18:29:05.620300: step 7145, loss 0.0203729, acc 0.984375
2017-09-08T18:29:06.215752: step 7146, loss 0.0142532, acc 1
2017-09-08T18:29:06.798262: step 7147, loss 0.0161076, acc 0.984375
2017-09-08T18:29:07.408167: step 7148, loss 0.00333505, acc 1
2017-09-08T18:29:07.981586: step 7149, loss 0.0200604, acc 0.984375
2017-09-08T18:29:08.537703: step 7150, loss 0.0468433, acc 0.984375

Evaluation:
2017-09-08T18:29:09.124850: step 7150, loss 0.232326, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7150

2017-09-08T18:29:11.989361: step 7151, loss 0.000788557, acc 1
2017-09-08T18:29:12.466335: step 7152, loss 0.000296145, acc 1
2017-09-08T18:29:13.001869: step 7153, loss 0.000446247, acc 1
2017-09-08T18:29:13.476864: step 7154, loss 0.000541719, acc 1
2017-09-08T18:29:14.054397: step 7155, loss 0.0654016, acc 0.984375
2017-09-08T18:29:14.635965: step 7156, loss 0.0232143, acc 0.984375
2017-09-08T18:29:15.245601: step 7157, loss 0.000549349, acc 1
2017-09-08T18:29:15.763822: step 7158, loss 0.000120691, acc 1
2017-09-08T18:29:16.232750: step 7159, loss 0.000345969, acc 1
2017-09-08T18:29:16.739960: step 7160, loss 0.000214797, acc 1
2017-09-08T18:29:17.222871: step 7161, loss 0.000780575, acc 1
2017-09-08T18:29:17.753475: step 7162, loss 0.000996633, acc 1
2017-09-08T18:29:18.281904: step 7163, loss 0.0312379, acc 0.984375
2017-09-08T18:29:18.868499: step 7164, loss 0.00355819, acc 1
2017-09-08T18:29:19.417224: step 7165, loss 0.00310809, acc 1
2017-09-08T18:29:20.053387: step 7166, loss 0.00151815, acc 1
2017-09-08T18:29:20.591042: step 7167, loss 0.056092, acc 0.96875
2017-09-08T18:29:21.202378: step 7168, loss 0.0691917, acc 0.953125
2017-09-08T18:29:21.810645: step 7169, loss 0.0156202, acc 1
2017-09-08T18:29:22.378850: step 7170, loss 0.014656, acc 0.984375
2017-09-08T18:29:23.036098: step 7171, loss 0.00132807, acc 1
2017-09-08T18:29:23.549119: step 7172, loss 0.0173322, acc 1
2017-09-08T18:29:24.001634: step 7173, loss 0.0043576, acc 1
2017-09-08T18:29:24.511817: step 7174, loss 0.0120804, acc 1
2017-09-08T18:29:25.002075: step 7175, loss 0.0641214, acc 0.953125
2017-09-08T18:29:25.584593: step 7176, loss 0.000300718, acc 1
2017-09-08T18:29:26.179655: step 7177, loss 0.00522808, acc 1
2017-09-08T18:29:26.751489: step 7178, loss 0.0479737, acc 0.984375
2017-09-08T18:29:27.318641: step 7179, loss 0.0341159, acc 0.984375
2017-09-08T18:29:27.906801: step 7180, loss 0.000141789, acc 1
2017-09-08T18:29:28.479722: step 7181, loss 0.00023998, acc 1
2017-09-08T18:29:29.061193: step 7182, loss 0.000292872, acc 1
2017-09-08T18:29:29.622795: step 7183, loss 0.010622, acc 1
2017-09-08T18:29:30.218478: step 7184, loss 0.0292044, acc 0.984375
2017-09-08T18:29:30.790328: step 7185, loss 0.00586771, acc 1
2017-09-08T18:29:31.365468: step 7186, loss 0.00108219, acc 1
2017-09-08T18:29:31.926604: step 7187, loss 0.000439298, acc 1
2017-09-08T18:29:32.527260: step 7188, loss 0.021611, acc 0.984375
2017-09-08T18:29:33.126133: step 7189, loss 0.0400824, acc 0.984375
2017-09-08T18:29:33.657475: step 7190, loss 0.000933836, acc 1
2017-09-08T18:29:34.239154: step 7191, loss 0.00236303, acc 1
2017-09-08T18:29:34.803131: step 7192, loss 0.000330085, acc 1
2017-09-08T18:29:35.383240: step 7193, loss 0.118175, acc 0.96875
2017-09-08T18:29:35.949156: step 7194, loss 0.000652003, acc 1
2017-09-08T18:29:36.562676: step 7195, loss 0.000537806, acc 1
2017-09-08T18:29:37.170554: step 7196, loss 0.0260624, acc 0.984375
2017-09-08T18:29:37.712833: step 7197, loss 0.0297992, acc 0.984375
2017-09-08T18:29:38.289765: step 7198, loss 0.0292025, acc 0.984375
2017-09-08T18:29:38.856191: step 7199, loss 0.0109384, acc 1
2017-09-08T18:29:39.467808: step 7200, loss 0.0440572, acc 0.984375

Evaluation:
2017-09-08T18:29:40.043815: step 7200, loss 0.240753, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7200

2017-09-08T18:29:42.468003: step 7201, loss 0.00186462, acc 1
2017-09-08T18:29:43.042760: step 7202, loss 0.00128926, acc 1
2017-09-08T18:29:43.616799: step 7203, loss 0.0374644, acc 0.984375
2017-09-08T18:29:44.202753: step 7204, loss 2.6892e-05, acc 1
2017-09-08T18:29:44.802204: step 7205, loss 0.101689, acc 0.984375
2017-09-08T18:29:45.368861: step 7206, loss 0.00265125, acc 1
2017-09-08T18:29:45.953445: step 7207, loss 0.000873629, acc 1
2017-09-08T18:29:46.472081: step 7208, loss 0.00609005, acc 1
2017-09-08T18:29:47.024605: step 7209, loss 0.0471253, acc 0.984375
2017-09-08T18:29:47.523712: step 7210, loss 0.0708859, acc 0.96875
2017-09-08T18:29:48.023724: step 7211, loss 0.000399871, acc 1
2017-09-08T18:29:48.563144: step 7212, loss 0.00101599, acc 1
2017-09-08T18:29:49.099964: step 7213, loss 0.00113897, acc 1
2017-09-08T18:29:49.697216: step 7214, loss 0.0251303, acc 0.984375
2017-09-08T18:29:50.274230: step 7215, loss 0.0113944, acc 0.984375
2017-09-08T18:29:50.902353: step 7216, loss 0.0163092, acc 0.984375
2017-09-08T18:29:51.470521: step 7217, loss 0.0274759, acc 0.984375
2017-09-08T18:29:52.019621: step 7218, loss 0.000162173, acc 1
2017-09-08T18:29:52.630606: step 7219, loss 0.0547162, acc 0.984375
2017-09-08T18:29:53.158166: step 7220, loss 0.00788662, acc 1
2017-09-08T18:29:53.781172: step 7221, loss 0.037301, acc 0.96875
2017-09-08T18:29:54.275243: step 7222, loss 0.000403756, acc 1
2017-09-08T18:29:54.759114: step 7223, loss 0.0202862, acc 0.984375
2017-09-08T18:29:55.255304: step 7224, loss 0.0898747, acc 0.96875
2017-09-08T18:29:55.756418: step 7225, loss 0.0117941, acc 1
2017-09-08T18:29:56.235725: step 7226, loss 0.00597607, acc 1
2017-09-08T18:29:56.789512: step 7227, loss 0.000886577, acc 1
2017-09-08T18:29:57.369452: step 7228, loss 0.00379563, acc 1
2017-09-08T18:29:57.936672: step 7229, loss 0.00180319, acc 1
2017-09-08T18:29:58.534852: step 7230, loss 0.00310582, acc 1
2017-09-08T18:29:59.115037: step 7231, loss 0.0601218, acc 0.984375
2017-09-08T18:29:59.677391: step 7232, loss 0.00135515, acc 1
2017-09-08T18:30:00.243914: step 7233, loss 0.00153809, acc 1
2017-09-08T18:30:00.817408: step 7234, loss 0.000594373, acc 1
2017-09-08T18:30:01.388016: step 7235, loss 0.0467155, acc 0.96875
2017-09-08T18:30:02.003614: step 7236, loss 0.000578888, acc 1
2017-09-08T18:30:02.551288: step 7237, loss 0.0317614, acc 0.984375
2017-09-08T18:30:03.146305: step 7238, loss 0.00354366, acc 1
2017-09-08T18:30:03.727633: step 7239, loss 0.000338909, acc 1
2017-09-08T18:30:04.302529: step 7240, loss 0.0347035, acc 0.984375
2017-09-08T18:30:04.923847: step 7241, loss 0.0439498, acc 0.96875
2017-09-08T18:30:05.541590: step 7242, loss 0.0708832, acc 0.984375
2017-09-08T18:30:06.130857: step 7243, loss 0.000669038, acc 1
2017-09-08T18:30:06.702258: step 7244, loss 0.00545762, acc 1
2017-09-08T18:30:07.274417: step 7245, loss 0.00404286, acc 1
2017-09-08T18:30:07.821617: step 7246, loss 0.0906065, acc 0.96875
2017-09-08T18:30:08.390103: step 7247, loss 0.00607825, acc 1
2017-09-08T18:30:08.979488: step 7248, loss 0.0163482, acc 1
2017-09-08T18:30:09.528393: step 7249, loss 0.000545505, acc 1
2017-09-08T18:30:10.075143: step 7250, loss 0.000396401, acc 1

Evaluation:
2017-09-08T18:30:10.662357: step 7250, loss 0.224761, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7250

2017-09-08T18:30:12.943950: step 7251, loss 0.0167789, acc 0.984375
2017-09-08T18:30:13.430419: step 7252, loss 0.00612946, acc 1
2017-09-08T18:30:14.007863: step 7253, loss 0.000455673, acc 1
2017-09-08T18:30:14.596937: step 7254, loss 0.0248164, acc 0.984375
2017-09-08T18:30:15.179338: step 7255, loss 0.00312663, acc 1
2017-09-08T18:30:15.760557: step 7256, loss 0.00243072, acc 1
2017-09-08T18:30:16.317260: step 7257, loss 0.00369813, acc 1
2017-09-08T18:30:16.923067: step 7258, loss 0.000281502, acc 1
2017-09-08T18:30:17.414677: step 7259, loss 8.49686e-05, acc 1
2017-09-08T18:30:17.902044: step 7260, loss 0.000783671, acc 1
2017-09-08T18:30:18.361559: step 7261, loss 0.00502289, acc 1
2017-09-08T18:30:18.861567: step 7262, loss 0.00118904, acc 1
2017-09-08T18:30:19.332263: step 7263, loss 0.0627131, acc 0.96875
2017-09-08T18:30:19.917103: step 7264, loss 0.0001881, acc 1
2017-09-08T18:30:20.489016: step 7265, loss 0.0175636, acc 0.984375
2017-09-08T18:30:21.070822: step 7266, loss 0.00054142, acc 1
2017-09-08T18:30:21.649111: step 7267, loss 0.0258725, acc 0.984375
2017-09-08T18:30:22.266655: step 7268, loss 0.00102758, acc 1
2017-09-08T18:30:22.853135: step 7269, loss 0.0117074, acc 1
2017-09-08T18:30:23.425808: step 7270, loss 0.0403621, acc 0.984375
2017-09-08T18:30:23.986980: step 7271, loss 0.00261447, acc 1
2017-09-08T18:30:24.558289: step 7272, loss 0.0117848, acc 1
2017-09-08T18:30:25.156062: step 7273, loss 0.100797, acc 0.984375
2017-09-08T18:30:25.662140: step 7274, loss 0.00538931, acc 1
2017-09-08T18:30:26.150887: step 7275, loss 0.000809222, acc 1
2017-09-08T18:30:26.706616: step 7276, loss 0.00711782, acc 1
2017-09-08T18:30:27.284763: step 7277, loss 0.000297189, acc 1
2017-09-08T18:30:27.855851: step 7278, loss 0.0240594, acc 0.984375
2017-09-08T18:30:28.448711: step 7279, loss 0.00968897, acc 1
2017-09-08T18:30:28.979415: step 7280, loss 0.00190901, acc 1
2017-09-08T18:30:29.563170: step 7281, loss 0.0011922, acc 1
2017-09-08T18:30:30.120736: step 7282, loss 0.00463592, acc 1
2017-09-08T18:30:30.694903: step 7283, loss 0.023112, acc 0.984375
2017-09-08T18:30:31.259919: step 7284, loss 0.00273507, acc 1
2017-09-08T18:30:31.845262: step 7285, loss 0.00235142, acc 1
2017-09-08T18:30:32.402461: step 7286, loss 0.0262739, acc 0.984375
2017-09-08T18:30:32.946852: step 7287, loss 0.000890687, acc 1
2017-09-08T18:30:33.542437: step 7288, loss 0.0124702, acc 1
2017-09-08T18:30:34.124414: step 7289, loss 0.0373242, acc 0.984375
2017-09-08T18:30:34.681749: step 7290, loss 0.0231709, acc 0.984375
2017-09-08T18:30:35.252960: step 7291, loss 0.000513221, acc 1
2017-09-08T18:30:35.814103: step 7292, loss 0.000469383, acc 1
2017-09-08T18:30:36.376879: step 7293, loss 0.0200695, acc 0.984375
2017-09-08T18:30:36.967545: step 7294, loss 0.000391411, acc 1
2017-09-08T18:30:37.541510: step 7295, loss 0.000663188, acc 1
2017-09-08T18:30:38.090260: step 7296, loss 0.0061134, acc 1
2017-09-08T18:30:38.621368: step 7297, loss 0.000458434, acc 1
2017-09-08T18:30:39.171545: step 7298, loss 0.0165158, acc 0.984375
2017-09-08T18:30:39.755832: step 7299, loss 0.00228026, acc 1
2017-09-08T18:30:40.351017: step 7300, loss 0.0223974, acc 0.984375

Evaluation:
2017-09-08T18:30:41.031448: step 7300, loss 0.226391, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7300

2017-09-08T18:30:44.357577: step 7301, loss 0.000335997, acc 1
2017-09-08T18:30:44.865312: step 7302, loss 0.0413661, acc 0.984375
2017-09-08T18:30:45.346844: step 7303, loss 0.00461297, acc 1
2017-09-08T18:30:45.905243: step 7304, loss 0.00596816, acc 1
2017-09-08T18:30:46.470388: step 7305, loss 0.0116789, acc 1
2017-09-08T18:30:47.006887: step 7306, loss 0.0366067, acc 0.984375
2017-09-08T18:30:47.584353: step 7307, loss 0.0581926, acc 0.984375
2017-09-08T18:30:48.242015: step 7308, loss 0.0008761, acc 1
2017-09-08T18:30:48.689604: step 7309, loss 0.0750323, acc 0.9375
2017-09-08T18:30:49.152820: step 7310, loss 0.0308568, acc 0.984375
2017-09-08T18:30:49.651966: step 7311, loss 0.021939, acc 0.984375
2017-09-08T18:30:50.151322: step 7312, loss 0.000493787, acc 1
2017-09-08T18:30:50.653635: step 7313, loss 0.000270189, acc 1
2017-09-08T18:30:51.219837: step 7314, loss 0.0183569, acc 0.984375
2017-09-08T18:30:51.757330: step 7315, loss 0.000195399, acc 1
2017-09-08T18:30:52.289601: step 7316, loss 0.000588549, acc 1
2017-09-08T18:30:52.851090: step 7317, loss 0.00658129, acc 1
2017-09-08T18:30:53.428586: step 7318, loss 0.000173064, acc 1
2017-09-08T18:30:53.976919: step 7319, loss 0.0210648, acc 0.984375
2017-09-08T18:30:54.533617: step 7320, loss 0.000266406, acc 1
2017-09-08T18:30:55.149168: step 7321, loss 0.00692293, acc 1
2017-09-08T18:30:55.677588: step 7322, loss 0.00705601, acc 1
2017-09-08T18:30:56.162538: step 7323, loss 0.0300749, acc 0.984375
2017-09-08T18:30:56.645178: step 7324, loss 0.000388543, acc 1
2017-09-08T18:30:57.185177: step 7325, loss 0.022057, acc 0.984375
2017-09-08T18:30:57.754933: step 7326, loss 0.0277846, acc 0.984375
2017-09-08T18:30:58.330030: step 7327, loss 0.0524798, acc 0.984375
2017-09-08T18:30:58.916528: step 7328, loss 0.000521923, acc 1
2017-09-08T18:30:59.485624: step 7329, loss 0.000235298, acc 1
2017-09-08T18:31:00.045563: step 7330, loss 0.00201059, acc 1
2017-09-08T18:31:00.606152: step 7331, loss 0.013991, acc 0.984375
2017-09-08T18:31:01.189322: step 7332, loss 0.00189506, acc 1
2017-09-08T18:31:01.735340: step 7333, loss 0.0381707, acc 0.984375
2017-09-08T18:31:02.285890: step 7334, loss 0.0271299, acc 0.984375
2017-09-08T18:31:02.848993: step 7335, loss 0.0763949, acc 0.96875
2017-09-08T18:31:03.459722: step 7336, loss 0.000230753, acc 1
2017-09-08T18:31:04.026936: step 7337, loss 0.000942714, acc 1
2017-09-08T18:31:04.607530: step 7338, loss 0.100271, acc 0.96875
2017-09-08T18:31:05.182225: step 7339, loss 0.0173011, acc 1
2017-09-08T18:31:05.745841: step 7340, loss 0.0617228, acc 0.953125
2017-09-08T18:31:06.318624: step 7341, loss 0.176569, acc 0.96875
2017-09-08T18:31:06.904490: step 7342, loss 0.0265545, acc 0.984375
2017-09-08T18:31:07.481101: step 7343, loss 0.00023495, acc 1
2017-09-08T18:31:08.062267: step 7344, loss 0.00721328, acc 1
2017-09-08T18:31:08.635437: step 7345, loss 0.0680453, acc 0.96875
2017-09-08T18:31:09.226476: step 7346, loss 0.0218872, acc 1
2017-09-08T18:31:09.781615: step 7347, loss 0.000800335, acc 1
2017-09-08T18:31:10.372122: step 7348, loss 0.116897, acc 0.96875
2017-09-08T18:31:10.916327: step 7349, loss 0.0183299, acc 0.984375
2017-09-08T18:31:11.420364: step 7350, loss 0.00847078, acc 1

Evaluation:
2017-09-08T18:31:12.050214: step 7350, loss 0.225558, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7350

2017-09-08T18:31:14.341015: step 7351, loss 0.0244575, acc 0.984375
2017-09-08T18:31:14.912583: step 7352, loss 0.00102382, acc 1
2017-09-08T18:31:15.483239: step 7353, loss 0.000353496, acc 1
2017-09-08T18:31:16.034004: step 7354, loss 0.00331906, acc 1
2017-09-08T18:31:16.566467: step 7355, loss 0.00718256, acc 1
2017-09-08T18:31:17.131581: step 7356, loss 0.0818717, acc 0.96875
2017-09-08T18:31:17.698549: step 7357, loss 0.000446345, acc 1
2017-09-08T18:31:18.309232: step 7358, loss 0.000711241, acc 1
2017-09-08T18:31:18.913216: step 7359, loss 0.00222914, acc 1
2017-09-08T18:31:19.429123: step 7360, loss 0.00111091, acc 1
2017-09-08T18:31:19.868417: step 7361, loss 0.000224658, acc 1
2017-09-08T18:31:20.358625: step 7362, loss 0.000581065, acc 1
2017-09-08T18:31:20.818706: step 7363, loss 0.0172543, acc 1
2017-09-08T18:31:21.415235: step 7364, loss 0.000566976, acc 1
2017-09-08T18:31:21.991903: step 7365, loss 0.00402961, acc 1
2017-09-08T18:31:22.631205: step 7366, loss 0.0241324, acc 0.984375
2017-09-08T18:31:23.182212: step 7367, loss 0.000822945, acc 1
2017-09-08T18:31:23.724565: step 7368, loss 0.0617233, acc 0.96875
2017-09-08T18:31:24.301243: step 7369, loss 0.0660353, acc 0.984375
2017-09-08T18:31:24.884475: step 7370, loss 0.0206672, acc 0.984375
2017-09-08T18:31:25.476831: step 7371, loss 0.000239194, acc 1
2017-09-08T18:31:26.077976: step 7372, loss 0.000165468, acc 1
2017-09-08T18:31:26.572226: step 7373, loss 0.000826264, acc 1
2017-09-08T18:31:27.070874: step 7374, loss 0.000322805, acc 1
2017-09-08T18:31:27.545104: step 7375, loss 0.0743269, acc 0.984375
2017-09-08T18:31:28.080097: step 7376, loss 0.00115524, acc 1
2017-09-08T18:31:28.593982: step 7377, loss 0.00209801, acc 1
2017-09-08T18:31:29.129891: step 7378, loss 0.000392184, acc 1
2017-09-08T18:31:29.717085: step 7379, loss 0.0294993, acc 0.984375
2017-09-08T18:31:30.304252: step 7380, loss 0.000707595, acc 1
2017-09-08T18:31:30.907604: step 7381, loss 0.000748156, acc 1
2017-09-08T18:31:31.513129: step 7382, loss 0.000286417, acc 1
2017-09-08T18:31:32.103077: step 7383, loss 0.00805926, acc 1
2017-09-08T18:31:32.669944: step 7384, loss 0.00370764, acc 1
2017-09-08T18:31:33.293092: step 7385, loss 0.0253505, acc 0.984375
2017-09-08T18:31:33.873227: step 7386, loss 0.000470743, acc 1
2017-09-08T18:31:34.403948: step 7387, loss 0.000589135, acc 1
2017-09-08T18:31:34.998926: step 7388, loss 0.0989234, acc 0.984375
2017-09-08T18:31:35.627211: step 7389, loss 0.0459536, acc 0.984375
2017-09-08T18:31:36.208083: step 7390, loss 0.00447697, acc 1
2017-09-08T18:31:36.783839: step 7391, loss 0.037975, acc 0.984375
2017-09-08T18:31:37.378974: step 7392, loss 0.026354, acc 0.984375
2017-09-08T18:31:37.917972: step 7393, loss 0.00888088, acc 1
2017-09-08T18:31:38.500045: step 7394, loss 0.000593462, acc 1
2017-09-08T18:31:39.090814: step 7395, loss 0.0126672, acc 0.984375
2017-09-08T18:31:39.642600: step 7396, loss 0.00357893, acc 1
2017-09-08T18:31:40.222511: step 7397, loss 0.0124351, acc 1
2017-09-08T18:31:40.823867: step 7398, loss 0.000152153, acc 1
2017-09-08T18:31:41.389951: step 7399, loss 0.056538, acc 0.984375
2017-09-08T18:31:41.945120: step 7400, loss 0.0315035, acc 0.984375

Evaluation:
2017-09-08T18:31:42.513101: step 7400, loss 0.251128, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7400

2017-09-08T18:31:45.736812: step 7401, loss 0.00937588, acc 1
2017-09-08T18:31:46.249630: step 7402, loss 0.0197581, acc 0.984375
2017-09-08T18:31:46.807949: step 7403, loss 0.011502, acc 1
2017-09-08T18:31:47.372229: step 7404, loss 0.00207612, acc 1
2017-09-08T18:31:47.946083: step 7405, loss 0.00482053, acc 1
2017-09-08T18:31:48.507477: step 7406, loss 0.00948231, acc 1
2017-09-08T18:31:49.085466: step 7407, loss 0.0640064, acc 0.96875
2017-09-08T18:31:49.646624: step 7408, loss 0.0438278, acc 0.96875
2017-09-08T18:31:50.161188: step 7409, loss 0.0156657, acc 0.984375
2017-09-08T18:31:50.653964: step 7410, loss 0.00422751, acc 1
2017-09-08T18:31:51.119117: step 7411, loss 0.00129585, acc 1
2017-09-08T18:31:51.603383: step 7412, loss 0.000210279, acc 1
2017-09-08T18:31:52.085924: step 7413, loss 0.0676929, acc 0.984375
2017-09-08T18:31:52.571771: step 7414, loss 0.00046027, acc 1
2017-09-08T18:31:53.063141: step 7415, loss 0.085808, acc 0.96875
2017-09-08T18:31:53.544388: step 7416, loss 0.000547807, acc 1
2017-09-08T18:31:54.025096: step 7417, loss 0.00577502, acc 1
2017-09-08T18:31:54.504904: step 7418, loss 0.0197807, acc 0.984375
2017-09-08T18:31:54.977639: step 7419, loss 0.000578189, acc 1
2017-09-08T18:31:55.479392: step 7420, loss 0.0451544, acc 0.984375
2017-09-08T18:31:55.952690: step 7421, loss 0.00476181, acc 1
2017-09-08T18:31:56.485590: step 7422, loss 0.00174993, acc 1
2017-09-08T18:31:56.868634: step 7423, loss 0.000830201, acc 1
2017-09-08T18:31:57.274120: step 7424, loss 0.0326835, acc 0.984375
2017-09-08T18:31:57.659995: step 7425, loss 0.000591221, acc 1
2017-09-08T18:31:58.137014: step 7426, loss 0.00374791, acc 1
2017-09-08T18:31:58.626597: step 7427, loss 0.0414174, acc 0.984375
2017-09-08T18:31:59.136187: step 7428, loss 0.00168775, acc 1
2017-09-08T18:31:59.636523: step 7429, loss 0.000122039, acc 1
2017-09-08T18:32:00.110423: step 7430, loss 0.0314607, acc 0.984375
2017-09-08T18:32:00.601493: step 7431, loss 0.00462102, acc 1
2017-09-08T18:32:01.084721: step 7432, loss 0.000800148, acc 1
2017-09-08T18:32:01.538569: step 7433, loss 0.0105254, acc 1
2017-09-08T18:32:02.040879: step 7434, loss 0.000167348, acc 1
2017-09-08T18:32:02.533758: step 7435, loss 0.026352, acc 0.984375
2017-09-08T18:32:03.047039: step 7436, loss 0.000214017, acc 1
2017-09-08T18:32:03.489954: step 7437, loss 0.000218892, acc 1
2017-09-08T18:32:03.990222: step 7438, loss 0.083198, acc 0.96875
2017-09-08T18:32:04.476199: step 7439, loss 0.0119118, acc 0.984375
2017-09-08T18:32:04.960415: step 7440, loss 0.0191297, acc 0.984375
2017-09-08T18:32:05.462459: step 7441, loss 0.0160697, acc 1
2017-09-08T18:32:05.957566: step 7442, loss 0.0313821, acc 1
2017-09-08T18:32:06.450079: step 7443, loss 0.00652509, acc 1
2017-09-08T18:32:06.919961: step 7444, loss 0.00821779, acc 1
2017-09-08T18:32:07.417705: step 7445, loss 0.000450913, acc 1
2017-09-08T18:32:07.888923: step 7446, loss 0.00282817, acc 1
2017-09-08T18:32:08.384541: step 7447, loss 0.0353468, acc 0.96875
2017-09-08T18:32:08.858745: step 7448, loss 0.000294891, acc 1
2017-09-08T18:32:09.342327: step 7449, loss 0.00029442, acc 1
2017-09-08T18:32:09.830905: step 7450, loss 0.0033956, acc 1

Evaluation:
2017-09-08T18:32:10.303703: step 7450, loss 0.252879, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7450

2017-09-08T18:32:12.300199: step 7451, loss 0.000509254, acc 1
2017-09-08T18:32:12.725754: step 7452, loss 0.000750885, acc 1
2017-09-08T18:32:13.154536: step 7453, loss 0.0155617, acc 0.984375
2017-09-08T18:32:13.608722: step 7454, loss 0.0481769, acc 0.96875
2017-09-08T18:32:14.055684: step 7455, loss 0.00975957, acc 1
2017-09-08T18:32:14.500736: step 7456, loss 0.0419145, acc 0.984375
2017-09-08T18:32:14.946352: step 7457, loss 0.00747262, acc 1
2017-09-08T18:32:15.407013: step 7458, loss 0.000292459, acc 1
2017-09-08T18:32:15.811019: step 7459, loss 0.00224854, acc 1
2017-09-08T18:32:16.277532: step 7460, loss 0.00336122, acc 1
2017-09-08T18:32:16.716111: step 7461, loss 0.00200227, acc 1
2017-09-08T18:32:17.141674: step 7462, loss 0.000897763, acc 1
2017-09-08T18:32:17.587292: step 7463, loss 0.00108106, acc 1
2017-09-08T18:32:18.060338: step 7464, loss 0.000804953, acc 1
2017-09-08T18:32:18.532689: step 7465, loss 0.000729166, acc 1
2017-09-08T18:32:19.016564: step 7466, loss 0.0155614, acc 0.984375
2017-09-08T18:32:19.498410: step 7467, loss 0.0406775, acc 0.984375
2017-09-08T18:32:19.981729: step 7468, loss 0.000263933, acc 1
2017-09-08T18:32:20.461374: step 7469, loss 0.0212172, acc 0.984375
2017-09-08T18:32:20.946860: step 7470, loss 0.0418974, acc 0.984375
2017-09-08T18:32:21.447188: step 7471, loss 0.00036066, acc 1
2017-09-08T18:32:22.006444: step 7472, loss 0.000321622, acc 1
2017-09-08T18:32:22.387527: step 7473, loss 0.000431434, acc 1
2017-09-08T18:32:22.789885: step 7474, loss 0.000134145, acc 1
2017-09-08T18:32:23.178834: step 7475, loss 0.000577317, acc 1
2017-09-08T18:32:23.571416: step 7476, loss 0.0428569, acc 0.984375
2017-09-08T18:32:23.989005: step 7477, loss 0.0148965, acc 0.984375
2017-09-08T18:32:24.527662: step 7478, loss 0.020168, acc 1
2017-09-08T18:32:25.033502: step 7479, loss 0.0212216, acc 0.984375
2017-09-08T18:32:25.513184: step 7480, loss 0.00153475, acc 1
2017-09-08T18:32:26.020637: step 7481, loss 0.000491808, acc 1
2017-09-08T18:32:26.494135: step 7482, loss 0.0200134, acc 0.984375
2017-09-08T18:32:26.999097: step 7483, loss 0.0414494, acc 0.96875
2017-09-08T18:32:27.506563: step 7484, loss 0.00920163, acc 1
2017-09-08T18:32:27.976242: step 7485, loss 0.0378744, acc 0.984375
2017-09-08T18:32:28.470625: step 7486, loss 0.130029, acc 0.96875
2017-09-08T18:32:28.948776: step 7487, loss 0.0364749, acc 0.96875
2017-09-08T18:32:29.464024: step 7488, loss 0.00146609, acc 1
2017-09-08T18:32:29.937943: step 7489, loss 0.0709541, acc 0.96875
2017-09-08T18:32:30.393392: step 7490, loss 0.0884335, acc 0.96875
2017-09-08T18:32:30.862372: step 7491, loss 0.0700059, acc 0.984375
2017-09-08T18:32:31.354058: step 7492, loss 0.0216022, acc 0.984375
2017-09-08T18:32:31.843489: step 7493, loss 9.81822e-05, acc 1
2017-09-08T18:32:32.340237: step 7494, loss 0.000996105, acc 1
2017-09-08T18:32:32.832844: step 7495, loss 0.0200236, acc 0.984375
2017-09-08T18:32:33.313755: step 7496, loss 0.0108437, acc 1
2017-09-08T18:32:33.821127: step 7497, loss 0.00107041, acc 1
2017-09-08T18:32:34.291247: step 7498, loss 0.0136524, acc 0.984375
2017-09-08T18:32:34.765228: step 7499, loss 0.0018818, acc 1
2017-09-08T18:32:35.253567: step 7500, loss 0.028758, acc 0.984375

Evaluation:
2017-09-08T18:32:35.775055: step 7500, loss 0.236764, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7500

2017-09-08T18:32:38.031754: step 7501, loss 0.0110583, acc 1
2017-09-08T18:32:38.487744: step 7502, loss 0.000304952, acc 1
2017-09-08T18:32:38.909738: step 7503, loss 0.000376285, acc 1
2017-09-08T18:32:39.391014: step 7504, loss 0.00865692, acc 1
2017-09-08T18:32:39.893261: step 7505, loss 0.0605086, acc 0.96875
2017-09-08T18:32:40.408065: step 7506, loss 0.000856044, acc 1
2017-09-08T18:32:40.906780: step 7507, loss 0.000529826, acc 1
2017-09-08T18:32:41.404723: step 7508, loss 0.000684321, acc 1
2017-09-08T18:32:41.883566: step 7509, loss 0.0620021, acc 0.96875
2017-09-08T18:32:42.363932: step 7510, loss 0.000735839, acc 1
2017-09-08T18:32:42.886196: step 7511, loss 0.0180175, acc 1
2017-09-08T18:32:43.382907: step 7512, loss 0.0185679, acc 0.984375
2017-09-08T18:32:43.883536: step 7513, loss 0.0436721, acc 0.96875
2017-09-08T18:32:44.373642: step 7514, loss 0.00506096, acc 1
2017-09-08T18:32:44.848125: step 7515, loss 0.0240356, acc 0.984375
2017-09-08T18:32:45.306899: step 7516, loss 0.0165618, acc 0.984375
2017-09-08T18:32:45.818305: step 7517, loss 0.00243007, acc 1
2017-09-08T18:32:46.282903: step 7518, loss 0.0492949, acc 0.984375
2017-09-08T18:32:46.759029: step 7519, loss 0.000263072, acc 1
2017-09-08T18:32:47.250462: step 7520, loss 0.000170497, acc 1
2017-09-08T18:32:47.733843: step 7521, loss 0.037119, acc 0.984375
2017-09-08T18:32:48.289392: step 7522, loss 0.00498436, acc 1
2017-09-08T18:32:48.693338: step 7523, loss 0.000312069, acc 1
2017-09-08T18:32:49.093752: step 7524, loss 0.000238878, acc 1
2017-09-08T18:32:49.496818: step 7525, loss 0.000321613, acc 1
2017-09-08T18:32:49.897881: step 7526, loss 0.000922156, acc 1
2017-09-08T18:32:50.383318: step 7527, loss 0.000356063, acc 1
2017-09-08T18:32:50.880723: step 7528, loss 0.000395049, acc 1
2017-09-08T18:32:51.391537: step 7529, loss 0.000144298, acc 1
2017-09-08T18:32:51.881963: step 7530, loss 0.00246657, acc 1
2017-09-08T18:32:52.389902: step 7531, loss 0.0752501, acc 0.984375
2017-09-08T18:32:52.902502: step 7532, loss 0.000491191, acc 1
2017-09-08T18:32:53.402688: step 7533, loss 0.00234254, acc 1
2017-09-08T18:32:53.915504: step 7534, loss 0.00311393, acc 1
2017-09-08T18:32:54.408369: step 7535, loss 0.000570947, acc 1
2017-09-08T18:32:54.901569: step 7536, loss 0.000385171, acc 1
2017-09-08T18:32:55.397348: step 7537, loss 0.024215, acc 0.984375
2017-09-08T18:32:55.894188: step 7538, loss 0.0116243, acc 1
2017-09-08T18:32:56.390538: step 7539, loss 0.0216718, acc 0.984375
2017-09-08T18:32:56.868378: step 7540, loss 0.000290962, acc 1
2017-09-08T18:32:57.383298: step 7541, loss 0.000314925, acc 1
2017-09-08T18:32:57.904527: step 7542, loss 0.00921589, acc 1
2017-09-08T18:32:58.389184: step 7543, loss 0.00661291, acc 1
2017-09-08T18:32:58.884757: step 7544, loss 0.00738044, acc 1
2017-09-08T18:32:59.380147: step 7545, loss 0.0166478, acc 0.984375
2017-09-08T18:32:59.835861: step 7546, loss 0.000526683, acc 1
2017-09-08T18:33:00.298843: step 7547, loss 6.77221e-05, acc 1
2017-09-08T18:33:00.788787: step 7548, loss 0.118526, acc 0.984375
2017-09-08T18:33:01.278581: step 7549, loss 0.0168271, acc 0.984375
2017-09-08T18:33:01.752736: step 7550, loss 0.00291433, acc 1

Evaluation:
2017-09-08T18:33:02.246386: step 7550, loss 0.241456, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7550

2017-09-08T18:33:05.084835: step 7551, loss 0.0239824, acc 0.984375
2017-09-08T18:33:05.557474: step 7552, loss 0.00154719, acc 1
2017-09-08T18:33:06.041775: step 7553, loss 0.00241411, acc 1
2017-09-08T18:33:06.539173: step 7554, loss 0.0108934, acc 1
2017-09-08T18:33:07.030193: step 7555, loss 0.0319163, acc 0.984375
2017-09-08T18:33:07.514123: step 7556, loss 0.0355815, acc 0.984375
2017-09-08T18:33:08.029252: step 7557, loss 0.000293758, acc 1
2017-09-08T18:33:08.530705: step 7558, loss 0.0130532, acc 0.984375
2017-09-08T18:33:08.990750: step 7559, loss 0.000933383, acc 1
2017-09-08T18:33:09.473827: step 7560, loss 0.111307, acc 0.96875
2017-09-08T18:33:09.991796: step 7561, loss 0.074957, acc 0.984375
2017-09-08T18:33:10.488763: step 7562, loss 0.0304909, acc 0.984375
2017-09-08T18:33:10.998664: step 7563, loss 0.0232752, acc 0.984375
2017-09-08T18:33:11.476560: step 7564, loss 0.0209129, acc 0.984375
2017-09-08T18:33:11.976264: step 7565, loss 0.0388575, acc 0.984375
2017-09-08T18:33:12.463903: step 7566, loss 0.0411991, acc 0.984375
2017-09-08T18:33:12.954573: step 7567, loss 0.00148611, acc 1
2017-09-08T18:33:13.396559: step 7568, loss 0.00127253, acc 1
2017-09-08T18:33:13.872780: step 7569, loss 0.0501807, acc 0.984375
2017-09-08T18:33:14.426029: step 7570, loss 0.0485202, acc 0.984375
2017-09-08T18:33:14.807346: step 7571, loss 0.000245817, acc 1
2017-09-08T18:33:15.193686: step 7572, loss 0.000360144, acc 1
2017-09-08T18:33:15.579853: step 7573, loss 0.00136502, acc 1
2017-09-08T18:33:15.977872: step 7574, loss 0.00695137, acc 1
2017-09-08T18:33:16.432396: step 7575, loss 0.0195519, acc 0.984375
2017-09-08T18:33:16.898350: step 7576, loss 0.000322498, acc 1
2017-09-08T18:33:17.391775: step 7577, loss 0.00873725, acc 1
2017-09-08T18:33:17.924107: step 7578, loss 0.00028104, acc 1
2017-09-08T18:33:18.422595: step 7579, loss 0.0289639, acc 0.984375
2017-09-08T18:33:18.903779: step 7580, loss 0.0750601, acc 0.984375
2017-09-08T18:33:19.406704: step 7581, loss 0.000545601, acc 1
2017-09-08T18:33:19.935097: step 7582, loss 0.000987776, acc 1
2017-09-08T18:33:20.411599: step 7583, loss 0.0116443, acc 1
2017-09-08T18:33:20.859403: step 7584, loss 0.000218499, acc 1
2017-09-08T18:33:21.336930: step 7585, loss 0.0393735, acc 0.984375
2017-09-08T18:33:21.809523: step 7586, loss 0.000473666, acc 1
2017-09-08T18:33:22.260432: step 7587, loss 0.00199045, acc 1
2017-09-08T18:33:22.747723: step 7588, loss 0.0586541, acc 0.984375
2017-09-08T18:33:23.251873: step 7589, loss 0.000206443, acc 1
2017-09-08T18:33:23.722911: step 7590, loss 0.00322766, acc 1
2017-09-08T18:33:24.190809: step 7591, loss 0.00668604, acc 1
2017-09-08T18:33:24.674569: step 7592, loss 0.0426884, acc 0.96875
2017-09-08T18:33:25.178708: step 7593, loss 8.48945e-05, acc 1
2017-09-08T18:33:25.666712: step 7594, loss 0.0023111, acc 1
2017-09-08T18:33:26.136129: step 7595, loss 0.000943377, acc 1
2017-09-08T18:33:26.600040: step 7596, loss 0.00043728, acc 1
2017-09-08T18:33:27.069241: step 7597, loss 0.00547173, acc 1
2017-09-08T18:33:27.534363: step 7598, loss 0.00121292, acc 1
2017-09-08T18:33:28.029970: step 7599, loss 0.000879104, acc 1
2017-09-08T18:33:28.501179: step 7600, loss 0.000315448, acc 1

Evaluation:
2017-09-08T18:33:28.987306: step 7600, loss 0.244062, acc 0.938129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7600

2017-09-08T18:33:30.910412: step 7601, loss 0.000289899, acc 1
2017-09-08T18:33:31.421906: step 7602, loss 0.000215759, acc 1
2017-09-08T18:33:31.928789: step 7603, loss 0.0342106, acc 0.984375
2017-09-08T18:33:32.450000: step 7604, loss 0.00572761, acc 1
2017-09-08T18:33:32.916307: step 7605, loss 0.000839168, acc 1
2017-09-08T18:33:33.358930: step 7606, loss 9.98952e-05, acc 1
2017-09-08T18:33:33.842609: step 7607, loss 0.00415317, acc 1
2017-09-08T18:33:34.340248: step 7608, loss 0.102309, acc 0.96875
2017-09-08T18:33:34.836086: step 7609, loss 0.00102399, acc 1
2017-09-08T18:33:35.317093: step 7610, loss 0.0509261, acc 0.984375
2017-09-08T18:33:35.813801: step 7611, loss 0.0415579, acc 0.984375
2017-09-08T18:33:36.296026: step 7612, loss 0.056398, acc 0.984375
2017-09-08T18:33:36.751920: step 7613, loss 0.000239166, acc 1
2017-09-08T18:33:37.228347: step 7614, loss 0.0163075, acc 0.984375
2017-09-08T18:33:37.701371: step 7615, loss 0.0165568, acc 0.984375
2017-09-08T18:33:38.173911: step 7616, loss 0.0114684, acc 1
2017-09-08T18:33:38.659858: step 7617, loss 0.0231613, acc 0.984375
2017-09-08T18:33:39.158244: step 7618, loss 0.000337441, acc 1
2017-09-08T18:33:39.629423: step 7619, loss 0.00197698, acc 1
2017-09-08T18:33:40.174528: step 7620, loss 0.0290063, acc 0.984375
2017-09-08T18:33:40.561004: step 7621, loss 0.00232123, acc 1
2017-09-08T18:33:40.940844: step 7622, loss 0.00106004, acc 1
2017-09-08T18:33:41.347361: step 7623, loss 0.00523126, acc 1
2017-09-08T18:33:41.744330: step 7624, loss 0.0470174, acc 0.984375
2017-09-08T18:33:42.131608: step 7625, loss 0.107351, acc 0.96875
2017-09-08T18:33:42.603435: step 7626, loss 0.000488084, acc 1
2017-09-08T18:33:43.091642: step 7627, loss 0.0205484, acc 0.984375
2017-09-08T18:33:43.598202: step 7628, loss 0.00870091, acc 1
2017-09-08T18:33:44.111104: step 7629, loss 0.0415383, acc 0.984375
2017-09-08T18:33:44.638664: step 7630, loss 0.0268408, acc 0.984375
2017-09-08T18:33:45.127757: step 7631, loss 0.000340355, acc 1
2017-09-08T18:33:45.585547: step 7632, loss 0.00304467, acc 1
2017-09-08T18:33:46.081553: step 7633, loss 0.000529386, acc 1
2017-09-08T18:33:46.553478: step 7634, loss 0.000349453, acc 1
2017-09-08T18:33:47.035969: step 7635, loss 0.0299066, acc 0.984375
2017-09-08T18:33:47.555295: step 7636, loss 0.00587488, acc 1
2017-09-08T18:33:48.049399: step 7637, loss 0.00275622, acc 1
2017-09-08T18:33:48.527652: step 7638, loss 0.086685, acc 0.96875
2017-09-08T18:33:49.012317: step 7639, loss 0.000186119, acc 1
2017-09-08T18:33:49.479804: step 7640, loss 0.0938192, acc 0.96875
2017-09-08T18:33:49.948994: step 7641, loss 0.000723648, acc 1
2017-09-08T18:33:50.442270: step 7642, loss 0.00016676, acc 1
2017-09-08T18:33:50.928919: step 7643, loss 0.032995, acc 0.984375
2017-09-08T18:33:51.365369: step 7644, loss 0.000276859, acc 1
2017-09-08T18:33:51.868457: step 7645, loss 0.00133362, acc 1
2017-09-08T18:33:52.351208: step 7646, loss 0.0162873, acc 0.984375
2017-09-08T18:33:52.826976: step 7647, loss 0.085847, acc 0.984375
2017-09-08T18:33:53.301482: step 7648, loss 0.000540372, acc 1
2017-09-08T18:33:53.781316: step 7649, loss 0.0703918, acc 0.984375
2017-09-08T18:33:54.246195: step 7650, loss 0.00143509, acc 1

Evaluation:
2017-09-08T18:33:54.756832: step 7650, loss 0.244224, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7650

2017-09-08T18:33:57.545765: step 7651, loss 0.00201906, acc 1
2017-09-08T18:33:58.038145: step 7652, loss 0.0105491, acc 1
2017-09-08T18:33:58.523872: step 7653, loss 0.0107888, acc 1
2017-09-08T18:33:59.009414: step 7654, loss 0.0246697, acc 0.984375
2017-09-08T18:33:59.477790: step 7655, loss 0.00678937, acc 1
2017-09-08T18:33:59.978629: step 7656, loss 0.0263952, acc 0.984375
2017-09-08T18:34:00.446568: step 7657, loss 0.000924262, acc 1
2017-09-08T18:34:00.920143: step 7658, loss 0.00905504, acc 1
2017-09-08T18:34:01.423996: step 7659, loss 0.00294673, acc 1
2017-09-08T18:34:01.929884: step 7660, loss 0.00199549, acc 1
2017-09-08T18:34:02.401103: step 7661, loss 0.00536258, acc 1
2017-09-08T18:34:02.888880: step 7662, loss 0.00141207, acc 1
2017-09-08T18:34:03.387067: step 7663, loss 0.00346077, acc 1
2017-09-08T18:34:03.893904: step 7664, loss 0.119331, acc 0.96875
2017-09-08T18:34:04.357364: step 7665, loss 0.0140211, acc 1
2017-09-08T18:34:04.852020: step 7666, loss 0.00101046, acc 1
2017-09-08T18:34:05.310432: step 7667, loss 0.00434762, acc 1
2017-09-08T18:34:05.792559: step 7668, loss 0.000222182, acc 1
2017-09-08T18:34:06.354479: step 7669, loss 0.000245295, acc 1
2017-09-08T18:34:06.749673: step 7670, loss 0.00333802, acc 1
2017-09-08T18:34:07.132657: step 7671, loss 0.129963, acc 0.9375
2017-09-08T18:34:07.529548: step 7672, loss 0.0330011, acc 0.984375
2017-09-08T18:34:08.008355: step 7673, loss 0.00680968, acc 1
2017-09-08T18:34:08.472705: step 7674, loss 0.0432419, acc 0.984375
2017-09-08T18:34:08.985183: step 7675, loss 0.0352617, acc 0.984375
2017-09-08T18:34:09.509528: step 7676, loss 0.00564368, acc 1
2017-09-08T18:34:10.015806: step 7677, loss 0.0691214, acc 0.984375
2017-09-08T18:34:10.491642: step 7678, loss 0.00116901, acc 1
2017-09-08T18:34:11.140254: step 7679, loss 0.0597424, acc 0.984375
2017-09-08T18:34:11.631791: step 7680, loss 0.00405381, acc 1
2017-09-08T18:34:12.138929: step 7681, loss 0.000129074, acc 1
2017-09-08T18:34:12.626277: step 7682, loss 0.0199259, acc 0.984375
2017-09-08T18:34:13.139152: step 7683, loss 0.0809112, acc 0.984375
2017-09-08T18:34:13.610624: step 7684, loss 0.00122532, acc 1
2017-09-08T18:34:14.092334: step 7685, loss 0.000349999, acc 1
2017-09-08T18:34:14.591438: step 7686, loss 0.0271097, acc 0.96875
2017-09-08T18:34:15.057352: step 7687, loss 0.0361809, acc 0.984375
2017-09-08T18:34:15.522105: step 7688, loss 0.00202306, acc 1
2017-09-08T18:34:15.940325: step 7689, loss 0.0357608, acc 0.96875
2017-09-08T18:34:16.408867: step 7690, loss 0.000285383, acc 1
2017-09-08T18:34:16.869762: step 7691, loss 0.000232201, acc 1
2017-09-08T18:34:17.282111: step 7692, loss 0.000122882, acc 1
2017-09-08T18:34:17.724779: step 7693, loss 0.000242509, acc 1
2017-09-08T18:34:18.187277: step 7694, loss 0.0852077, acc 0.96875
2017-09-08T18:34:18.645669: step 7695, loss 0.000446024, acc 1
2017-09-08T18:34:19.065060: step 7696, loss 0.000504176, acc 1
2017-09-08T18:34:19.507471: step 7697, loss 0.0227698, acc 0.984375
2017-09-08T18:34:19.938952: step 7698, loss 0.00522429, acc 1
2017-09-08T18:34:20.385548: step 7699, loss 0.00471142, acc 1
2017-09-08T18:34:20.806320: step 7700, loss 0.000210808, acc 1

Evaluation:
2017-09-08T18:34:21.726167: step 7700, loss 0.250067, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7700

2017-09-08T18:34:23.545835: step 7701, loss 0.000300693, acc 1
2017-09-08T18:34:24.034187: step 7702, loss 0.0215829, acc 0.984375
2017-09-08T18:34:24.511189: step 7703, loss 0.000192383, acc 1
2017-09-08T18:34:24.992562: step 7704, loss 0.00459027, acc 1
2017-09-08T18:34:25.485925: step 7705, loss 0.0423377, acc 0.984375
2017-09-08T18:34:25.997829: step 7706, loss 0.0002116, acc 1
2017-09-08T18:34:26.476976: step 7707, loss 0.0237989, acc 0.984375
2017-09-08T18:34:26.949359: step 7708, loss 0.0751765, acc 0.984375
2017-09-08T18:34:27.421708: step 7709, loss 0.000409017, acc 1
2017-09-08T18:34:27.909848: step 7710, loss 0.0487598, acc 0.984375
2017-09-08T18:34:28.415185: step 7711, loss 0.00249458, acc 1
2017-09-08T18:34:28.886430: step 7712, loss 0.0320246, acc 0.984375
2017-09-08T18:34:29.387185: step 7713, loss 0.0172593, acc 0.984375
2017-09-08T18:34:29.904388: step 7714, loss 0.000634059, acc 1
2017-09-08T18:34:30.402606: step 7715, loss 0.0384576, acc 0.984375
2017-09-08T18:34:30.890263: step 7716, loss 0.000170069, acc 1
2017-09-08T18:34:31.336429: step 7717, loss 0.0088374, acc 1
2017-09-08T18:34:32.088583: step 7718, loss 0.000729804, acc 1
2017-09-08T18:34:32.618876: step 7719, loss 9.18834e-05, acc 1
2017-09-08T18:34:33.024300: step 7720, loss 0.0459219, acc 0.96875
2017-09-08T18:34:33.437140: step 7721, loss 0.0551653, acc 0.96875
2017-09-08T18:34:33.843705: step 7722, loss 0.00248839, acc 1
2017-09-08T18:34:34.243383: step 7723, loss 0.000748358, acc 1
2017-09-08T18:34:34.632934: step 7724, loss 0.000693003, acc 1
2017-09-08T18:34:35.141918: step 7725, loss 0.0141535, acc 1
2017-09-08T18:34:35.681198: step 7726, loss 0.0341748, acc 0.984375
2017-09-08T18:34:36.173874: step 7727, loss 0.000693416, acc 1
2017-09-08T18:34:36.675610: step 7728, loss 0.000177738, acc 1
2017-09-08T18:34:37.722967: step 7729, loss 0.00187424, acc 1
2017-09-08T18:34:38.187609: step 7730, loss 0.000657971, acc 1
2017-09-08T18:34:38.646662: step 7731, loss 0.000537808, acc 1
2017-09-08T18:34:39.123056: step 7732, loss 0.065159, acc 0.984375
2017-09-08T18:34:39.610078: step 7733, loss 0.000657825, acc 1
2017-09-08T18:34:40.090598: step 7734, loss 0.00586578, acc 1
2017-09-08T18:34:40.613142: step 7735, loss 0.000492076, acc 1
2017-09-08T18:34:41.082747: step 7736, loss 0.00682705, acc 1
2017-09-08T18:34:41.589403: step 7737, loss 0.0408043, acc 0.984375
2017-09-08T18:34:42.064381: step 7738, loss 0.00604487, acc 1
2017-09-08T18:34:42.555022: step 7739, loss 0.000261992, acc 1
2017-09-08T18:34:43.010198: step 7740, loss 0.000944346, acc 1
2017-09-08T18:34:43.515986: step 7741, loss 0.00240859, acc 1
2017-09-08T18:34:43.950388: step 7742, loss 0.00675798, acc 1
2017-09-08T18:34:44.456144: step 7743, loss 0.00115511, acc 1
2017-09-08T18:34:44.947563: step 7744, loss 0.000522329, acc 1
2017-09-08T18:34:45.426915: step 7745, loss 0.00112342, acc 1
2017-09-08T18:34:45.918885: step 7746, loss 0.000579302, acc 1
2017-09-08T18:34:46.383539: step 7747, loss 0.0299854, acc 0.984375
2017-09-08T18:34:46.864395: step 7748, loss 0.000124861, acc 1
2017-09-08T18:34:47.344398: step 7749, loss 0.122, acc 0.96875
2017-09-08T18:34:47.825672: step 7750, loss 0.00130837, acc 1

Evaluation:
2017-09-08T18:34:48.323478: step 7750, loss 0.263896, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7750

2017-09-08T18:34:50.435262: step 7751, loss 0.000553878, acc 1
2017-09-08T18:34:50.924637: step 7752, loss 0.000849576, acc 1
2017-09-08T18:34:51.369785: step 7753, loss 0.00945763, acc 1
2017-09-08T18:34:51.854374: step 7754, loss 0.0182614, acc 0.984375
2017-09-08T18:34:52.362736: step 7755, loss 0.0187964, acc 0.984375
2017-09-08T18:34:52.844300: step 7756, loss 0.00381203, acc 1
2017-09-08T18:34:53.335469: step 7757, loss 0.0449407, acc 0.984375
2017-09-08T18:34:53.826952: step 7758, loss 0.00437201, acc 1
2017-09-08T18:34:54.310996: step 7759, loss 0.0002156, acc 1
2017-09-08T18:34:54.779506: step 7760, loss 0.0104574, acc 1
2017-09-08T18:34:55.236654: step 7761, loss 0.00132852, acc 1
2017-09-08T18:34:55.726288: step 7762, loss 0.000940833, acc 1
2017-09-08T18:34:56.206638: step 7763, loss 0.0399149, acc 0.984375
2017-09-08T18:34:56.701379: step 7764, loss 0.000181142, acc 1
2017-09-08T18:34:57.192341: step 7765, loss 0.000990986, acc 1
2017-09-08T18:34:57.676110: step 7766, loss 0.00843332, acc 1
2017-09-08T18:34:58.156899: step 7767, loss 0.000499784, acc 1
2017-09-08T18:34:58.639757: step 7768, loss 0.0235485, acc 0.984375
2017-09-08T18:34:59.136459: step 7769, loss 0.181413, acc 0.921875
2017-09-08T18:34:59.693959: step 7770, loss 0.0291698, acc 0.984375
2017-09-08T18:35:00.095167: step 7771, loss 0.0130083, acc 0.984375
2017-09-08T18:35:00.490158: step 7772, loss 0.000273576, acc 1
2017-09-08T18:35:00.881333: step 7773, loss 0.00277004, acc 1
2017-09-08T18:35:01.305958: step 7774, loss 0.000512409, acc 1
2017-09-08T18:35:01.778482: step 7775, loss 0.0229693, acc 1
2017-09-08T18:35:02.248826: step 7776, loss 0.00183874, acc 1
2017-09-08T18:35:02.780934: step 7777, loss 0.0026142, acc 1
2017-09-08T18:35:03.278640: step 7778, loss 0.00850572, acc 1
2017-09-08T18:35:03.761007: step 7779, loss 0.0243771, acc 0.984375
2017-09-08T18:35:04.243969: step 7780, loss 0.0105443, acc 1
2017-09-08T18:35:04.736352: step 7781, loss 0.000522758, acc 1
2017-09-08T18:35:05.217442: step 7782, loss 0.00455615, acc 1
2017-09-08T18:35:05.676373: step 7783, loss 0.0357183, acc 0.984375
2017-09-08T18:35:06.183015: step 7784, loss 0.000435043, acc 1
2017-09-08T18:35:06.688662: step 7785, loss 9.67889e-05, acc 1
2017-09-08T18:35:07.157285: step 7786, loss 0.000297886, acc 1
2017-09-08T18:35:07.630689: step 7787, loss 0.00808504, acc 1
2017-09-08T18:35:08.116740: step 7788, loss 0.0575856, acc 0.984375
2017-09-08T18:35:08.613223: step 7789, loss 0.00249101, acc 1
2017-09-08T18:35:09.107174: step 7790, loss 0.00350276, acc 1
2017-09-08T18:35:09.600963: step 7791, loss 0.062959, acc 0.984375
2017-09-08T18:35:10.076789: step 7792, loss 0.000151208, acc 1
2017-09-08T18:35:10.548152: step 7793, loss 0.020197, acc 0.984375
2017-09-08T18:35:11.021206: step 7794, loss 0.000326807, acc 1
2017-09-08T18:35:11.514178: step 7795, loss 0.010378, acc 1
2017-09-08T18:35:11.992660: step 7796, loss 0.0112361, acc 0.984375
2017-09-08T18:35:12.513734: step 7797, loss 0.0926733, acc 0.96875
2017-09-08T18:35:12.981331: step 7798, loss 0.0203099, acc 0.984375
2017-09-08T18:35:13.476999: step 7799, loss 0.000453988, acc 1
2017-09-08T18:35:13.978028: step 7800, loss 0.000480164, acc 1

Evaluation:
2017-09-08T18:35:14.515568: step 7800, loss 0.251302, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7800

2017-09-08T18:35:17.624801: step 7801, loss 0.00042527, acc 1
2017-09-08T18:35:18.105079: step 7802, loss 9.75102e-05, acc 1
2017-09-08T18:35:18.596072: step 7803, loss 0.000400784, acc 1
2017-09-08T18:35:19.057457: step 7804, loss 0.0423234, acc 0.984375
2017-09-08T18:35:19.536057: step 7805, loss 0.00103341, acc 1
2017-09-08T18:35:19.994159: step 7806, loss 0.014814, acc 1
2017-09-08T18:35:20.470366: step 7807, loss 0.0289067, acc 0.984375
2017-09-08T18:35:20.954717: step 7808, loss 0.00774794, acc 1
2017-09-08T18:35:21.439087: step 7809, loss 0.0036715, acc 1
2017-09-08T18:35:21.959760: step 7810, loss 0.00315052, acc 1
2017-09-08T18:35:22.460101: step 7811, loss 0.0378699, acc 0.984375
2017-09-08T18:35:22.929081: step 7812, loss 0.101762, acc 0.953125
2017-09-08T18:35:23.423222: step 7813, loss 0.000452684, acc 1
2017-09-08T18:35:23.949756: step 7814, loss 0.035887, acc 0.984375
2017-09-08T18:35:24.469983: step 7815, loss 0.0114672, acc 1
2017-09-08T18:35:24.993828: step 7816, loss 0.0164233, acc 0.984375
2017-09-08T18:35:25.382549: step 7817, loss 0.000863285, acc 1
2017-09-08T18:35:25.770607: step 7818, loss 0.000547589, acc 1
2017-09-08T18:35:26.168248: step 7819, loss 0.0158862, acc 1
2017-09-08T18:35:26.561092: step 7820, loss 0.000484338, acc 1
2017-09-08T18:35:27.019954: step 7821, loss 0.0002919, acc 1
2017-09-08T18:35:27.529754: step 7822, loss 0.0478627, acc 0.984375
2017-09-08T18:35:28.021186: step 7823, loss 0.000386635, acc 1
2017-09-08T18:35:28.502037: step 7824, loss 0.00452909, acc 1
2017-09-08T18:35:28.992500: step 7825, loss 0.0385482, acc 0.984375
2017-09-08T18:35:29.485262: step 7826, loss 0.000453761, acc 1
2017-09-08T18:35:29.967743: step 7827, loss 0.0267692, acc 0.984375
2017-09-08T18:35:30.466740: step 7828, loss 0.0321057, acc 0.984375
2017-09-08T18:35:30.950965: step 7829, loss 0.000120854, acc 1
2017-09-08T18:35:31.444811: step 7830, loss 0.0485135, acc 0.984375
2017-09-08T18:35:31.926080: step 7831, loss 0.0600915, acc 0.96875
2017-09-08T18:35:32.430768: step 7832, loss 0.000865821, acc 1
2017-09-08T18:35:32.944956: step 7833, loss 0.000185593, acc 1
2017-09-08T18:35:33.407275: step 7834, loss 0.0135519, acc 1
2017-09-08T18:35:33.882783: step 7835, loss 0.0046841, acc 1
2017-09-08T18:35:34.334622: step 7836, loss 0.0001917, acc 1
2017-09-08T18:35:34.795000: step 7837, loss 0.0188161, acc 1
2017-09-08T18:35:35.277052: step 7838, loss 0.000463282, acc 1
2017-09-08T18:35:35.766636: step 7839, loss 0.0581483, acc 0.984375
2017-09-08T18:35:36.210299: step 7840, loss 0.00373463, acc 1
2017-09-08T18:35:36.712282: step 7841, loss 0.00972907, acc 1
2017-09-08T18:35:37.195853: step 7842, loss 0.00287228, acc 1
2017-09-08T18:35:37.683427: step 7843, loss 0.0748505, acc 0.96875
2017-09-08T18:35:38.169559: step 7844, loss 0.000361886, acc 1
2017-09-08T18:35:38.654652: step 7845, loss 0.000159594, acc 1
2017-09-08T18:35:39.161405: step 7846, loss 0.000533388, acc 1
2017-09-08T18:35:39.628783: step 7847, loss 0.0137282, acc 0.984375
2017-09-08T18:35:40.110289: step 7848, loss 0.0202983, acc 0.984375
2017-09-08T18:35:40.610940: step 7849, loss 0.00549413, acc 1
2017-09-08T18:35:41.078050: step 7850, loss 0.0106666, acc 1

Evaluation:
2017-09-08T18:35:41.570985: step 7850, loss 0.243531, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7850

2017-09-08T18:35:43.595001: step 7851, loss 0.00619587, acc 1
2017-09-08T18:35:44.112595: step 7852, loss 0.00315555, acc 1
2017-09-08T18:35:44.600250: step 7853, loss 0.000520473, acc 1
2017-09-08T18:35:45.068432: step 7854, loss 0.0212094, acc 0.984375
2017-09-08T18:35:45.558832: step 7855, loss 0.000773788, acc 1
2017-09-08T18:35:46.042576: step 7856, loss 0.00328819, acc 1
2017-09-08T18:35:46.526479: step 7857, loss 0.000277721, acc 1
2017-09-08T18:35:47.007186: step 7858, loss 0.00288013, acc 1
2017-09-08T18:35:47.510682: step 7859, loss 0.0231673, acc 0.984375
2017-09-08T18:35:47.995113: step 7860, loss 0.00284888, acc 1
2017-09-08T18:35:48.487438: step 7861, loss 0.00662665, acc 1
2017-09-08T18:35:48.981869: step 7862, loss 0.000181045, acc 1
2017-09-08T18:35:49.474625: step 7863, loss 0.000682779, acc 1
2017-09-08T18:35:49.945482: step 7864, loss 0.0132133, acc 1
2017-09-08T18:35:50.422341: step 7865, loss 0.0193437, acc 0.984375
2017-09-08T18:35:50.984563: step 7866, loss 0.0119206, acc 1
2017-09-08T18:35:51.381330: step 7867, loss 0.000594493, acc 1
2017-09-08T18:35:51.776266: step 7868, loss 0.000364443, acc 1
2017-09-08T18:35:52.160892: step 7869, loss 0.00811391, acc 1
2017-09-08T18:35:52.545052: step 7870, loss 0.0092483, acc 1
2017-09-08T18:35:52.938909: step 7871, loss 0.00258115, acc 1
2017-09-08T18:35:53.430657: step 7872, loss 0.000406756, acc 1
2017-09-08T18:35:53.942907: step 7873, loss 7.68329e-05, acc 1
2017-09-08T18:35:54.452555: step 7874, loss 0.0297771, acc 0.984375
2017-09-08T18:35:54.918872: step 7875, loss 0.000419099, acc 1
2017-09-08T18:35:55.420140: step 7876, loss 0.00782864, acc 1
2017-09-08T18:35:55.888595: step 7877, loss 0.000444432, acc 1
2017-09-08T18:35:56.386035: step 7878, loss 0.0749644, acc 0.96875
2017-09-08T18:35:56.847942: step 7879, loss 0.00874828, acc 1
2017-09-08T18:35:57.343742: step 7880, loss 0.000649537, acc 1
2017-09-08T18:35:57.837127: step 7881, loss 0.0128517, acc 1
2017-09-08T18:35:58.314979: step 7882, loss 0.00488209, acc 1
2017-09-08T18:35:58.816596: step 7883, loss 0.0212634, acc 0.984375
2017-09-08T18:35:59.271386: step 7884, loss 0.0139858, acc 1
2017-09-08T18:35:59.742873: step 7885, loss 0.00019848, acc 1
2017-09-08T18:36:00.230541: step 7886, loss 0.114737, acc 0.953125
2017-09-08T18:36:00.720030: step 7887, loss 0.0275218, acc 0.984375
2017-09-08T18:36:01.630023: step 7888, loss 0.0011747, acc 1
2017-09-08T18:36:02.134858: step 7889, loss 0.000141664, acc 1
2017-09-08T18:36:02.628622: step 7890, loss 0.000366887, acc 1
2017-09-08T18:36:03.076747: step 7891, loss 0.00676796, acc 1
2017-09-08T18:36:03.554327: step 7892, loss 0.0217247, acc 0.984375
2017-09-08T18:36:04.051548: step 7893, loss 0.0181003, acc 0.984375
2017-09-08T18:36:04.533665: step 7894, loss 0.0517166, acc 0.984375
2017-09-08T18:36:05.030777: step 7895, loss 0.00145121, acc 1
2017-09-08T18:36:05.525593: step 7896, loss 0.00724432, acc 1
2017-09-08T18:36:05.988422: step 7897, loss 0.000122771, acc 1
2017-09-08T18:36:06.475789: step 7898, loss 0.000929799, acc 1
2017-09-08T18:36:06.915881: step 7899, loss 0.000271055, acc 1
2017-09-08T18:36:07.391510: step 7900, loss 0.0250566, acc 0.984375

Evaluation:
2017-09-08T18:36:07.883386: step 7900, loss 0.252521, acc 0.92518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7900

2017-09-08T18:36:10.735431: step 7901, loss 0.00122885, acc 1
2017-09-08T18:36:11.200187: step 7902, loss 0.000598275, acc 1
2017-09-08T18:36:11.683413: step 7903, loss 0.00274864, acc 1
2017-09-08T18:36:12.206997: step 7904, loss 0.00110295, acc 1
2017-09-08T18:36:12.710677: step 7905, loss 0.00907618, acc 1
2017-09-08T18:36:13.213330: step 7906, loss 0.00298674, acc 1
2017-09-08T18:36:13.709644: step 7907, loss 0.000338668, acc 1
2017-09-08T18:36:14.190908: step 7908, loss 0.000744266, acc 1
2017-09-08T18:36:14.661352: step 7909, loss 0.0334165, acc 0.984375
2017-09-08T18:36:15.142530: step 7910, loss 0.0386943, acc 0.984375
2017-09-08T18:36:15.614435: step 7911, loss 0.012864, acc 1
2017-09-08T18:36:16.143513: step 7912, loss 0.0154419, acc 0.984375
2017-09-08T18:36:16.615912: step 7913, loss 0.000904533, acc 1
2017-09-08T18:36:17.091081: step 7914, loss 0.0410504, acc 0.96875
2017-09-08T18:36:17.647732: step 7915, loss 0.00100833, acc 1
2017-09-08T18:36:18.042516: step 7916, loss 0.0600626, acc 0.984375
2017-09-08T18:36:18.451354: step 7917, loss 0.00054367, acc 1
2017-09-08T18:36:18.835961: step 7918, loss 0.000121976, acc 1
2017-09-08T18:36:19.319741: step 7919, loss 0.0186826, acc 0.984375
2017-09-08T18:36:19.822650: step 7920, loss 0.0392866, acc 0.984375
2017-09-08T18:36:20.307540: step 7921, loss 0.000661439, acc 1
2017-09-08T18:36:20.801068: step 7922, loss 0.0956738, acc 0.96875
2017-09-08T18:36:21.321880: step 7923, loss 0.000107866, acc 1
2017-09-08T18:36:21.829172: step 7924, loss 0.0360055, acc 0.984375
2017-09-08T18:36:22.337689: step 7925, loss 0.00101144, acc 1
2017-09-08T18:36:22.810194: step 7926, loss 0.0131134, acc 0.984375
2017-09-08T18:36:23.318450: step 7927, loss 0.000237726, acc 1
2017-09-08T18:36:23.815750: step 7928, loss 0.0411155, acc 0.984375
2017-09-08T18:36:24.325285: step 7929, loss 0.0211102, acc 0.984375
2017-09-08T18:36:24.819618: step 7930, loss 0.0404423, acc 0.984375
2017-09-08T18:36:25.283542: step 7931, loss 0.0226869, acc 0.984375
2017-09-08T18:36:25.770705: step 7932, loss 0.000149132, acc 1
2017-09-08T18:36:26.256051: step 7933, loss 0.0504993, acc 0.984375
2017-09-08T18:36:26.730715: step 7934, loss 0.0162969, acc 1
2017-09-08T18:36:27.209563: step 7935, loss 0.0111664, acc 1
2017-09-08T18:36:27.705530: step 7936, loss 0.0550193, acc 0.984375
2017-09-08T18:36:28.190541: step 7937, loss 0.00497844, acc 1
2017-09-08T18:36:28.621020: step 7938, loss 0.0484913, acc 0.980392
2017-09-08T18:36:29.132267: step 7939, loss 0.000739843, acc 1
2017-09-08T18:36:29.615639: step 7940, loss 0.000318659, acc 1
2017-09-08T18:36:30.106190: step 7941, loss 0.0126797, acc 1
2017-09-08T18:36:30.610383: step 7942, loss 0.00306203, acc 1
2017-09-08T18:36:31.073945: step 7943, loss 0.0110738, acc 1
2017-09-08T18:36:31.551235: step 7944, loss 0.000774059, acc 1
2017-09-08T18:36:32.042001: step 7945, loss 0.00345257, acc 1
2017-09-08T18:36:32.525071: step 7946, loss 0.0489437, acc 0.96875
2017-09-08T18:36:33.017623: step 7947, loss 0.0328707, acc 0.96875
2017-09-08T18:36:33.515107: step 7948, loss 0.000620811, acc 1
2017-09-08T18:36:33.977310: step 7949, loss 0.0134654, acc 0.984375
2017-09-08T18:36:34.464557: step 7950, loss 0.0414358, acc 0.984375

Evaluation:
2017-09-08T18:36:34.999197: step 7950, loss 0.256256, acc 0.923741

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-7950

2017-09-08T18:36:37.041424: step 7951, loss 0.000933789, acc 1
2017-09-08T18:36:37.519509: step 7952, loss 0.000381766, acc 1
2017-09-08T18:36:38.013563: step 7953, loss 0.0329915, acc 0.984375
2017-09-08T18:36:38.515843: step 7954, loss 0.00549691, acc 1
2017-09-08T18:36:39.011860: step 7955, loss 0.02992, acc 0.984375
2017-09-08T18:36:39.505435: step 7956, loss 0.0983409, acc 0.953125
2017-09-08T18:36:39.995269: step 7957, loss 0.0571591, acc 0.96875
2017-09-08T18:36:40.464306: step 7958, loss 0.0592945, acc 0.984375
2017-09-08T18:36:40.966760: step 7959, loss 0.000352646, acc 1
2017-09-08T18:36:41.455463: step 7960, loss 0.013695, acc 1
2017-09-08T18:36:41.950625: step 7961, loss 0.0353471, acc 0.984375
2017-09-08T18:36:42.444112: step 7962, loss 0.0302856, acc 0.984375
2017-09-08T18:36:42.923491: step 7963, loss 0.00784378, acc 1
2017-09-08T18:36:43.501091: step 7964, loss 0.00907472, acc 1
2017-09-08T18:36:43.929118: step 7965, loss 0.000869561, acc 1
2017-09-08T18:36:44.303627: step 7966, loss 0.000377295, acc 1
2017-09-08T18:36:44.691669: step 7967, loss 0.0221149, acc 0.984375
2017-09-08T18:36:45.090485: step 7968, loss 0.0272531, acc 0.984375
2017-09-08T18:36:45.488482: step 7969, loss 0.000283567, acc 1
2017-09-08T18:36:45.950900: step 7970, loss 9.64588e-05, acc 1
2017-09-08T18:36:46.464115: step 7971, loss 0.00040206, acc 1
2017-09-08T18:36:46.934485: step 7972, loss 0.087862, acc 0.953125
2017-09-08T18:36:47.421876: step 7973, loss 0.000467143, acc 1
2017-09-08T18:36:47.873963: step 7974, loss 0.0207343, acc 1
2017-09-08T18:36:48.370593: step 7975, loss 0.0560934, acc 0.984375
2017-09-08T18:36:48.849642: step 7976, loss 0.000373896, acc 1
2017-09-08T18:36:49.334434: step 7977, loss 0.0758577, acc 0.96875
2017-09-08T18:36:49.816073: step 7978, loss 0.00230725, acc 1
2017-09-08T18:36:50.288647: step 7979, loss 0.00137736, acc 1
2017-09-08T18:36:50.785573: step 7980, loss 0.0520472, acc 0.984375
2017-09-08T18:36:51.284248: step 7981, loss 0.000240692, acc 1
2017-09-08T18:36:51.770627: step 7982, loss 0.0430862, acc 0.984375
2017-09-08T18:36:52.255708: step 7983, loss 0.0314235, acc 0.984375
2017-09-08T18:36:52.738223: step 7984, loss 0.0185309, acc 0.984375
2017-09-08T18:36:53.214848: step 7985, loss 0.000192597, acc 1
2017-09-08T18:36:53.716158: step 7986, loss 0.000329048, acc 1
2017-09-08T18:36:54.220684: step 7987, loss 0.00181872, acc 1
2017-09-08T18:36:54.689245: step 7988, loss 0.0399084, acc 0.96875
2017-09-08T18:36:55.193084: step 7989, loss 0.0132524, acc 0.984375
2017-09-08T18:36:55.708642: step 7990, loss 0.000427125, acc 1
2017-09-08T18:36:56.191481: step 7991, loss 0.00184699, acc 1
2017-09-08T18:36:56.670246: step 7992, loss 0.0865254, acc 0.96875
2017-09-08T18:36:57.153245: step 7993, loss 0.0131567, acc 0.984375
2017-09-08T18:36:57.641500: step 7994, loss 0.000277586, acc 1
2017-09-08T18:36:58.122192: step 7995, loss 0.0177976, acc 0.984375
2017-09-08T18:36:58.608052: step 7996, loss 0.0171164, acc 1
2017-09-08T18:36:59.055261: step 7997, loss 0.00984823, acc 1
2017-09-08T18:36:59.532179: step 7998, loss 0.0109502, acc 1
2017-09-08T18:36:59.999161: step 7999, loss 0.0226358, acc 0.984375
2017-09-08T18:37:00.469171: step 8000, loss 0.000489575, acc 1

Evaluation:
2017-09-08T18:37:00.954236: step 8000, loss 0.265592, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8000

2017-09-08T18:37:03.364373: step 8001, loss 0.0104195, acc 1
2017-09-08T18:37:03.896023: step 8002, loss 0.048814, acc 0.984375
2017-09-08T18:37:04.415852: step 8003, loss 0.001142, acc 1
2017-09-08T18:37:04.937666: step 8004, loss 0.000754856, acc 1
2017-09-08T18:37:05.398890: step 8005, loss 0.0215474, acc 0.984375
2017-09-08T18:37:05.893129: step 8006, loss 0.0103438, acc 1
2017-09-08T18:37:06.387039: step 8007, loss 0.00223966, acc 1
2017-09-08T18:37:06.889174: step 8008, loss 0.0297728, acc 0.984375
2017-09-08T18:37:07.370698: step 8009, loss 0.0424512, acc 0.984375
2017-09-08T18:37:07.863434: step 8010, loss 0.00455898, acc 1
2017-09-08T18:37:08.349908: step 8011, loss 0.000223211, acc 1
2017-09-08T18:37:08.817289: step 8012, loss 0.0499908, acc 0.96875
2017-09-08T18:37:09.295250: step 8013, loss 0.000416569, acc 1
2017-09-08T18:37:09.797867: step 8014, loss 0.00203581, acc 1
2017-09-08T18:37:10.177718: step 8015, loss 0.000462887, acc 1
2017-09-08T18:37:10.579040: step 8016, loss 0.000559493, acc 1
2017-09-08T18:37:10.964497: step 8017, loss 0.00537385, acc 1
2017-09-08T18:37:11.409693: step 8018, loss 0.00016121, acc 1
2017-09-08T18:37:11.861945: step 8019, loss 0.00562408, acc 1
2017-09-08T18:37:12.327681: step 8020, loss 0.00585754, acc 1
2017-09-08T18:37:12.802653: step 8021, loss 0.0104164, acc 1
2017-09-08T18:37:13.281879: step 8022, loss 0.0319252, acc 0.984375
2017-09-08T18:37:13.746111: step 8023, loss 0.00440755, acc 1
2017-09-08T18:37:14.230691: step 8024, loss 0.00316361, acc 1
2017-09-08T18:37:14.726854: step 8025, loss 0.000587395, acc 1
2017-09-08T18:37:15.219584: step 8026, loss 0.0327366, acc 0.984375
2017-09-08T18:37:15.735539: step 8027, loss 0.0282899, acc 0.984375
2017-09-08T18:37:16.227229: step 8028, loss 0.000421912, acc 1
2017-09-08T18:37:16.719024: step 8029, loss 0.0071296, acc 1
2017-09-08T18:37:17.198183: step 8030, loss 0.00832644, acc 1
2017-09-08T18:37:17.697863: step 8031, loss 0.00038356, acc 1
2017-09-08T18:37:18.172683: step 8032, loss 0.00017086, acc 1
2017-09-08T18:37:18.652010: step 8033, loss 0.000264808, acc 1
2017-09-08T18:37:19.139358: step 8034, loss 0.000478048, acc 1
2017-09-08T18:37:19.607999: step 8035, loss 0.000376829, acc 1
2017-09-08T18:37:20.080800: step 8036, loss 0.0665283, acc 0.960784
2017-09-08T18:37:20.609833: step 8037, loss 0.00651021, acc 1
2017-09-08T18:37:21.102371: step 8038, loss 0.002006, acc 1
2017-09-08T18:37:21.591940: step 8039, loss 0.0017843, acc 1
2017-09-08T18:37:22.070641: step 8040, loss 0.00120217, acc 1
2017-09-08T18:37:22.534592: step 8041, loss 0.0359184, acc 0.96875
2017-09-08T18:37:22.990726: step 8042, loss 0.0154397, acc 0.984375
2017-09-08T18:37:23.436574: step 8043, loss 0.00804647, acc 1
2017-09-08T18:37:23.960950: step 8044, loss 0.000997207, acc 1
2017-09-08T18:37:24.444425: step 8045, loss 0.0310979, acc 0.984375
2017-09-08T18:37:24.934798: step 8046, loss 0.00064713, acc 1
2017-09-08T18:37:25.429437: step 8047, loss 0.00017826, acc 1
2017-09-08T18:37:25.895974: step 8048, loss 0.0241247, acc 0.984375
2017-09-08T18:37:26.382774: step 8049, loss 0.000638111, acc 1
2017-09-08T18:37:26.875404: step 8050, loss 0.00924587, acc 1

Evaluation:
2017-09-08T18:37:27.488475: step 8050, loss 0.270757, acc 0.928058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8050

2017-09-08T18:37:30.175518: step 8051, loss 0.033741, acc 0.984375
2017-09-08T18:37:30.638646: step 8052, loss 0.0846206, acc 0.96875
2017-09-08T18:37:31.149882: step 8053, loss 0.0198107, acc 1
2017-09-08T18:37:31.643051: step 8054, loss 0.0254815, acc 0.984375
2017-09-08T18:37:32.142846: step 8055, loss 0.000150392, acc 1
2017-09-08T18:37:32.629178: step 8056, loss 0.000682844, acc 1
2017-09-08T18:37:33.129397: step 8057, loss 0.0500921, acc 0.984375
2017-09-08T18:37:33.604366: step 8058, loss 0.00518243, acc 1
2017-09-08T18:37:34.072627: step 8059, loss 0.0288062, acc 0.984375
2017-09-08T18:37:34.553191: step 8060, loss 0.0193508, acc 0.984375
2017-09-08T18:37:35.120315: step 8061, loss 0.0432349, acc 0.984375
2017-09-08T18:37:35.500279: step 8062, loss 0.000405278, acc 1
2017-09-08T18:37:35.896387: step 8063, loss 0.00337423, acc 1
2017-09-08T18:37:36.280022: step 8064, loss 0.0186555, acc 0.984375
2017-09-08T18:37:36.689197: step 8065, loss 0.0011271, acc 1
2017-09-08T18:37:37.192313: step 8066, loss 0.0100572, acc 1
2017-09-08T18:37:37.706297: step 8067, loss 0.0113406, acc 1
2017-09-08T18:37:38.149888: step 8068, loss 0.0101435, acc 1
2017-09-08T18:37:38.595844: step 8069, loss 0.0208137, acc 0.984375
2017-09-08T18:37:39.100240: step 8070, loss 0.000269701, acc 1
2017-09-08T18:37:39.554321: step 8071, loss 0.000517518, acc 1
2017-09-08T18:37:40.058195: step 8072, loss 0.0569855, acc 0.984375
2017-09-08T18:37:40.558675: step 8073, loss 0.000139064, acc 1
2017-09-08T18:37:41.021680: step 8074, loss 0.00798911, acc 1
2017-09-08T18:37:41.501958: step 8075, loss 0.000341506, acc 1
2017-09-08T18:37:41.996928: step 8076, loss 0.00103688, acc 1
2017-09-08T18:37:42.514143: step 8077, loss 0.0411207, acc 0.984375
2017-09-08T18:37:43.022592: step 8078, loss 0.0159209, acc 0.984375
2017-09-08T18:37:43.527744: step 8079, loss 0.000352981, acc 1
2017-09-08T18:37:44.009994: step 8080, loss 0.00298757, acc 1
2017-09-08T18:37:44.468065: step 8081, loss 0.00232957, acc 1
2017-09-08T18:37:44.975845: step 8082, loss 0.00818617, acc 1
2017-09-08T18:37:45.444610: step 8083, loss 0.000102149, acc 1
2017-09-08T18:37:45.921632: step 8084, loss 0.000886746, acc 1
2017-09-08T18:37:46.409026: step 8085, loss 0.00745386, acc 1
2017-09-08T18:37:46.898252: step 8086, loss 0.00033595, acc 1
2017-09-08T18:37:47.406109: step 8087, loss 0.000612019, acc 1
2017-09-08T18:37:47.910922: step 8088, loss 0.0301963, acc 0.984375
2017-09-08T18:37:48.422282: step 8089, loss 0.0164052, acc 0.984375
2017-09-08T18:37:48.899290: step 8090, loss 0.071694, acc 0.96875
2017-09-08T18:37:49.419114: step 8091, loss 0.00621959, acc 1
2017-09-08T18:37:49.923680: step 8092, loss 0.0325272, acc 0.96875
2017-09-08T18:37:50.399395: step 8093, loss 0.000255095, acc 1
2017-09-08T18:37:50.885490: step 8094, loss 0.000402059, acc 1
2017-09-08T18:37:51.361285: step 8095, loss 0.00998083, acc 1
2017-09-08T18:37:51.879292: step 8096, loss 0.0162561, acc 0.984375
2017-09-08T18:37:52.391708: step 8097, loss 0.000166874, acc 1
2017-09-08T18:37:52.883940: step 8098, loss 0.0266299, acc 0.984375
2017-09-08T18:37:53.374721: step 8099, loss 0.0117288, acc 0.984375
2017-09-08T18:37:53.883478: step 8100, loss 0.00360927, acc 1

Evaluation:
2017-09-08T18:37:54.492138: step 8100, loss 0.242077, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8100

2017-09-08T18:37:56.433230: step 8101, loss 0.000214536, acc 1
2017-09-08T18:37:56.903484: step 8102, loss 0.0329954, acc 0.984375
2017-09-08T18:37:57.428401: step 8103, loss 0.0279787, acc 0.984375
2017-09-08T18:37:57.900467: step 8104, loss 0.0274553, acc 0.984375
2017-09-08T18:37:58.384140: step 8105, loss 0.00014062, acc 1
2017-09-08T18:37:58.887015: step 8106, loss 0.0326103, acc 0.984375
2017-09-08T18:37:59.397003: step 8107, loss 0.0404176, acc 0.984375
2017-09-08T18:37:59.887296: step 8108, loss 0.000386885, acc 1
2017-09-08T18:38:00.340930: step 8109, loss 0.0736797, acc 0.984375
2017-09-08T18:38:00.846692: step 8110, loss 0.0616812, acc 0.984375
2017-09-08T18:38:01.391554: step 8111, loss 0.156171, acc 0.96875
2017-09-08T18:38:01.773444: step 8112, loss 0.00016147, acc 1
2017-09-08T18:38:02.166388: step 8113, loss 0.0192022, acc 0.984375
2017-09-08T18:38:02.582455: step 8114, loss 0.0273779, acc 0.984375
2017-09-08T18:38:02.990893: step 8115, loss 0.0290857, acc 0.984375
2017-09-08T18:38:03.384619: step 8116, loss 0.000442567, acc 1
2017-09-08T18:38:03.765971: step 8117, loss 0.00921426, acc 1
2017-09-08T18:38:04.205386: step 8118, loss 0.000138915, acc 1
2017-09-08T18:38:04.644344: step 8119, loss 0.00477704, acc 1
2017-09-08T18:38:05.083251: step 8120, loss 0.000449369, acc 1
2017-09-08T18:38:05.583623: step 8121, loss 0.00069293, acc 1
2017-09-08T18:38:06.041189: step 8122, loss 0.00657267, acc 1
2017-09-08T18:38:06.564574: step 8123, loss 0.000285291, acc 1
2017-09-08T18:38:07.005961: step 8124, loss 0.00269347, acc 1
2017-09-08T18:38:07.489182: step 8125, loss 0.00015517, acc 1
2017-09-08T18:38:07.958253: step 8126, loss 0.00614056, acc 1
2017-09-08T18:38:08.462658: step 8127, loss 0.0197389, acc 0.984375
2017-09-08T18:38:08.943067: step 8128, loss 0.00370508, acc 1
2017-09-08T18:38:09.418220: step 8129, loss 0.0325558, acc 0.984375
2017-09-08T18:38:09.883426: step 8130, loss 0.0680875, acc 0.96875
2017-09-08T18:38:10.351707: step 8131, loss 0.0123448, acc 1
2017-09-08T18:38:10.828610: step 8132, loss 0.0629103, acc 0.984375
2017-09-08T18:38:11.314922: step 8133, loss 0.0245683, acc 0.984375
2017-09-08T18:38:11.778396: step 8134, loss 0.000105706, acc 1
2017-09-08T18:38:12.255940: step 8135, loss 0.013345, acc 1
2017-09-08T18:38:12.778742: step 8136, loss 0.0102231, acc 1
2017-09-08T18:38:13.257125: step 8137, loss 0.014869, acc 0.984375
2017-09-08T18:38:13.756597: step 8138, loss 0.0165428, acc 0.984375
2017-09-08T18:38:14.260371: step 8139, loss 0.00830359, acc 1
2017-09-08T18:38:14.739966: step 8140, loss 5.13829e-05, acc 1
2017-09-08T18:38:15.235681: step 8141, loss 0.000282417, acc 1
2017-09-08T18:38:15.711004: step 8142, loss 0.0139716, acc 1
2017-09-08T18:38:16.198983: step 8143, loss 0.000479787, acc 1
2017-09-08T18:38:16.673848: step 8144, loss 0.00683667, acc 1
2017-09-08T18:38:17.129907: step 8145, loss 0.0010366, acc 1
2017-09-08T18:38:17.604279: step 8146, loss 0.0583119, acc 0.96875
2017-09-08T18:38:18.084449: step 8147, loss 0.0204188, acc 0.984375
2017-09-08T18:38:18.554374: step 8148, loss 0.00646525, acc 1
2017-09-08T18:38:19.028179: step 8149, loss 0.000371094, acc 1
2017-09-08T18:38:19.533929: step 8150, loss 0.014593, acc 1

Evaluation:
2017-09-08T18:38:20.045806: step 8150, loss 0.238933, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8150

2017-09-08T18:38:22.716423: step 8151, loss 0.018802, acc 0.984375
2017-09-08T18:38:23.223217: step 8152, loss 0.0206582, acc 0.984375
2017-09-08T18:38:23.685028: step 8153, loss 0.000175675, acc 1
2017-09-08T18:38:24.183167: step 8154, loss 0.000236768, acc 1
2017-09-08T18:38:24.659454: step 8155, loss 0.0360198, acc 0.984375
2017-09-08T18:38:25.150782: step 8156, loss 0.000136933, acc 1
2017-09-08T18:38:25.601233: step 8157, loss 0.000826431, acc 1
2017-09-08T18:38:26.111823: step 8158, loss 0.0008995, acc 1
2017-09-08T18:38:26.574976: step 8159, loss 0.00573138, acc 1
2017-09-08T18:38:27.059581: step 8160, loss 0.000976481, acc 1
2017-09-08T18:38:27.524455: step 8161, loss 0.00433017, acc 1
2017-09-08T18:38:28.073397: step 8162, loss 0.0174035, acc 1
2017-09-08T18:38:28.475152: step 8163, loss 0.000551456, acc 1
2017-09-08T18:38:28.863311: step 8164, loss 0.00405336, acc 1
2017-09-08T18:38:29.252787: step 8165, loss 0.010606, acc 1
2017-09-08T18:38:29.688921: step 8166, loss 0.000479573, acc 1
2017-09-08T18:38:30.166044: step 8167, loss 0.0115041, acc 1
2017-09-08T18:38:30.597709: step 8168, loss 0.00124576, acc 1
2017-09-08T18:38:31.070737: step 8169, loss 0.0245076, acc 0.984375
2017-09-08T18:38:31.517968: step 8170, loss 0.00199622, acc 1
2017-09-08T18:38:31.991870: step 8171, loss 0.029024, acc 0.984375
2017-09-08T18:38:32.465688: step 8172, loss 0.000789703, acc 1
2017-09-08T18:38:32.951272: step 8173, loss 0.000128404, acc 1
2017-09-08T18:38:33.440373: step 8174, loss 5.45729e-05, acc 1
2017-09-08T18:38:33.925530: step 8175, loss 0.0152241, acc 1
2017-09-08T18:38:34.400497: step 8176, loss 0.00101614, acc 1
2017-09-08T18:38:34.906318: step 8177, loss 0.000846754, acc 1
2017-09-08T18:38:35.384710: step 8178, loss 0.000400593, acc 1
2017-09-08T18:38:35.882818: step 8179, loss 0.043723, acc 0.96875
2017-09-08T18:38:36.365599: step 8180, loss 0.00103238, acc 1
2017-09-08T18:38:36.837621: step 8181, loss 0.0021589, acc 1
2017-09-08T18:38:37.298291: step 8182, loss 0.0610652, acc 0.984375
2017-09-08T18:38:37.804291: step 8183, loss 0.0170839, acc 1
2017-09-08T18:38:38.279154: step 8184, loss 0.00019111, acc 1
2017-09-08T18:38:38.760241: step 8185, loss 0.0307186, acc 0.96875
2017-09-08T18:38:39.222210: step 8186, loss 0.00101822, acc 1
2017-09-08T18:38:40.003118: step 8187, loss 0.000684654, acc 1
2017-09-08T18:38:40.460102: step 8188, loss 0.00629271, acc 1
2017-09-08T18:38:40.943340: step 8189, loss 0.000263355, acc 1
2017-09-08T18:38:41.453816: step 8190, loss 0.000122622, acc 1
2017-09-08T18:38:41.915750: step 8191, loss 0.0111561, acc 0.984375
2017-09-08T18:38:42.395980: step 8192, loss 0.000700288, acc 1
2017-09-08T18:38:42.891355: step 8193, loss 0.00645618, acc 1
2017-09-08T18:38:43.381412: step 8194, loss 0.00380626, acc 1
2017-09-08T18:38:43.880662: step 8195, loss 0.0233044, acc 0.984375
2017-09-08T18:38:44.373343: step 8196, loss 0.000222137, acc 1
2017-09-08T18:38:44.784001: step 8197, loss 0.00526317, acc 1
2017-09-08T18:38:45.235075: step 8198, loss 0.0477507, acc 0.984375
2017-09-08T18:38:45.670101: step 8199, loss 0.00652248, acc 1
2017-09-08T18:38:46.093109: step 8200, loss 0.00567525, acc 1

Evaluation:
2017-09-08T18:38:46.643242: step 8200, loss 0.257391, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8200

2017-09-08T18:38:48.457061: step 8201, loss 0.0520722, acc 0.984375
2017-09-08T18:38:48.922340: step 8202, loss 0.000122003, acc 1
2017-09-08T18:38:49.376539: step 8203, loss 0.0858019, acc 0.984375
2017-09-08T18:38:50.057765: step 8204, loss 0.000975362, acc 1
2017-09-08T18:38:50.496152: step 8205, loss 0.0449509, acc 0.984375
2017-09-08T18:38:50.979294: step 8206, loss 0.0119146, acc 1
2017-09-08T18:38:51.395329: step 8207, loss 0.04313, acc 0.984375
2017-09-08T18:38:51.825130: step 8208, loss 0.00600167, acc 1
2017-09-08T18:38:52.270772: step 8209, loss 0.0138236, acc 0.984375
2017-09-08T18:38:52.716176: step 8210, loss 0.00234379, acc 1
2017-09-08T18:38:53.147277: step 8211, loss 0.0546761, acc 0.984375
2017-09-08T18:38:53.562607: step 8212, loss 0.000651092, acc 1
2017-09-08T18:38:54.076373: step 8213, loss 0.0712185, acc 0.984375
2017-09-08T18:38:54.472622: step 8214, loss 0.00104786, acc 1
2017-09-08T18:38:54.875903: step 8215, loss 8.50706e-05, acc 1
2017-09-08T18:38:55.265663: step 8216, loss 0.00845063, acc 1
2017-09-08T18:38:55.652091: step 8217, loss 0.00491158, acc 1
2017-09-08T18:38:56.038262: step 8218, loss 0.00393322, acc 1
2017-09-08T18:38:56.496979: step 8219, loss 0.0251041, acc 0.984375
2017-09-08T18:38:56.990113: step 8220, loss 0.0619713, acc 0.984375
2017-09-08T18:38:57.458508: step 8221, loss 0.0435476, acc 0.984375
2017-09-08T18:38:57.953909: step 8222, loss 0.053506, acc 0.984375
2017-09-08T18:38:58.431047: step 8223, loss 0.00205576, acc 1
2017-09-08T18:38:58.911661: step 8224, loss 0.0270538, acc 0.984375
2017-09-08T18:38:59.392780: step 8225, loss 0.000286666, acc 1
2017-09-08T18:38:59.843557: step 8226, loss 0.000815611, acc 1
2017-09-08T18:39:00.327811: step 8227, loss 0.0315867, acc 0.984375
2017-09-08T18:39:00.823917: step 8228, loss 0.000284136, acc 1
2017-09-08T18:39:01.325688: step 8229, loss 0.0357124, acc 0.984375
2017-09-08T18:39:01.798853: step 8230, loss 0.155378, acc 0.953125
2017-09-08T18:39:02.288003: step 8231, loss 0.00783896, acc 1
2017-09-08T18:39:02.750106: step 8232, loss 0.00102099, acc 1
2017-09-08T18:39:03.270263: step 8233, loss 4.27258e-05, acc 1
2017-09-08T18:39:03.742560: step 8234, loss 0.044062, acc 0.96875
2017-09-08T18:39:04.238040: step 8235, loss 0.0105832, acc 1
2017-09-08T18:39:04.754444: step 8236, loss 0.0115179, acc 1
2017-09-08T18:39:05.224739: step 8237, loss 0.00575721, acc 1
2017-09-08T18:39:05.673671: step 8238, loss 0.0119113, acc 1
2017-09-08T18:39:06.152991: step 8239, loss 0.018778, acc 0.984375
2017-09-08T18:39:06.646994: step 8240, loss 9.77188e-05, acc 1
2017-09-08T18:39:07.114586: step 8241, loss 0.000408044, acc 1
2017-09-08T18:39:07.585938: step 8242, loss 0.000530094, acc 1
2017-09-08T18:39:08.087472: step 8243, loss 0.00479229, acc 1
2017-09-08T18:39:08.572278: step 8244, loss 0.00105061, acc 1
2017-09-08T18:39:09.084215: step 8245, loss 0.000719993, acc 1
2017-09-08T18:39:09.516616: step 8246, loss 0.000316959, acc 1
2017-09-08T18:39:09.997572: step 8247, loss 0.00527631, acc 1
2017-09-08T18:39:10.463394: step 8248, loss 0.00487556, acc 1
2017-09-08T18:39:10.965405: step 8249, loss 0.0235414, acc 0.984375
2017-09-08T18:39:11.448389: step 8250, loss 0.00027014, acc 1

Evaluation:
2017-09-08T18:39:11.940134: step 8250, loss 0.238375, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8250

2017-09-08T18:39:14.452083: step 8251, loss 0.00793783, acc 1
2017-09-08T18:39:14.949384: step 8252, loss 0.0198622, acc 0.984375
2017-09-08T18:39:15.449535: step 8253, loss 0.0169981, acc 0.984375
2017-09-08T18:39:15.959646: step 8254, loss 0.00278012, acc 1
2017-09-08T18:39:16.440248: step 8255, loss 0.000152205, acc 1
2017-09-08T18:39:16.946625: step 8256, loss 0.0387704, acc 0.984375
2017-09-08T18:39:17.421390: step 8257, loss 0.00425436, acc 1
2017-09-08T18:39:17.910129: step 8258, loss 0.0271905, acc 0.984375
2017-09-08T18:39:18.397273: step 8259, loss 0.0155153, acc 0.984375
2017-09-08T18:39:18.884231: step 8260, loss 0.00018848, acc 1
2017-09-08T18:39:19.380217: step 8261, loss 0.0412651, acc 0.984375
2017-09-08T18:39:19.889574: step 8262, loss 0.000609774, acc 1
2017-09-08T18:39:20.464107: step 8263, loss 0.0215828, acc 0.984375
2017-09-08T18:39:20.878383: step 8264, loss 0.000670193, acc 1
2017-09-08T18:39:21.291470: step 8265, loss 0.00377764, acc 1
2017-09-08T18:39:21.681216: step 8266, loss 0.000198884, acc 1
2017-09-08T18:39:22.127494: step 8267, loss 0.014493, acc 0.984375
2017-09-08T18:39:22.604736: step 8268, loss 0.00635271, acc 1
2017-09-08T18:39:23.081571: step 8269, loss 0.0180661, acc 0.984375
2017-09-08T18:39:23.505054: step 8270, loss 0.0271328, acc 0.984375
2017-09-08T18:39:23.970390: step 8271, loss 0.0200597, acc 0.984375
2017-09-08T18:39:24.411540: step 8272, loss 0.000766895, acc 1
2017-09-08T18:39:24.839094: step 8273, loss 0.000130608, acc 1
2017-09-08T18:39:25.302451: step 8274, loss 0.00598325, acc 1
2017-09-08T18:39:25.713586: step 8275, loss 0.0680549, acc 0.96875
2017-09-08T18:39:26.160159: step 8276, loss 0.0122952, acc 1
2017-09-08T18:39:26.621259: step 8277, loss 0.000205608, acc 1
2017-09-08T18:39:27.089753: step 8278, loss 0.000206834, acc 1
2017-09-08T18:39:27.561220: step 8279, loss 0.00897721, acc 1
2017-09-08T18:39:28.043694: step 8280, loss 0.0266637, acc 0.984375
2017-09-08T18:39:28.529858: step 8281, loss 0.0509457, acc 0.984375
2017-09-08T18:39:29.008485: step 8282, loss 0.0872098, acc 0.96875
2017-09-08T18:39:29.492228: step 8283, loss 0.00276799, acc 1
2017-09-08T18:39:29.964506: step 8284, loss 0.0309821, acc 0.984375
2017-09-08T18:39:30.479294: step 8285, loss 0.0298912, acc 0.984375
2017-09-08T18:39:30.947917: step 8286, loss 0.00157716, acc 1
2017-09-08T18:39:31.418906: step 8287, loss 0.0377991, acc 0.984375
2017-09-08T18:39:31.951304: step 8288, loss 0.000287932, acc 1
2017-09-08T18:39:32.414050: step 8289, loss 0.000225092, acc 1
2017-09-08T18:39:32.895362: step 8290, loss 0.000384304, acc 1
2017-09-08T18:39:33.404359: step 8291, loss 0.0421219, acc 0.984375
2017-09-08T18:39:33.904210: step 8292, loss 0.000335231, acc 1
2017-09-08T18:39:34.350328: step 8293, loss 0.000614878, acc 1
2017-09-08T18:39:34.847640: step 8294, loss 0.00961794, acc 1
2017-09-08T18:39:35.349955: step 8295, loss 0.021553, acc 0.984375
2017-09-08T18:39:35.840659: step 8296, loss 0.000799023, acc 1
2017-09-08T18:39:36.306041: step 8297, loss 0.000111314, acc 1
2017-09-08T18:39:36.789002: step 8298, loss 0.00268426, acc 1
2017-09-08T18:39:37.258810: step 8299, loss 0.0158821, acc 0.984375
2017-09-08T18:39:37.743533: step 8300, loss 0.0162033, acc 0.984375

Evaluation:
2017-09-08T18:39:38.237554: step 8300, loss 0.243117, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8300

2017-09-08T18:39:41.042297: step 8301, loss 0.00377299, acc 1
2017-09-08T18:39:41.534104: step 8302, loss 0.000366435, acc 1
2017-09-08T18:39:42.041925: step 8303, loss 0.0352378, acc 0.984375
2017-09-08T18:39:42.533045: step 8304, loss 0.000189945, acc 1
2017-09-08T18:39:43.022806: step 8305, loss 0.111257, acc 0.9375
2017-09-08T18:39:43.509039: step 8306, loss 0.000724785, acc 1
2017-09-08T18:39:43.998546: step 8307, loss 0.0150953, acc 1
2017-09-08T18:39:44.486079: step 8308, loss 0.000254345, acc 1
2017-09-08T18:39:44.984255: step 8309, loss 0.000968051, acc 1
2017-09-08T18:39:45.474526: step 8310, loss 0.000384202, acc 1
2017-09-08T18:39:46.042667: step 8311, loss 0.000144327, acc 1
2017-09-08T18:39:46.402687: step 8312, loss 0.101549, acc 0.984375
2017-09-08T18:39:46.797858: step 8313, loss 0.000678942, acc 1
2017-09-08T18:39:47.197239: step 8314, loss 0.000812734, acc 1
2017-09-08T18:39:47.573565: step 8315, loss 0.000144129, acc 1
2017-09-08T18:39:48.059402: step 8316, loss 0.00309336, acc 1
2017-09-08T18:39:48.564138: step 8317, loss 0.00101402, acc 1
2017-09-08T18:39:49.041354: step 8318, loss 0.0275554, acc 0.984375
2017-09-08T18:39:49.566648: step 8319, loss 0.115757, acc 0.96875
2017-09-08T18:39:50.046973: step 8320, loss 0.0103378, acc 1
2017-09-08T18:39:50.515340: step 8321, loss 0.0025793, acc 1
2017-09-08T18:39:50.995176: step 8322, loss 0.00488653, acc 1
2017-09-08T18:39:51.499391: step 8323, loss 0.000871507, acc 1
2017-09-08T18:39:51.977801: step 8324, loss 0.000507805, acc 1
2017-09-08T18:39:52.504316: step 8325, loss 0.0530443, acc 0.984375
2017-09-08T18:39:52.992726: step 8326, loss 0.000188425, acc 1
2017-09-08T18:39:53.432907: step 8327, loss 0.0562339, acc 0.984375
2017-09-08T18:39:53.922633: step 8328, loss 0.010544, acc 1
2017-09-08T18:39:54.440356: step 8329, loss 0.00706412, acc 1
2017-09-08T18:39:54.893982: step 8330, loss 0.118514, acc 0.960784
2017-09-08T18:39:55.411619: step 8331, loss 0.00124062, acc 1
2017-09-08T18:39:55.893910: step 8332, loss 0.0742666, acc 0.96875
2017-09-08T18:39:56.416636: step 8333, loss 0.000622537, acc 1
2017-09-08T18:39:56.887951: step 8334, loss 0.000409556, acc 1
2017-09-08T18:39:57.395254: step 8335, loss 0.000610544, acc 1
2017-09-08T18:39:57.874918: step 8336, loss 0.0268748, acc 0.984375
2017-09-08T18:39:58.363628: step 8337, loss 0.0106134, acc 1
2017-09-08T18:39:58.872948: step 8338, loss 0.000243018, acc 1
2017-09-08T18:39:59.352401: step 8339, loss 0.00120765, acc 1
2017-09-08T18:39:59.828071: step 8340, loss 0.0365682, acc 0.96875
2017-09-08T18:40:00.314302: step 8341, loss 0.025839, acc 0.984375
2017-09-08T18:40:00.799005: step 8342, loss 0.0340872, acc 0.984375
2017-09-08T18:40:01.286078: step 8343, loss 0.0371437, acc 0.984375
2017-09-08T18:40:01.774328: step 8344, loss 0.00127003, acc 1
2017-09-08T18:40:02.270678: step 8345, loss 0.00147462, acc 1
2017-09-08T18:40:02.721834: step 8346, loss 0.0248471, acc 1
2017-09-08T18:40:03.208783: step 8347, loss 0.0222691, acc 0.984375
2017-09-08T18:40:03.692778: step 8348, loss 0.000186843, acc 1
2017-09-08T18:40:04.222059: step 8349, loss 0.000251751, acc 1
2017-09-08T18:40:04.704455: step 8350, loss 0.000178083, acc 1

Evaluation:
2017-09-08T18:40:05.189282: step 8350, loss 0.243466, acc 0.936691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8350

2017-09-08T18:40:06.989128: step 8351, loss 0.000585817, acc 1
2017-09-08T18:40:07.459271: step 8352, loss 0.000536391, acc 1
2017-09-08T18:40:07.887740: step 8353, loss 0.000669601, acc 1
2017-09-08T18:40:08.382116: step 8354, loss 0.00190808, acc 1
2017-09-08T18:40:08.865934: step 8355, loss 0.000335241, acc 1
2017-09-08T18:40:09.318984: step 8356, loss 0.00030504, acc 1
2017-09-08T18:40:09.790814: step 8357, loss 0.0350235, acc 0.984375
2017-09-08T18:40:10.286674: step 8358, loss 0.000311691, acc 1
2017-09-08T18:40:10.775600: step 8359, loss 0.000899676, acc 1
2017-09-08T18:40:11.276181: step 8360, loss 0.00181414, acc 1
2017-09-08T18:40:11.836252: step 8361, loss 0.000416856, acc 1
2017-09-08T18:40:12.221830: step 8362, loss 0.0125763, acc 1
2017-09-08T18:40:12.616567: step 8363, loss 0.0385818, acc 0.984375
2017-09-08T18:40:13.007539: step 8364, loss 0.0240358, acc 0.984375
2017-09-08T18:40:13.418692: step 8365, loss 0.000313584, acc 1
2017-09-08T18:40:13.803985: step 8366, loss 0.0676837, acc 0.984375
2017-09-08T18:40:14.197314: step 8367, loss 0.0291188, acc 0.984375
2017-09-08T18:40:14.649198: step 8368, loss 0.0326845, acc 0.984375
2017-09-08T18:40:15.137318: step 8369, loss 0.0354901, acc 0.984375
2017-09-08T18:40:15.597187: step 8370, loss 0.00173272, acc 1
2017-09-08T18:40:16.068965: step 8371, loss 0.0137197, acc 1
2017-09-08T18:40:16.507955: step 8372, loss 0.0274298, acc 0.984375
2017-09-08T18:40:16.971422: step 8373, loss 9.78346e-05, acc 1
2017-09-08T18:40:17.418223: step 8374, loss 0.112473, acc 0.96875
2017-09-08T18:40:17.845374: step 8375, loss 0.0232329, acc 0.984375
2017-09-08T18:40:18.304603: step 8376, loss 0.0241833, acc 0.984375
2017-09-08T18:40:18.742422: step 8377, loss 0.0776443, acc 0.984375
2017-09-08T18:40:19.206037: step 8378, loss 0.000421834, acc 1
2017-09-08T18:40:19.760478: step 8379, loss 0.00736471, acc 1
2017-09-08T18:40:20.231809: step 8380, loss 0.00706658, acc 1
2017-09-08T18:40:20.684548: step 8381, loss 0.01246, acc 1
2017-09-08T18:40:21.144971: step 8382, loss 0.000174852, acc 1
2017-09-08T18:40:21.598469: step 8383, loss 0.0197044, acc 0.984375
2017-09-08T18:40:22.128295: step 8384, loss 0.00592226, acc 1
2017-09-08T18:40:22.627272: step 8385, loss 0.0149548, acc 1
2017-09-08T18:40:23.091253: step 8386, loss 0.0135671, acc 0.984375
2017-09-08T18:40:23.573761: step 8387, loss 0.00316723, acc 1
2017-09-08T18:40:24.079908: step 8388, loss 0.0194727, acc 0.984375
2017-09-08T18:40:24.571118: step 8389, loss 0.000341292, acc 1
2017-09-08T18:40:25.042937: step 8390, loss 0.00350164, acc 1
2017-09-08T18:40:25.524078: step 8391, loss 0.00272, acc 1
2017-09-08T18:40:26.038732: step 8392, loss 0.000857391, acc 1
2017-09-08T18:40:26.529888: step 8393, loss 0.000507913, acc 1
2017-09-08T18:40:27.031180: step 8394, loss 0.0186055, acc 0.984375
2017-09-08T18:40:27.504947: step 8395, loss 0.0174941, acc 0.984375
2017-09-08T18:40:27.988522: step 8396, loss 0.000158119, acc 1
2017-09-08T18:40:28.466013: step 8397, loss 0.000701175, acc 1
2017-09-08T18:40:28.949566: step 8398, loss 0.000201056, acc 1
2017-09-08T18:40:29.428444: step 8399, loss 0.000114918, acc 1
2017-09-08T18:40:29.888684: step 8400, loss 0.00027799, acc 1

Evaluation:
2017-09-08T18:40:30.495837: step 8400, loss 0.236735, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8400

2017-09-08T18:40:33.201331: step 8401, loss 0.00236809, acc 1
2017-09-08T18:40:33.730306: step 8402, loss 0.000171721, acc 1
2017-09-08T18:40:34.218018: step 8403, loss 0.00401576, acc 1
2017-09-08T18:40:34.709338: step 8404, loss 0.00641867, acc 1
2017-09-08T18:40:35.218275: step 8405, loss 0.00469238, acc 1
2017-09-08T18:40:35.703422: step 8406, loss 0.00371188, acc 1
2017-09-08T18:40:36.188537: step 8407, loss 0.0404855, acc 0.984375
2017-09-08T18:40:36.685187: step 8408, loss 0.00010505, acc 1
2017-09-08T18:40:37.171735: step 8409, loss 0.00104662, acc 1
2017-09-08T18:40:37.653557: step 8410, loss 0.0547206, acc 0.96875
2017-09-08T18:40:38.211613: step 8411, loss 0.00092471, acc 1
2017-09-08T18:40:38.589664: step 8412, loss 0.00185173, acc 1
2017-09-08T18:40:38.974681: step 8413, loss 0.0472001, acc 0.984375
2017-09-08T18:40:39.352222: step 8414, loss 0.0331035, acc 0.984375
2017-09-08T18:40:39.737393: step 8415, loss 0.0220179, acc 0.984375
2017-09-08T18:40:40.199046: step 8416, loss 0.0756051, acc 0.96875
2017-09-08T18:40:40.685937: step 8417, loss 0.029227, acc 0.984375
2017-09-08T18:40:41.167679: step 8418, loss 7.39758e-05, acc 1
2017-09-08T18:40:41.639852: step 8419, loss 0.00882984, acc 1
2017-09-08T18:40:42.140426: step 8420, loss 0.00170387, acc 1
2017-09-08T18:40:42.634044: step 8421, loss 5.85049e-05, acc 1
2017-09-08T18:40:43.116156: step 8422, loss 0.102937, acc 0.96875
2017-09-08T18:40:43.601713: step 8423, loss 0.0376475, acc 0.96875
2017-09-08T18:40:44.116522: step 8424, loss 9.60973e-05, acc 1
2017-09-08T18:40:44.587578: step 8425, loss 5.26913e-05, acc 1
2017-09-08T18:40:45.049495: step 8426, loss 0.0699655, acc 0.96875
2017-09-08T18:40:45.568771: step 8427, loss 0.00422697, acc 1
2017-09-08T18:40:46.050722: step 8428, loss 0.00186209, acc 1
2017-09-08T18:40:46.525901: step 8429, loss 0.00914467, acc 1
2017-09-08T18:40:47.010134: step 8430, loss 0.000264251, acc 1
2017-09-08T18:40:47.520004: step 8431, loss 0.000166868, acc 1
2017-09-08T18:40:47.996366: step 8432, loss 0.0367304, acc 0.984375
2017-09-08T18:40:48.480427: step 8433, loss 0.000262979, acc 1
2017-09-08T18:40:48.964493: step 8434, loss 0.0324403, acc 0.984375
2017-09-08T18:40:49.451381: step 8435, loss 0.000941401, acc 1
2017-09-08T18:40:49.916292: step 8436, loss 0.000253549, acc 1
2017-09-08T18:40:50.407349: step 8437, loss 0.0126041, acc 0.984375
2017-09-08T18:40:50.931873: step 8438, loss 0.00412025, acc 1
2017-09-08T18:40:51.382730: step 8439, loss 8.67301e-05, acc 1
2017-09-08T18:40:51.842470: step 8440, loss 0.00107827, acc 1
2017-09-08T18:40:52.294954: step 8441, loss 0.0265427, acc 0.984375
2017-09-08T18:40:52.766208: step 8442, loss 0.00238982, acc 1
2017-09-08T18:40:53.280145: step 8443, loss 0.0480696, acc 0.984375
2017-09-08T18:40:53.751622: step 8444, loss 0.0216995, acc 0.984375
2017-09-08T18:40:54.225802: step 8445, loss 0.0082068, acc 1
2017-09-08T18:40:54.746998: step 8446, loss 0.070684, acc 0.953125
2017-09-08T18:40:55.224612: step 8447, loss 0.00543771, acc 1
2017-09-08T18:40:55.697153: step 8448, loss 0.0254642, acc 0.984375
2017-09-08T18:40:56.196271: step 8449, loss 0.000269446, acc 1
2017-09-08T18:40:56.674908: step 8450, loss 0.046534, acc 0.984375

Evaluation:
2017-09-08T18:40:57.172329: step 8450, loss 0.244475, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8450

2017-09-08T18:40:58.903912: step 8451, loss 0.000749653, acc 1
2017-09-08T18:40:59.359873: step 8452, loss 0.0261949, acc 0.984375
2017-09-08T18:40:59.829591: step 8453, loss 0.000253323, acc 1
2017-09-08T18:41:00.312806: step 8454, loss 0.000248296, acc 1
2017-09-08T18:41:00.783278: step 8455, loss 0.0196838, acc 0.984375
2017-09-08T18:41:01.265823: step 8456, loss 0.0406642, acc 0.96875
2017-09-08T18:41:01.756241: step 8457, loss 0.055035, acc 0.96875
2017-09-08T18:41:02.249760: step 8458, loss 0.0371295, acc 0.984375
2017-09-08T18:41:02.765030: step 8459, loss 0.000212605, acc 1
2017-09-08T18:41:03.229122: step 8460, loss 0.0061924, acc 1
2017-09-08T18:41:03.680548: step 8461, loss 0.0174412, acc 0.984375
2017-09-08T18:41:04.219491: step 8462, loss 0.00852823, acc 1
2017-09-08T18:41:04.596185: step 8463, loss 0.000705054, acc 1
2017-09-08T18:41:04.974939: step 8464, loss 0.0750162, acc 0.96875
2017-09-08T18:41:05.384027: step 8465, loss 0.000260613, acc 1
2017-09-08T18:41:05.794235: step 8466, loss 9.63286e-05, acc 1
2017-09-08T18:41:06.196673: step 8467, loss 0.0133617, acc 1
2017-09-08T18:41:06.589836: step 8468, loss 0.000223413, acc 1
2017-09-08T18:41:07.039236: step 8469, loss 0.000257534, acc 1
2017-09-08T18:41:07.511739: step 8470, loss 0.00035436, acc 1
2017-09-08T18:41:07.985364: step 8471, loss 0.024612, acc 0.984375
2017-09-08T18:41:08.472352: step 8472, loss 0.0347965, acc 0.984375
2017-09-08T18:41:08.938324: step 8473, loss 0.115661, acc 0.96875
2017-09-08T18:41:09.439390: step 8474, loss 0.000376345, acc 1
2017-09-08T18:41:09.911147: step 8475, loss 0.00026128, acc 1
2017-09-08T18:41:10.408362: step 8476, loss 0.000298413, acc 1
2017-09-08T18:41:10.907487: step 8477, loss 0.00513457, acc 1
2017-09-08T18:41:11.416330: step 8478, loss 0.00316381, acc 1
2017-09-08T18:41:11.889910: step 8479, loss 0.00065461, acc 1
2017-09-08T18:41:12.386369: step 8480, loss 0.00471574, acc 1
2017-09-08T18:41:12.857639: step 8481, loss 0.0350729, acc 0.96875
2017-09-08T18:41:13.360680: step 8482, loss 0.0703185, acc 0.984375
2017-09-08T18:41:13.879127: step 8483, loss 0.017442, acc 1
2017-09-08T18:41:14.375484: step 8484, loss 0.0800998, acc 0.984375
2017-09-08T18:41:14.864393: step 8485, loss 0.000615219, acc 1
2017-09-08T18:41:15.341760: step 8486, loss 0.0236999, acc 0.984375
2017-09-08T18:41:15.831684: step 8487, loss 0.0148952, acc 0.984375
2017-09-08T18:41:16.330344: step 8488, loss 0.000332246, acc 1
2017-09-08T18:41:16.825867: step 8489, loss 0.00220345, acc 1
2017-09-08T18:41:17.294485: step 8490, loss 0.0309328, acc 0.984375
2017-09-08T18:41:17.807872: step 8491, loss 5.89889e-05, acc 1
2017-09-08T18:41:18.292213: step 8492, loss 0.000585513, acc 1
2017-09-08T18:41:18.790580: step 8493, loss 0.00327547, acc 1
2017-09-08T18:41:19.293300: step 8494, loss 0.000402688, acc 1
2017-09-08T18:41:19.765112: step 8495, loss 0.00232354, acc 1
2017-09-08T18:41:20.235833: step 8496, loss 0.0474635, acc 0.96875
2017-09-08T18:41:20.709212: step 8497, loss 0.00571972, acc 1
2017-09-08T18:41:21.173209: step 8498, loss 0.0064657, acc 1
2017-09-08T18:41:21.663640: step 8499, loss 0.0165114, acc 0.984375
2017-09-08T18:41:22.133205: step 8500, loss 0.0540563, acc 0.96875

Evaluation:
2017-09-08T18:41:22.651392: step 8500, loss 0.257717, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8500

2017-09-08T18:41:25.099689: step 8501, loss 0.0481226, acc 0.96875
2017-09-08T18:41:25.607406: step 8502, loss 0.0265848, acc 0.984375
2017-09-08T18:41:26.105256: step 8503, loss 0.00952869, acc 1
2017-09-08T18:41:26.572499: step 8504, loss 0.0454617, acc 0.984375
2017-09-08T18:41:27.078177: step 8505, loss 0.00180923, acc 1
2017-09-08T18:41:27.581229: step 8506, loss 0.15367, acc 0.953125
2017-09-08T18:41:28.061780: step 8507, loss 0.00277605, acc 1
2017-09-08T18:41:28.543125: step 8508, loss 0.00359056, acc 1
2017-09-08T18:41:29.050402: step 8509, loss 0.0178717, acc 0.984375
2017-09-08T18:41:29.559654: step 8510, loss 0.000425142, acc 1
2017-09-08T18:41:30.035589: step 8511, loss 0.0378539, acc 0.984375
2017-09-08T18:41:30.514607: step 8512, loss 0.000287126, acc 1
2017-09-08T18:41:31.061592: step 8513, loss 0.000558739, acc 1
2017-09-08T18:41:31.421993: step 8514, loss 0.010019, acc 1
2017-09-08T18:41:31.823409: step 8515, loss 0.00351755, acc 1
2017-09-08T18:41:32.218890: step 8516, loss 0.000287828, acc 1
2017-09-08T18:41:32.637623: step 8517, loss 0.0899497, acc 0.96875
2017-09-08T18:41:33.088057: step 8518, loss 0.0657847, acc 0.984375
2017-09-08T18:41:34.324188: step 8519, loss 0.00529168, acc 1
2017-09-08T18:41:34.784317: step 8520, loss 9.08907e-05, acc 1
2017-09-08T18:41:35.253452: step 8521, loss 0.00783908, acc 1
2017-09-08T18:41:35.739304: step 8522, loss 0.000193152, acc 1
2017-09-08T18:41:36.258561: step 8523, loss 0.0226053, acc 1
2017-09-08T18:41:36.725584: step 8524, loss 0.0312236, acc 0.984375
2017-09-08T18:41:37.204826: step 8525, loss 0.0178368, acc 0.984375
2017-09-08T18:41:37.655329: step 8526, loss 0.0391407, acc 0.980392
2017-09-08T18:41:38.127470: step 8527, loss 0.000227237, acc 1
2017-09-08T18:41:38.626617: step 8528, loss 0.0133854, acc 0.984375
2017-09-08T18:41:39.126611: step 8529, loss 0.000475776, acc 1
2017-09-08T18:41:39.600249: step 8530, loss 0.000845199, acc 1
2017-09-08T18:41:40.046432: step 8531, loss 0.00015358, acc 1
2017-09-08T18:41:40.574213: step 8532, loss 0.00936851, acc 1
2017-09-08T18:41:41.075732: step 8533, loss 0.000263834, acc 1
2017-09-08T18:41:41.580698: step 8534, loss 0.0033258, acc 1
2017-09-08T18:41:42.068591: step 8535, loss 0.0492185, acc 0.984375
2017-09-08T18:41:42.563740: step 8536, loss 0.0187785, acc 0.984375
2017-09-08T18:41:43.027080: step 8537, loss 0.000678792, acc 1
2017-09-08T18:41:43.485761: step 8538, loss 0.023275, acc 0.984375
2017-09-08T18:41:43.982698: step 8539, loss 0.000265358, acc 1
2017-09-08T18:41:44.427682: step 8540, loss 0.0268616, acc 0.984375
2017-09-08T18:41:44.906497: step 8541, loss 0.022073, acc 0.984375
2017-09-08T18:41:45.414292: step 8542, loss 0.0893005, acc 0.96875
2017-09-08T18:41:45.911091: step 8543, loss 2.93206e-05, acc 1
2017-09-08T18:41:46.405025: step 8544, loss 0.0592489, acc 0.984375
2017-09-08T18:41:46.896399: step 8545, loss 0.0254063, acc 0.984375
2017-09-08T18:41:47.348680: step 8546, loss 3.97077e-05, acc 1
2017-09-08T18:41:47.811701: step 8547, loss 0.000178562, acc 1
2017-09-08T18:41:48.272294: step 8548, loss 0.000731122, acc 1
2017-09-08T18:41:48.746058: step 8549, loss 0.000166648, acc 1
2017-09-08T18:41:49.237837: step 8550, loss 0.0229398, acc 0.984375

Evaluation:
2017-09-08T18:41:49.740152: step 8550, loss 0.231929, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8550

2017-09-08T18:41:52.804960: step 8551, loss 0.0304509, acc 0.984375
2017-09-08T18:41:53.320113: step 8552, loss 8.81726e-05, acc 1
2017-09-08T18:41:53.807699: step 8553, loss 0.0150993, acc 0.984375
2017-09-08T18:41:54.296971: step 8554, loss 0.0296804, acc 0.984375
2017-09-08T18:41:54.821715: step 8555, loss 0.0171935, acc 1
2017-09-08T18:41:55.289624: step 8556, loss 0.00156588, acc 1
2017-09-08T18:41:55.766558: step 8557, loss 0.0161131, acc 1
2017-09-08T18:41:56.276966: step 8558, loss 0.0526152, acc 0.96875
2017-09-08T18:41:56.810368: step 8559, loss 0.0148918, acc 0.984375
2017-09-08T18:41:57.199586: step 8560, loss 5.25467e-05, acc 1
2017-09-08T18:41:57.577386: step 8561, loss 0.0377023, acc 0.984375
2017-09-08T18:41:57.983716: step 8562, loss 0.000870424, acc 1
2017-09-08T18:41:58.375246: step 8563, loss 0.000142686, acc 1
2017-09-08T18:41:58.847125: step 8564, loss 0.0184928, acc 0.984375
2017-09-08T18:41:59.329163: step 8565, loss 6.94104e-05, acc 1
2017-09-08T18:41:59.835400: step 8566, loss 0.000199261, acc 1
2017-09-08T18:42:00.318541: step 8567, loss 0.00913242, acc 1
2017-09-08T18:42:00.822037: step 8568, loss 0.0109965, acc 1
2017-09-08T18:42:01.287558: step 8569, loss 0.0370759, acc 0.984375
2017-09-08T18:42:01.761983: step 8570, loss 0.00435792, acc 1
2017-09-08T18:42:02.264006: step 8571, loss 0.00195768, acc 1
2017-09-08T18:42:02.751407: step 8572, loss 0.0264446, acc 0.984375
2017-09-08T18:42:03.205025: step 8573, loss 0.00219008, acc 1
2017-09-08T18:42:03.700106: step 8574, loss 0.000190934, acc 1
2017-09-08T18:42:04.190740: step 8575, loss 0.0184525, acc 0.984375
2017-09-08T18:42:04.702364: step 8576, loss 0.000654706, acc 1
2017-09-08T18:42:05.202134: step 8577, loss 0.00736767, acc 1
2017-09-08T18:42:05.678780: step 8578, loss 0.000400376, acc 1
2017-09-08T18:42:06.162849: step 8579, loss 0.00502177, acc 1
2017-09-08T18:42:06.631345: step 8580, loss 0.00420197, acc 1
2017-09-08T18:42:07.118609: step 8581, loss 0.0307764, acc 0.984375
2017-09-08T18:42:07.610093: step 8582, loss 0.025396, acc 0.984375
2017-09-08T18:42:08.066811: step 8583, loss 9.24139e-05, acc 1
2017-09-08T18:42:08.529296: step 8584, loss 0.00847538, acc 1
2017-09-08T18:42:08.983418: step 8585, loss 0.00460968, acc 1
2017-09-08T18:42:09.450631: step 8586, loss 0.033785, acc 0.984375
2017-09-08T18:42:09.948336: step 8587, loss 0.0178017, acc 0.984375
2017-09-08T18:42:10.454168: step 8588, loss 0.000488307, acc 1
2017-09-08T18:42:10.939065: step 8589, loss 0.00588925, acc 1
2017-09-08T18:42:11.420424: step 8590, loss 0.0197734, acc 0.984375
2017-09-08T18:42:11.882086: step 8591, loss 0.000143327, acc 1
2017-09-08T18:42:12.376991: step 8592, loss 0.000453553, acc 1
2017-09-08T18:42:12.861759: step 8593, loss 0.00183229, acc 1
2017-09-08T18:42:13.349121: step 8594, loss 0.000314006, acc 1
2017-09-08T18:42:13.853471: step 8595, loss 0.000206248, acc 1
2017-09-08T18:42:14.337718: step 8596, loss 8.8989e-05, acc 1
2017-09-08T18:42:14.813747: step 8597, loss 0.00606328, acc 1
2017-09-08T18:42:15.286996: step 8598, loss 0.000108583, acc 1
2017-09-08T18:42:15.776741: step 8599, loss 0.0832946, acc 0.96875
2017-09-08T18:42:16.259798: step 8600, loss 0.000529592, acc 1

Evaluation:
2017-09-08T18:42:16.752584: step 8600, loss 0.245211, acc 0.935252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8600

2017-09-08T18:42:18.485117: step 8601, loss 0.000208506, acc 1
2017-09-08T18:42:18.933612: step 8602, loss 0.0764713, acc 0.96875
2017-09-08T18:42:19.401605: step 8603, loss 0.0188239, acc 0.984375
2017-09-08T18:42:19.866397: step 8604, loss 0.0311446, acc 0.984375
2017-09-08T18:42:21.290310: step 8605, loss 0.0455541, acc 0.984375
2017-09-08T18:42:21.757806: step 8606, loss 0.00035246, acc 1
2017-09-08T18:42:22.244648: step 8607, loss 0.0173194, acc 0.984375
2017-09-08T18:42:22.724571: step 8608, loss 0.00240959, acc 1
2017-09-08T18:42:23.240416: step 8609, loss 0.000202422, acc 1
2017-09-08T18:42:23.621600: step 8610, loss 0.000104484, acc 1
2017-09-08T18:42:23.986595: step 8611, loss 0.00143743, acc 1
2017-09-08T18:42:24.379957: step 8612, loss 0.0228384, acc 0.984375
2017-09-08T18:42:24.773179: step 8613, loss 0.010157, acc 1
2017-09-08T18:42:25.166778: step 8614, loss 0.00284304, acc 1
2017-09-08T18:42:25.534792: step 8615, loss 0.0183309, acc 1
2017-09-08T18:42:26.037157: step 8616, loss 0.00049232, acc 1
2017-09-08T18:42:26.526902: step 8617, loss 0.00258217, acc 1
2017-09-08T18:42:26.999693: step 8618, loss 0.000205357, acc 1
2017-09-08T18:42:27.462994: step 8619, loss 0.0264325, acc 0.984375
2017-09-08T18:42:27.944738: step 8620, loss 0.0411716, acc 0.96875
2017-09-08T18:42:28.414286: step 8621, loss 0.0599297, acc 0.984375
2017-09-08T18:42:28.932607: step 8622, loss 0.00951529, acc 1
2017-09-08T18:42:29.411479: step 8623, loss 0.0244316, acc 0.984375
2017-09-08T18:42:29.863844: step 8624, loss 0.000229532, acc 1
2017-09-08T18:42:30.345537: step 8625, loss 0.000126234, acc 1
2017-09-08T18:42:30.861594: step 8626, loss 0.000217954, acc 1
2017-09-08T18:42:32.008527: step 8627, loss 0.000296778, acc 1
2017-09-08T18:42:32.519039: step 8628, loss 0.051233, acc 0.96875
2017-09-08T18:42:33.026582: step 8629, loss 0.000966788, acc 1
2017-09-08T18:42:33.518456: step 8630, loss 0.00112546, acc 1
2017-09-08T18:42:34.000813: step 8631, loss 0.00649234, acc 1
2017-09-08T18:42:34.491038: step 8632, loss 0.000883242, acc 1
2017-09-08T18:42:34.953677: step 8633, loss 0.00862035, acc 1
2017-09-08T18:42:35.445779: step 8634, loss 0.0115462, acc 1
2017-09-08T18:42:35.931771: step 8635, loss 0.0080518, acc 1
2017-09-08T18:42:36.975775: step 8636, loss 0.0417273, acc 0.984375
2017-09-08T18:42:37.477140: step 8637, loss 0.0521462, acc 0.984375
2017-09-08T18:42:37.959422: step 8638, loss 0.000225574, acc 1
2017-09-08T18:42:38.456723: step 8639, loss 0.0104675, acc 1
2017-09-08T18:42:38.963235: step 8640, loss 0.0062223, acc 1
2017-09-08T18:42:39.441706: step 8641, loss 0.00353093, acc 1
2017-09-08T18:42:39.909505: step 8642, loss 0.000778345, acc 1
2017-09-08T18:42:40.373903: step 8643, loss 0.0246851, acc 0.984375
2017-09-08T18:42:40.862256: step 8644, loss 0.00968006, acc 1
2017-09-08T18:42:41.370330: step 8645, loss 0.00907974, acc 1
2017-09-08T18:42:42.391481: step 8646, loss 0.0314818, acc 0.984375
2017-09-08T18:42:42.884002: step 8647, loss 0.000202874, acc 1
2017-09-08T18:42:43.345303: step 8648, loss 0.000216967, acc 1
2017-09-08T18:42:43.814257: step 8649, loss 0.00855211, acc 1
2017-09-08T18:42:44.296446: step 8650, loss 0.00987323, acc 1

Evaluation:
2017-09-08T18:42:44.810623: step 8650, loss 0.239749, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8650

2017-09-08T18:42:47.686924: step 8651, loss 0.00834657, acc 1
2017-09-08T18:42:48.159262: step 8652, loss 0.0141195, acc 0.984375
2017-09-08T18:42:48.590750: step 8653, loss 0.000881981, acc 1
2017-09-08T18:42:49.052008: step 8654, loss 0.0104152, acc 1
2017-09-08T18:42:49.484454: step 8655, loss 0.000273328, acc 1
2017-09-08T18:42:50.028849: step 8656, loss 0.0822627, acc 0.984375
2017-09-08T18:42:50.402592: step 8657, loss 0.00112759, acc 1
2017-09-08T18:42:50.793999: step 8658, loss 0.141616, acc 0.953125
2017-09-08T18:42:51.160130: step 8659, loss 0.00777847, acc 1
2017-09-08T18:42:51.586441: step 8660, loss 0.0563187, acc 0.96875
2017-09-08T18:42:52.055969: step 8661, loss 0.000168763, acc 1
2017-09-08T18:42:52.476646: step 8662, loss 0.000408513, acc 1
2017-09-08T18:42:52.958230: step 8663, loss 0.00320064, acc 1
2017-09-08T18:42:53.419285: step 8664, loss 0.00403562, acc 1
2017-09-08T18:42:53.892717: step 8665, loss 0.0345402, acc 0.984375
2017-09-08T18:42:54.367477: step 8666, loss 7.00378e-05, acc 1
2017-09-08T18:42:54.866712: step 8667, loss 0.000256279, acc 1
2017-09-08T18:42:55.395387: step 8668, loss 0.0230537, acc 0.984375
2017-09-08T18:42:55.863163: step 8669, loss 0.0110333, acc 1
2017-09-08T18:42:56.382525: step 8670, loss 0.00726639, acc 1
2017-09-08T18:42:56.858539: step 8671, loss 0.00905982, acc 1
2017-09-08T18:42:57.354178: step 8672, loss 0.000851866, acc 1
2017-09-08T18:42:57.863772: step 8673, loss 0.0234212, acc 0.984375
2017-09-08T18:42:58.354259: step 8674, loss 0.0104673, acc 1
2017-09-08T18:42:58.833937: step 8675, loss 0.000180525, acc 1
2017-09-08T18:42:59.286516: step 8676, loss 9.59647e-05, acc 1
2017-09-08T18:42:59.799003: step 8677, loss 0.0101126, acc 1
2017-09-08T18:43:00.294903: step 8678, loss 0.00642529, acc 1
2017-09-08T18:43:00.771219: step 8679, loss 0.000279565, acc 1
2017-09-08T18:43:01.240905: step 8680, loss 0.103862, acc 0.96875
2017-09-08T18:43:01.704708: step 8681, loss 0.0833213, acc 0.96875
2017-09-08T18:43:02.160527: step 8682, loss 0.00128639, acc 1
2017-09-08T18:43:02.676804: step 8683, loss 0.000216594, acc 1
2017-09-08T18:43:03.138787: step 8684, loss 0.0477414, acc 0.984375
2017-09-08T18:43:03.647570: step 8685, loss 0.0316496, acc 0.984375
2017-09-08T18:43:04.118143: step 8686, loss 0.000186077, acc 1
2017-09-08T18:43:04.614816: step 8687, loss 0.126109, acc 0.96875
2017-09-08T18:43:05.097896: step 8688, loss 0.0414245, acc 0.984375
2017-09-08T18:43:05.563887: step 8689, loss 0.0465577, acc 0.984375
2017-09-08T18:43:06.012467: step 8690, loss 9.04671e-05, acc 1
2017-09-08T18:43:06.481570: step 8691, loss 0.000266406, acc 1
2017-09-08T18:43:06.974765: step 8692, loss 0.000115304, acc 1
2017-09-08T18:43:07.456158: step 8693, loss 0.0092765, acc 1
2017-09-08T18:43:07.983165: step 8694, loss 6.83558e-05, acc 1
2017-09-08T18:43:08.468394: step 8695, loss 0.0185706, acc 0.984375
2017-09-08T18:43:08.954878: step 8696, loss 0.0286709, acc 0.984375
2017-09-08T18:43:09.467825: step 8697, loss 0.0204476, acc 0.984375
2017-09-08T18:43:09.978596: step 8698, loss 0.00100978, acc 1
2017-09-08T18:43:10.457000: step 8699, loss 0.000523257, acc 1
2017-09-08T18:43:10.925705: step 8700, loss 0.000103829, acc 1

Evaluation:
2017-09-08T18:43:11.391224: step 8700, loss 0.265956, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8700

2017-09-08T18:43:13.263357: step 8701, loss 0.0745945, acc 0.96875
2017-09-08T18:43:13.740583: step 8702, loss 7.92401e-05, acc 1
2017-09-08T18:43:14.223806: step 8703, loss 0.0118591, acc 0.984375
2017-09-08T18:43:14.707955: step 8704, loss 0.123151, acc 0.96875
2017-09-08T18:43:15.196146: step 8705, loss 0.0210156, acc 0.984375
2017-09-08T18:43:15.699473: step 8706, loss 0.00328276, acc 1
2017-09-08T18:43:16.258814: step 8707, loss 0.0128601, acc 0.984375
2017-09-08T18:43:16.640999: step 8708, loss 0.000256929, acc 1
2017-09-08T18:43:17.041917: step 8709, loss 0.0167229, acc 0.984375
2017-09-08T18:43:17.412720: step 8710, loss 0.0214096, acc 0.984375
2017-09-08T18:43:17.807205: step 8711, loss 0.000452583, acc 1
2017-09-08T18:43:18.191705: step 8712, loss 0.003334, acc 1
2017-09-08T18:43:18.653025: step 8713, loss 0.000212008, acc 1
2017-09-08T18:43:19.153634: step 8714, loss 0.0164031, acc 0.984375
2017-09-08T18:43:19.614209: step 8715, loss 0.0298376, acc 0.984375
2017-09-08T18:43:20.092510: step 8716, loss 0.00568357, acc 1
2017-09-08T18:43:20.570312: step 8717, loss 0.0119217, acc 0.984375
2017-09-08T18:43:21.040530: step 8718, loss 0.000175115, acc 1
2017-09-08T18:43:21.508651: step 8719, loss 0.000422354, acc 1
2017-09-08T18:43:21.997571: step 8720, loss 0.000284645, acc 1
2017-09-08T18:43:22.480632: step 8721, loss 0.00152768, acc 1
2017-09-08T18:43:22.942646: step 8722, loss 0.00163815, acc 1
2017-09-08T18:43:23.432520: step 8723, loss 0.000181308, acc 1
2017-09-08T18:43:23.878385: step 8724, loss 0.00347485, acc 1
2017-09-08T18:43:24.400412: step 8725, loss 0.00185932, acc 1
2017-09-08T18:43:24.884792: step 8726, loss 0.0117715, acc 0.984375
2017-09-08T18:43:25.383589: step 8727, loss 0.000468953, acc 1
2017-09-08T18:43:25.883668: step 8728, loss 0.0701521, acc 0.96875
2017-09-08T18:43:26.368563: step 8729, loss 0.00264911, acc 1
2017-09-08T18:43:26.841033: step 8730, loss 0.0223191, acc 0.984375
2017-09-08T18:43:27.330923: step 8731, loss 0.000277084, acc 1
2017-09-08T18:43:27.803647: step 8732, loss 0.000174807, acc 1
2017-09-08T18:43:28.266052: step 8733, loss 0.00297333, acc 1
2017-09-08T18:43:28.756080: step 8734, loss 0.0115692, acc 1
2017-09-08T18:43:29.242794: step 8735, loss 0.0103534, acc 1
2017-09-08T18:43:29.757290: step 8736, loss 0.00045503, acc 1
2017-09-08T18:43:30.284828: step 8737, loss 0.0290437, acc 0.984375
2017-09-08T18:43:30.777265: step 8738, loss 0.000701526, acc 1
2017-09-08T18:43:31.254413: step 8739, loss 0.0129637, acc 1
2017-09-08T18:43:31.706597: step 8740, loss 0.000193191, acc 1
2017-09-08T18:43:32.180666: step 8741, loss 0.00651032, acc 1
2017-09-08T18:43:32.692712: step 8742, loss 0.050454, acc 0.984375
2017-09-08T18:43:33.180394: step 8743, loss 0.0102869, acc 1
2017-09-08T18:43:33.686072: step 8744, loss 0.00814018, acc 1
2017-09-08T18:43:34.170498: step 8745, loss 0.0117714, acc 1
2017-09-08T18:43:34.655360: step 8746, loss 0.000841265, acc 1
2017-09-08T18:43:35.123194: step 8747, loss 0.0434969, acc 0.984375
2017-09-08T18:43:35.636961: step 8748, loss 0.0216026, acc 0.984375
2017-09-08T18:43:36.105590: step 8749, loss 7.51244e-05, acc 1
2017-09-08T18:43:36.614145: step 8750, loss 0.0360914, acc 0.984375

Evaluation:
2017-09-08T18:43:37.118235: step 8750, loss 0.261286, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8750

2017-09-08T18:43:39.575702: step 8751, loss 0.0129421, acc 1
2017-09-08T18:43:40.048277: step 8752, loss 0.0228996, acc 0.984375
2017-09-08T18:43:40.564159: step 8753, loss 0.00156978, acc 1
2017-09-08T18:43:41.052058: step 8754, loss 0.042415, acc 0.984375
2017-09-08T18:43:41.515510: step 8755, loss 0.0156467, acc 0.984375
2017-09-08T18:43:42.013310: step 8756, loss 0.000637199, acc 1
2017-09-08T18:43:42.582659: step 8757, loss 0.000108569, acc 1
2017-09-08T18:43:42.976965: step 8758, loss 8.92915e-05, acc 1
2017-09-08T18:43:43.380819: step 8759, loss 0.000153311, acc 1
2017-09-08T18:43:43.768155: step 8760, loss 0.016842, acc 0.984375
2017-09-08T18:43:44.178796: step 8761, loss 0.043775, acc 0.984375
2017-09-08T18:43:44.635060: step 8762, loss 0.000164282, acc 1
2017-09-08T18:43:45.090348: step 8763, loss 0.033654, acc 0.984375
2017-09-08T18:43:45.543061: step 8764, loss 0.000102449, acc 1
2017-09-08T18:43:45.996899: step 8765, loss 0.000290164, acc 1
2017-09-08T18:43:46.493084: step 8766, loss 0.0146873, acc 1
2017-09-08T18:43:47.016271: step 8767, loss 0.0493388, acc 0.96875
2017-09-08T18:43:47.526851: step 8768, loss 6.01498e-05, acc 1
2017-09-08T18:43:48.025101: step 8769, loss 0.000317327, acc 1
2017-09-08T18:43:48.530074: step 8770, loss 6.0699e-05, acc 1
2017-09-08T18:43:49.009073: step 8771, loss 0.000104251, acc 1
2017-09-08T18:43:49.468818: step 8772, loss 0.0107792, acc 1
2017-09-08T18:43:49.983501: step 8773, loss 6.42909e-05, acc 1
2017-09-08T18:43:50.471006: step 8774, loss 0.000327253, acc 1
2017-09-08T18:43:50.959378: step 8775, loss 0.0569543, acc 0.96875
2017-09-08T18:43:51.456056: step 8776, loss 0.00203433, acc 1
2017-09-08T18:43:51.945287: step 8777, loss 0.000110992, acc 1
2017-09-08T18:43:52.461136: step 8778, loss 0.0738034, acc 0.984375
2017-09-08T18:43:52.905096: step 8779, loss 0.0662888, acc 0.96875
2017-09-08T18:43:53.393057: step 8780, loss 0.000844669, acc 1
2017-09-08T18:43:53.877704: step 8781, loss 3.66502e-05, acc 1
2017-09-08T18:43:54.357630: step 8782, loss 0.0180577, acc 0.984375
2017-09-08T18:43:54.842275: step 8783, loss 0.000208727, acc 1
2017-09-08T18:43:55.324425: step 8784, loss 0.000130886, acc 1
2017-09-08T18:43:55.819721: step 8785, loss 0.00109235, acc 1
2017-09-08T18:43:56.289975: step 8786, loss 0.0179142, acc 0.984375
2017-09-08T18:43:56.796260: step 8787, loss 0.0170744, acc 0.984375
2017-09-08T18:43:57.313755: step 8788, loss 0.0277772, acc 0.984375
2017-09-08T18:43:57.812727: step 8789, loss 0.0143655, acc 0.984375
2017-09-08T18:43:58.283284: step 8790, loss 0.000990534, acc 1
2017-09-08T18:43:58.772134: step 8791, loss 0.0562709, acc 0.96875
2017-09-08T18:43:59.263103: step 8792, loss 0.110384, acc 0.96875
2017-09-08T18:43:59.738407: step 8793, loss 0.00175177, acc 1
2017-09-08T18:44:00.232991: step 8794, loss 0.00130091, acc 1
2017-09-08T18:44:00.684440: step 8795, loss 0.0225875, acc 0.984375
2017-09-08T18:44:01.171014: step 8796, loss 0.000686644, acc 1
2017-09-08T18:44:01.635460: step 8797, loss 0.0135652, acc 1
2017-09-08T18:44:02.121347: step 8798, loss 0.0003097, acc 1
2017-09-08T18:44:02.583186: step 8799, loss 0.00777987, acc 1
2017-09-08T18:44:03.070861: step 8800, loss 0.00299872, acc 1

Evaluation:
2017-09-08T18:44:03.552948: step 8800, loss 0.251996, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8800

2017-09-08T18:44:06.140219: step 8801, loss 0.0421493, acc 0.984375
2017-09-08T18:44:06.591852: step 8802, loss 0.0732391, acc 0.984375
2017-09-08T18:44:07.054380: step 8803, loss 0.00636231, acc 1
2017-09-08T18:44:07.538352: step 8804, loss 0.00647501, acc 1
2017-09-08T18:44:08.042268: step 8805, loss 0.0231242, acc 0.984375
2017-09-08T18:44:08.439993: step 8806, loss 9.91797e-05, acc 1
2017-09-08T18:44:08.806829: step 8807, loss 0.0104335, acc 1
2017-09-08T18:44:09.192136: step 8808, loss 0.0674547, acc 0.96875
2017-09-08T18:44:09.586401: step 8809, loss 0.0773077, acc 0.953125
2017-09-08T18:44:10.042676: step 8810, loss 0.00404976, acc 1
2017-09-08T18:44:10.525201: step 8811, loss 0.00644092, acc 1
2017-09-08T18:44:11.033012: step 8812, loss 0.0414641, acc 0.984375
2017-09-08T18:44:11.549742: step 8813, loss 0.026583, acc 0.984375
2017-09-08T18:44:12.024670: step 8814, loss 0.0133673, acc 0.984375
2017-09-08T18:44:12.504660: step 8815, loss 0.000136243, acc 1
2017-09-08T18:44:13.009603: step 8816, loss 0.0205576, acc 0.984375
2017-09-08T18:44:13.522422: step 8817, loss 0.00191027, acc 1
2017-09-08T18:44:13.981738: step 8818, loss 0.000358999, acc 1
2017-09-08T18:44:14.468857: step 8819, loss 0.000122496, acc 1
2017-09-08T18:44:14.909296: step 8820, loss 0.000884316, acc 1
2017-09-08T18:44:15.405996: step 8821, loss 0.0169798, acc 0.984375
2017-09-08T18:44:15.870193: step 8822, loss 0.00168539, acc 1
2017-09-08T18:44:16.360079: step 8823, loss 0.0397745, acc 0.984375
2017-09-08T18:44:16.851833: step 8824, loss 0.0504561, acc 0.984375
2017-09-08T18:44:17.324378: step 8825, loss 0.0286679, acc 0.984375
2017-09-08T18:44:17.806550: step 8826, loss 0.000580407, acc 1
2017-09-08T18:44:18.250309: step 8827, loss 0.00261875, acc 1
2017-09-08T18:44:18.766749: step 8828, loss 0.000170135, acc 1
2017-09-08T18:44:19.251668: step 8829, loss 0.00304595, acc 1
2017-09-08T18:44:19.700005: step 8830, loss 0.00450647, acc 1
2017-09-08T18:44:20.187459: step 8831, loss 0.00326186, acc 1
2017-09-08T18:44:20.662661: step 8832, loss 0.0323527, acc 0.984375
2017-09-08T18:44:21.149715: step 8833, loss 9.23183e-05, acc 1
2017-09-08T18:44:21.622774: step 8834, loss 0.000248501, acc 1
2017-09-08T18:44:22.096329: step 8835, loss 0.000560763, acc 1
2017-09-08T18:44:22.561136: step 8836, loss 0.000100402, acc 1
2017-09-08T18:44:23.025805: step 8837, loss 0.0402265, acc 0.984375
2017-09-08T18:44:23.515044: step 8838, loss 0.00292684, acc 1
2017-09-08T18:44:23.980437: step 8839, loss 0.0355109, acc 0.984375
2017-09-08T18:44:24.468171: step 8840, loss 0.024774, acc 0.984375
2017-09-08T18:44:24.956415: step 8841, loss 0.00460676, acc 1
2017-09-08T18:44:25.449784: step 8842, loss 0.0361868, acc 0.984375
2017-09-08T18:44:25.924428: step 8843, loss 0.00446946, acc 1
2017-09-08T18:44:26.370953: step 8844, loss 0.00994837, acc 1
2017-09-08T18:44:26.876365: step 8845, loss 0.000737185, acc 1
2017-09-08T18:44:27.373897: step 8846, loss 0.000982262, acc 1
2017-09-08T18:44:27.873395: step 8847, loss 0.0152118, acc 0.984375
2017-09-08T18:44:28.371133: step 8848, loss 0.0266839, acc 0.984375
2017-09-08T18:44:28.866832: step 8849, loss 0.00484823, acc 1
2017-09-08T18:44:29.350448: step 8850, loss 0.0172637, acc 0.984375

Evaluation:
2017-09-08T18:44:29.846161: step 8850, loss 0.249495, acc 0.928058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8850

2017-09-08T18:44:31.825967: step 8851, loss 0.00156182, acc 1
2017-09-08T18:44:32.288602: step 8852, loss 0.0165942, acc 0.984375
2017-09-08T18:44:33.470315: step 8853, loss 0.0503278, acc 0.984375
2017-09-08T18:44:33.947586: step 8854, loss 0.000375444, acc 1
2017-09-08T18:44:34.532120: step 8855, loss 9.81346e-05, acc 1
2017-09-08T18:44:34.922550: step 8856, loss 0.00951321, acc 1
2017-09-08T18:44:35.313794: step 8857, loss 0.0347894, acc 0.96875
2017-09-08T18:44:35.703668: step 8858, loss 0.000392192, acc 1
2017-09-08T18:44:36.099012: step 8859, loss 0.000629983, acc 1
2017-09-08T18:44:36.484804: step 8860, loss 0.000265685, acc 1
2017-09-08T18:44:36.955144: step 8861, loss 0.032518, acc 0.984375
2017-09-08T18:44:37.433977: step 8862, loss 0.00023542, acc 1
2017-09-08T18:44:37.914809: step 8863, loss 0.052444, acc 0.96875
2017-09-08T18:44:38.412114: step 8864, loss 0.000185434, acc 1
2017-09-08T18:44:38.876197: step 8865, loss 0.000610899, acc 1
2017-09-08T18:44:39.374782: step 8866, loss 0.0192135, acc 1
2017-09-08T18:44:39.858122: step 8867, loss 0.000196403, acc 1
2017-09-08T18:44:40.351863: step 8868, loss 0.000219073, acc 1
2017-09-08T18:44:40.829776: step 8869, loss 0.104435, acc 0.953125
2017-09-08T18:44:41.321241: step 8870, loss 0.0562826, acc 0.96875
2017-09-08T18:44:41.794813: step 8871, loss 0.000196498, acc 1
2017-09-08T18:44:42.267571: step 8872, loss 0.0236088, acc 0.984375
2017-09-08T18:44:42.746427: step 8873, loss 0.000134942, acc 1
2017-09-08T18:44:43.220806: step 8874, loss 0.0439213, acc 0.984375
2017-09-08T18:44:43.697564: step 8875, loss 0.0404801, acc 0.984375
2017-09-08T18:44:44.181047: step 8876, loss 0.0543209, acc 0.984375
2017-09-08T18:44:44.657425: step 8877, loss 0.000249719, acc 1
2017-09-08T18:44:45.157705: step 8878, loss 0.0250971, acc 0.984375
2017-09-08T18:44:45.619334: step 8879, loss 0.000703274, acc 1
2017-09-08T18:44:46.089863: step 8880, loss 0.000332056, acc 1
2017-09-08T18:44:46.590745: step 8881, loss 0.0298202, acc 0.984375
2017-09-08T18:44:47.068703: step 8882, loss 0.088336, acc 0.953125
2017-09-08T18:44:47.550968: step 8883, loss 9.90833e-05, acc 1
2017-09-08T18:44:48.031200: step 8884, loss 7.61096e-05, acc 1
2017-09-08T18:44:48.520601: step 8885, loss 0.0167983, acc 0.984375
2017-09-08T18:44:48.997238: step 8886, loss 0.00662996, acc 1
2017-09-08T18:44:49.485422: step 8887, loss 0.000172369, acc 1
2017-09-08T18:44:49.957274: step 8888, loss 0.00317237, acc 1
2017-09-08T18:44:50.449287: step 8889, loss 0.0114779, acc 1
2017-09-08T18:44:50.928714: step 8890, loss 0.0208598, acc 0.984375
2017-09-08T18:44:51.393589: step 8891, loss 0.000114867, acc 1
2017-09-08T18:44:51.869477: step 8892, loss 0.0380971, acc 0.984375
2017-09-08T18:44:52.351450: step 8893, loss 0.00799066, acc 1
2017-09-08T18:44:52.857245: step 8894, loss 0.0393131, acc 0.96875
2017-09-08T18:44:53.325394: step 8895, loss 0.00136561, acc 1
2017-09-08T18:44:53.820910: step 8896, loss 0.0138955, acc 0.984375
2017-09-08T18:44:54.315091: step 8897, loss 0.0283152, acc 0.984375
2017-09-08T18:44:54.823257: step 8898, loss 0.000280953, acc 1
2017-09-08T18:44:55.283492: step 8899, loss 6.10916e-05, acc 1
2017-09-08T18:44:55.752731: step 8900, loss 0.00013574, acc 1

Evaluation:
2017-09-08T18:44:56.311093: step 8900, loss 0.255662, acc 0.928058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8900

2017-09-08T18:44:59.232088: step 8901, loss 6.07273e-05, acc 1
2017-09-08T18:44:59.699373: step 8902, loss 0.000711375, acc 1
2017-09-08T18:45:00.151988: step 8903, loss 0.00816946, acc 1
2017-09-08T18:45:00.680273: step 8904, loss 0.000406591, acc 1
2017-09-08T18:45:01.071991: step 8905, loss 0.0237936, acc 0.984375
2017-09-08T18:45:01.467354: step 8906, loss 0.0667503, acc 0.96875
2017-09-08T18:45:01.895487: step 8907, loss 8.07969e-05, acc 1
2017-09-08T18:45:02.368022: step 8908, loss 0.0125306, acc 1
2017-09-08T18:45:02.846624: step 8909, loss 0.000173433, acc 1
2017-09-08T18:45:03.289903: step 8910, loss 0.000824433, acc 1
2017-09-08T18:45:03.738472: step 8911, loss 0.0311519, acc 0.984375
2017-09-08T18:45:04.299357: step 8912, loss 0.000203362, acc 1
2017-09-08T18:45:04.726287: step 8913, loss 0.0402216, acc 0.984375
2017-09-08T18:45:05.177937: step 8914, loss 0.0127794, acc 1
2017-09-08T18:45:05.646757: step 8915, loss 0.000792858, acc 1
2017-09-08T18:45:06.121190: step 8916, loss 0.0192859, acc 0.984375
2017-09-08T18:45:06.613175: step 8917, loss 0.000114753, acc 1
2017-09-08T18:45:07.063853: step 8918, loss 0.0574217, acc 0.960784
2017-09-08T18:45:07.544526: step 8919, loss 0.039725, acc 0.984375
2017-09-08T18:45:08.054579: step 8920, loss 0.0186704, acc 1
2017-09-08T18:45:08.542472: step 8921, loss 0.000252368, acc 1
2017-09-08T18:45:09.017710: step 8922, loss 0.000427456, acc 1
2017-09-08T18:45:09.486494: step 8923, loss 0.0327549, acc 0.984375
2017-09-08T18:45:09.958685: step 8924, loss 0.000286771, acc 1
2017-09-08T18:45:10.453738: step 8925, loss 0.0331124, acc 0.96875
2017-09-08T18:45:10.962837: step 8926, loss 5.28516e-05, acc 1
2017-09-08T18:45:11.437211: step 8927, loss 0.000796505, acc 1
2017-09-08T18:45:11.917493: step 8928, loss 0.000177023, acc 1
2017-09-08T18:45:12.392283: step 8929, loss 0.00213555, acc 1
2017-09-08T18:45:12.883837: step 8930, loss 0.0321924, acc 0.984375
2017-09-08T18:45:13.388222: step 8931, loss 0.0159571, acc 0.984375
2017-09-08T18:45:13.864855: step 8932, loss 0.0095158, acc 1
2017-09-08T18:45:14.367552: step 8933, loss 0.0100698, acc 1
2017-09-08T18:45:14.827166: step 8934, loss 0.000113615, acc 1
2017-09-08T18:45:15.308650: step 8935, loss 0.0191326, acc 1
2017-09-08T18:45:15.795140: step 8936, loss 0.017741, acc 0.984375
2017-09-08T18:45:16.306056: step 8937, loss 0.00294737, acc 1
2017-09-08T18:45:16.790742: step 8938, loss 0.000152488, acc 1
2017-09-08T18:45:17.266672: step 8939, loss 0.00500827, acc 1
2017-09-08T18:45:17.766992: step 8940, loss 0.042929, acc 0.96875
2017-09-08T18:45:18.250120: step 8941, loss 0.017294, acc 0.984375
2017-09-08T18:45:18.760550: step 8942, loss 0.0604749, acc 0.984375
2017-09-08T18:45:19.259781: step 8943, loss 0.00486461, acc 1
2017-09-08T18:45:19.711124: step 8944, loss 0.0634416, acc 0.984375
2017-09-08T18:45:20.209333: step 8945, loss 0.0627404, acc 0.953125
2017-09-08T18:45:20.732899: step 8946, loss 0.00837565, acc 1
2017-09-08T18:45:21.229342: step 8947, loss 0.0248641, acc 0.984375
2017-09-08T18:45:21.725054: step 8948, loss 0.00864644, acc 1
2017-09-08T18:45:22.226559: step 8949, loss 0.0221318, acc 0.984375
2017-09-08T18:45:22.735761: step 8950, loss 0.000354099, acc 1

Evaluation:
2017-09-08T18:45:23.251864: step 8950, loss 0.247349, acc 0.92518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-8950

2017-09-08T18:45:25.274818: step 8951, loss 0.00223314, acc 1
2017-09-08T18:45:25.787346: step 8952, loss 0.000289348, acc 1
2017-09-08T18:45:26.338464: step 8953, loss 0.000121616, acc 1
2017-09-08T18:45:26.733566: step 8954, loss 0.000430099, acc 1
2017-09-08T18:45:27.115093: step 8955, loss 0.000173252, acc 1
2017-09-08T18:45:27.489035: step 8956, loss 0.0548443, acc 0.984375
2017-09-08T18:45:27.880865: step 8957, loss 0.0720812, acc 0.984375
2017-09-08T18:45:28.277320: step 8958, loss 0.000147907, acc 1
2017-09-08T18:45:28.686604: step 8959, loss 0.00693685, acc 1
2017-09-08T18:45:29.132789: step 8960, loss 0.00100252, acc 1
2017-09-08T18:45:29.588450: step 8961, loss 0.000669803, acc 1
2017-09-08T18:45:30.000316: step 8962, loss 0.00361979, acc 1
2017-09-08T18:45:30.445722: step 8963, loss 0.000609389, acc 1
2017-09-08T18:45:30.890697: step 8964, loss 8.37115e-05, acc 1
2017-09-08T18:45:31.364670: step 8965, loss 0.0136263, acc 0.984375
2017-09-08T18:45:31.869053: step 8966, loss 0.003375, acc 1
2017-09-08T18:45:32.386289: step 8967, loss 0.00065018, acc 1
2017-09-08T18:45:32.882174: step 8968, loss 0.000104962, acc 1
2017-09-08T18:45:33.335820: step 8969, loss 0.000182969, acc 1
2017-09-08T18:45:33.825060: step 8970, loss 0.0161026, acc 0.984375
2017-09-08T18:45:34.326901: step 8971, loss 0.000113066, acc 1
2017-09-08T18:45:34.828349: step 8972, loss 0.000450476, acc 1
2017-09-08T18:45:35.325252: step 8973, loss 0.00229295, acc 1
2017-09-08T18:45:35.800610: step 8974, loss 0.000208375, acc 1
2017-09-08T18:45:36.282666: step 8975, loss 7.92798e-05, acc 1
2017-09-08T18:45:36.768016: step 8976, loss 0.000556729, acc 1
2017-09-08T18:45:37.250648: step 8977, loss 0.000137974, acc 1
2017-09-08T18:45:37.712485: step 8978, loss 0.104977, acc 0.953125
2017-09-08T18:45:38.184595: step 8979, loss 0.0157605, acc 1
2017-09-08T18:45:38.682187: step 8980, loss 8.15561e-05, acc 1
2017-09-08T18:45:39.176307: step 8981, loss 0.0320655, acc 0.984375
2017-09-08T18:45:39.671097: step 8982, loss 0.00283269, acc 1
2017-09-08T18:45:40.163373: step 8983, loss 0.000451196, acc 1
2017-09-08T18:45:40.654599: step 8984, loss 0.000527148, acc 1
2017-09-08T18:45:41.130391: step 8985, loss 0.0645068, acc 0.984375
2017-09-08T18:45:41.603390: step 8986, loss 0.102157, acc 0.96875
2017-09-08T18:45:42.103580: step 8987, loss 0.000646978, acc 1
2017-09-08T18:45:42.589378: step 8988, loss 0.000452416, acc 1
2017-09-08T18:45:43.075206: step 8989, loss 0.000161859, acc 1
2017-09-08T18:45:43.575520: step 8990, loss 0.00991203, acc 1
2017-09-08T18:45:44.043947: step 8991, loss 0.0125929, acc 0.984375
2017-09-08T18:45:44.523244: step 8992, loss 0.018608, acc 0.984375
2017-09-08T18:45:44.996355: step 8993, loss 0.000952134, acc 1
2017-09-08T18:45:45.481620: step 8994, loss 0.0262783, acc 0.984375
2017-09-08T18:45:45.950892: step 8995, loss 0.000259963, acc 1
2017-09-08T18:45:46.412587: step 8996, loss 0.00287126, acc 1
2017-09-08T18:45:46.895324: step 8997, loss 0.0233764, acc 0.984375
2017-09-08T18:45:47.406322: step 8998, loss 0.0228286, acc 0.984375
2017-09-08T18:45:47.900253: step 8999, loss 7.05898e-05, acc 1
2017-09-08T18:45:48.357788: step 9000, loss 0.000628292, acc 1

Evaluation:
2017-09-08T18:45:48.845716: step 9000, loss 0.288048, acc 0.933813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9000

2017-09-08T18:45:51.277194: step 9001, loss 0.0206773, acc 0.984375
2017-09-08T18:45:51.776502: step 9002, loss 0.000327122, acc 1
2017-09-08T18:45:52.265882: step 9003, loss 0.0192646, acc 0.984375
2017-09-08T18:45:52.677314: step 9004, loss 0.00177327, acc 1
2017-09-08T18:45:53.187093: step 9005, loss 0.0131431, acc 0.984375
2017-09-08T18:45:53.557053: step 9006, loss 0.000436022, acc 1
2017-09-08T18:45:53.942947: step 9007, loss 0.0567304, acc 0.96875
2017-09-08T18:45:54.346947: step 9008, loss 0.000388801, acc 1
2017-09-08T18:45:54.823271: step 9009, loss 0.000256411, acc 1
2017-09-08T18:45:55.287344: step 9010, loss 0.000287457, acc 1
2017-09-08T18:45:55.770411: step 9011, loss 0.0459133, acc 0.984375
2017-09-08T18:45:56.273314: step 9012, loss 9.04338e-05, acc 1
2017-09-08T18:45:56.728018: step 9013, loss 0.000128544, acc 1
2017-09-08T18:45:57.194023: step 9014, loss 0.0977839, acc 0.984375
2017-09-08T18:45:57.677602: step 9015, loss 0.00761489, acc 1
2017-09-08T18:45:58.147545: step 9016, loss 0.0583317, acc 0.980392
2017-09-08T18:45:58.671402: step 9017, loss 0.0304474, acc 0.984375
2017-09-08T18:45:59.178059: step 9018, loss 0.0275525, acc 0.984375
2017-09-08T18:45:59.678144: step 9019, loss 0.0419398, acc 0.984375
2017-09-08T18:46:00.138492: step 9020, loss 0.021083, acc 0.984375
2017-09-08T18:46:00.630268: step 9021, loss 0.0119596, acc 1
2017-09-08T18:46:01.161464: step 9022, loss 0.000166435, acc 1
2017-09-08T18:46:01.644931: step 9023, loss 6.4077e-05, acc 1
2017-09-08T18:46:02.131960: step 9024, loss 0.00592993, acc 1
2017-09-08T18:46:02.620094: step 9025, loss 0.00134185, acc 1
2017-09-08T18:46:03.134324: step 9026, loss 0.0122563, acc 1
2017-09-08T18:46:03.612569: step 9027, loss 4.6392e-05, acc 1
2017-09-08T18:46:04.108616: step 9028, loss 0.00888055, acc 1
2017-09-08T18:46:04.573558: step 9029, loss 0.000534534, acc 1
2017-09-08T18:46:05.066242: step 9030, loss 0.00840088, acc 1
2017-09-08T18:46:05.552633: step 9031, loss 0.0149724, acc 1
2017-09-08T18:46:06.007013: step 9032, loss 0.00848317, acc 1
2017-09-08T18:46:06.473308: step 9033, loss 0.0491668, acc 0.984375
2017-09-08T18:46:06.960602: step 9034, loss 0.000152828, acc 1
2017-09-08T18:46:07.466909: step 9035, loss 0.017649, acc 0.984375
2017-09-08T18:46:07.940742: step 9036, loss 0.0453462, acc 0.984375
2017-09-08T18:46:08.433670: step 9037, loss 0.0305802, acc 0.984375
2017-09-08T18:46:08.951260: step 9038, loss 0.023634, acc 0.984375
2017-09-08T18:46:09.427490: step 9039, loss 0.00564323, acc 1
2017-09-08T18:46:09.917264: step 9040, loss 0.000171786, acc 1
2017-09-08T18:46:10.407529: step 9041, loss 0.000809738, acc 1
2017-09-08T18:46:10.897690: step 9042, loss 0.00181684, acc 1
2017-09-08T18:46:11.374729: step 9043, loss 0.0839717, acc 0.96875
2017-09-08T18:46:11.868114: step 9044, loss 8.93524e-05, acc 1
2017-09-08T18:46:12.324267: step 9045, loss 0.000817733, acc 1
2017-09-08T18:46:12.813555: step 9046, loss 0.000859076, acc 1
2017-09-08T18:46:13.349370: step 9047, loss 0.0229226, acc 0.984375
2017-09-08T18:46:13.815571: step 9048, loss 0.000755248, acc 1
2017-09-08T18:46:14.292830: step 9049, loss 0.121199, acc 0.953125
2017-09-08T18:46:14.811388: step 9050, loss 0.0209657, acc 0.984375

Evaluation:
2017-09-08T18:46:15.311000: step 9050, loss 0.256075, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9050

2017-09-08T18:46:18.085725: step 9051, loss 0.000883256, acc 1
2017-09-08T18:46:18.621805: step 9052, loss 0.0169955, acc 0.984375
2017-09-08T18:46:19.003233: step 9053, loss 8.86211e-05, acc 1
2017-09-08T18:46:19.388079: step 9054, loss 0.0159998, acc 0.984375
2017-09-08T18:46:19.779831: step 9055, loss 0.00233398, acc 1
2017-09-08T18:46:20.154398: step 9056, loss 0.0391607, acc 0.984375
2017-09-08T18:46:20.582189: step 9057, loss 0.129898, acc 0.96875
2017-09-08T18:46:21.023986: step 9058, loss 7.47688e-05, acc 1
2017-09-08T18:46:21.471827: step 9059, loss 0.0125743, acc 1
2017-09-08T18:46:21.940022: step 9060, loss 5.32009e-05, acc 1
2017-09-08T18:46:22.387192: step 9061, loss 0.0527064, acc 0.984375
2017-09-08T18:46:22.856049: step 9062, loss 0.0522565, acc 0.984375
2017-09-08T18:46:23.340214: step 9063, loss 0.000222912, acc 1
2017-09-08T18:46:23.792897: step 9064, loss 0.000108489, acc 1
2017-09-08T18:46:24.214113: step 9065, loss 9.54091e-05, acc 1
2017-09-08T18:46:24.646483: step 9066, loss 0.0471035, acc 0.953125
2017-09-08T18:46:25.098985: step 9067, loss 0.000292469, acc 1
2017-09-08T18:46:25.548132: step 9068, loss 0.00132068, acc 1
2017-09-08T18:46:26.000529: step 9069, loss 0.0211913, acc 0.984375
2017-09-08T18:46:26.453161: step 9070, loss 0.0026203, acc 1
2017-09-08T18:46:26.928898: step 9071, loss 0.000289026, acc 1
2017-09-08T18:46:27.408673: step 9072, loss 0.0122425, acc 0.984375
2017-09-08T18:46:27.915031: step 9073, loss 0.0319914, acc 0.984375
2017-09-08T18:46:28.417718: step 9074, loss 0.00369431, acc 1
2017-09-08T18:46:28.895312: step 9075, loss 0.000827758, acc 1
2017-09-08T18:46:29.344773: step 9076, loss 0.00131508, acc 1
2017-09-08T18:46:29.840126: step 9077, loss 0.00035679, acc 1
2017-09-08T18:46:30.339586: step 9078, loss 0.000930259, acc 1
2017-09-08T18:46:30.818105: step 9079, loss 0.0453308, acc 0.984375
2017-09-08T18:46:31.323712: step 9080, loss 0.0392717, acc 0.984375
2017-09-08T18:46:31.838637: step 9081, loss 0.00846619, acc 1
2017-09-08T18:46:32.336138: step 9082, loss 0.000413859, acc 1
2017-09-08T18:46:32.810630: step 9083, loss 0.00982005, acc 1
2017-09-08T18:46:33.294836: step 9084, loss 0.000970014, acc 1
2017-09-08T18:46:33.754059: step 9085, loss 0.0390824, acc 0.96875
2017-09-08T18:46:34.255430: step 9086, loss 0.00633071, acc 1
2017-09-08T18:46:34.732012: step 9087, loss 0.00129913, acc 1
2017-09-08T18:46:35.204582: step 9088, loss 0.000346204, acc 1
2017-09-08T18:46:35.670249: step 9089, loss 0.000820081, acc 1
2017-09-08T18:46:36.138127: step 9090, loss 0.000552059, acc 1
2017-09-08T18:46:36.601517: step 9091, loss 0.00271261, acc 1
2017-09-08T18:46:37.087733: step 9092, loss 0.000597714, acc 1
2017-09-08T18:46:37.577080: step 9093, loss 0.00187101, acc 1
2017-09-08T18:46:38.074968: step 9094, loss 0.029086, acc 0.984375
2017-09-08T18:46:38.578834: step 9095, loss 0.0237485, acc 0.984375
2017-09-08T18:46:39.074660: step 9096, loss 0.00182208, acc 1
2017-09-08T18:46:39.565180: step 9097, loss 0.0370067, acc 0.984375
2017-09-08T18:46:40.058194: step 9098, loss 0.0590557, acc 0.984375
2017-09-08T18:46:40.551120: step 9099, loss 0.00960254, acc 1
2017-09-08T18:46:41.043129: step 9100, loss 0.0226912, acc 0.984375

Evaluation:
2017-09-08T18:46:41.530411: step 9100, loss 0.261685, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9100

2017-09-08T18:46:43.333290: step 9101, loss 0.000190457, acc 1
2017-09-08T18:46:43.785694: step 9102, loss 0.000133847, acc 1
2017-09-08T18:46:44.342088: step 9103, loss 0.0001891, acc 1
2017-09-08T18:46:44.725809: step 9104, loss 0.0124849, acc 1
2017-09-08T18:46:45.110873: step 9105, loss 0.0469055, acc 0.984375
2017-09-08T18:46:45.504045: step 9106, loss 0.00427544, acc 1
2017-09-08T18:46:45.881971: step 9107, loss 0.0022842, acc 1
2017-09-08T18:46:46.258157: step 9108, loss 5.99702e-05, acc 1
2017-09-08T18:46:46.621564: step 9109, loss 0.000721968, acc 1
2017-09-08T18:46:47.080730: step 9110, loss 0.0108479, acc 1
2017-09-08T18:46:47.575613: step 9111, loss 0.0148939, acc 0.984375
2017-09-08T18:46:48.058176: step 9112, loss 9.64231e-05, acc 1
2017-09-08T18:46:48.514951: step 9113, loss 0.00258854, acc 1
2017-09-08T18:46:48.943148: step 9114, loss 0.0257202, acc 0.980392
2017-09-08T18:46:49.456555: step 9115, loss 0.00129403, acc 1
2017-09-08T18:46:49.927863: step 9116, loss 5.69811e-05, acc 1
2017-09-08T18:46:50.405584: step 9117, loss 0.00574153, acc 1
2017-09-08T18:46:50.902088: step 9118, loss 0.0110919, acc 1
2017-09-08T18:46:51.377821: step 9119, loss 0.00037329, acc 1
2017-09-08T18:46:51.881489: step 9120, loss 5.77523e-05, acc 1
2017-09-08T18:46:52.343709: step 9121, loss 4.99577e-05, acc 1
2017-09-08T18:46:52.817547: step 9122, loss 0.00162542, acc 1
2017-09-08T18:46:53.316918: step 9123, loss 0.0157398, acc 0.984375
2017-09-08T18:46:53.809871: step 9124, loss 0.000353814, acc 1
2017-09-08T18:46:54.288769: step 9125, loss 0.00367799, acc 1
2017-09-08T18:46:54.764733: step 9126, loss 0.000318039, acc 1
2017-09-08T18:46:55.254092: step 9127, loss 0.000569725, acc 1
2017-09-08T18:46:55.710397: step 9128, loss 9.05918e-05, acc 1
2017-09-08T18:46:56.196880: step 9129, loss 0.0119687, acc 1
2017-09-08T18:46:56.695407: step 9130, loss 0.0257457, acc 0.984375
2017-09-08T18:46:57.211273: step 9131, loss 0.000377128, acc 1
2017-09-08T18:46:57.675659: step 9132, loss 0.00434326, acc 1
2017-09-08T18:46:58.143642: step 9133, loss 0.0359456, acc 0.984375
2017-09-08T18:46:58.631844: step 9134, loss 0.000125095, acc 1
2017-09-08T18:46:59.085097: step 9135, loss 0.00448093, acc 1
2017-09-08T18:46:59.584073: step 9136, loss 0.0482669, acc 0.96875
2017-09-08T18:47:00.090417: step 9137, loss 0.00732885, acc 1
2017-09-08T18:47:00.572672: step 9138, loss 0.000432202, acc 1
2017-09-08T18:47:01.028652: step 9139, loss 0.00685017, acc 1
2017-09-08T18:47:01.497675: step 9140, loss 0.162308, acc 0.953125
2017-09-08T18:47:02.001811: step 9141, loss 0.00011333, acc 1
2017-09-08T18:47:02.473660: step 9142, loss 0.000392108, acc 1
2017-09-08T18:47:02.974020: step 9143, loss 7.68697e-05, acc 1
2017-09-08T18:47:03.459258: step 9144, loss 0.000400705, acc 1
2017-09-08T18:47:03.950488: step 9145, loss 0.0178153, acc 0.984375
2017-09-08T18:47:04.453948: step 9146, loss 3.51967e-05, acc 1
2017-09-08T18:47:04.915464: step 9147, loss 0.0200737, acc 0.984375
2017-09-08T18:47:05.418460: step 9148, loss 0.000764978, acc 1
2017-09-08T18:47:05.889312: step 9149, loss 0.0209688, acc 0.984375
2017-09-08T18:47:06.377743: step 9150, loss 6.2012e-05, acc 1

Evaluation:
2017-09-08T18:47:06.922278: step 9150, loss 0.281695, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9150

2017-09-08T18:47:09.870424: step 9151, loss 0.000173634, acc 1
2017-09-08T18:47:10.357292: step 9152, loss 0.0186573, acc 0.984375
2017-09-08T18:47:10.829222: step 9153, loss 0.000523078, acc 1
2017-09-08T18:47:11.238035: step 9154, loss 0.00526105, acc 1
2017-09-08T18:47:11.634939: step 9155, loss 0.0395396, acc 0.984375
2017-09-08T18:47:12.019402: step 9156, loss 0.00477592, acc 1
2017-09-08T18:47:12.536607: step 9157, loss 0.00157572, acc 1
2017-09-08T18:47:13.033854: step 9158, loss 0.0305508, acc 0.984375
2017-09-08T18:47:13.510902: step 9159, loss 0.000392184, acc 1
2017-09-08T18:47:13.985696: step 9160, loss 0.0207364, acc 0.984375
2017-09-08T18:47:14.507410: step 9161, loss 0.00020578, acc 1
2017-09-08T18:47:14.972871: step 9162, loss 0.000471694, acc 1
2017-09-08T18:47:15.481853: step 9163, loss 0.0686838, acc 0.984375
2017-09-08T18:47:15.987852: step 9164, loss 0.00191045, acc 1
2017-09-08T18:47:16.462791: step 9165, loss 6.38522e-05, acc 1
2017-09-08T18:47:16.939655: step 9166, loss 0.0329, acc 0.984375
2017-09-08T18:47:17.433699: step 9167, loss 0.00141525, acc 1
2017-09-08T18:47:17.944110: step 9168, loss 0.00104971, acc 1
2017-09-08T18:47:18.419988: step 9169, loss 0.00530673, acc 1
2017-09-08T18:47:18.896335: step 9170, loss 0.0016497, acc 1
2017-09-08T18:47:19.434155: step 9171, loss 0.000421407, acc 1
2017-09-08T18:47:19.917392: step 9172, loss 0.0769026, acc 0.953125
2017-09-08T18:47:20.413947: step 9173, loss 0.0300244, acc 0.984375
2017-09-08T18:47:20.906086: step 9174, loss 0.0292133, acc 0.984375
2017-09-08T18:47:21.398395: step 9175, loss 0.000198082, acc 1
2017-09-08T18:47:21.929340: step 9176, loss 0.000508728, acc 1
2017-09-08T18:47:22.414960: step 9177, loss 0.016909, acc 1
2017-09-08T18:47:22.896446: step 9178, loss 0.00809252, acc 1
2017-09-08T18:47:23.383047: step 9179, loss 0.00599396, acc 1
2017-09-08T18:47:23.868162: step 9180, loss 0.0700935, acc 0.953125
2017-09-08T18:47:24.359501: step 9181, loss 0.00292589, acc 1
2017-09-08T18:47:24.865746: step 9182, loss 0.000665168, acc 1
2017-09-08T18:47:25.349900: step 9183, loss 0.023208, acc 0.984375
2017-09-08T18:47:25.813934: step 9184, loss 0.000132131, acc 1
2017-09-08T18:47:26.308160: step 9185, loss 0.0375776, acc 0.984375
2017-09-08T18:47:26.821273: step 9186, loss 0.000616346, acc 1
2017-09-08T18:47:27.299290: step 9187, loss 0.00175933, acc 1
2017-09-08T18:47:27.793671: step 9188, loss 0.0984678, acc 0.953125
2017-09-08T18:47:28.267188: step 9189, loss 0.00221884, acc 1
2017-09-08T18:47:28.741921: step 9190, loss 0.075846, acc 0.96875
2017-09-08T18:47:29.239217: step 9191, loss 0.129909, acc 0.96875
2017-09-08T18:47:29.734184: step 9192, loss 0.000898785, acc 1
2017-09-08T18:47:30.197578: step 9193, loss 0.0326905, acc 0.984375
2017-09-08T18:47:30.643820: step 9194, loss 0.000503384, acc 1
2017-09-08T18:47:31.112918: step 9195, loss 0.0143485, acc 1
2017-09-08T18:47:31.625396: step 9196, loss 0.000389034, acc 1
2017-09-08T18:47:32.082345: step 9197, loss 0.000127757, acc 1
2017-09-08T18:47:32.584402: step 9198, loss 8.75788e-05, acc 1
2017-09-08T18:47:33.079248: step 9199, loss 0.0295033, acc 0.984375
2017-09-08T18:47:33.604027: step 9200, loss 0.00106698, acc 1

Evaluation:
2017-09-08T18:47:34.129411: step 9200, loss 0.290052, acc 0.92518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9200

2017-09-08T18:47:35.867533: step 9201, loss 0.00803197, acc 1
2017-09-08T18:47:36.328062: step 9202, loss 0.000215223, acc 1
2017-09-08T18:47:36.880759: step 9203, loss 0.000857942, acc 1
2017-09-08T18:47:37.271238: step 9204, loss 0.0235239, acc 0.984375
2017-09-08T18:47:37.683401: step 9205, loss 0.0965538, acc 0.984375
2017-09-08T18:47:38.072854: step 9206, loss 0.017269, acc 0.984375
2017-09-08T18:47:38.474642: step 9207, loss 0.00121737, acc 1
2017-09-08T18:47:38.886044: step 9208, loss 0.000787246, acc 1
2017-09-08T18:47:39.285703: step 9209, loss 0.0556879, acc 0.96875
2017-09-08T18:47:39.745786: step 9210, loss 0.00919401, acc 1
2017-09-08T18:47:40.249644: step 9211, loss 0.0479534, acc 0.984375
2017-09-08T18:47:40.683959: step 9212, loss 3.61938e-05, acc 1
2017-09-08T18:47:41.172747: step 9213, loss 0.0566153, acc 0.953125
2017-09-08T18:47:41.673441: step 9214, loss 0.00558699, acc 1
2017-09-08T18:47:42.198568: step 9215, loss 0.0387861, acc 0.96875
2017-09-08T18:47:42.703441: step 9216, loss 0.00254045, acc 1
2017-09-08T18:47:43.184573: step 9217, loss 0.0416228, acc 0.984375
2017-09-08T18:47:43.653627: step 9218, loss 0.000809593, acc 1
2017-09-08T18:47:44.131820: step 9219, loss 0.0134473, acc 1
2017-09-08T18:47:44.607304: step 9220, loss 0.00481361, acc 1
2017-09-08T18:47:45.103372: step 9221, loss 0.000143482, acc 1
2017-09-08T18:47:45.602824: step 9222, loss 0.0480573, acc 0.984375
2017-09-08T18:47:46.106863: step 9223, loss 0.00259788, acc 1
2017-09-08T18:47:46.578559: step 9224, loss 7.13039e-05, acc 1
2017-09-08T18:47:47.070251: step 9225, loss 0.000285337, acc 1
2017-09-08T18:47:47.549822: step 9226, loss 0.00342236, acc 1
2017-09-08T18:47:48.043171: step 9227, loss 0.000139398, acc 1
2017-09-08T18:47:48.532155: step 9228, loss 0.0163334, acc 1
2017-09-08T18:47:49.045166: step 9229, loss 3.18034e-05, acc 1
2017-09-08T18:47:49.522143: step 9230, loss 0.0397095, acc 0.984375
2017-09-08T18:47:49.979792: step 9231, loss 0.0785767, acc 0.984375
2017-09-08T18:47:50.507210: step 9232, loss 0.000195156, acc 1
2017-09-08T18:47:51.010201: step 9233, loss 0.00105191, acc 1
2017-09-08T18:47:51.473321: step 9234, loss 0.0131743, acc 1
2017-09-08T18:47:51.950251: step 9235, loss 0.000185578, acc 1
2017-09-08T18:47:52.439721: step 9236, loss 0.000183649, acc 1
2017-09-08T18:47:52.926779: step 9237, loss 0.000119073, acc 1
2017-09-08T18:47:53.382634: step 9238, loss 0.0135886, acc 1
2017-09-08T18:47:53.870003: step 9239, loss 9.18706e-05, acc 1
2017-09-08T18:47:54.346852: step 9240, loss 0.000163, acc 1
2017-09-08T18:47:54.841759: step 9241, loss 5.65445e-05, acc 1
2017-09-08T18:47:55.301752: step 9242, loss 0.0115744, acc 0.984375
2017-09-08T18:47:55.808103: step 9243, loss 0.00199153, acc 1
2017-09-08T18:47:56.307720: step 9244, loss 0.042773, acc 0.984375
2017-09-08T18:47:56.827849: step 9245, loss 0.000120784, acc 1
2017-09-08T18:47:57.286331: step 9246, loss 0.0177886, acc 0.984375
2017-09-08T18:47:57.758434: step 9247, loss 0.0229763, acc 0.984375
2017-09-08T18:47:58.247828: step 9248, loss 7.52925e-05, acc 1
2017-09-08T18:47:58.728674: step 9249, loss 0.000771781, acc 1
2017-09-08T18:47:59.208639: step 9250, loss 9.42569e-05, acc 1

Evaluation:
2017-09-08T18:47:59.692877: step 9250, loss 0.273717, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9250

2017-09-08T18:48:01.830663: step 9251, loss 0.00815232, acc 1
2017-09-08T18:48:02.304774: step 9252, loss 0.0301329, acc 0.984375
2017-09-08T18:48:02.777208: step 9253, loss 0.039724, acc 0.984375
2017-09-08T18:48:03.240374: step 9254, loss 0.00213002, acc 1
2017-09-08T18:48:03.786982: step 9255, loss 0.00194961, acc 1
2017-09-08T18:48:04.172866: step 9256, loss 0.000156503, acc 1
2017-09-08T18:48:04.547552: step 9257, loss 0.00510967, acc 1
2017-09-08T18:48:04.948538: step 9258, loss 0.000983996, acc 1
2017-09-08T18:48:05.418413: step 9259, loss 0.000672375, acc 1
2017-09-08T18:48:05.873167: step 9260, loss 0.00457269, acc 1
2017-09-08T18:48:06.352434: step 9261, loss 4.45731e-05, acc 1
2017-09-08T18:48:06.811906: step 9262, loss 0.0029905, acc 1
2017-09-08T18:48:07.302397: step 9263, loss 0.0483625, acc 0.984375
2017-09-08T18:48:07.767269: step 9264, loss 0.000268825, acc 1
2017-09-08T18:48:08.245228: step 9265, loss 0.0313632, acc 0.984375
2017-09-08T18:48:08.712924: step 9266, loss 0.0485885, acc 0.984375
2017-09-08T18:48:09.219307: step 9267, loss 0.0252288, acc 0.984375
2017-09-08T18:48:09.618448: step 9268, loss 0.000429831, acc 1
2017-09-08T18:48:10.087227: step 9269, loss 0.00223126, acc 1
2017-09-08T18:48:10.585026: step 9270, loss 0.0418935, acc 0.96875
2017-09-08T18:48:11.082108: step 9271, loss 0.0596098, acc 0.984375
2017-09-08T18:48:11.599696: step 9272, loss 0.000863474, acc 1
2017-09-08T18:48:12.076768: step 9273, loss 6.24417e-05, acc 1
2017-09-08T18:48:12.564436: step 9274, loss 0.0426928, acc 0.984375
2017-09-08T18:48:13.044939: step 9275, loss 0.000320461, acc 1
2017-09-08T18:48:13.516572: step 9276, loss 0.0422569, acc 0.96875
2017-09-08T18:48:14.018066: step 9277, loss 0.0236398, acc 0.984375
2017-09-08T18:48:14.503246: step 9278, loss 0.0944253, acc 0.953125
2017-09-08T18:48:14.999568: step 9279, loss 0.000806966, acc 1
2017-09-08T18:48:15.481717: step 9280, loss 0.0199091, acc 0.984375
2017-09-08T18:48:15.989903: step 9281, loss 0.00987318, acc 1
2017-09-08T18:48:16.480009: step 9282, loss 0.0304486, acc 0.984375
2017-09-08T18:48:16.972452: step 9283, loss 0.000458108, acc 1
2017-09-08T18:48:17.468049: step 9284, loss 0.000211405, acc 1
2017-09-08T18:48:17.952551: step 9285, loss 0.0792852, acc 0.96875
2017-09-08T18:48:18.447295: step 9286, loss 0.000633308, acc 1
2017-09-08T18:48:18.955691: step 9287, loss 0.000647799, acc 1
2017-09-08T18:48:19.419293: step 9288, loss 0.0176639, acc 0.984375
2017-09-08T18:48:19.927907: step 9289, loss 0.0176902, acc 0.984375
2017-09-08T18:48:20.411669: step 9290, loss 8.65025e-05, acc 1
2017-09-08T18:48:20.886372: step 9291, loss 0.00705587, acc 1
2017-09-08T18:48:21.361380: step 9292, loss 0.0537786, acc 0.984375
2017-09-08T18:48:21.867526: step 9293, loss 0.0206727, acc 0.984375
2017-09-08T18:48:22.361028: step 9294, loss 0.0113458, acc 1
2017-09-08T18:48:22.849474: step 9295, loss 0.0435883, acc 0.96875
2017-09-08T18:48:23.331894: step 9296, loss 0.000297728, acc 1
2017-09-08T18:48:23.806654: step 9297, loss 0.0207567, acc 1
2017-09-08T18:48:24.256896: step 9298, loss 0.000346919, acc 1
2017-09-08T18:48:24.757191: step 9299, loss 8.38844e-05, acc 1
2017-09-08T18:48:25.238054: step 9300, loss 0.00013426, acc 1

Evaluation:
2017-09-08T18:48:25.730240: step 9300, loss 0.281239, acc 0.92518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9300

2017-09-08T18:48:28.392594: step 9301, loss 0.0294274, acc 0.984375
2017-09-08T18:48:28.829761: step 9302, loss 0.000117373, acc 1
2017-09-08T18:48:29.266793: step 9303, loss 0.0742555, acc 0.96875
2017-09-08T18:48:29.679464: step 9304, loss 0.000105187, acc 1
2017-09-08T18:48:30.160384: step 9305, loss 0.000499158, acc 1
2017-09-08T18:48:30.548874: step 9306, loss 7.52903e-05, acc 1
2017-09-08T18:48:30.937747: step 9307, loss 0.0172872, acc 0.984375
2017-09-08T18:48:31.344606: step 9308, loss 0.000469523, acc 1
2017-09-08T18:48:31.752059: step 9309, loss 0.00804818, acc 1
2017-09-08T18:48:32.201604: step 9310, loss 0.00034698, acc 1
2017-09-08T18:48:32.676393: step 9311, loss 0.00534693, acc 1
2017-09-08T18:48:33.153638: step 9312, loss 0.0648324, acc 0.984375
2017-09-08T18:48:33.641617: step 9313, loss 6.16215e-05, acc 1
2017-09-08T18:48:34.136497: step 9314, loss 0.000666289, acc 1
2017-09-08T18:48:34.620540: step 9315, loss 0.000851031, acc 1
2017-09-08T18:48:35.128813: step 9316, loss 0.0386562, acc 0.984375
2017-09-08T18:48:35.628182: step 9317, loss 5.92024e-05, acc 1
2017-09-08T18:48:36.125643: step 9318, loss 0.00281784, acc 1
2017-09-08T18:48:36.599983: step 9319, loss 0.00714429, acc 1
2017-09-08T18:48:37.081809: step 9320, loss 0.0255328, acc 0.984375
2017-09-08T18:48:37.583082: step 9321, loss 0.00888988, acc 1
2017-09-08T18:48:38.077087: step 9322, loss 0.0059399, acc 1
2017-09-08T18:48:38.581069: step 9323, loss 0.000367224, acc 1
2017-09-08T18:48:39.080314: step 9324, loss 0.00444979, acc 1
2017-09-08T18:48:39.567717: step 9325, loss 0.000139148, acc 1
2017-09-08T18:48:40.040031: step 9326, loss 0.0429544, acc 0.984375
2017-09-08T18:48:40.519178: step 9327, loss 0.0469545, acc 0.984375
2017-09-08T18:48:41.014185: step 9328, loss 0.000211813, acc 1
2017-09-08T18:48:41.507220: step 9329, loss 0.0291548, acc 0.984375
2017-09-08T18:48:41.988419: step 9330, loss 0.000313377, acc 1
2017-09-08T18:48:42.474972: step 9331, loss 0.00076678, acc 1
2017-09-08T18:48:42.978699: step 9332, loss 0.0209873, acc 0.984375
2017-09-08T18:48:43.478127: step 9333, loss 8.52771e-05, acc 1
2017-09-08T18:48:43.946694: step 9334, loss 0.00576435, acc 1
2017-09-08T18:48:44.420145: step 9335, loss 0.0203999, acc 0.984375
2017-09-08T18:48:44.892746: step 9336, loss 8.32256e-05, acc 1
2017-09-08T18:48:45.347955: step 9337, loss 0.0064538, acc 1
2017-09-08T18:48:45.861190: step 9338, loss 0.00395442, acc 1
2017-09-08T18:48:46.337433: step 9339, loss 0.0191852, acc 0.984375
2017-09-08T18:48:46.840601: step 9340, loss 0.00119091, acc 1
2017-09-08T18:48:47.288370: step 9341, loss 0.0342948, acc 0.984375
2017-09-08T18:48:47.771858: step 9342, loss 0.0046901, acc 1
2017-09-08T18:48:48.239071: step 9343, loss 8.52792e-05, acc 1
2017-09-08T18:48:48.703435: step 9344, loss 0.000775993, acc 1
2017-09-08T18:48:49.194019: step 9345, loss 0.00192616, acc 1
2017-09-08T18:48:49.686376: step 9346, loss 0.0390093, acc 0.96875
2017-09-08T18:48:50.171350: step 9347, loss 7.07705e-05, acc 1
2017-09-08T18:48:50.654462: step 9348, loss 0.000242441, acc 1
2017-09-08T18:48:51.114749: step 9349, loss 0.000927273, acc 1
2017-09-08T18:48:51.571530: step 9350, loss 0.00427968, acc 1

Evaluation:
2017-09-08T18:48:52.090443: step 9350, loss 0.261822, acc 0.92518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9350

2017-09-08T18:48:53.853231: step 9351, loss 0.000792657, acc 1
2017-09-08T18:48:54.354074: step 9352, loss 0.000434788, acc 1
2017-09-08T18:48:54.850673: step 9353, loss 0.000500585, acc 1
2017-09-08T18:48:55.367166: step 9354, loss 0.0161399, acc 0.984375
2017-09-08T18:48:55.865244: step 9355, loss 0.0271481, acc 0.984375
2017-09-08T18:48:56.413845: step 9356, loss 0.000147956, acc 1
2017-09-08T18:48:56.824332: step 9357, loss 0.000234906, acc 1
2017-09-08T18:48:57.210991: step 9358, loss 0.00830219, acc 1
2017-09-08T18:48:57.612083: step 9359, loss 0.00165722, acc 1
2017-09-08T18:48:58.022745: step 9360, loss 0.000152511, acc 1
2017-09-08T18:48:58.426327: step 9361, loss 0.0587885, acc 0.96875
2017-09-08T18:48:58.908379: step 9362, loss 0.0345922, acc 0.96875
2017-09-08T18:48:59.445660: step 9363, loss 0.023754, acc 0.984375
2017-09-08T18:48:59.930370: step 9364, loss 0.0266566, acc 0.984375
2017-09-08T18:49:00.401591: step 9365, loss 0.0161593, acc 1
2017-09-08T18:49:00.912417: step 9366, loss 0.000120231, acc 1
2017-09-08T18:49:01.381531: step 9367, loss 0.0153502, acc 0.984375
2017-09-08T18:49:01.870281: step 9368, loss 0.024376, acc 0.984375
2017-09-08T18:49:02.372816: step 9369, loss 0.0390114, acc 0.984375
2017-09-08T18:49:02.867073: step 9370, loss 0.00693346, acc 1
2017-09-08T18:49:03.368545: step 9371, loss 0.0120115, acc 1
2017-09-08T18:49:03.849062: step 9372, loss 0.0104477, acc 1
2017-09-08T18:49:04.351769: step 9373, loss 0.000169383, acc 1
2017-09-08T18:49:04.814223: step 9374, loss 9.6984e-05, acc 1
2017-09-08T18:49:05.284085: step 9375, loss 0.0881731, acc 0.953125
2017-09-08T18:49:05.755728: step 9376, loss 0.0352963, acc 0.984375
2017-09-08T18:49:06.230958: step 9377, loss 0.000290206, acc 1
2017-09-08T18:49:06.703363: step 9378, loss 0.046372, acc 0.984375
2017-09-08T18:49:07.165670: step 9379, loss 0.0346891, acc 0.96875
2017-09-08T18:49:07.612228: step 9380, loss 0.00772323, acc 1
2017-09-08T18:49:08.133093: step 9381, loss 0.00085998, acc 1
2017-09-08T18:49:08.613332: step 9382, loss 0.000123993, acc 1
2017-09-08T18:49:09.101268: step 9383, loss 0.00801401, acc 1
2017-09-08T18:49:09.523590: step 9384, loss 0.000532202, acc 1
2017-09-08T18:49:10.016884: step 9385, loss 0.00435673, acc 1
2017-09-08T18:49:10.547639: step 9386, loss 0.0323448, acc 0.984375
2017-09-08T18:49:11.030964: step 9387, loss 0.000109322, acc 1
2017-09-08T18:49:11.502457: step 9388, loss 0.00165112, acc 1
2017-09-08T18:49:11.965320: step 9389, loss 0.00162949, acc 1
2017-09-08T18:49:12.469659: step 9390, loss 0.0037838, acc 1
2017-09-08T18:49:12.984018: step 9391, loss 0.0172419, acc 0.984375
2017-09-08T18:49:13.454744: step 9392, loss 0.0112241, acc 1
2017-09-08T18:49:13.952395: step 9393, loss 0.00998522, acc 1
2017-09-08T18:49:14.436616: step 9394, loss 0.00203232, acc 1
2017-09-08T18:49:14.930891: step 9395, loss 0.00199649, acc 1
2017-09-08T18:49:15.424135: step 9396, loss 0.025564, acc 0.984375
2017-09-08T18:49:15.931489: step 9397, loss 0.0492998, acc 0.96875
2017-09-08T18:49:16.386434: step 9398, loss 0.000273906, acc 1
2017-09-08T18:49:16.877362: step 9399, loss 0.0391667, acc 0.984375
2017-09-08T18:49:17.344858: step 9400, loss 0.00208404, acc 1

Evaluation:
2017-09-08T18:49:17.845628: step 9400, loss 0.254267, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9400

2017-09-08T18:49:20.680310: step 9401, loss 0.00233769, acc 1
2017-09-08T18:49:21.160934: step 9402, loss 0.0030352, acc 1
2017-09-08T18:49:21.632969: step 9403, loss 0.00597792, acc 1
2017-09-08T18:49:22.144311: step 9404, loss 0.0448704, acc 0.96875
2017-09-08T18:49:22.670777: step 9405, loss 0.000257796, acc 1
2017-09-08T18:49:23.124610: step 9406, loss 0.00136471, acc 1
2017-09-08T18:49:23.521925: step 9407, loss 0.00100577, acc 1
2017-09-08T18:49:23.867706: step 9408, loss 0.00052019, acc 1
2017-09-08T18:49:24.338901: step 9409, loss 0.0132824, acc 1
2017-09-08T18:49:24.806479: step 9410, loss 0.00733151, acc 1
2017-09-08T18:49:25.316896: step 9411, loss 0.0392153, acc 0.984375
2017-09-08T18:49:25.805169: step 9412, loss 0.0147186, acc 0.984375
2017-09-08T18:49:26.310765: step 9413, loss 0.00578599, acc 1
2017-09-08T18:49:26.828527: step 9414, loss 0.00719207, acc 1
2017-09-08T18:49:27.324251: step 9415, loss 0.0148045, acc 1
2017-09-08T18:49:27.798673: step 9416, loss 0.00042742, acc 1
2017-09-08T18:49:28.293825: step 9417, loss 0.0270951, acc 0.984375
2017-09-08T18:49:28.762955: step 9418, loss 0.0276034, acc 0.984375
2017-09-08T18:49:29.250717: step 9419, loss 0.0210809, acc 0.984375
2017-09-08T18:49:29.743967: step 9420, loss 0.000547267, acc 1
2017-09-08T18:49:30.218348: step 9421, loss 0.000189179, acc 1
2017-09-08T18:49:30.689858: step 9422, loss 0.00973398, acc 1
2017-09-08T18:49:31.169570: step 9423, loss 0.000134133, acc 1
2017-09-08T18:49:31.638291: step 9424, loss 0.00309575, acc 1
2017-09-08T18:49:32.143580: step 9425, loss 0.00052918, acc 1
2017-09-08T18:49:32.614994: step 9426, loss 0.0150456, acc 0.984375
2017-09-08T18:49:33.102549: step 9427, loss 0.00775383, acc 1
2017-09-08T18:49:33.573074: step 9428, loss 0.00112162, acc 1
2017-09-08T18:49:34.071814: step 9429, loss 0.000310774, acc 1
2017-09-08T18:49:34.568430: step 9430, loss 0.0234537, acc 0.984375
2017-09-08T18:49:35.047883: step 9431, loss 0.000340726, acc 1
2017-09-08T18:49:35.537792: step 9432, loss 0.0259536, acc 0.984375
2017-09-08T18:49:36.042111: step 9433, loss 0.0254542, acc 0.96875
2017-09-08T18:49:36.493673: step 9434, loss 0.00727887, acc 1
2017-09-08T18:49:36.984104: step 9435, loss 0.0309481, acc 0.984375
2017-09-08T18:49:37.486047: step 9436, loss 0.0154231, acc 0.984375
2017-09-08T18:49:37.991198: step 9437, loss 0.000912074, acc 1
2017-09-08T18:49:38.479536: step 9438, loss 0.0133159, acc 1
2017-09-08T18:49:38.984769: step 9439, loss 0.0317429, acc 0.984375
2017-09-08T18:49:39.468020: step 9440, loss 0.0164385, acc 0.984375
2017-09-08T18:49:39.942513: step 9441, loss 0.000234313, acc 1
2017-09-08T18:49:40.405706: step 9442, loss 0.0845912, acc 0.96875
2017-09-08T18:49:40.897868: step 9443, loss 0.000146169, acc 1
2017-09-08T18:49:41.388287: step 9444, loss 0.0105338, acc 1
2017-09-08T18:49:41.863581: step 9445, loss 0.00152532, acc 1
2017-09-08T18:49:42.313593: step 9446, loss 0.00076954, acc 1
2017-09-08T18:49:42.799256: step 9447, loss 0.000379916, acc 1
2017-09-08T18:49:43.298019: step 9448, loss 0.000116305, acc 1
2017-09-08T18:49:43.796840: step 9449, loss 0.000127049, acc 1
2017-09-08T18:49:44.253658: step 9450, loss 0.051468, acc 0.984375

Evaluation:
2017-09-08T18:49:44.733121: step 9450, loss 0.26171, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9450

2017-09-08T18:49:46.540627: step 9451, loss 0.00148525, acc 1
2017-09-08T18:49:47.030452: step 9452, loss 0.0312766, acc 0.984375
2017-09-08T18:49:47.559264: step 9453, loss 0.0174294, acc 0.984375
2017-09-08T18:49:48.020279: step 9454, loss 0.000690878, acc 1
2017-09-08T18:49:48.539501: step 9455, loss 0.0147247, acc 0.984375
2017-09-08T18:49:48.918263: step 9456, loss 8.57452e-05, acc 1
2017-09-08T18:49:49.302644: step 9457, loss 0.0012898, acc 1
2017-09-08T18:49:49.685644: step 9458, loss 0.0248, acc 0.984375
2017-09-08T18:49:50.087355: step 9459, loss 0.0332348, acc 0.984375
2017-09-08T18:49:50.468906: step 9460, loss 0.0252707, acc 0.984375
2017-09-08T18:49:50.881252: step 9461, loss 0.0360475, acc 0.984375
2017-09-08T18:49:51.357178: step 9462, loss 0.0124077, acc 1
2017-09-08T18:49:51.834998: step 9463, loss 0.0128366, acc 0.984375
2017-09-08T18:49:52.322409: step 9464, loss 0.000104594, acc 1
2017-09-08T18:49:52.788496: step 9465, loss 0.000931814, acc 1
2017-09-08T18:49:53.278835: step 9466, loss 0.0225369, acc 0.984375
2017-09-08T18:49:53.767909: step 9467, loss 0.000114421, acc 1
2017-09-08T18:49:54.257790: step 9468, loss 0.0300022, acc 0.984375
2017-09-08T18:49:54.743254: step 9469, loss 0.00474898, acc 1
2017-09-08T18:49:55.247060: step 9470, loss 0.00196587, acc 1
2017-09-08T18:49:55.698888: step 9471, loss 0.000267627, acc 1
2017-09-08T18:49:56.171400: step 9472, loss 0.000123188, acc 1
2017-09-08T18:49:56.638303: step 9473, loss 0.000194961, acc 1
2017-09-08T18:49:57.125018: step 9474, loss 0.00321649, acc 1
2017-09-08T18:49:57.635808: step 9475, loss 0.00999508, acc 1
2017-09-08T18:49:58.151443: step 9476, loss 0.000243751, acc 1
2017-09-08T18:49:58.628500: step 9477, loss 7.30022e-05, acc 1
2017-09-08T18:49:59.129170: step 9478, loss 0.000322395, acc 1
2017-09-08T18:49:59.616815: step 9479, loss 0.00719928, acc 1
2017-09-08T18:50:00.117743: step 9480, loss 0.00795203, acc 1
2017-09-08T18:50:00.605729: step 9481, loss 0.0145917, acc 0.984375
2017-09-08T18:50:01.092581: step 9482, loss 0.0576648, acc 0.96875
2017-09-08T18:50:01.592107: step 9483, loss 0.0253194, acc 0.984375
2017-09-08T18:50:02.074025: step 9484, loss 0.00561444, acc 1
2017-09-08T18:50:02.554999: step 9485, loss 0.0346071, acc 0.984375
2017-09-08T18:50:03.033183: step 9486, loss 3.00015e-05, acc 1
2017-09-08T18:50:03.530773: step 9487, loss 0.00214839, acc 1
2017-09-08T18:50:04.023763: step 9488, loss 0.0137162, acc 1
2017-09-08T18:50:04.534317: step 9489, loss 0.0124203, acc 1
2017-09-08T18:50:05.040362: step 9490, loss 0.00139095, acc 1
2017-09-08T18:50:05.557670: step 9491, loss 0.0869366, acc 0.96875
2017-09-08T18:50:06.043853: step 9492, loss 0.00479678, acc 1
2017-09-08T18:50:06.544039: step 9493, loss 7.44745e-05, acc 1
2017-09-08T18:50:07.046236: step 9494, loss 0.00256991, acc 1
2017-09-08T18:50:07.547274: step 9495, loss 0.000114796, acc 1
2017-09-08T18:50:08.052356: step 9496, loss 0.000341213, acc 1
2017-09-08T18:50:08.514795: step 9497, loss 0.0260956, acc 0.984375
2017-09-08T18:50:09.016506: step 9498, loss 0.0001035, acc 1
2017-09-08T18:50:09.506182: step 9499, loss 0.00196669, acc 1
2017-09-08T18:50:09.961773: step 9500, loss 0.0010999, acc 1

Evaluation:
2017-09-08T18:50:10.510272: step 9500, loss 0.255771, acc 0.92518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9500

2017-09-08T18:50:12.682335: step 9501, loss 0.00205409, acc 1
2017-09-08T18:50:13.123152: step 9502, loss 0.00186153, acc 1
2017-09-08T18:50:13.564284: step 9503, loss 0.0070922, acc 1
2017-09-08T18:50:14.039069: step 9504, loss 0.000717417, acc 1
2017-09-08T18:50:14.526783: step 9505, loss 0.000220867, acc 1
2017-09-08T18:50:14.962608: step 9506, loss 0.0536001, acc 0.980392
2017-09-08T18:50:15.513252: step 9507, loss 0.000769922, acc 1
2017-09-08T18:50:15.899186: step 9508, loss 0.00082018, acc 1
2017-09-08T18:50:16.281343: step 9509, loss 0.0037698, acc 1
2017-09-08T18:50:16.664909: step 9510, loss 0.000214915, acc 1
2017-09-08T18:50:17.152222: step 9511, loss 0.0366202, acc 0.96875
2017-09-08T18:50:17.640713: step 9512, loss 0.0597493, acc 0.984375
2017-09-08T18:50:18.115277: step 9513, loss 0.0278665, acc 0.984375
2017-09-08T18:50:18.607078: step 9514, loss 0.000555519, acc 1
2017-09-08T18:50:19.070229: step 9515, loss 0.0355176, acc 0.984375
2017-09-08T18:50:19.551577: step 9516, loss 8.24978e-05, acc 1
2017-09-08T18:50:20.021252: step 9517, loss 0.0120747, acc 1
2017-09-08T18:50:20.481191: step 9518, loss 0.0757349, acc 0.96875
2017-09-08T18:50:20.974990: step 9519, loss 0.0268303, acc 0.984375
2017-09-08T18:50:21.446613: step 9520, loss 0.00176814, acc 1
2017-09-08T18:50:21.937801: step 9521, loss 0.00954206, acc 1
2017-09-08T18:50:22.442749: step 9522, loss 0.0103978, acc 1
2017-09-08T18:50:22.909574: step 9523, loss 0.000172548, acc 1
2017-09-08T18:50:23.397651: step 9524, loss 0.00214258, acc 1
2017-09-08T18:50:23.891771: step 9525, loss 0.00182234, acc 1
2017-09-08T18:50:24.361676: step 9526, loss 0.00869386, acc 1
2017-09-08T18:50:24.839167: step 9527, loss 0.0390394, acc 0.984375
2017-09-08T18:50:25.306201: step 9528, loss 0.000225584, acc 1
2017-09-08T18:50:25.806016: step 9529, loss 0.000662691, acc 1
2017-09-08T18:50:26.263350: step 9530, loss 0.0382039, acc 0.984375
2017-09-08T18:50:26.723074: step 9531, loss 6.73891e-05, acc 1
2017-09-08T18:50:27.227170: step 9532, loss 0.00511683, acc 1
2017-09-08T18:50:27.733161: step 9533, loss 0.00156054, acc 1
2017-09-08T18:50:28.190757: step 9534, loss 0.000961676, acc 1
2017-09-08T18:50:28.662323: step 9535, loss 0.00485843, acc 1
2017-09-08T18:50:29.119900: step 9536, loss 0.00638421, acc 1
2017-09-08T18:50:29.581449: step 9537, loss 8.14651e-05, acc 1
2017-09-08T18:50:30.087642: step 9538, loss 0.000370163, acc 1
2017-09-08T18:50:30.608050: step 9539, loss 0.0299506, acc 0.984375
2017-09-08T18:50:31.086124: step 9540, loss 0.0117041, acc 1
2017-09-08T18:50:31.588629: step 9541, loss 0.0588892, acc 0.984375
2017-09-08T18:50:32.075691: step 9542, loss 0.0106598, acc 1
2017-09-08T18:50:32.573715: step 9543, loss 0.00192909, acc 1
2017-09-08T18:50:33.063608: step 9544, loss 0.00190849, acc 1
2017-09-08T18:50:33.552930: step 9545, loss 0.0179464, acc 0.984375
2017-09-08T18:50:34.055272: step 9546, loss 0.00258387, acc 1
2017-09-08T18:50:34.533719: step 9547, loss 0.0419971, acc 0.984375
2017-09-08T18:50:34.995716: step 9548, loss 0.0257017, acc 0.984375
2017-09-08T18:50:35.474715: step 9549, loss 0.00188667, acc 1
2017-09-08T18:50:35.956320: step 9550, loss 0.00043119, acc 1

Evaluation:
2017-09-08T18:50:36.437337: step 9550, loss 0.268239, acc 0.929496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9550

2017-09-08T18:50:39.905780: step 9551, loss 6.50706e-05, acc 1
2017-09-08T18:50:40.375131: step 9552, loss 0.00165177, acc 1
2017-09-08T18:50:40.868892: step 9553, loss 0.0542287, acc 0.96875
2017-09-08T18:50:41.434263: step 9554, loss 0.0186442, acc 0.984375
2017-09-08T18:50:42.491963: step 9555, loss 0.000140986, acc 1
2017-09-08T18:50:42.887580: step 9556, loss 0.044388, acc 0.984375
2017-09-08T18:50:43.261878: step 9557, loss 0.0168347, acc 0.984375
2017-09-08T18:50:43.658775: step 9558, loss 0.00885549, acc 1
2017-09-08T18:50:44.150356: step 9559, loss 0.00477822, acc 1
2017-09-08T18:50:44.654398: step 9560, loss 0.0256289, acc 0.984375
2017-09-08T18:50:45.112209: step 9561, loss 0.00283109, acc 1
2017-09-08T18:50:45.646163: step 9562, loss 0.000463151, acc 1
2017-09-08T18:50:46.139170: step 9563, loss 0.000171667, acc 1
2017-09-08T18:50:46.621278: step 9564, loss 0.0933257, acc 0.984375
2017-09-08T18:50:47.075055: step 9565, loss 0.0530573, acc 0.96875
2017-09-08T18:50:47.559012: step 9566, loss 0.0154846, acc 0.984375
2017-09-08T18:50:48.047200: step 9567, loss 2.79615e-05, acc 1
2017-09-08T18:50:48.524257: step 9568, loss 0.0180577, acc 0.984375
2017-09-08T18:50:49.010146: step 9569, loss 0.000631647, acc 1
2017-09-08T18:50:49.519020: step 9570, loss 0.000224368, acc 1
2017-09-08T18:50:50.002630: step 9571, loss 3.56646e-05, acc 1
2017-09-08T18:50:50.510677: step 9572, loss 0.0139126, acc 0.984375
2017-09-08T18:50:51.000181: step 9573, loss 0.000954908, acc 1
2017-09-08T18:50:51.504748: step 9574, loss 4.7e-05, acc 1
2017-09-08T18:50:51.971707: step 9575, loss 0.0001317, acc 1
2017-09-08T18:50:52.488794: step 9576, loss 0.00920211, acc 1
2017-09-08T18:50:52.961054: step 9577, loss 0.0123495, acc 0.984375
2017-09-08T18:50:53.449702: step 9578, loss 0.000129579, acc 1
2017-09-08T18:50:53.905049: step 9579, loss 0.000169166, acc 1
2017-09-08T18:50:54.385360: step 9580, loss 0.0148197, acc 0.984375
2017-09-08T18:50:54.852846: step 9581, loss 0.000334542, acc 1
2017-09-08T18:50:55.341160: step 9582, loss 0.0177799, acc 0.984375
2017-09-08T18:50:55.809035: step 9583, loss 0.00207545, acc 1
2017-09-08T18:50:56.306585: step 9584, loss 0.00395686, acc 1
2017-09-08T18:50:56.805694: step 9585, loss 0.0110032, acc 1
2017-09-08T18:50:57.278414: step 9586, loss 0.0380433, acc 0.984375
2017-09-08T18:50:57.782555: step 9587, loss 0.0358159, acc 0.984375
2017-09-08T18:50:58.293089: step 9588, loss 0.000149746, acc 1
2017-09-08T18:50:58.754725: step 9589, loss 0.00244633, acc 1
2017-09-08T18:50:59.214658: step 9590, loss 0.00253598, acc 1
2017-09-08T18:50:59.705541: step 9591, loss 6.06763e-05, acc 1
2017-09-08T18:51:00.192294: step 9592, loss 5.89857e-05, acc 1
2017-09-08T18:51:00.658598: step 9593, loss 0.000608449, acc 1
2017-09-08T18:51:01.153902: step 9594, loss 0.00189731, acc 1
2017-09-08T18:51:01.637542: step 9595, loss 0.0383338, acc 0.984375
2017-09-08T18:51:02.134741: step 9596, loss 0.042635, acc 0.984375
2017-09-08T18:51:02.634353: step 9597, loss 0.050914, acc 0.984375
2017-09-08T18:51:03.104734: step 9598, loss 0.0347287, acc 0.984375
2017-09-08T18:51:03.571282: step 9599, loss 0.00544573, acc 1
2017-09-08T18:51:04.056431: step 9600, loss 0.0146918, acc 0.984375

Evaluation:
2017-09-08T18:51:04.564110: step 9600, loss 0.273205, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9600

2017-09-08T18:51:06.385203: step 9601, loss 0.000159184, acc 1
2017-09-08T18:51:06.832074: step 9602, loss 0.0298571, acc 0.984375
2017-09-08T18:51:07.274256: step 9603, loss 0.038411, acc 0.984375
2017-09-08T18:51:07.634761: step 9604, loss 0.0486903, acc 0.980392
2017-09-08T18:51:08.155154: step 9605, loss 0.00760863, acc 1
2017-09-08T18:51:08.548350: step 9606, loss 0.000553283, acc 1
2017-09-08T18:51:08.949342: step 9607, loss 6.66991e-05, acc 1
2017-09-08T18:51:09.355614: step 9608, loss 0.00940626, acc 1
2017-09-08T18:51:09.757796: step 9609, loss 8.48424e-05, acc 1
2017-09-08T18:51:10.138552: step 9610, loss 0.00972844, acc 1
2017-09-08T18:51:10.522655: step 9611, loss 0.000419724, acc 1
2017-09-08T18:51:10.922556: step 9612, loss 0.000198203, acc 1
2017-09-08T18:51:11.309404: step 9613, loss 0.000717896, acc 1
2017-09-08T18:51:11.699616: step 9614, loss 6.48245e-05, acc 1
2017-09-08T18:51:12.094070: step 9615, loss 9.4178e-05, acc 1
2017-09-08T18:51:12.474732: step 9616, loss 0.00141012, acc 1
2017-09-08T18:51:12.862205: step 9617, loss 3.7638e-05, acc 1
2017-09-08T18:51:13.235930: step 9618, loss 0.000107559, acc 1
2017-09-08T18:51:13.631316: step 9619, loss 0.00145252, acc 1
2017-09-08T18:51:14.048941: step 9620, loss 0.00446168, acc 1
2017-09-08T18:51:14.446243: step 9621, loss 0.00224857, acc 1
2017-09-08T18:51:14.848798: step 9622, loss 0.0212113, acc 0.984375
2017-09-08T18:51:15.226013: step 9623, loss 0.12351, acc 0.953125
2017-09-08T18:51:15.612390: step 9624, loss 0.000185947, acc 1
2017-09-08T18:51:15.993839: step 9625, loss 0.0125176, acc 1
2017-09-08T18:51:16.381723: step 9626, loss 0.0502372, acc 0.984375
2017-09-08T18:51:16.767990: step 9627, loss 3.08596e-05, acc 1
2017-09-08T18:51:17.151556: step 9628, loss 8.72355e-05, acc 1
2017-09-08T18:51:17.537774: step 9629, loss 9.93573e-05, acc 1
2017-09-08T18:51:17.933735: step 9630, loss 0.0408155, acc 0.984375
2017-09-08T18:51:18.331624: step 9631, loss 0.0284478, acc 0.984375
2017-09-08T18:51:18.712254: step 9632, loss 0.00238214, acc 1
2017-09-08T18:51:19.100621: step 9633, loss 0.0347943, acc 0.984375
2017-09-08T18:51:19.486984: step 9634, loss 7.21049e-05, acc 1
2017-09-08T18:51:19.860631: step 9635, loss 1.02199e-05, acc 1
2017-09-08T18:51:20.236052: step 9636, loss 0.00176389, acc 1
2017-09-08T18:51:20.622134: step 9637, loss 0.00173315, acc 1
2017-09-08T18:51:21.017038: step 9638, loss 5.34911e-05, acc 1
2017-09-08T18:51:21.411689: step 9639, loss 0.00634258, acc 1
2017-09-08T18:51:21.818894: step 9640, loss 5.43203e-05, acc 1
2017-09-08T18:51:22.216993: step 9641, loss 0.0200433, acc 1
2017-09-08T18:51:22.604318: step 9642, loss 0.041879, acc 0.984375
2017-09-08T18:51:22.990106: step 9643, loss 0.000103084, acc 1
2017-09-08T18:51:23.402071: step 9644, loss 0.000628422, acc 1
2017-09-08T18:51:23.795787: step 9645, loss 0.000240235, acc 1
2017-09-08T18:51:24.172525: step 9646, loss 0.0633882, acc 0.984375
2017-09-08T18:51:24.570452: step 9647, loss 0.0360145, acc 0.984375
2017-09-08T18:51:24.972103: step 9648, loss 0.000170907, acc 1
2017-09-08T18:51:25.364113: step 9649, loss 8.92188e-05, acc 1
2017-09-08T18:51:25.743182: step 9650, loss 0.00118567, acc 1

Evaluation:
2017-09-08T18:51:26.161012: step 9650, loss 0.267744, acc 0.926619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9650

2017-09-08T18:51:28.250773: step 9651, loss 0.000283836, acc 1
2017-09-08T18:51:28.725423: step 9652, loss 0.000133656, acc 1
2017-09-08T18:51:29.116254: step 9653, loss 0.00169112, acc 1
2017-09-08T18:51:29.508020: step 9654, loss 0.0253984, acc 0.984375
2017-09-08T18:51:29.915694: step 9655, loss 0.0545547, acc 0.984375
2017-09-08T18:51:30.307924: step 9656, loss 0.00312489, acc 1
2017-09-08T18:51:30.715596: step 9657, loss 0.0299556, acc 0.984375
2017-09-08T18:51:31.115497: step 9658, loss 0.0296033, acc 0.984375
2017-09-08T18:51:31.511639: step 9659, loss 0.0172829, acc 0.984375
2017-09-08T18:51:31.914449: step 9660, loss 3.47295e-05, acc 1
2017-09-08T18:51:32.307701: step 9661, loss 0.111881, acc 0.96875
2017-09-08T18:51:32.736401: step 9662, loss 0.00439507, acc 1
2017-09-08T18:51:33.134215: step 9663, loss 0.0230414, acc 0.984375
2017-09-08T18:51:33.527403: step 9664, loss 0.0716817, acc 0.984375
2017-09-08T18:51:33.905363: step 9665, loss 7.60369e-05, acc 1
2017-09-08T18:51:34.290405: step 9666, loss 0.000321484, acc 1
2017-09-08T18:51:34.684063: step 9667, loss 0.000112169, acc 1
2017-09-08T18:51:35.076115: step 9668, loss 0.0156586, acc 0.984375
2017-09-08T18:51:35.460950: step 9669, loss 0.00691667, acc 1
2017-09-08T18:51:35.838990: step 9670, loss 0.000863718, acc 1
2017-09-08T18:51:36.207125: step 9671, loss 8.41466e-05, acc 1
2017-09-08T18:51:36.594301: step 9672, loss 0.0142757, acc 0.984375
2017-09-08T18:51:36.981700: step 9673, loss 0.000344635, acc 1
2017-09-08T18:51:37.354617: step 9674, loss 0.000146842, acc 1
2017-09-08T18:51:37.731180: step 9675, loss 0.0186003, acc 0.984375
2017-09-08T18:51:38.132571: step 9676, loss 0.0138904, acc 1
2017-09-08T18:51:38.526088: step 9677, loss 0.0321603, acc 0.984375
2017-09-08T18:51:38.920901: step 9678, loss 0.00171829, acc 1
2017-09-08T18:51:39.329448: step 9679, loss 0.000456642, acc 1
2017-09-08T18:51:39.718112: step 9680, loss 0.00258051, acc 1
2017-09-08T18:51:40.113637: step 9681, loss 0.00690665, acc 1
2017-09-08T18:51:40.500947: step 9682, loss 0.000163759, acc 1
2017-09-08T18:51:40.885583: step 9683, loss 0.050704, acc 0.984375
2017-09-08T18:51:41.279355: step 9684, loss 0.000109298, acc 1
2017-09-08T18:51:41.666908: step 9685, loss 0.00114217, acc 1
2017-09-08T18:51:42.065820: step 9686, loss 0.00295965, acc 1
2017-09-08T18:51:42.457776: step 9687, loss 0.00747733, acc 1
2017-09-08T18:51:42.845304: step 9688, loss 0.0865113, acc 0.984375
2017-09-08T18:51:43.253502: step 9689, loss 0.0549445, acc 0.953125
2017-09-08T18:51:43.637470: step 9690, loss 0.0316562, acc 0.984375
2017-09-08T18:51:44.053369: step 9691, loss 0.0641087, acc 0.984375
2017-09-08T18:51:44.459501: step 9692, loss 0.0238832, acc 0.984375
2017-09-08T18:51:44.850609: step 9693, loss 0.03867, acc 0.984375
2017-09-08T18:51:45.252721: step 9694, loss 0.0300491, acc 0.984375
2017-09-08T18:51:45.661801: step 9695, loss 0.0239198, acc 0.984375
2017-09-08T18:51:46.053007: step 9696, loss 0.00375937, acc 1
2017-09-08T18:51:46.457209: step 9697, loss 0.0417314, acc 0.984375
2017-09-08T18:51:46.845468: step 9698, loss 0.0325388, acc 0.96875
2017-09-08T18:51:47.227453: step 9699, loss 0.000315324, acc 1
2017-09-08T18:51:47.605935: step 9700, loss 0.000899615, acc 1

Evaluation:
2017-09-08T18:51:48.019658: step 9700, loss 0.280966, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9700

2017-09-08T18:51:49.552032: step 9701, loss 3.05406e-05, acc 1
2017-09-08T18:51:49.900154: step 9702, loss 0.0732842, acc 0.980392
2017-09-08T18:51:50.292017: step 9703, loss 0.0172921, acc 0.984375
2017-09-08T18:51:50.697723: step 9704, loss 0.0170232, acc 0.984375
2017-09-08T18:51:51.117433: step 9705, loss 0.035306, acc 0.984375
2017-09-08T18:51:51.511605: step 9706, loss 0.000222644, acc 1
2017-09-08T18:51:51.897025: step 9707, loss 0.00076416, acc 1
2017-09-08T18:51:52.281010: step 9708, loss 0.00086054, acc 1
2017-09-08T18:51:52.673967: step 9709, loss 0.0857203, acc 0.96875
2017-09-08T18:51:53.064832: step 9710, loss 0.0334412, acc 0.984375
2017-09-08T18:51:53.469844: step 9711, loss 0.000744884, acc 1
2017-09-08T18:51:53.844905: step 9712, loss 0.00423311, acc 1
2017-09-08T18:51:54.243649: step 9713, loss 0.00103341, acc 1
2017-09-08T18:51:54.628823: step 9714, loss 0.000100148, acc 1
2017-09-08T18:51:55.031933: step 9715, loss 0.00271548, acc 1
2017-09-08T18:51:55.418321: step 9716, loss 0.00181094, acc 1
2017-09-08T18:51:55.827327: step 9717, loss 0.00144539, acc 1
2017-09-08T18:51:56.217404: step 9718, loss 0.000376613, acc 1
2017-09-08T18:51:56.603641: step 9719, loss 0.000108532, acc 1
2017-09-08T18:51:56.968316: step 9720, loss 3.94188e-05, acc 1
2017-09-08T18:51:57.365223: step 9721, loss 0.00979801, acc 1
2017-09-08T18:51:57.749993: step 9722, loss 0.0249094, acc 0.984375
2017-09-08T18:51:58.153737: step 9723, loss 0.000120796, acc 1
2017-09-08T18:51:58.561919: step 9724, loss 0.0169381, acc 0.984375
2017-09-08T18:51:58.937027: step 9725, loss 0.00175354, acc 1
2017-09-08T18:51:59.336928: step 9726, loss 0.00264646, acc 1
2017-09-08T18:51:59.730992: step 9727, loss 7.43791e-05, acc 1
2017-09-08T18:52:00.176887: step 9728, loss 4.56198e-05, acc 1
2017-09-08T18:52:00.565986: step 9729, loss 0.0313294, acc 0.984375
2017-09-08T18:52:00.957129: step 9730, loss 0.0635623, acc 0.96875
2017-09-08T18:52:01.350646: step 9731, loss 0.0140025, acc 0.984375
2017-09-08T18:52:01.726865: step 9732, loss 9.22293e-05, acc 1
2017-09-08T18:52:02.103251: step 9733, loss 0.000702251, acc 1
2017-09-08T18:52:02.487518: step 9734, loss 0.0023991, acc 1
2017-09-08T18:52:02.874214: step 9735, loss 8.27868e-05, acc 1
2017-09-08T18:52:03.260316: step 9736, loss 0.00295733, acc 1
2017-09-08T18:52:03.648710: step 9737, loss 0.0463991, acc 0.984375
2017-09-08T18:52:04.034894: step 9738, loss 0.0464032, acc 0.984375
2017-09-08T18:52:04.449205: step 9739, loss 0.00043606, acc 1
2017-09-08T18:52:04.829800: step 9740, loss 0.0556151, acc 0.96875
2017-09-08T18:52:05.248357: step 9741, loss 0.047961, acc 0.96875
2017-09-08T18:52:05.637006: step 9742, loss 0.0177123, acc 0.984375
2017-09-08T18:52:06.040289: step 9743, loss 0.000201103, acc 1
2017-09-08T18:52:06.424662: step 9744, loss 0.000160695, acc 1
2017-09-08T18:52:06.818268: step 9745, loss 0.0239011, acc 0.984375
2017-09-08T18:52:07.216616: step 9746, loss 0.000209782, acc 1
2017-09-08T18:52:07.610214: step 9747, loss 0.00197059, acc 1
2017-09-08T18:52:07.993411: step 9748, loss 0.00692927, acc 1
2017-09-08T18:52:08.384048: step 9749, loss 0.0133433, acc 1
2017-09-08T18:52:08.791965: step 9750, loss 0.0122353, acc 1

Evaluation:
2017-09-08T18:52:09.211470: step 9750, loss 0.276303, acc 0.932374

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9750

2017-09-08T18:52:10.948534: step 9751, loss 0.0159456, acc 1
2017-09-08T18:52:11.354239: step 9752, loss 0.000352461, acc 1
2017-09-08T18:52:11.727264: step 9753, loss 0.0189182, acc 0.984375
2017-09-08T18:52:12.116305: step 9754, loss 0.000186564, acc 1
2017-09-08T18:52:12.515802: step 9755, loss 0.00244526, acc 1
2017-09-08T18:52:12.905487: step 9756, loss 9.15971e-05, acc 1
2017-09-08T18:52:13.299280: step 9757, loss 0.0321969, acc 0.984375
2017-09-08T18:52:13.719496: step 9758, loss 0.000917151, acc 1
2017-09-08T18:52:14.130124: step 9759, loss 0.0109367, acc 1
2017-09-08T18:52:14.528671: step 9760, loss 0.0353924, acc 0.984375
2017-09-08T18:52:14.947227: step 9761, loss 0.00249454, acc 1
2017-09-08T18:52:15.329798: step 9762, loss 0.0277802, acc 0.984375
2017-09-08T18:52:15.730676: step 9763, loss 0.0413413, acc 0.984375
2017-09-08T18:52:16.129283: step 9764, loss 0.0451139, acc 0.984375
2017-09-08T18:52:16.537181: step 9765, loss 0.00217936, acc 1
2017-09-08T18:52:16.918603: step 9766, loss 0.000394536, acc 1
2017-09-08T18:52:17.315202: step 9767, loss 0.000184284, acc 1
2017-09-08T18:52:17.694122: step 9768, loss 0.0405231, acc 0.984375
2017-09-08T18:52:18.094347: step 9769, loss 0.00925005, acc 1
2017-09-08T18:52:18.488188: step 9770, loss 7.87626e-05, acc 1
2017-09-08T18:52:18.870936: step 9771, loss 0.00271897, acc 1
2017-09-08T18:52:19.268814: step 9772, loss 0.000258725, acc 1
2017-09-08T18:52:19.648229: step 9773, loss 0.00480274, acc 1
2017-09-08T18:52:20.048645: step 9774, loss 0.00337699, acc 1
2017-09-08T18:52:20.440716: step 9775, loss 4.47634e-05, acc 1
2017-09-08T18:52:20.840663: step 9776, loss 0.0120118, acc 0.984375
2017-09-08T18:52:21.222606: step 9777, loss 0.0505413, acc 0.984375
2017-09-08T18:52:21.622055: step 9778, loss 0.0456354, acc 0.984375
2017-09-08T18:52:22.023610: step 9779, loss 4.29367e-05, acc 1
2017-09-08T18:52:22.419970: step 9780, loss 0.0279441, acc 0.984375
2017-09-08T18:52:22.821949: step 9781, loss 4.62572e-05, acc 1
2017-09-08T18:52:23.217208: step 9782, loss 6.43762e-05, acc 1
2017-09-08T18:52:23.609772: step 9783, loss 0.0647126, acc 0.953125
2017-09-08T18:52:24.010601: step 9784, loss 0.066011, acc 0.984375
2017-09-08T18:52:24.410768: step 9785, loss 0.000149931, acc 1
2017-09-08T18:52:24.783288: step 9786, loss 0.000670228, acc 1
2017-09-08T18:52:25.164312: step 9787, loss 0.0487558, acc 0.96875
2017-09-08T18:52:25.572797: step 9788, loss 0.0234679, acc 0.984375
2017-09-08T18:52:25.973261: step 9789, loss 0.0141457, acc 1
2017-09-08T18:52:26.348612: step 9790, loss 0.109552, acc 0.96875
2017-09-08T18:52:26.752538: step 9791, loss 0.000203616, acc 1
2017-09-08T18:52:27.145704: step 9792, loss 0.000155264, acc 1
2017-09-08T18:52:27.538398: step 9793, loss 0.000170063, acc 1
2017-09-08T18:52:27.938125: step 9794, loss 0.00036714, acc 1
2017-09-08T18:52:28.330947: step 9795, loss 0.000122592, acc 1
2017-09-08T18:52:28.724102: step 9796, loss 0.000529743, acc 1
2017-09-08T18:52:29.097788: step 9797, loss 0.0305781, acc 0.96875
2017-09-08T18:52:29.497795: step 9798, loss 0.00590653, acc 1
2017-09-08T18:52:29.890365: step 9799, loss 0.00505687, acc 1
2017-09-08T18:52:30.257843: step 9800, loss 0.000172896, acc 1

Evaluation:
2017-09-08T18:52:30.689123: step 9800, loss 0.321329, acc 0.930935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-glove/cnn-text-classification-tf/runs/1504907767/checkpoints/model-9800

