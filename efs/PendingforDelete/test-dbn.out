
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=50
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=128
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=50
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=100
NUM_FILTERS=128

Loading data...
Vocabulary Size: 78626
Train/Dev split: 6259/695
Writing to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440

Load glove file /home/xxliu10/bigdata/myglove.txt
glove file has been loaded

2017-09-09T15:20:49.935742: step 1, loss 2.86424, acc 0.203125
2017-09-09T15:20:50.184895: step 2, loss 2.71096, acc 0.234375
2017-09-09T15:20:50.419342: step 3, loss 2.84029, acc 0.390625
2017-09-09T15:20:50.661033: step 4, loss 3.25074, acc 0.28125
2017-09-09T15:20:50.910812: step 5, loss 3.1086, acc 0.25
2017-09-09T15:20:51.141994: step 6, loss 2.49999, acc 0.25
2017-09-09T15:20:51.372192: step 7, loss 2.29081, acc 0.265625
2017-09-09T15:20:51.599708: step 8, loss 2.47574, acc 0.25
2017-09-09T15:20:51.826584: step 9, loss 2.2686, acc 0.265625
2017-09-09T15:20:52.068146: step 10, loss 2.56028, acc 0.203125
2017-09-09T15:20:52.301366: step 11, loss 2.38345, acc 0.21875
2017-09-09T15:20:52.531456: step 12, loss 2.30969, acc 0.21875
2017-09-09T15:20:52.759645: step 13, loss 2.08915, acc 0.234375
2017-09-09T15:20:52.995905: step 14, loss 2.26556, acc 0.28125
2017-09-09T15:20:53.232042: step 15, loss 1.95519, acc 0.421875
2017-09-09T15:20:53.475700: step 16, loss 2.1306, acc 0.28125
2017-09-09T15:20:53.705791: step 17, loss 2.40597, acc 0.25
2017-09-09T15:20:53.939446: step 18, loss 1.81395, acc 0.34375
2017-09-09T15:20:54.173183: step 19, loss 2.14073, acc 0.265625
2017-09-09T15:20:54.401290: step 20, loss 1.89074, acc 0.234375
2017-09-09T15:20:54.632540: step 21, loss 1.85487, acc 0.3125
2017-09-09T15:20:54.870102: step 22, loss 1.725, acc 0.359375
2017-09-09T15:20:55.098396: step 23, loss 1.61836, acc 0.359375
2017-09-09T15:20:55.330058: step 24, loss 1.96388, acc 0.1875
2017-09-09T15:20:55.557100: step 25, loss 1.87143, acc 0.1875
2017-09-09T15:20:55.786605: step 26, loss 1.72468, acc 0.21875
2017-09-09T15:20:56.014787: step 27, loss 1.75452, acc 0.296875
2017-09-09T15:20:56.242579: step 28, loss 1.7522, acc 0.34375
2017-09-09T15:20:56.472162: step 29, loss 1.74654, acc 0.28125
2017-09-09T15:20:56.702647: step 30, loss 1.82528, acc 0.296875
2017-09-09T15:20:56.931961: step 31, loss 1.53799, acc 0.265625
2017-09-09T15:20:57.166362: step 32, loss 1.53044, acc 0.34375
2017-09-09T15:20:57.398428: step 33, loss 1.67543, acc 0.296875
2017-09-09T15:20:57.627712: step 34, loss 1.80324, acc 0.25
2017-09-09T15:20:57.853436: step 35, loss 1.61379, acc 0.359375
2017-09-09T15:20:58.075270: step 36, loss 1.48956, acc 0.3125
2017-09-09T15:20:58.304482: step 37, loss 1.52292, acc 0.296875
2017-09-09T15:20:58.532586: step 38, loss 1.53925, acc 0.28125
2017-09-09T15:20:58.760148: step 39, loss 1.70335, acc 0.21875
2017-09-09T15:20:58.991644: step 40, loss 1.66881, acc 0.265625
2017-09-09T15:20:59.216887: step 41, loss 1.60209, acc 0.375
2017-09-09T15:20:59.447573: step 42, loss 1.82803, acc 0.296875
2017-09-09T15:20:59.676022: step 43, loss 1.49979, acc 0.328125
2017-09-09T15:20:59.903595: step 44, loss 1.53841, acc 0.34375
2017-09-09T15:21:00.131682: step 45, loss 1.4907, acc 0.34375
2017-09-09T15:21:00.362323: step 46, loss 1.56605, acc 0.375
2017-09-09T15:21:00.589518: step 47, loss 1.58064, acc 0.328125
2017-09-09T15:21:00.818879: step 48, loss 1.65631, acc 0.265625
2017-09-09T15:21:01.040258: step 49, loss 1.48765, acc 0.375
2017-09-09T15:21:01.263378: step 50, loss 1.4778, acc 0.296875

Evaluation:
2017-09-09T15:21:01.320789: step 50, loss 1.58039, acc 0.290647

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-50

2017-09-09T15:21:03.104071: step 51, loss 1.49899, acc 0.3125
2017-09-09T15:21:03.329038: step 52, loss 1.60919, acc 0.28125
2017-09-09T15:21:03.552739: step 53, loss 1.40733, acc 0.328125
2017-09-09T15:21:03.780068: step 54, loss 1.60439, acc 0.328125
2017-09-09T15:21:04.005442: step 55, loss 1.46109, acc 0.375
2017-09-09T15:21:04.227931: step 56, loss 1.55728, acc 0.296875
2017-09-09T15:21:04.461534: step 57, loss 1.67263, acc 0.234375
2017-09-09T15:21:04.686907: step 58, loss 1.51882, acc 0.265625
2017-09-09T15:21:04.914265: step 59, loss 1.52551, acc 0.34375
2017-09-09T15:21:05.144732: step 60, loss 1.57516, acc 0.28125
2017-09-09T15:21:05.375861: step 61, loss 1.48421, acc 0.296875
2017-09-09T15:21:05.603232: step 62, loss 1.52362, acc 0.28125
2017-09-09T15:21:05.820174: step 63, loss 1.53438, acc 0.375
2017-09-09T15:21:06.044895: step 64, loss 1.56424, acc 0.28125
2017-09-09T15:21:06.264709: step 65, loss 1.55992, acc 0.359375
2017-09-09T15:21:06.492336: step 66, loss 1.56365, acc 0.3125
2017-09-09T15:21:06.718789: step 67, loss 1.63988, acc 0.203125
2017-09-09T15:21:06.945422: step 68, loss 1.50204, acc 0.34375
2017-09-09T15:21:07.176652: step 69, loss 1.46317, acc 0.390625
2017-09-09T15:21:07.411842: step 70, loss 1.47806, acc 0.421875
2017-09-09T15:21:07.636365: step 71, loss 1.54732, acc 0.265625
2017-09-09T15:21:07.866649: step 72, loss 1.51279, acc 0.3125
2017-09-09T15:21:08.092347: step 73, loss 1.59532, acc 0.359375
2017-09-09T15:21:08.321624: step 74, loss 1.45018, acc 0.296875
2017-09-09T15:21:08.543762: step 75, loss 1.45832, acc 0.359375
2017-09-09T15:21:08.766767: step 76, loss 1.5747, acc 0.328125
2017-09-09T15:21:08.991340: step 77, loss 1.50128, acc 0.296875
2017-09-09T15:21:09.224038: step 78, loss 1.55354, acc 0.3125
2017-09-09T15:21:09.449170: step 79, loss 1.48081, acc 0.34375
2017-09-09T15:21:09.670813: step 80, loss 1.61082, acc 0.28125
2017-09-09T15:21:09.896550: step 81, loss 1.43063, acc 0.359375
2017-09-09T15:21:10.117711: step 82, loss 1.61675, acc 0.3125
2017-09-09T15:21:10.341811: step 83, loss 1.5818, acc 0.328125
2017-09-09T15:21:10.569310: step 84, loss 1.52307, acc 0.296875
2017-09-09T15:21:10.800547: step 85, loss 1.50129, acc 0.328125
2017-09-09T15:21:11.036634: step 86, loss 1.60671, acc 0.203125
2017-09-09T15:21:11.261716: step 87, loss 1.54198, acc 0.234375
2017-09-09T15:21:11.489264: step 88, loss 1.52975, acc 0.359375
2017-09-09T15:21:11.716681: step 89, loss 1.44707, acc 0.375
2017-09-09T15:21:11.941234: step 90, loss 1.53417, acc 0.296875
2017-09-09T15:21:12.166999: step 91, loss 1.56239, acc 0.28125
2017-09-09T15:21:12.395389: step 92, loss 1.58602, acc 0.25
2017-09-09T15:21:12.613812: step 93, loss 1.57067, acc 0.328125
2017-09-09T15:21:12.837223: step 94, loss 1.46412, acc 0.265625
2017-09-09T15:21:13.066512: step 95, loss 1.53322, acc 0.3125
2017-09-09T15:21:13.290026: step 96, loss 1.58602, acc 0.34375
2017-09-09T15:21:13.511592: step 97, loss 1.48204, acc 0.390625
2017-09-09T15:21:13.727173: step 98, loss 1.57904, acc 0.294118
2017-09-09T15:21:13.953044: step 99, loss 1.38426, acc 0.421875
2017-09-09T15:21:14.174184: step 100, loss 1.55939, acc 0.296875

Evaluation:
2017-09-09T15:21:14.222355: step 100, loss 1.52561, acc 0.290647

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-100

2017-09-09T15:21:15.991724: step 101, loss 1.45115, acc 0.390625
2017-09-09T15:21:16.215672: step 102, loss 1.51922, acc 0.34375
2017-09-09T15:21:16.446904: step 103, loss 1.4409, acc 0.390625
2017-09-09T15:21:16.678898: step 104, loss 1.53653, acc 0.3125
2017-09-09T15:21:16.899986: step 105, loss 1.61864, acc 0.3125
2017-09-09T15:21:17.126611: step 106, loss 1.40828, acc 0.359375
2017-09-09T15:21:17.353061: step 107, loss 1.55767, acc 0.359375
2017-09-09T15:21:17.581255: step 108, loss 1.52459, acc 0.375
2017-09-09T15:21:17.808743: step 109, loss 1.49555, acc 0.34375
2017-09-09T15:21:18.034297: step 110, loss 1.394, acc 0.453125
2017-09-09T15:21:18.273202: step 111, loss 1.4576, acc 0.328125
2017-09-09T15:21:18.498527: step 112, loss 1.61428, acc 0.25
2017-09-09T15:21:18.721545: step 113, loss 1.45341, acc 0.375
2017-09-09T15:21:18.942422: step 114, loss 1.43252, acc 0.390625
2017-09-09T15:21:19.162715: step 115, loss 1.49389, acc 0.328125
2017-09-09T15:21:19.394974: step 116, loss 1.51215, acc 0.28125
2017-09-09T15:21:19.624364: step 117, loss 1.47543, acc 0.3125
2017-09-09T15:21:19.851207: step 118, loss 1.58706, acc 0.28125
2017-09-09T15:21:20.077855: step 119, loss 1.58445, acc 0.234375
2017-09-09T15:21:20.301466: step 120, loss 1.45679, acc 0.40625
2017-09-09T15:21:20.527431: step 121, loss 1.41179, acc 0.4375
2017-09-09T15:21:20.756790: step 122, loss 1.52191, acc 0.328125
2017-09-09T15:21:21.000184: step 123, loss 1.55464, acc 0.296875
2017-09-09T15:21:21.239716: step 124, loss 1.57019, acc 0.34375
2017-09-09T15:21:21.470095: step 125, loss 1.53635, acc 0.3125
2017-09-09T15:21:21.691343: step 126, loss 1.5001, acc 0.40625
2017-09-09T15:21:21.919314: step 127, loss 1.54245, acc 0.34375
2017-09-09T15:21:22.141977: step 128, loss 1.50246, acc 0.359375
2017-09-09T15:21:22.369817: step 129, loss 1.51473, acc 0.375
2017-09-09T15:21:22.596298: step 130, loss 1.54276, acc 0.265625
2017-09-09T15:21:22.830682: step 131, loss 1.54451, acc 0.234375
2017-09-09T15:21:23.053978: step 132, loss 1.52565, acc 0.3125
2017-09-09T15:21:23.301662: step 133, loss 1.54487, acc 0.28125
2017-09-09T15:21:23.536869: step 134, loss 1.45023, acc 0.296875
2017-09-09T15:21:23.761054: step 135, loss 1.50348, acc 0.25
2017-09-09T15:21:23.989246: step 136, loss 1.4531, acc 0.34375
2017-09-09T15:21:24.216147: step 137, loss 1.50671, acc 0.359375
2017-09-09T15:21:24.442879: step 138, loss 1.55104, acc 0.359375
2017-09-09T15:21:24.667382: step 139, loss 1.50807, acc 0.359375
2017-09-09T15:21:24.890910: step 140, loss 1.48652, acc 0.328125
2017-09-09T15:21:25.117682: step 141, loss 1.56253, acc 0.3125
2017-09-09T15:21:25.345695: step 142, loss 1.50191, acc 0.34375
2017-09-09T15:21:25.574717: step 143, loss 1.41753, acc 0.4375
2017-09-09T15:21:25.798543: step 144, loss 1.46189, acc 0.359375
2017-09-09T15:21:26.029521: step 145, loss 1.4473, acc 0.328125
2017-09-09T15:21:26.260449: step 146, loss 1.42125, acc 0.4375
2017-09-09T15:21:26.481725: step 147, loss 1.51212, acc 0.3125
2017-09-09T15:21:26.707220: step 148, loss 1.33693, acc 0.5
2017-09-09T15:21:26.926102: step 149, loss 1.51806, acc 0.296875
2017-09-09T15:21:27.166414: step 150, loss 1.54475, acc 0.34375

Evaluation:
2017-09-09T15:21:27.214984: step 150, loss 1.5253, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-150

2017-09-09T15:21:28.678393: step 151, loss 1.545, acc 0.34375
2017-09-09T15:21:28.913709: step 152, loss 1.53474, acc 0.28125
2017-09-09T15:21:29.135451: step 153, loss 1.56648, acc 0.328125
2017-09-09T15:21:29.357091: step 154, loss 1.51137, acc 0.3125
2017-09-09T15:21:29.586100: step 155, loss 1.45163, acc 0.40625
2017-09-09T15:21:29.804126: step 156, loss 1.58323, acc 0.328125
2017-09-09T15:21:30.030346: step 157, loss 1.49514, acc 0.40625
2017-09-09T15:21:30.249843: step 158, loss 1.52158, acc 0.375
2017-09-09T15:21:30.468040: step 159, loss 1.46553, acc 0.484375
2017-09-09T15:21:30.699930: step 160, loss 1.40275, acc 0.375
2017-09-09T15:21:30.923560: step 161, loss 1.46971, acc 0.375
2017-09-09T15:21:31.158752: step 162, loss 1.54359, acc 0.3125
2017-09-09T15:21:31.379591: step 163, loss 1.46437, acc 0.40625
2017-09-09T15:21:31.607829: step 164, loss 1.50793, acc 0.3125
2017-09-09T15:21:31.834469: step 165, loss 1.48853, acc 0.359375
2017-09-09T15:21:32.051503: step 166, loss 1.50617, acc 0.390625
2017-09-09T15:21:32.271488: step 167, loss 1.48411, acc 0.3125
2017-09-09T15:21:32.497603: step 168, loss 1.49326, acc 0.265625
2017-09-09T15:21:32.720960: step 169, loss 1.50801, acc 0.3125
2017-09-09T15:21:32.947433: step 170, loss 1.58469, acc 0.25
2017-09-09T15:21:33.168763: step 171, loss 1.5361, acc 0.34375
2017-09-09T15:21:33.388669: step 172, loss 1.47431, acc 0.34375
2017-09-09T15:21:33.614254: step 173, loss 1.55073, acc 0.421875
2017-09-09T15:21:33.835887: step 174, loss 1.54001, acc 0.296875
2017-09-09T15:21:34.053508: step 175, loss 1.53083, acc 0.3125
2017-09-09T15:21:34.272768: step 176, loss 1.45453, acc 0.46875
2017-09-09T15:21:34.501933: step 177, loss 1.47045, acc 0.375
2017-09-09T15:21:34.732232: step 178, loss 1.48489, acc 0.375
2017-09-09T15:21:34.954158: step 179, loss 1.49319, acc 0.3125
2017-09-09T15:21:35.180910: step 180, loss 1.46577, acc 0.390625
2017-09-09T15:21:35.403663: step 181, loss 1.50502, acc 0.3125
2017-09-09T15:21:35.634285: step 182, loss 1.42115, acc 0.453125
2017-09-09T15:21:35.852561: step 183, loss 1.49564, acc 0.375
2017-09-09T15:21:36.084514: step 184, loss 1.4856, acc 0.375
2017-09-09T15:21:36.304557: step 185, loss 1.48745, acc 0.40625
2017-09-09T15:21:36.523591: step 186, loss 1.43389, acc 0.40625
2017-09-09T15:21:36.747925: step 187, loss 1.50984, acc 0.375
2017-09-09T15:21:36.973090: step 188, loss 1.49573, acc 0.359375
2017-09-09T15:21:37.195465: step 189, loss 1.48348, acc 0.296875
2017-09-09T15:21:37.433592: step 190, loss 1.54863, acc 0.328125
2017-09-09T15:21:37.662801: step 191, loss 1.47418, acc 0.28125
2017-09-09T15:21:37.887424: step 192, loss 1.53653, acc 0.296875
2017-09-09T15:21:38.118542: step 193, loss 1.63635, acc 0.21875
2017-09-09T15:21:38.345520: step 194, loss 1.39058, acc 0.359375
2017-09-09T15:21:38.565436: step 195, loss 1.39012, acc 0.390625
2017-09-09T15:21:38.785520: step 196, loss 1.38338, acc 0.411765
2017-09-09T15:21:39.017526: step 197, loss 1.4667, acc 0.328125
2017-09-09T15:21:39.248344: step 198, loss 1.45774, acc 0.390625
2017-09-09T15:21:39.468793: step 199, loss 1.41603, acc 0.375
2017-09-09T15:21:39.692636: step 200, loss 1.41651, acc 0.40625

Evaluation:
2017-09-09T15:21:39.736946: step 200, loss 1.5253, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-200

2017-09-09T15:21:41.517393: step 201, loss 1.53535, acc 0.265625
2017-09-09T15:21:41.739501: step 202, loss 1.52265, acc 0.34375
2017-09-09T15:21:41.959665: step 203, loss 1.41815, acc 0.328125
2017-09-09T15:21:42.188401: step 204, loss 1.53005, acc 0.265625
2017-09-09T15:21:42.412170: step 205, loss 1.58594, acc 0.21875
2017-09-09T15:21:42.635919: step 206, loss 1.41611, acc 0.40625
2017-09-09T15:21:42.860207: step 207, loss 1.47876, acc 0.359375
2017-09-09T15:21:43.078963: step 208, loss 1.46028, acc 0.4375
2017-09-09T15:21:43.296709: step 209, loss 1.53674, acc 0.296875
2017-09-09T15:21:43.521871: step 210, loss 1.51632, acc 0.4375
2017-09-09T15:21:43.746437: step 211, loss 1.42367, acc 0.484375
2017-09-09T15:21:43.972300: step 212, loss 1.39793, acc 0.453125
2017-09-09T15:21:44.202797: step 213, loss 1.42765, acc 0.390625
2017-09-09T15:21:44.424054: step 214, loss 1.47602, acc 0.359375
2017-09-09T15:21:44.650123: step 215, loss 1.39319, acc 0.390625
2017-09-09T15:21:44.870703: step 216, loss 1.34691, acc 0.46875
2017-09-09T15:21:45.094174: step 217, loss 1.37402, acc 0.4375
2017-09-09T15:21:45.317271: step 218, loss 1.40973, acc 0.34375
2017-09-09T15:21:45.548437: step 219, loss 1.38912, acc 0.421875
2017-09-09T15:21:45.926340: step 220, loss 1.42201, acc 0.421875
2017-09-09T15:21:46.149649: step 221, loss 1.63777, acc 0.171875
2017-09-09T15:21:46.367158: step 222, loss 1.43779, acc 0.46875
2017-09-09T15:21:46.592806: step 223, loss 1.56934, acc 0.328125
2017-09-09T15:21:46.822929: step 224, loss 1.52075, acc 0.359375
2017-09-09T15:21:47.043771: step 225, loss 1.48971, acc 0.40625
2017-09-09T15:21:47.275230: step 226, loss 1.42372, acc 0.328125
2017-09-09T15:21:47.501328: step 227, loss 1.47736, acc 0.359375
2017-09-09T15:21:47.727042: step 228, loss 1.43006, acc 0.40625
2017-09-09T15:21:47.950525: step 229, loss 1.49793, acc 0.3125
2017-09-09T15:21:48.181195: step 230, loss 1.44559, acc 0.375
2017-09-09T15:21:48.411181: step 231, loss 1.50661, acc 0.375
2017-09-09T15:21:48.640790: step 232, loss 1.45419, acc 0.421875
2017-09-09T15:21:48.860464: step 233, loss 1.48177, acc 0.375
2017-09-09T15:21:49.093144: step 234, loss 1.44561, acc 0.40625
2017-09-09T15:21:49.315972: step 235, loss 1.53051, acc 0.34375
2017-09-09T15:21:49.541873: step 236, loss 1.47285, acc 0.359375
2017-09-09T15:21:49.764472: step 237, loss 1.4563, acc 0.359375
2017-09-09T15:21:49.993048: step 238, loss 1.43352, acc 0.46875
2017-09-09T15:21:50.219000: step 239, loss 1.43139, acc 0.4375
2017-09-09T15:21:50.445969: step 240, loss 1.51574, acc 0.40625
2017-09-09T15:21:50.669683: step 241, loss 1.50107, acc 0.328125
2017-09-09T15:21:50.901586: step 242, loss 1.40886, acc 0.421875
2017-09-09T15:21:51.129922: step 243, loss 1.47738, acc 0.375
2017-09-09T15:21:51.352002: step 244, loss 1.48756, acc 0.453125
2017-09-09T15:21:51.580863: step 245, loss 1.41804, acc 0.40625
2017-09-09T15:21:51.806366: step 246, loss 1.3795, acc 0.515625
2017-09-09T15:21:52.043236: step 247, loss 1.48118, acc 0.3125
2017-09-09T15:21:52.278025: step 248, loss 1.45297, acc 0.4375
2017-09-09T15:21:52.515512: step 249, loss 1.44536, acc 0.296875
2017-09-09T15:21:52.742616: step 250, loss 1.47724, acc 0.34375

Evaluation:
2017-09-09T15:21:52.791746: step 250, loss 1.5185, acc 0.316547

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-250

2017-09-09T15:21:54.791916: step 251, loss 1.49716, acc 0.328125
2017-09-09T15:21:55.017619: step 252, loss 1.58488, acc 0.3125
2017-09-09T15:21:55.237445: step 253, loss 1.44694, acc 0.4375
2017-09-09T15:21:55.463338: step 254, loss 1.45877, acc 0.40625
2017-09-09T15:21:55.699821: step 255, loss 1.4745, acc 0.359375
2017-09-09T15:21:55.926860: step 256, loss 1.44764, acc 0.46875
2017-09-09T15:21:56.161194: step 257, loss 1.43994, acc 0.53125
2017-09-09T15:21:56.400852: step 258, loss 1.40972, acc 0.453125
2017-09-09T15:21:56.629307: step 259, loss 1.44375, acc 0.359375
2017-09-09T15:21:56.851659: step 260, loss 1.52753, acc 0.25
2017-09-09T15:21:57.084098: step 261, loss 1.49329, acc 0.234375
2017-09-09T15:21:57.310467: step 262, loss 1.50399, acc 0.28125
2017-09-09T15:21:57.530381: step 263, loss 1.51092, acc 0.40625
2017-09-09T15:21:57.751796: step 264, loss 1.51653, acc 0.359375
2017-09-09T15:21:57.975075: step 265, loss 1.48425, acc 0.390625
2017-09-09T15:21:58.204234: step 266, loss 1.40832, acc 0.453125
2017-09-09T15:21:58.437238: step 267, loss 1.43886, acc 0.40625
2017-09-09T15:21:58.657345: step 268, loss 1.49086, acc 0.390625
2017-09-09T15:21:58.880025: step 269, loss 1.41562, acc 0.421875
2017-09-09T15:21:59.095534: step 270, loss 1.41592, acc 0.390625
2017-09-09T15:21:59.319747: step 271, loss 1.34752, acc 0.421875
2017-09-09T15:21:59.539519: step 272, loss 1.47987, acc 0.34375
2017-09-09T15:21:59.763554: step 273, loss 1.52265, acc 0.328125
2017-09-09T15:21:59.987279: step 274, loss 1.52589, acc 0.28125
2017-09-09T15:22:00.212330: step 275, loss 1.3525, acc 0.5
2017-09-09T15:22:00.437008: step 276, loss 1.49113, acc 0.390625
2017-09-09T15:22:00.657147: step 277, loss 1.47469, acc 0.34375
2017-09-09T15:22:00.879222: step 278, loss 1.57903, acc 0.28125
2017-09-09T15:22:01.105465: step 279, loss 1.47283, acc 0.328125
2017-09-09T15:22:01.332847: step 280, loss 1.40563, acc 0.421875
2017-09-09T15:22:01.562093: step 281, loss 1.45286, acc 0.421875
2017-09-09T15:22:01.795038: step 282, loss 1.43902, acc 0.421875
2017-09-09T15:22:02.016468: step 283, loss 1.44135, acc 0.46875
2017-09-09T15:22:02.237153: step 284, loss 1.44846, acc 0.390625
2017-09-09T15:22:02.458164: step 285, loss 1.40639, acc 0.5
2017-09-09T15:22:02.683927: step 286, loss 1.46941, acc 0.359375
2017-09-09T15:22:02.910709: step 287, loss 1.47897, acc 0.40625
2017-09-09T15:22:03.128281: step 288, loss 1.47874, acc 0.34375
2017-09-09T15:22:03.355828: step 289, loss 1.41807, acc 0.421875
2017-09-09T15:22:03.581931: step 290, loss 1.42127, acc 0.421875
2017-09-09T15:22:03.809515: step 291, loss 1.43405, acc 0.546875
2017-09-09T15:22:04.038181: step 292, loss 1.42446, acc 0.390625
2017-09-09T15:22:04.271250: step 293, loss 1.31997, acc 0.53125
2017-09-09T15:22:04.493086: step 294, loss 1.45157, acc 0.411765
2017-09-09T15:22:04.725252: step 295, loss 1.50348, acc 0.359375
2017-09-09T15:22:04.948029: step 296, loss 1.32966, acc 0.546875
2017-09-09T15:22:05.169130: step 297, loss 1.43643, acc 0.4375
2017-09-09T15:22:05.398729: step 298, loss 1.42082, acc 0.46875
2017-09-09T15:22:05.625382: step 299, loss 1.42342, acc 0.40625
2017-09-09T15:22:05.852676: step 300, loss 1.42636, acc 0.4375

Evaluation:
2017-09-09T15:22:05.903161: step 300, loss 1.51625, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-300

2017-09-09T15:22:07.423214: step 301, loss 1.29811, acc 0.546875
2017-09-09T15:22:07.653825: step 302, loss 1.40608, acc 0.4375
2017-09-09T15:22:07.884347: step 303, loss 1.36876, acc 0.5
2017-09-09T15:22:08.116421: step 304, loss 1.40868, acc 0.46875
2017-09-09T15:22:08.351362: step 305, loss 1.4217, acc 0.5
2017-09-09T15:22:08.581681: step 306, loss 1.52261, acc 0.359375
2017-09-09T15:22:08.802350: step 307, loss 1.51658, acc 0.4375
2017-09-09T15:22:09.030268: step 308, loss 1.35069, acc 0.546875
2017-09-09T15:22:09.256030: step 309, loss 1.39198, acc 0.5
2017-09-09T15:22:09.485991: step 310, loss 1.29199, acc 0.703125
2017-09-09T15:22:09.707906: step 311, loss 1.32055, acc 0.625
2017-09-09T15:22:09.931226: step 312, loss 1.45065, acc 0.4375
2017-09-09T15:22:10.154114: step 313, loss 1.38793, acc 0.5
2017-09-09T15:22:10.378099: step 314, loss 1.36642, acc 0.453125
2017-09-09T15:22:10.599131: step 315, loss 1.49483, acc 0.390625
2017-09-09T15:22:10.818327: step 316, loss 1.3137, acc 0.5
2017-09-09T15:22:11.042560: step 317, loss 1.38111, acc 0.546875
2017-09-09T15:22:11.280863: step 318, loss 1.29651, acc 0.5
2017-09-09T15:22:11.510130: step 319, loss 1.38085, acc 0.421875
2017-09-09T15:22:11.738748: step 320, loss 1.41244, acc 0.453125
2017-09-09T15:22:11.968252: step 321, loss 1.29676, acc 0.484375
2017-09-09T15:22:12.195266: step 322, loss 1.34412, acc 0.5
2017-09-09T15:22:12.424194: step 323, loss 1.37365, acc 0.421875
2017-09-09T15:22:12.650457: step 324, loss 1.42493, acc 0.390625
2017-09-09T15:22:12.873885: step 325, loss 1.34074, acc 0.453125
2017-09-09T15:22:13.099101: step 326, loss 1.42374, acc 0.46875
2017-09-09T15:22:13.321376: step 327, loss 1.40858, acc 0.46875
2017-09-09T15:22:13.543069: step 328, loss 1.33775, acc 0.515625
2017-09-09T15:22:13.798670: step 329, loss 1.34427, acc 0.546875
2017-09-09T15:22:14.028462: step 330, loss 1.34178, acc 0.53125
2017-09-09T15:22:14.277547: step 331, loss 1.46598, acc 0.4375
2017-09-09T15:22:14.525316: step 332, loss 1.35997, acc 0.46875
2017-09-09T15:22:14.754606: step 333, loss 1.33678, acc 0.5
2017-09-09T15:22:14.997242: step 334, loss 1.33588, acc 0.53125
2017-09-09T15:22:15.270978: step 335, loss 1.31319, acc 0.484375
2017-09-09T15:22:15.508736: step 336, loss 1.32139, acc 0.484375
2017-09-09T15:22:15.742336: step 337, loss 1.37777, acc 0.484375
2017-09-09T15:22:15.993025: step 338, loss 1.34415, acc 0.421875
2017-09-09T15:22:16.242965: step 339, loss 1.35892, acc 0.46875
2017-09-09T15:22:16.491901: step 340, loss 1.41498, acc 0.328125
2017-09-09T15:22:16.742974: step 341, loss 1.2788, acc 0.515625
2017-09-09T15:22:17.023880: step 342, loss 1.44864, acc 0.3125
2017-09-09T15:22:17.280096: step 343, loss 1.35322, acc 0.46875
2017-09-09T15:22:17.513880: step 344, loss 1.37789, acc 0.390625
2017-09-09T15:22:17.759979: step 345, loss 1.38033, acc 0.515625
2017-09-09T15:22:18.003855: step 346, loss 1.29648, acc 0.5
2017-09-09T15:22:18.242570: step 347, loss 1.43627, acc 0.4375
2017-09-09T15:22:18.480587: step 348, loss 1.32322, acc 0.484375
2017-09-09T15:22:18.735657: step 349, loss 1.40695, acc 0.40625
2017-09-09T15:22:19.580290: step 350, loss 1.35004, acc 0.546875

Evaluation:
2017-09-09T15:22:19.627725: step 350, loss 1.51387, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-350

2017-09-09T15:22:21.985887: step 351, loss 1.38726, acc 0.5
2017-09-09T15:22:22.278051: step 352, loss 1.39187, acc 0.4375
2017-09-09T15:22:22.531184: step 353, loss 1.29155, acc 0.546875
2017-09-09T15:22:22.764171: step 354, loss 1.45237, acc 0.359375
2017-09-09T15:22:23.029300: step 355, loss 1.34697, acc 0.484375
2017-09-09T15:22:23.285398: step 356, loss 1.37688, acc 0.515625
2017-09-09T15:22:23.524444: step 357, loss 1.38775, acc 0.421875
2017-09-09T15:22:23.782903: step 358, loss 1.39548, acc 0.4375
2017-09-09T15:22:24.741274: step 359, loss 1.30605, acc 0.515625
2017-09-09T15:22:24.980449: step 360, loss 1.35053, acc 0.46875
2017-09-09T15:22:25.228700: step 361, loss 1.32057, acc 0.578125
2017-09-09T15:22:25.521189: step 362, loss 1.47867, acc 0.359375
2017-09-09T15:22:25.758927: step 363, loss 1.2627, acc 0.578125
2017-09-09T15:22:26.002902: step 364, loss 1.38088, acc 0.46875
2017-09-09T15:22:26.251784: step 365, loss 1.26388, acc 0.53125
2017-09-09T15:22:26.493441: step 366, loss 1.27444, acc 0.484375
2017-09-09T15:22:26.739766: step 367, loss 1.31681, acc 0.484375
2017-09-09T15:22:26.990451: step 368, loss 1.30613, acc 0.5
2017-09-09T15:22:27.260350: step 369, loss 1.27555, acc 0.46875
2017-09-09T15:22:27.507074: step 370, loss 1.32344, acc 0.53125
2017-09-09T15:22:27.743402: step 371, loss 1.42008, acc 0.359375
2017-09-09T15:22:28.003138: step 372, loss 1.36861, acc 0.4375
2017-09-09T15:22:28.257251: step 373, loss 1.42943, acc 0.328125
2017-09-09T15:22:28.519648: step 374, loss 1.4099, acc 0.46875
2017-09-09T15:22:28.783916: step 375, loss 1.3916, acc 0.5
2017-09-09T15:22:29.028905: step 376, loss 1.28246, acc 0.5625
2017-09-09T15:22:29.292770: step 377, loss 1.40501, acc 0.515625
2017-09-09T15:22:29.533449: step 378, loss 1.2392, acc 0.578125
2017-09-09T15:22:29.817307: step 379, loss 1.33483, acc 0.5625
2017-09-09T15:22:30.083086: step 380, loss 1.3666, acc 0.484375
2017-09-09T15:22:30.332894: step 381, loss 1.43147, acc 0.4375
2017-09-09T15:22:30.610771: step 382, loss 1.45898, acc 0.40625
2017-09-09T15:22:30.891971: step 383, loss 1.41882, acc 0.4375
2017-09-09T15:22:31.157109: step 384, loss 1.32229, acc 0.53125
2017-09-09T15:22:31.394887: step 385, loss 1.24043, acc 0.578125
2017-09-09T15:22:31.656007: step 386, loss 1.27033, acc 0.515625
2017-09-09T15:22:31.902827: step 387, loss 1.34741, acc 0.421875
2017-09-09T15:22:32.140468: step 388, loss 1.18714, acc 0.625
2017-09-09T15:22:32.369203: step 389, loss 1.2418, acc 0.53125
2017-09-09T15:22:32.657447: step 390, loss 1.40716, acc 0.46875
2017-09-09T15:22:32.887912: step 391, loss 1.24, acc 0.5625
2017-09-09T15:22:33.112216: step 392, loss 1.31854, acc 0.509804
2017-09-09T15:22:33.337619: step 393, loss 1.20471, acc 0.5
2017-09-09T15:22:33.562880: step 394, loss 1.18267, acc 0.546875
2017-09-09T15:22:33.784985: step 395, loss 1.12154, acc 0.546875
2017-09-09T15:22:34.014895: step 396, loss 1.18284, acc 0.546875
2017-09-09T15:22:34.248654: step 397, loss 1.3484, acc 0.4375
2017-09-09T15:22:34.540157: step 398, loss 1.2528, acc 0.515625
2017-09-09T15:22:34.784877: step 399, loss 1.24064, acc 0.53125
2017-09-09T15:22:35.030608: step 400, loss 1.18777, acc 0.546875

Evaluation:
2017-09-09T15:22:35.124278: step 400, loss 1.51235, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-400

2017-09-09T15:22:37.236827: step 401, loss 1.21554, acc 0.546875
2017-09-09T15:22:37.479125: step 402, loss 1.20531, acc 0.5625
2017-09-09T15:22:37.770587: step 403, loss 1.23762, acc 0.59375
2017-09-09T15:22:38.057592: step 404, loss 1.19146, acc 0.515625
2017-09-09T15:22:38.328586: step 405, loss 1.21502, acc 0.5625
2017-09-09T15:22:38.563634: step 406, loss 1.19193, acc 0.59375
2017-09-09T15:22:38.835534: step 407, loss 1.2176, acc 0.5
2017-09-09T15:22:39.122814: step 408, loss 1.28206, acc 0.53125
2017-09-09T15:22:39.393621: step 409, loss 1.07192, acc 0.65625
2017-09-09T15:22:39.629233: step 410, loss 1.15136, acc 0.59375
2017-09-09T15:22:39.917285: step 411, loss 1.1589, acc 0.609375
2017-09-09T15:22:40.187750: step 412, loss 1.17457, acc 0.484375
2017-09-09T15:22:40.422023: step 413, loss 1.28043, acc 0.46875
2017-09-09T15:22:40.686656: step 414, loss 1.15102, acc 0.609375
2017-09-09T15:22:40.941821: step 415, loss 1.18709, acc 0.546875
2017-09-09T15:22:41.186960: step 416, loss 1.32394, acc 0.453125
2017-09-09T15:22:41.452866: step 417, loss 1.29475, acc 0.515625
2017-09-09T15:22:41.714512: step 418, loss 1.09214, acc 0.671875
2017-09-09T15:22:41.955937: step 419, loss 1.12618, acc 0.625
2017-09-09T15:22:42.215662: step 420, loss 1.23767, acc 0.546875
2017-09-09T15:22:42.493519: step 421, loss 1.27654, acc 0.46875
2017-09-09T15:22:42.753011: step 422, loss 1.10374, acc 0.59375
2017-09-09T15:22:42.995757: step 423, loss 1.24331, acc 0.546875
2017-09-09T15:22:43.282178: step 424, loss 1.09724, acc 0.640625
2017-09-09T15:22:43.538138: step 425, loss 1.13788, acc 0.578125
2017-09-09T15:22:43.801376: step 426, loss 1.18569, acc 0.578125
2017-09-09T15:22:44.054538: step 427, loss 1.11501, acc 0.625
2017-09-09T15:22:44.354530: step 428, loss 1.14023, acc 0.59375
2017-09-09T15:22:44.600399: step 429, loss 1.14513, acc 0.5625
2017-09-09T15:22:44.848924: step 430, loss 1.0529, acc 0.625
2017-09-09T15:22:45.115821: step 431, loss 1.15647, acc 0.53125
2017-09-09T15:22:45.353213: step 432, loss 1.02678, acc 0.640625
2017-09-09T15:22:45.609371: step 433, loss 1.12147, acc 0.546875
2017-09-09T15:22:45.891194: step 434, loss 1.24036, acc 0.515625
2017-09-09T15:22:46.156795: step 435, loss 1.21481, acc 0.484375
2017-09-09T15:22:46.398828: step 436, loss 1.09322, acc 0.578125
2017-09-09T15:22:46.645806: step 437, loss 1.03075, acc 0.625
2017-09-09T15:22:46.908357: step 438, loss 1.14307, acc 0.5625
2017-09-09T15:22:47.141960: step 439, loss 1.13993, acc 0.578125
2017-09-09T15:22:47.421805: step 440, loss 1.01114, acc 0.6875
2017-09-09T15:22:47.705172: step 441, loss 1.22176, acc 0.515625
2017-09-09T15:22:47.939347: step 442, loss 1.04306, acc 0.578125
2017-09-09T15:22:48.246073: step 443, loss 1.13065, acc 0.546875
2017-09-09T15:22:48.525705: step 444, loss 1.20945, acc 0.515625
2017-09-09T15:22:48.769054: step 445, loss 0.997227, acc 0.703125
2017-09-09T15:22:49.033132: step 446, loss 1.04304, acc 0.640625
2017-09-09T15:22:49.334067: step 447, loss 1.09396, acc 0.578125
2017-09-09T15:22:49.634346: step 448, loss 1.07915, acc 0.609375
2017-09-09T15:22:49.903591: step 449, loss 1.22654, acc 0.53125
2017-09-09T15:22:50.160697: step 450, loss 1.23383, acc 0.453125

Evaluation:
2017-09-09T15:22:50.256019: step 450, loss 1.50845, acc 0.299281

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-450

2017-09-09T15:22:52.557255: step 451, loss 1.01754, acc 0.640625
2017-09-09T15:22:52.801928: step 452, loss 1.04472, acc 0.65625
2017-09-09T15:22:53.092706: step 453, loss 1.11531, acc 0.578125
2017-09-09T15:22:53.314085: step 454, loss 1.10137, acc 0.546875
2017-09-09T15:22:53.533941: step 455, loss 1.10809, acc 0.578125
2017-09-09T15:22:53.758042: step 456, loss 1.14597, acc 0.5625
2017-09-09T15:22:53.983033: step 457, loss 1.02866, acc 0.65625
2017-09-09T15:22:54.220115: step 458, loss 1.07111, acc 0.640625
2017-09-09T15:22:54.457166: step 459, loss 1.15988, acc 0.5
2017-09-09T15:22:54.681652: step 460, loss 1.13506, acc 0.546875
2017-09-09T15:22:54.958043: step 461, loss 1.08387, acc 0.59375
2017-09-09T15:22:55.226263: step 462, loss 0.994883, acc 0.6875
2017-09-09T15:22:55.479658: step 463, loss 1.19073, acc 0.46875
2017-09-09T15:22:55.731760: step 464, loss 1.17218, acc 0.515625
2017-09-09T15:22:56.013560: step 465, loss 1.05427, acc 0.625
2017-09-09T15:22:56.285443: step 466, loss 0.944088, acc 0.703125
2017-09-09T15:22:56.515986: step 467, loss 0.953642, acc 0.703125
2017-09-09T15:22:56.803204: step 468, loss 1.21176, acc 0.515625
2017-09-09T15:22:57.065250: step 469, loss 1.04448, acc 0.640625
2017-09-09T15:22:57.308968: step 470, loss 0.950272, acc 0.625
2017-09-09T15:22:57.570917: step 471, loss 0.898598, acc 0.6875
2017-09-09T15:22:57.845057: step 472, loss 0.949311, acc 0.625
2017-09-09T15:22:58.080777: step 473, loss 0.986898, acc 0.59375
2017-09-09T15:22:58.343053: step 474, loss 1.0779, acc 0.578125
2017-09-09T15:22:58.609028: step 475, loss 1.04048, acc 0.578125
2017-09-09T15:22:58.851741: step 476, loss 1.04088, acc 0.5625
2017-09-09T15:22:59.107423: step 477, loss 1.0182, acc 0.640625
2017-09-09T15:22:59.396824: step 478, loss 1.03708, acc 0.578125
2017-09-09T15:22:59.686871: step 479, loss 0.992153, acc 0.640625
2017-09-09T15:22:59.928521: step 480, loss 1.05361, acc 0.5625
2017-09-09T15:23:00.196984: step 481, loss 1.0989, acc 0.5625
2017-09-09T15:23:00.431213: step 482, loss 0.94179, acc 0.671875
2017-09-09T15:23:00.670929: step 483, loss 0.901182, acc 0.703125
2017-09-09T15:23:00.943198: step 484, loss 1.02405, acc 0.640625
2017-09-09T15:23:01.224721: step 485, loss 1.03097, acc 0.546875
2017-09-09T15:23:01.507813: step 486, loss 1.08974, acc 0.53125
2017-09-09T15:23:01.773982: step 487, loss 1.05426, acc 0.546875
2017-09-09T15:23:02.027354: step 488, loss 1.09172, acc 0.65625
2017-09-09T15:23:02.255894: step 489, loss 1.00981, acc 0.640625
2017-09-09T15:23:02.522281: step 490, loss 1.20153, acc 0.431373
2017-09-09T15:23:02.789585: step 491, loss 1.01071, acc 0.546875
2017-09-09T15:23:03.032243: step 492, loss 0.833446, acc 0.6875
2017-09-09T15:23:03.296305: step 493, loss 0.89236, acc 0.609375
2017-09-09T15:23:03.601506: step 494, loss 1.00553, acc 0.484375
2017-09-09T15:23:03.854916: step 495, loss 0.964927, acc 0.625
2017-09-09T15:23:04.094023: step 496, loss 0.898084, acc 0.578125
2017-09-09T15:23:04.413088: step 497, loss 0.915535, acc 0.5625
2017-09-09T15:23:04.690952: step 498, loss 0.833633, acc 0.640625
2017-09-09T15:23:04.933901: step 499, loss 0.735084, acc 0.734375
2017-09-09T15:23:05.189737: step 500, loss 0.95658, acc 0.59375

Evaluation:
2017-09-09T15:23:05.242220: step 500, loss 1.51268, acc 0.299281

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-500

2017-09-09T15:23:07.946459: step 501, loss 0.825848, acc 0.671875
2017-09-09T15:23:08.213980: step 502, loss 1.0191, acc 0.53125
2017-09-09T15:23:08.453324: step 503, loss 1.01721, acc 0.578125
2017-09-09T15:23:08.728177: step 504, loss 0.893354, acc 0.609375
2017-09-09T15:23:09.006292: step 505, loss 0.793208, acc 0.671875
2017-09-09T15:23:09.257353: step 506, loss 0.927071, acc 0.578125
2017-09-09T15:23:09.570179: step 507, loss 0.685262, acc 0.78125
2017-09-09T15:23:09.848667: step 508, loss 0.82564, acc 0.640625
2017-09-09T15:23:10.087351: step 509, loss 0.803233, acc 0.703125
2017-09-09T15:23:10.365968: step 510, loss 0.802594, acc 0.671875
2017-09-09T15:23:10.632294: step 511, loss 0.790834, acc 0.671875
2017-09-09T15:23:10.880542: step 512, loss 0.855458, acc 0.59375
2017-09-09T15:23:11.156078: step 513, loss 0.707552, acc 0.734375
2017-09-09T15:23:11.434707: step 514, loss 0.689526, acc 0.734375
2017-09-09T15:23:11.666170: step 515, loss 0.823637, acc 0.640625
2017-09-09T15:23:11.944529: step 516, loss 0.904036, acc 0.578125
2017-09-09T15:23:12.233675: step 517, loss 0.702824, acc 0.6875
2017-09-09T15:23:12.485441: step 518, loss 0.867896, acc 0.65625
2017-09-09T15:23:12.739914: step 519, loss 0.765484, acc 0.71875
2017-09-09T15:23:12.997994: step 520, loss 0.914938, acc 0.59375
2017-09-09T15:23:13.240304: step 521, loss 0.768085, acc 0.671875
2017-09-09T15:23:13.501549: step 522, loss 0.855446, acc 0.6875
2017-09-09T15:23:13.751727: step 523, loss 0.72425, acc 0.78125
2017-09-09T15:23:14.028850: step 524, loss 0.970996, acc 0.625
2017-09-09T15:23:14.254489: step 525, loss 0.855731, acc 0.703125
2017-09-09T15:23:14.477887: step 526, loss 0.782744, acc 0.703125
2017-09-09T15:23:14.704247: step 527, loss 0.689119, acc 0.78125
2017-09-09T15:23:14.925432: step 528, loss 0.821441, acc 0.703125
2017-09-09T15:23:15.150387: step 529, loss 0.904902, acc 0.71875
2017-09-09T15:23:15.432843: step 530, loss 0.867226, acc 0.671875
2017-09-09T15:23:15.678558: step 531, loss 0.673742, acc 0.75
2017-09-09T15:23:15.953780: step 532, loss 0.910427, acc 0.640625
2017-09-09T15:23:16.227897: step 533, loss 0.677378, acc 0.828125
2017-09-09T15:23:16.471696: step 534, loss 0.904103, acc 0.6875
2017-09-09T15:23:16.742098: step 535, loss 0.827832, acc 0.6875
2017-09-09T15:23:17.007777: step 536, loss 0.889218, acc 0.6875
2017-09-09T15:23:17.276436: step 537, loss 0.718247, acc 0.796875
2017-09-09T15:23:17.528205: step 538, loss 0.7075, acc 0.8125
2017-09-09T15:23:17.798332: step 539, loss 0.82152, acc 0.65625
2017-09-09T15:23:18.055747: step 540, loss 0.745939, acc 0.734375
2017-09-09T15:23:18.305548: step 541, loss 0.730987, acc 0.71875
2017-09-09T15:23:18.588611: step 542, loss 0.845404, acc 0.65625
2017-09-09T15:23:18.878728: step 543, loss 0.755457, acc 0.78125
2017-09-09T15:23:19.140007: step 544, loss 0.734419, acc 0.71875
2017-09-09T15:23:19.421493: step 545, loss 0.902845, acc 0.640625
2017-09-09T15:23:19.714442: step 546, loss 0.875892, acc 0.609375
2017-09-09T15:23:19.972585: step 547, loss 0.773849, acc 0.671875
2017-09-09T15:23:20.212881: step 548, loss 0.806722, acc 0.6875
2017-09-09T15:23:20.505909: step 549, loss 0.80871, acc 0.71875
2017-09-09T15:23:20.788419: step 550, loss 0.81118, acc 0.734375

Evaluation:
2017-09-09T15:23:20.842249: step 550, loss 1.50254, acc 0.328058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-550

2017-09-09T15:23:22.594471: step 551, loss 0.684338, acc 0.78125
2017-09-09T15:23:22.863525: step 552, loss 0.613779, acc 0.8125
2017-09-09T15:23:23.124145: step 553, loss 0.886891, acc 0.640625
2017-09-09T15:23:23.409476: step 554, loss 0.695515, acc 0.84375
2017-09-09T15:23:23.663196: step 555, loss 0.672605, acc 0.734375
2017-09-09T15:23:23.916255: step 556, loss 0.708209, acc 0.75
2017-09-09T15:23:24.188470: step 557, loss 0.835124, acc 0.625
2017-09-09T15:23:24.418785: step 558, loss 0.761931, acc 0.765625
2017-09-09T15:23:24.657589: step 559, loss 0.792657, acc 0.734375
2017-09-09T15:23:24.931237: step 560, loss 0.70218, acc 0.78125
2017-09-09T15:23:25.197915: step 561, loss 0.696052, acc 0.765625
2017-09-09T15:23:25.438864: step 562, loss 0.702804, acc 0.796875
2017-09-09T15:23:25.702708: step 563, loss 0.71609, acc 0.796875
2017-09-09T15:23:25.947633: step 564, loss 0.652448, acc 0.8125
2017-09-09T15:23:26.250453: step 565, loss 0.703584, acc 0.75
2017-09-09T15:23:26.521198: step 566, loss 0.749753, acc 0.75
2017-09-09T15:23:26.788366: step 567, loss 0.785153, acc 0.6875
2017-09-09T15:23:27.032830: step 568, loss 0.710585, acc 0.765625
2017-09-09T15:23:27.297983: step 569, loss 0.742276, acc 0.703125
2017-09-09T15:23:27.583073: step 570, loss 0.736628, acc 0.734375
2017-09-09T15:23:27.815528: step 571, loss 0.665855, acc 0.796875
2017-09-09T15:23:28.070718: step 572, loss 0.84919, acc 0.671875
2017-09-09T15:23:28.340932: step 573, loss 0.733447, acc 0.765625
2017-09-09T15:23:28.616322: step 574, loss 0.788966, acc 0.671875
2017-09-09T15:23:28.855738: step 575, loss 0.761002, acc 0.734375
2017-09-09T15:23:29.135451: step 576, loss 0.745251, acc 0.78125
2017-09-09T15:23:29.402156: step 577, loss 0.784182, acc 0.765625
2017-09-09T15:23:29.670974: step 578, loss 0.654924, acc 0.796875
2017-09-09T15:23:30.009316: step 579, loss 0.793049, acc 0.8125
2017-09-09T15:23:30.293635: step 580, loss 0.733912, acc 0.765625
2017-09-09T15:23:30.558950: step 581, loss 0.695132, acc 0.8125
2017-09-09T15:23:30.810076: step 582, loss 0.704909, acc 0.734375
2017-09-09T15:23:31.096220: step 583, loss 0.718302, acc 0.828125
2017-09-09T15:23:31.382079: step 584, loss 0.618502, acc 0.796875
2017-09-09T15:23:31.627456: step 585, loss 0.739476, acc 0.6875
2017-09-09T15:23:31.905440: step 586, loss 0.811267, acc 0.625
2017-09-09T15:23:32.167304: step 587, loss 0.707496, acc 0.71875
2017-09-09T15:23:32.408563: step 588, loss 0.578349, acc 0.823529
2017-09-09T15:23:32.674702: step 589, loss 0.607101, acc 0.796875
2017-09-09T15:23:32.950969: step 590, loss 0.707791, acc 0.8125
2017-09-09T15:23:33.188532: step 591, loss 0.759086, acc 0.765625
2017-09-09T15:23:33.473314: step 592, loss 0.472829, acc 0.875
2017-09-09T15:23:33.760865: step 593, loss 0.610346, acc 0.84375
2017-09-09T15:23:34.036546: step 594, loss 0.507964, acc 0.890625
2017-09-09T15:23:34.288678: step 595, loss 0.687923, acc 0.796875
2017-09-09T15:23:34.534669: step 596, loss 0.561111, acc 0.84375
2017-09-09T15:23:34.853039: step 597, loss 0.594948, acc 0.84375
2017-09-09T15:23:35.083243: step 598, loss 0.59814, acc 0.8125
2017-09-09T15:23:35.313026: step 599, loss 0.699505, acc 0.71875
2017-09-09T15:23:35.547575: step 600, loss 0.662837, acc 0.78125

Evaluation:
2017-09-09T15:23:35.599448: step 600, loss 1.51838, acc 0.316547

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-600

2017-09-09T15:23:38.435810: step 601, loss 0.490992, acc 0.84375
2017-09-09T15:23:38.701221: step 602, loss 0.624969, acc 0.765625
2017-09-09T15:23:38.933364: step 603, loss 0.723626, acc 0.734375
2017-09-09T15:23:39.203413: step 604, loss 0.72204, acc 0.78125
2017-09-09T15:23:39.456287: step 605, loss 0.534106, acc 0.890625
2017-09-09T15:23:39.709069: step 606, loss 0.597959, acc 0.765625
2017-09-09T15:23:39.985886: step 607, loss 0.476704, acc 0.890625
2017-09-09T15:23:40.218969: step 608, loss 0.558515, acc 0.8125
2017-09-09T15:23:40.464312: step 609, loss 0.593279, acc 0.84375
2017-09-09T15:23:40.766437: step 610, loss 0.583178, acc 0.75
2017-09-09T15:23:41.008556: step 611, loss 0.521788, acc 0.84375
2017-09-09T15:23:41.255242: step 612, loss 0.730855, acc 0.65625
2017-09-09T15:23:41.532205: step 613, loss 0.759573, acc 0.71875
2017-09-09T15:23:41.782862: step 614, loss 0.615695, acc 0.796875
2017-09-09T15:23:42.045892: step 615, loss 0.529591, acc 0.875
2017-09-09T15:23:42.339342: step 616, loss 0.6416, acc 0.78125
2017-09-09T15:23:42.626564: step 617, loss 0.573752, acc 0.84375
2017-09-09T15:23:42.865415: step 618, loss 0.47137, acc 0.875
2017-09-09T15:23:43.141046: step 619, loss 0.590112, acc 0.828125
2017-09-09T15:23:43.430523: step 620, loss 0.540452, acc 0.84375
2017-09-09T15:23:43.689920: step 621, loss 0.604114, acc 0.84375
2017-09-09T15:23:43.925522: step 622, loss 0.546464, acc 0.796875
2017-09-09T15:23:44.213413: step 623, loss 0.604754, acc 0.8125
2017-09-09T15:23:44.468932: step 624, loss 0.652155, acc 0.78125
2017-09-09T15:23:44.711148: step 625, loss 0.502893, acc 0.859375
2017-09-09T15:23:44.997346: step 626, loss 0.557421, acc 0.921875
2017-09-09T15:23:45.264410: step 627, loss 0.529142, acc 0.859375
2017-09-09T15:23:45.498049: step 628, loss 0.637695, acc 0.78125
2017-09-09T15:23:45.782057: step 629, loss 0.483587, acc 0.921875
2017-09-09T15:23:46.056969: step 630, loss 0.620705, acc 0.8125
2017-09-09T15:23:46.302510: step 631, loss 0.603873, acc 0.828125
2017-09-09T15:23:46.578214: step 632, loss 0.548858, acc 0.828125
2017-09-09T15:23:46.858270: step 633, loss 0.65593, acc 0.734375
2017-09-09T15:23:47.111057: step 634, loss 0.578506, acc 0.8125
2017-09-09T15:23:47.377758: step 635, loss 0.492517, acc 0.875
2017-09-09T15:23:47.672828: step 636, loss 0.568279, acc 0.859375
2017-09-09T15:23:47.943410: step 637, loss 0.539821, acc 0.875
2017-09-09T15:23:48.194274: step 638, loss 0.653937, acc 0.734375
2017-09-09T15:23:48.478992: step 639, loss 0.474183, acc 0.921875
2017-09-09T15:23:48.714167: step 640, loss 0.456403, acc 0.875
2017-09-09T15:23:48.991910: step 641, loss 0.536698, acc 0.84375
2017-09-09T15:23:49.270417: step 642, loss 0.554817, acc 0.828125
2017-09-09T15:23:49.512852: step 643, loss 0.494057, acc 0.828125
2017-09-09T15:23:49.764439: step 644, loss 0.386707, acc 0.84375
2017-09-09T15:23:50.030381: step 645, loss 0.603756, acc 0.859375
2017-09-09T15:23:50.302507: step 646, loss 0.44681, acc 0.875
2017-09-09T15:23:50.554304: step 647, loss 0.463411, acc 0.921875
2017-09-09T15:23:50.805785: step 648, loss 0.674758, acc 0.765625
2017-09-09T15:23:51.059530: step 649, loss 0.467882, acc 0.890625
2017-09-09T15:23:51.311823: step 650, loss 0.697537, acc 0.78125

Evaluation:
2017-09-09T15:23:51.416342: step 650, loss 1.52148, acc 0.326619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-650

2017-09-09T15:23:53.390892: step 651, loss 0.525844, acc 0.875
2017-09-09T15:23:53.672582: step 652, loss 0.389309, acc 0.9375
2017-09-09T15:23:53.914349: step 653, loss 0.634504, acc 0.828125
2017-09-09T15:23:54.163654: step 654, loss 0.480516, acc 0.890625
2017-09-09T15:23:54.437153: step 655, loss 0.475509, acc 0.890625
2017-09-09T15:23:54.685449: step 656, loss 0.509284, acc 0.859375
2017-09-09T15:23:54.952337: step 657, loss 0.62806, acc 0.8125
2017-09-09T15:23:55.209727: step 658, loss 0.538727, acc 0.84375
2017-09-09T15:23:55.438117: step 659, loss 0.482704, acc 0.890625
2017-09-09T15:23:55.812509: step 660, loss 0.622717, acc 0.78125
2017-09-09T15:23:56.041267: step 661, loss 0.52356, acc 0.875
2017-09-09T15:23:56.269356: step 662, loss 0.572557, acc 0.84375
2017-09-09T15:23:56.496736: step 663, loss 0.623899, acc 0.78125
2017-09-09T15:23:56.726654: step 664, loss 0.485986, acc 0.875
2017-09-09T15:23:56.958453: step 665, loss 0.512926, acc 0.859375
2017-09-09T15:23:57.209004: step 666, loss 0.543014, acc 0.859375
2017-09-09T15:23:57.437014: step 667, loss 0.5565, acc 0.890625
2017-09-09T15:23:57.663959: step 668, loss 0.565426, acc 0.890625
2017-09-09T15:23:57.946784: step 669, loss 0.61171, acc 0.828125
2017-09-09T15:23:58.203956: step 670, loss 0.524407, acc 0.859375
2017-09-09T15:23:58.439934: step 671, loss 0.543604, acc 0.828125
2017-09-09T15:23:58.696938: step 672, loss 0.551311, acc 0.8125
2017-09-09T15:23:58.930072: step 673, loss 0.587174, acc 0.8125
2017-09-09T15:23:59.186001: step 674, loss 0.356599, acc 0.953125
2017-09-09T15:23:59.431206: step 675, loss 0.562906, acc 0.796875
2017-09-09T15:23:59.687023: step 676, loss 0.388781, acc 0.953125
2017-09-09T15:23:59.948878: step 677, loss 0.685049, acc 0.78125
2017-09-09T15:24:00.248138: step 678, loss 0.430297, acc 0.890625
2017-09-09T15:24:00.521903: step 679, loss 0.429062, acc 0.875
2017-09-09T15:24:00.778854: step 680, loss 0.468965, acc 0.859375
2017-09-09T15:24:01.048763: step 681, loss 0.494452, acc 0.875
2017-09-09T15:24:01.319336: step 682, loss 0.367699, acc 0.890625
2017-09-09T15:24:01.564636: step 683, loss 0.543682, acc 0.828125
2017-09-09T15:24:01.828116: step 684, loss 0.498457, acc 0.90625
2017-09-09T15:24:02.071290: step 685, loss 0.563361, acc 0.828125
2017-09-09T15:24:02.319470: step 686, loss 0.498109, acc 0.901961
2017-09-09T15:24:02.619032: step 687, loss 0.370747, acc 0.921875
2017-09-09T15:24:02.850414: step 688, loss 0.458542, acc 0.921875
2017-09-09T15:24:03.121138: step 689, loss 0.439204, acc 0.890625
2017-09-09T15:24:03.400698: step 690, loss 0.562679, acc 0.84375
2017-09-09T15:24:03.665492: step 691, loss 0.454738, acc 0.828125
2017-09-09T15:24:03.915221: step 692, loss 0.378485, acc 0.90625
2017-09-09T15:24:04.195413: step 693, loss 0.409444, acc 0.9375
2017-09-09T15:24:04.454515: step 694, loss 0.451679, acc 0.859375
2017-09-09T15:24:04.709584: step 695, loss 0.521827, acc 0.84375
2017-09-09T15:24:04.965898: step 696, loss 0.576006, acc 0.8125
2017-09-09T15:24:05.232827: step 697, loss 0.415054, acc 0.875
2017-09-09T15:24:05.469494: step 698, loss 0.415857, acc 0.875
2017-09-09T15:24:05.754866: step 699, loss 0.522417, acc 0.859375
2017-09-09T15:24:06.029487: step 700, loss 0.485182, acc 0.796875

Evaluation:
2017-09-09T15:24:06.088354: step 700, loss 1.53988, acc 0.322302

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-700

2017-09-09T15:24:08.069820: step 701, loss 0.564569, acc 0.84375
2017-09-09T15:24:08.354902: step 702, loss 0.501469, acc 0.875
2017-09-09T15:24:08.615228: step 703, loss 0.503808, acc 0.875
2017-09-09T15:24:08.849099: step 704, loss 0.414969, acc 0.84375
2017-09-09T15:24:09.146148: step 705, loss 0.432895, acc 0.921875
2017-09-09T15:24:09.380958: step 706, loss 0.652895, acc 0.796875
2017-09-09T15:24:09.643405: step 707, loss 0.317483, acc 0.921875
2017-09-09T15:24:09.925185: step 708, loss 0.522449, acc 0.859375
2017-09-09T15:24:10.212647: step 709, loss 0.498921, acc 0.8125
2017-09-09T15:24:10.471374: step 710, loss 0.386931, acc 0.875
2017-09-09T15:24:10.765177: step 711, loss 0.476996, acc 0.890625
2017-09-09T15:24:11.035871: step 712, loss 0.397846, acc 0.859375
2017-09-09T15:24:11.272655: step 713, loss 0.400496, acc 0.9375
2017-09-09T15:24:11.563310: step 714, loss 0.493719, acc 0.9375
2017-09-09T15:24:11.850890: step 715, loss 0.410592, acc 0.921875
2017-09-09T15:24:12.120987: step 716, loss 0.389079, acc 0.9375
2017-09-09T15:24:12.371084: step 717, loss 0.459827, acc 0.84375
2017-09-09T15:24:12.658286: step 718, loss 0.41642, acc 0.875
2017-09-09T15:24:12.903322: step 719, loss 0.431905, acc 0.859375
2017-09-09T15:24:13.146400: step 720, loss 0.509393, acc 0.828125
2017-09-09T15:24:13.431330: step 721, loss 0.373863, acc 0.890625
2017-09-09T15:24:13.707677: step 722, loss 0.369374, acc 0.9375
2017-09-09T15:24:13.950374: step 723, loss 0.287251, acc 0.921875
2017-09-09T15:24:14.223339: step 724, loss 0.536272, acc 0.84375
2017-09-09T15:24:14.498710: step 725, loss 0.349732, acc 0.921875
2017-09-09T15:24:14.780673: step 726, loss 0.455957, acc 0.90625
2017-09-09T15:24:15.038755: step 727, loss 0.410348, acc 0.890625
2017-09-09T15:24:15.332643: step 728, loss 0.403512, acc 0.859375
2017-09-09T15:24:15.613730: step 729, loss 0.337761, acc 0.9375
2017-09-09T15:24:15.870407: step 730, loss 0.435224, acc 0.859375
2017-09-09T15:24:16.148976: step 731, loss 0.390242, acc 0.84375
2017-09-09T15:24:16.395354: step 732, loss 0.449295, acc 0.890625
2017-09-09T15:24:16.660912: step 733, loss 0.395793, acc 0.890625
2017-09-09T15:24:16.938245: step 734, loss 0.342436, acc 0.90625
2017-09-09T15:24:17.180820: step 735, loss 0.358102, acc 0.921875
2017-09-09T15:24:17.437613: step 736, loss 0.364943, acc 0.90625
2017-09-09T15:24:17.715829: step 737, loss 0.470973, acc 0.875
2017-09-09T15:24:17.953172: step 738, loss 0.455095, acc 0.890625
2017-09-09T15:24:18.286736: step 739, loss 0.437002, acc 0.84375
2017-09-09T15:24:18.516704: step 740, loss 0.407325, acc 0.890625
2017-09-09T15:24:18.742189: step 741, loss 0.410155, acc 0.890625
2017-09-09T15:24:18.982793: step 742, loss 0.467248, acc 0.828125
2017-09-09T15:24:19.211489: step 743, loss 0.380442, acc 0.84375
2017-09-09T15:24:19.444321: step 744, loss 0.359619, acc 0.90625
2017-09-09T15:24:19.730440: step 745, loss 0.300081, acc 0.953125
2017-09-09T15:24:19.978704: step 746, loss 0.401775, acc 0.859375
2017-09-09T15:24:20.232542: step 747, loss 0.396937, acc 0.890625
2017-09-09T15:24:20.519848: step 748, loss 0.499271, acc 0.796875
2017-09-09T15:24:20.788063: step 749, loss 0.460492, acc 0.859375
2017-09-09T15:24:21.047444: step 750, loss 0.428091, acc 0.90625

Evaluation:
2017-09-09T15:24:21.146998: step 750, loss 1.51269, acc 0.322302

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-750

2017-09-09T15:24:24.374438: step 751, loss 0.364465, acc 0.875
2017-09-09T15:24:24.686708: step 752, loss 0.311108, acc 0.953125
2017-09-09T15:24:24.953413: step 753, loss 0.41249, acc 0.890625
2017-09-09T15:24:25.203172: step 754, loss 0.392398, acc 0.9375
2017-09-09T15:24:25.505597: step 755, loss 0.386834, acc 0.90625
2017-09-09T15:24:25.753027: step 756, loss 0.39219, acc 0.90625
2017-09-09T15:24:25.999890: step 757, loss 0.474635, acc 0.875
2017-09-09T15:24:26.287295: step 758, loss 0.376736, acc 0.9375
2017-09-09T15:24:26.529730: step 759, loss 0.387813, acc 0.859375
2017-09-09T15:24:26.782291: step 760, loss 0.477205, acc 0.875
2017-09-09T15:24:27.056291: step 761, loss 0.389109, acc 0.859375
2017-09-09T15:24:27.377881: step 762, loss 0.372387, acc 0.875
2017-09-09T15:24:27.629408: step 763, loss 0.348745, acc 0.953125
2017-09-09T15:24:27.875573: step 764, loss 0.276294, acc 0.953125
2017-09-09T15:24:28.174427: step 765, loss 0.434635, acc 0.875
2017-09-09T15:24:28.427067: step 766, loss 0.323887, acc 0.9375
2017-09-09T15:24:28.674805: step 767, loss 0.363091, acc 0.90625
2017-09-09T15:24:28.967742: step 768, loss 0.347994, acc 0.875
2017-09-09T15:24:29.253150: step 769, loss 0.339971, acc 0.875
2017-09-09T15:24:29.539301: step 770, loss 0.331224, acc 0.90625
2017-09-09T15:24:29.830778: step 771, loss 0.398588, acc 0.921875
2017-09-09T15:24:30.156087: step 772, loss 0.314579, acc 0.953125
2017-09-09T15:24:30.418191: step 773, loss 0.338456, acc 0.921875
2017-09-09T15:24:30.656897: step 774, loss 0.393508, acc 0.9375
2017-09-09T15:24:30.930385: step 775, loss 0.414666, acc 0.890625
2017-09-09T15:24:31.227674: step 776, loss 0.474887, acc 0.890625
2017-09-09T15:24:31.465399: step 777, loss 0.319548, acc 0.90625
2017-09-09T15:24:31.724997: step 778, loss 0.325758, acc 0.921875
2017-09-09T15:24:32.025456: step 779, loss 0.260084, acc 0.984375
2017-09-09T15:24:32.264264: step 780, loss 0.569759, acc 0.8125
2017-09-09T15:24:32.547944: step 781, loss 0.32677, acc 0.875
2017-09-09T15:24:32.844390: step 782, loss 0.382697, acc 0.875
2017-09-09T15:24:33.087155: step 783, loss 0.352194, acc 0.890625
2017-09-09T15:24:33.337475: step 784, loss 0.388643, acc 0.901961
2017-09-09T15:24:33.621824: step 785, loss 0.366156, acc 0.90625
2017-09-09T15:24:33.915030: step 786, loss 0.312611, acc 0.953125
2017-09-09T15:24:34.185718: step 787, loss 0.417374, acc 0.875
2017-09-09T15:24:34.448810: step 788, loss 0.280566, acc 0.96875
2017-09-09T15:24:34.733505: step 789, loss 0.351408, acc 0.90625
2017-09-09T15:24:34.981241: step 790, loss 0.303219, acc 0.9375
2017-09-09T15:24:35.225107: step 791, loss 0.26628, acc 0.953125
2017-09-09T15:24:35.532947: step 792, loss 0.350429, acc 0.890625
2017-09-09T15:24:35.805529: step 793, loss 0.384425, acc 0.875
2017-09-09T15:24:36.062065: step 794, loss 0.33395, acc 0.921875
2017-09-09T15:24:36.327301: step 795, loss 0.245049, acc 0.9375
2017-09-09T15:24:36.627154: step 796, loss 0.392706, acc 0.84375
2017-09-09T15:24:36.899915: step 797, loss 0.313299, acc 0.9375
2017-09-09T15:24:37.142834: step 798, loss 0.414985, acc 0.875
2017-09-09T15:24:37.426671: step 799, loss 0.291196, acc 0.953125
2017-09-09T15:24:37.708406: step 800, loss 0.338467, acc 0.921875

Evaluation:
2017-09-09T15:24:37.771630: step 800, loss 1.50312, acc 0.320863

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-800

2017-09-09T15:24:39.442884: step 801, loss 0.404867, acc 0.890625
2017-09-09T15:24:39.673271: step 802, loss 0.324013, acc 0.90625
2017-09-09T15:24:39.923592: step 803, loss 0.34774, acc 0.890625
2017-09-09T15:24:40.154131: step 804, loss 0.36476, acc 0.875
2017-09-09T15:24:40.394817: step 805, loss 0.230421, acc 0.96875
2017-09-09T15:24:40.687387: step 806, loss 0.386393, acc 0.921875
2017-09-09T15:24:40.962420: step 807, loss 0.27969, acc 0.921875
2017-09-09T15:24:41.234461: step 808, loss 0.338991, acc 0.9375
2017-09-09T15:24:41.486485: step 809, loss 0.287167, acc 0.9375
2017-09-09T15:24:41.756926: step 810, loss 0.308023, acc 0.90625
2017-09-09T15:24:42.056689: step 811, loss 0.367991, acc 0.921875
2017-09-09T15:24:42.296336: step 812, loss 0.310247, acc 0.9375
2017-09-09T15:24:42.566277: step 813, loss 0.279828, acc 0.921875
2017-09-09T15:24:42.830034: step 814, loss 0.317795, acc 0.890625
2017-09-09T15:24:43.123901: step 815, loss 0.264626, acc 0.953125
2017-09-09T15:24:43.381012: step 816, loss 0.326602, acc 0.9375
2017-09-09T15:24:43.674396: step 817, loss 0.297863, acc 0.921875
2017-09-09T15:24:43.937831: step 818, loss 0.365562, acc 0.921875
2017-09-09T15:24:44.207404: step 819, loss 0.363389, acc 0.96875
2017-09-09T15:24:44.483677: step 820, loss 0.334637, acc 0.921875
2017-09-09T15:24:44.762212: step 821, loss 0.295678, acc 0.921875
2017-09-09T15:24:45.011116: step 822, loss 0.374013, acc 0.90625
2017-09-09T15:24:45.311608: step 823, loss 0.248128, acc 0.984375
2017-09-09T15:24:45.588193: step 824, loss 0.213655, acc 0.984375
2017-09-09T15:24:45.828422: step 825, loss 0.302962, acc 0.953125
2017-09-09T15:24:46.089377: step 826, loss 0.246165, acc 0.921875
2017-09-09T15:24:46.398286: step 827, loss 0.339905, acc 0.875
2017-09-09T15:24:46.650962: step 828, loss 0.276699, acc 0.96875
2017-09-09T15:24:46.911090: step 829, loss 0.328328, acc 0.921875
2017-09-09T15:24:47.207658: step 830, loss 0.295862, acc 0.921875
2017-09-09T15:24:47.486699: step 831, loss 0.349272, acc 0.90625
2017-09-09T15:24:47.725686: step 832, loss 0.319215, acc 0.9375
2017-09-09T15:24:47.992533: step 833, loss 0.241418, acc 0.96875
2017-09-09T15:24:48.262471: step 834, loss 0.284822, acc 0.921875
2017-09-09T15:24:48.517352: step 835, loss 0.335295, acc 0.921875
2017-09-09T15:24:48.772507: step 836, loss 0.313565, acc 0.953125
2017-09-09T15:24:49.072212: step 837, loss 0.279142, acc 0.90625
2017-09-09T15:24:49.342926: step 838, loss 0.404974, acc 0.890625
2017-09-09T15:24:49.592621: step 839, loss 0.262985, acc 0.96875
2017-09-09T15:24:49.898753: step 840, loss 0.32389, acc 0.921875
2017-09-09T15:24:50.177114: step 841, loss 0.397902, acc 0.859375
2017-09-09T15:24:50.459429: step 842, loss 0.312569, acc 0.921875
2017-09-09T15:24:50.767707: step 843, loss 0.25989, acc 0.953125
2017-09-09T15:24:51.070831: step 844, loss 0.381521, acc 0.875
2017-09-09T15:24:51.346708: step 845, loss 0.246931, acc 0.9375
2017-09-09T15:24:51.584645: step 846, loss 0.315057, acc 0.90625
2017-09-09T15:24:51.884329: step 847, loss 0.250996, acc 0.984375
2017-09-09T15:24:52.176497: step 848, loss 0.217126, acc 0.953125
2017-09-09T15:24:52.430293: step 849, loss 0.300413, acc 0.921875
2017-09-09T15:24:52.704529: step 850, loss 0.37794, acc 0.890625

Evaluation:
2017-09-09T15:24:52.757088: step 850, loss 1.49943, acc 0.328058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-850

2017-09-09T15:24:55.200950: step 851, loss 0.422185, acc 0.859375
2017-09-09T15:24:55.472359: step 852, loss 0.256584, acc 0.921875
2017-09-09T15:24:55.712992: step 853, loss 0.290844, acc 0.953125
2017-09-09T15:24:55.992487: step 854, loss 0.377233, acc 0.890625
2017-09-09T15:24:56.275365: step 855, loss 0.253385, acc 0.9375
2017-09-09T15:24:56.543275: step 856, loss 0.240713, acc 0.921875
2017-09-09T15:24:56.789960: step 857, loss 0.240848, acc 0.953125
2017-09-09T15:24:57.098542: step 858, loss 0.223607, acc 0.96875
2017-09-09T15:24:57.356743: step 859, loss 0.318208, acc 0.921875
2017-09-09T15:24:57.609999: step 860, loss 0.31865, acc 0.9375
2017-09-09T15:24:57.904101: step 861, loss 0.304264, acc 0.921875
2017-09-09T15:24:58.155855: step 862, loss 0.313994, acc 0.90625
2017-09-09T15:24:58.412121: step 863, loss 0.351193, acc 0.890625
2017-09-09T15:24:58.714450: step 864, loss 0.28353, acc 0.9375
2017-09-09T15:24:59.000825: step 865, loss 0.385722, acc 0.859375
2017-09-09T15:24:59.243591: step 866, loss 0.236072, acc 0.953125
2017-09-09T15:24:59.489285: step 867, loss 0.341472, acc 0.90625
2017-09-09T15:24:59.729946: step 868, loss 0.201382, acc 0.96875
2017-09-09T15:25:00.021607: step 869, loss 0.280045, acc 0.90625
2017-09-09T15:25:00.249715: step 870, loss 0.251405, acc 0.96875
2017-09-09T15:25:00.478863: step 871, loss 0.323872, acc 0.9375
2017-09-09T15:25:00.717272: step 872, loss 0.277041, acc 0.9375
2017-09-09T15:25:00.947791: step 873, loss 0.301937, acc 0.921875
2017-09-09T15:25:01.182625: step 874, loss 0.279912, acc 0.90625
2017-09-09T15:25:01.408361: step 875, loss 0.368963, acc 0.875
2017-09-09T15:25:01.649585: step 876, loss 0.230014, acc 0.953125
2017-09-09T15:25:01.931707: step 877, loss 0.406718, acc 0.84375
2017-09-09T15:25:02.231024: step 878, loss 0.177537, acc 0.96875
2017-09-09T15:25:02.470345: step 879, loss 0.30861, acc 0.921875
2017-09-09T15:25:02.756975: step 880, loss 0.273096, acc 0.921875
2017-09-09T15:25:03.053661: step 881, loss 0.266562, acc 0.9375
2017-09-09T15:25:03.299081: step 882, loss 0.247377, acc 0.921569
2017-09-09T15:25:03.568709: step 883, loss 0.350491, acc 0.921875
2017-09-09T15:25:03.846644: step 884, loss 0.233234, acc 0.953125
2017-09-09T15:25:04.122559: step 885, loss 0.211176, acc 0.96875
2017-09-09T15:25:04.389754: step 886, loss 0.313902, acc 0.9375
2017-09-09T15:25:04.640747: step 887, loss 0.275107, acc 0.9375
2017-09-09T15:25:04.916361: step 888, loss 0.161395, acc 0.984375
2017-09-09T15:25:05.159495: step 889, loss 0.375322, acc 0.890625
2017-09-09T15:25:05.426172: step 890, loss 0.299636, acc 0.9375
2017-09-09T15:25:05.688802: step 891, loss 0.144209, acc 0.984375
2017-09-09T15:25:05.953767: step 892, loss 0.272004, acc 0.921875
2017-09-09T15:25:06.244303: step 893, loss 0.214784, acc 0.96875
2017-09-09T15:25:06.487772: step 894, loss 0.247828, acc 0.953125
2017-09-09T15:25:06.749179: step 895, loss 0.249398, acc 0.96875
2017-09-09T15:25:07.048589: step 896, loss 0.239432, acc 0.921875
2017-09-09T15:25:07.319777: step 897, loss 0.29131, acc 0.953125
2017-09-09T15:25:07.566177: step 898, loss 0.215789, acc 0.921875
2017-09-09T15:25:07.828886: step 899, loss 0.260796, acc 0.953125
2017-09-09T15:25:08.108343: step 900, loss 0.220272, acc 0.921875

Evaluation:
2017-09-09T15:25:08.163147: step 900, loss 1.49623, acc 0.328058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-900

2017-09-09T15:25:09.946554: step 901, loss 0.176006, acc 0.953125
2017-09-09T15:25:10.195552: step 902, loss 0.394818, acc 0.859375
2017-09-09T15:25:10.481753: step 903, loss 0.298034, acc 0.875
2017-09-09T15:25:10.727970: step 904, loss 0.3119, acc 0.953125
2017-09-09T15:25:10.977355: step 905, loss 0.280235, acc 0.9375
2017-09-09T15:25:11.258825: step 906, loss 0.318029, acc 0.84375
2017-09-09T15:25:11.505063: step 907, loss 0.232701, acc 0.9375
2017-09-09T15:25:11.769068: step 908, loss 0.213681, acc 0.921875
2017-09-09T15:25:12.072903: step 909, loss 0.267348, acc 0.890625
2017-09-09T15:25:12.354414: step 910, loss 0.335058, acc 0.921875
2017-09-09T15:25:12.633973: step 911, loss 0.204602, acc 0.984375
2017-09-09T15:25:12.893086: step 912, loss 0.2, acc 0.984375
2017-09-09T15:25:13.184196: step 913, loss 0.241605, acc 0.9375
2017-09-09T15:25:13.462690: step 914, loss 0.295808, acc 0.921875
2017-09-09T15:25:13.720882: step 915, loss 0.215356, acc 0.9375
2017-09-09T15:25:13.997212: step 916, loss 0.266227, acc 0.9375
2017-09-09T15:25:14.289590: step 917, loss 0.272978, acc 0.953125
2017-09-09T15:25:14.539534: step 918, loss 0.29458, acc 0.96875
2017-09-09T15:25:14.810280: step 919, loss 0.353673, acc 0.875
2017-09-09T15:25:15.114227: step 920, loss 0.232299, acc 0.9375
2017-09-09T15:25:15.392060: step 921, loss 0.229294, acc 0.984375
2017-09-09T15:25:15.643391: step 922, loss 0.211574, acc 0.9375
2017-09-09T15:25:15.950103: step 923, loss 0.1642, acc 0.953125
2017-09-09T15:25:16.241863: step 924, loss 0.225704, acc 0.96875
2017-09-09T15:25:16.506186: step 925, loss 0.228355, acc 0.953125
2017-09-09T15:25:16.778169: step 926, loss 0.198364, acc 0.984375
2017-09-09T15:25:17.041581: step 927, loss 0.15955, acc 0.984375
2017-09-09T15:25:17.315854: step 928, loss 0.22496, acc 0.9375
2017-09-09T15:25:17.578860: step 929, loss 0.227223, acc 0.953125
2017-09-09T15:25:17.914871: step 930, loss 0.243052, acc 0.9375
2017-09-09T15:25:18.181919: step 931, loss 0.185944, acc 1
2017-09-09T15:25:18.418672: step 932, loss 0.167761, acc 0.953125
2017-09-09T15:25:18.698149: step 933, loss 0.161361, acc 0.984375
2017-09-09T15:25:18.950155: step 934, loss 0.222542, acc 0.953125
2017-09-09T15:25:19.244581: step 935, loss 0.1979, acc 0.96875
2017-09-09T15:25:19.535093: step 936, loss 0.186606, acc 0.984375
2017-09-09T15:25:19.803354: step 937, loss 0.200492, acc 0.96875
2017-09-09T15:25:20.054588: step 938, loss 0.265314, acc 0.953125
2017-09-09T15:25:20.342858: step 939, loss 0.24523, acc 0.953125
2017-09-09T15:25:20.581218: step 940, loss 0.184069, acc 0.9375
2017-09-09T15:25:20.829042: step 941, loss 0.195356, acc 0.9375
2017-09-09T15:25:21.107951: step 942, loss 0.170787, acc 0.96875
2017-09-09T15:25:21.347089: step 943, loss 0.287088, acc 0.9375
2017-09-09T15:25:21.665931: step 944, loss 0.281269, acc 0.921875
2017-09-09T15:25:21.905123: step 945, loss 0.291969, acc 0.890625
2017-09-09T15:25:22.151260: step 946, loss 0.278212, acc 0.953125
2017-09-09T15:25:22.379642: step 947, loss 0.297437, acc 0.890625
2017-09-09T15:25:22.613400: step 948, loss 0.239158, acc 0.953125
2017-09-09T15:25:22.850823: step 949, loss 0.199119, acc 0.953125
2017-09-09T15:25:23.136550: step 950, loss 0.165398, acc 0.984375

Evaluation:
2017-09-09T15:25:23.187276: step 950, loss 1.55497, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-950

2017-09-09T15:25:25.574640: step 951, loss 0.225315, acc 0.9375
2017-09-09T15:25:25.827047: step 952, loss 0.218945, acc 0.953125
2017-09-09T15:25:26.106620: step 953, loss 0.163758, acc 0.96875
2017-09-09T15:25:26.392741: step 954, loss 0.332927, acc 0.875
2017-09-09T15:25:26.649325: step 955, loss 0.218757, acc 0.953125
2017-09-09T15:25:26.917468: step 956, loss 0.214888, acc 0.953125
2017-09-09T15:25:27.190037: step 957, loss 0.264986, acc 0.9375
2017-09-09T15:25:27.431115: step 958, loss 0.203807, acc 0.96875
2017-09-09T15:25:27.684070: step 959, loss 0.205067, acc 0.953125
2017-09-09T15:25:27.983657: step 960, loss 0.177274, acc 0.96875
2017-09-09T15:25:28.252088: step 961, loss 0.19799, acc 0.921875
2017-09-09T15:25:28.533719: step 962, loss 0.320736, acc 0.875
2017-09-09T15:25:28.827974: step 963, loss 0.165231, acc 0.96875
2017-09-09T15:25:29.063902: step 964, loss 0.310519, acc 0.890625
2017-09-09T15:25:29.323605: step 965, loss 0.311088, acc 0.953125
2017-09-09T15:25:29.618329: step 966, loss 0.175852, acc 0.96875
2017-09-09T15:25:29.892797: step 967, loss 0.222923, acc 0.953125
2017-09-09T15:25:30.139496: step 968, loss 0.227772, acc 0.953125
2017-09-09T15:25:30.424751: step 969, loss 0.219873, acc 0.921875
2017-09-09T15:25:30.739386: step 970, loss 0.205022, acc 0.9375
2017-09-09T15:25:31.014454: step 971, loss 0.142474, acc 0.96875
2017-09-09T15:25:31.282548: step 972, loss 0.171856, acc 0.984375
2017-09-09T15:25:31.564797: step 973, loss 0.177807, acc 0.96875
2017-09-09T15:25:31.822731: step 974, loss 0.176225, acc 0.9375
2017-09-09T15:25:32.073902: step 975, loss 0.090269, acc 0.984375
2017-09-09T15:25:32.340229: step 976, loss 0.180344, acc 0.984375
2017-09-09T15:25:32.629449: step 977, loss 0.322591, acc 0.875
2017-09-09T15:25:32.883328: step 978, loss 0.152991, acc 0.984375
2017-09-09T15:25:33.169127: step 979, loss 0.134467, acc 0.984375
2017-09-09T15:25:33.447213: step 980, loss 0.301292, acc 0.882353
2017-09-09T15:25:33.750034: step 981, loss 0.1998, acc 0.921875
2017-09-09T15:25:34.036464: step 982, loss 0.191259, acc 0.953125
2017-09-09T15:25:34.364925: step 983, loss 0.122434, acc 0.984375
2017-09-09T15:25:34.615568: step 984, loss 0.132262, acc 0.984375
2017-09-09T15:25:34.887758: step 985, loss 0.269394, acc 0.953125
2017-09-09T15:25:35.150337: step 986, loss 0.194968, acc 0.96875
2017-09-09T15:25:35.438608: step 987, loss 0.146127, acc 0.96875
2017-09-09T15:25:35.688070: step 988, loss 0.192959, acc 0.96875
2017-09-09T15:25:35.937737: step 989, loss 0.212321, acc 0.96875
2017-09-09T15:25:36.226745: step 990, loss 0.152087, acc 0.984375
2017-09-09T15:25:36.501100: step 991, loss 0.260371, acc 0.921875
2017-09-09T15:25:36.755638: step 992, loss 0.204486, acc 0.90625
2017-09-09T15:25:37.050401: step 993, loss 0.246096, acc 0.984375
2017-09-09T15:25:37.363335: step 994, loss 0.180721, acc 0.9375
2017-09-09T15:25:37.606431: step 995, loss 0.13121, acc 0.9375
2017-09-09T15:25:37.908638: step 996, loss 0.161019, acc 0.953125
2017-09-09T15:25:38.189240: step 997, loss 0.269228, acc 0.90625
2017-09-09T15:25:38.466967: step 998, loss 0.102977, acc 1
2017-09-09T15:25:38.724307: step 999, loss 0.194287, acc 0.96875
2017-09-09T15:25:39.019125: step 1000, loss 0.219693, acc 0.96875

Evaluation:
2017-09-09T15:25:39.074384: step 1000, loss 1.53471, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1000

2017-09-09T15:25:42.136997: step 1001, loss 0.211325, acc 0.90625
2017-09-09T15:25:42.463659: step 1002, loss 0.199798, acc 0.953125
2017-09-09T15:25:42.703127: step 1003, loss 0.127589, acc 0.953125
2017-09-09T15:25:42.943508: step 1004, loss 0.141949, acc 1
2017-09-09T15:25:43.177007: step 1005, loss 0.309234, acc 0.875
2017-09-09T15:25:43.410306: step 1006, loss 0.163665, acc 0.96875
2017-09-09T15:25:43.648586: step 1007, loss 0.184675, acc 0.96875
2017-09-09T15:25:43.886667: step 1008, loss 0.158661, acc 0.984375
2017-09-09T15:25:44.118829: step 1009, loss 0.249768, acc 0.9375
2017-09-09T15:25:44.419165: step 1010, loss 0.259162, acc 0.9375
2017-09-09T15:25:44.693500: step 1011, loss 0.208105, acc 0.96875
2017-09-09T15:25:44.957861: step 1012, loss 0.180311, acc 0.9375
2017-09-09T15:25:45.262163: step 1013, loss 0.204513, acc 0.953125
2017-09-09T15:25:45.543696: step 1014, loss 0.173402, acc 0.9375
2017-09-09T15:25:45.829046: step 1015, loss 0.17925, acc 0.96875
2017-09-09T15:25:46.094577: step 1016, loss 0.0777808, acc 0.984375
2017-09-09T15:25:46.385701: step 1017, loss 0.158039, acc 0.984375
2017-09-09T15:25:46.666426: step 1018, loss 0.157375, acc 0.96875
2017-09-09T15:25:46.914751: step 1019, loss 0.209839, acc 0.953125
2017-09-09T15:25:47.194074: step 1020, loss 0.160503, acc 0.953125
2017-09-09T15:25:47.475185: step 1021, loss 0.271834, acc 0.890625
2017-09-09T15:25:47.766958: step 1022, loss 0.257538, acc 0.875
2017-09-09T15:25:48.033418: step 1023, loss 0.218582, acc 0.9375
2017-09-09T15:25:48.327590: step 1024, loss 0.184325, acc 0.953125
2017-09-09T15:25:48.613060: step 1025, loss 0.136157, acc 0.984375
2017-09-09T15:25:48.849784: step 1026, loss 0.165184, acc 0.96875
2017-09-09T15:25:49.131666: step 1027, loss 0.176891, acc 0.96875
2017-09-09T15:25:49.420678: step 1028, loss 0.166025, acc 0.96875
2017-09-09T15:25:49.663495: step 1029, loss 0.213408, acc 0.984375
2017-09-09T15:25:49.942670: step 1030, loss 0.1981, acc 0.9375
2017-09-09T15:25:50.209160: step 1031, loss 0.207076, acc 0.9375
2017-09-09T15:25:50.485191: step 1032, loss 0.169542, acc 0.96875
2017-09-09T15:25:50.743005: step 1033, loss 0.12153, acc 0.984375
2017-09-09T15:25:51.032865: step 1034, loss 0.204156, acc 0.921875
2017-09-09T15:25:51.315752: step 1035, loss 0.201322, acc 0.96875
2017-09-09T15:25:51.575502: step 1036, loss 0.216728, acc 0.9375
2017-09-09T15:25:51.853735: step 1037, loss 0.175245, acc 0.96875
2017-09-09T15:25:52.118517: step 1038, loss 0.192456, acc 0.953125
2017-09-09T15:25:52.363857: step 1039, loss 0.19127, acc 0.953125
2017-09-09T15:25:52.709929: step 1040, loss 0.137333, acc 0.984375
2017-09-09T15:25:52.989949: step 1041, loss 0.25939, acc 0.921875
2017-09-09T15:25:53.275611: step 1042, loss 0.149355, acc 0.96875
2017-09-09T15:25:53.544392: step 1043, loss 0.175031, acc 0.984375
2017-09-09T15:25:53.841281: step 1044, loss 0.160505, acc 0.953125
2017-09-09T15:25:54.088754: step 1045, loss 0.13844, acc 0.96875
2017-09-09T15:25:54.338348: step 1046, loss 0.230033, acc 0.953125
2017-09-09T15:25:54.623559: step 1047, loss 0.19955, acc 0.9375
2017-09-09T15:25:54.921095: step 1048, loss 0.16514, acc 0.984375
2017-09-09T15:25:55.215635: step 1049, loss 0.176531, acc 0.921875
2017-09-09T15:25:55.455794: step 1050, loss 0.124588, acc 0.984375

Evaluation:
2017-09-09T15:25:55.532426: step 1050, loss 1.547, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1050

2017-09-09T15:25:57.303049: step 1051, loss 0.124382, acc 0.984375
2017-09-09T15:25:57.603591: step 1052, loss 0.182023, acc 0.921875
2017-09-09T15:25:57.888433: step 1053, loss 0.212791, acc 0.9375
2017-09-09T15:25:58.140049: step 1054, loss 0.157579, acc 0.984375
2017-09-09T15:25:58.451267: step 1055, loss 0.181005, acc 0.9375
2017-09-09T15:25:58.736800: step 1056, loss 0.195143, acc 0.9375
2017-09-09T15:25:58.983677: step 1057, loss 0.18216, acc 0.953125
2017-09-09T15:25:59.245188: step 1058, loss 0.227415, acc 0.953125
2017-09-09T15:25:59.527202: step 1059, loss 0.135493, acc 0.96875
2017-09-09T15:25:59.782258: step 1060, loss 0.126463, acc 0.96875
2017-09-09T15:26:00.050931: step 1061, loss 0.190865, acc 0.9375
2017-09-09T15:26:00.325463: step 1062, loss 0.213004, acc 0.921875
2017-09-09T15:26:00.593789: step 1063, loss 0.127163, acc 0.953125
2017-09-09T15:26:00.857444: step 1064, loss 0.206363, acc 0.921875
2017-09-09T15:26:01.149699: step 1065, loss 0.113614, acc 0.984375
2017-09-09T15:26:01.429497: step 1066, loss 0.187182, acc 0.9375
2017-09-09T15:26:01.701427: step 1067, loss 0.172234, acc 0.9375
2017-09-09T15:26:02.007119: step 1068, loss 0.0697677, acc 1
2017-09-09T15:26:02.278085: step 1069, loss 0.220683, acc 0.953125
2017-09-09T15:26:02.579010: step 1070, loss 0.209787, acc 0.921875
2017-09-09T15:26:02.841799: step 1071, loss 0.193969, acc 0.953125
2017-09-09T15:26:03.128617: step 1072, loss 0.10977, acc 1
2017-09-09T15:26:03.422721: step 1073, loss 0.11861, acc 0.96875
2017-09-09T15:26:03.661335: step 1074, loss 0.260422, acc 0.921875
2017-09-09T15:26:04.019709: step 1075, loss 0.192715, acc 0.953125
2017-09-09T15:26:04.283752: step 1076, loss 0.210769, acc 0.9375
2017-09-09T15:26:04.533793: step 1077, loss 0.158388, acc 0.96875
2017-09-09T15:26:04.774524: step 1078, loss 0.143302, acc 0.980392
2017-09-09T15:26:05.017938: step 1079, loss 0.192539, acc 0.953125
2017-09-09T15:26:05.249385: step 1080, loss 0.102924, acc 0.953125
2017-09-09T15:26:05.480321: step 1081, loss 0.150685, acc 0.953125
2017-09-09T15:26:05.708349: step 1082, loss 0.134179, acc 0.984375
2017-09-09T15:26:05.983833: step 1083, loss 0.133999, acc 0.984375
2017-09-09T15:26:06.228888: step 1084, loss 0.169457, acc 0.9375
2017-09-09T15:26:06.497364: step 1085, loss 0.0781331, acc 0.96875
2017-09-09T15:26:06.787238: step 1086, loss 0.101217, acc 0.96875
2017-09-09T15:26:07.061994: step 1087, loss 0.19933, acc 0.953125
2017-09-09T15:26:07.314022: step 1088, loss 0.146959, acc 0.984375
2017-09-09T15:26:07.592767: step 1089, loss 0.136181, acc 0.96875
2017-09-09T15:26:07.876087: step 1090, loss 0.182516, acc 0.984375
2017-09-09T15:26:08.121262: step 1091, loss 0.0856949, acc 0.984375
2017-09-09T15:26:08.418158: step 1092, loss 0.0934071, acc 1
2017-09-09T15:26:08.710455: step 1093, loss 0.23349, acc 0.890625
2017-09-09T15:26:08.982146: step 1094, loss 0.15996, acc 0.984375
2017-09-09T15:26:09.231545: step 1095, loss 0.123552, acc 0.984375
2017-09-09T15:26:09.495125: step 1096, loss 0.193343, acc 0.921875
2017-09-09T15:26:09.736010: step 1097, loss 0.161388, acc 0.953125
2017-09-09T15:26:09.996657: step 1098, loss 0.133056, acc 0.953125
2017-09-09T15:26:10.255054: step 1099, loss 0.0799398, acc 1
2017-09-09T15:26:10.505623: step 1100, loss 0.0801618, acc 0.984375

Evaluation:
2017-09-09T15:26:10.636414: step 1100, loss 1.54455, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1100

2017-09-09T15:26:13.468800: step 1101, loss 0.139452, acc 0.96875
2017-09-09T15:26:13.771348: step 1102, loss 0.226546, acc 0.90625
2017-09-09T15:26:14.068160: step 1103, loss 0.181307, acc 0.90625
2017-09-09T15:26:14.358681: step 1104, loss 0.186909, acc 0.9375
2017-09-09T15:26:14.615013: step 1105, loss 0.14189, acc 0.984375
2017-09-09T15:26:14.900085: step 1106, loss 0.117716, acc 0.984375
2017-09-09T15:26:15.183366: step 1107, loss 0.0835517, acc 0.984375
2017-09-09T15:26:15.469180: step 1108, loss 0.140041, acc 0.96875
2017-09-09T15:26:15.729622: step 1109, loss 0.118742, acc 0.96875
2017-09-09T15:26:16.037169: step 1110, loss 0.150027, acc 0.984375
2017-09-09T15:26:16.319519: step 1111, loss 0.12734, acc 0.96875
2017-09-09T15:26:16.577641: step 1112, loss 0.0882933, acc 1
2017-09-09T15:26:16.891783: step 1113, loss 0.123441, acc 0.96875
2017-09-09T15:26:17.166125: step 1114, loss 0.117847, acc 0.96875
2017-09-09T15:26:17.451378: step 1115, loss 0.176862, acc 0.953125
2017-09-09T15:26:17.735571: step 1116, loss 0.0817942, acc 0.984375
2017-09-09T15:26:18.031265: step 1117, loss 0.182565, acc 0.953125
2017-09-09T15:26:18.295121: step 1118, loss 0.180268, acc 0.953125
2017-09-09T15:26:18.550623: step 1119, loss 0.176141, acc 0.953125
2017-09-09T15:26:18.854845: step 1120, loss 0.198011, acc 0.953125
2017-09-09T15:26:19.153816: step 1121, loss 0.154123, acc 0.984375
2017-09-09T15:26:19.403476: step 1122, loss 0.206901, acc 0.921875
2017-09-09T15:26:19.685740: step 1123, loss 0.0975603, acc 0.984375
2017-09-09T15:26:19.952741: step 1124, loss 0.171966, acc 0.953125
2017-09-09T15:26:20.193552: step 1125, loss 0.224837, acc 0.9375
2017-09-09T15:26:20.481426: step 1126, loss 0.120099, acc 0.984375
2017-09-09T15:26:20.785341: step 1127, loss 0.158271, acc 0.9375
2017-09-09T15:26:21.037232: step 1128, loss 0.196613, acc 0.953125
2017-09-09T15:26:21.339816: step 1129, loss 0.147044, acc 0.96875
2017-09-09T15:26:21.607486: step 1130, loss 0.156664, acc 0.953125
2017-09-09T15:26:21.879120: step 1131, loss 0.145052, acc 0.96875
2017-09-09T15:26:22.170734: step 1132, loss 0.138525, acc 0.984375
2017-09-09T15:26:22.457213: step 1133, loss 0.189098, acc 0.96875
2017-09-09T15:26:22.703532: step 1134, loss 0.112195, acc 0.984375
2017-09-09T15:26:22.984200: step 1135, loss 0.170318, acc 0.9375
2017-09-09T15:26:23.258242: step 1136, loss 0.0850591, acc 0.984375
2017-09-09T15:26:23.517057: step 1137, loss 0.118375, acc 0.984375
2017-09-09T15:26:23.827906: step 1138, loss 0.146862, acc 0.953125
2017-09-09T15:26:24.109314: step 1139, loss 0.2434, acc 0.9375
2017-09-09T15:26:24.377037: step 1140, loss 0.135076, acc 0.9375
2017-09-09T15:26:24.699142: step 1141, loss 0.136212, acc 1
2017-09-09T15:26:25.056750: step 1142, loss 0.171527, acc 0.953125
2017-09-09T15:26:25.345113: step 1143, loss 0.144053, acc 0.96875
2017-09-09T15:26:25.599385: step 1144, loss 0.161731, acc 0.953125
2017-09-09T15:26:25.883590: step 1145, loss 0.209475, acc 0.90625
2017-09-09T15:26:26.155116: step 1146, loss 0.135065, acc 0.96875
2017-09-09T15:26:26.410396: step 1147, loss 0.15864, acc 0.9375
2017-09-09T15:26:26.658932: step 1148, loss 0.164028, acc 0.953125
2017-09-09T15:26:26.906326: step 1149, loss 0.157086, acc 0.984375
2017-09-09T15:26:27.142229: step 1150, loss 0.197397, acc 0.953125

Evaluation:
2017-09-09T15:26:27.197808: step 1150, loss 1.58952, acc 0.306475

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1150

2017-09-09T15:26:29.321830: step 1151, loss 0.159497, acc 0.96875
2017-09-09T15:26:29.623494: step 1152, loss 0.147099, acc 0.984375
2017-09-09T15:26:29.933355: step 1153, loss 0.0927914, acc 1
2017-09-09T15:26:30.215308: step 1154, loss 0.175308, acc 0.953125
2017-09-09T15:26:30.492299: step 1155, loss 0.184766, acc 0.9375
2017-09-09T15:26:30.743293: step 1156, loss 0.145767, acc 0.953125
2017-09-09T15:26:31.034375: step 1157, loss 0.148062, acc 0.96875
2017-09-09T15:26:31.338094: step 1158, loss 0.150439, acc 0.96875
2017-09-09T15:26:31.623563: step 1159, loss 0.125902, acc 0.96875
2017-09-09T15:26:31.881745: step 1160, loss 0.150626, acc 0.96875
2017-09-09T15:26:32.196644: step 1161, loss 0.0721202, acc 1
2017-09-09T15:26:32.444841: step 1162, loss 0.271331, acc 0.875
2017-09-09T15:26:32.704114: step 1163, loss 0.172215, acc 0.96875
2017-09-09T15:26:33.006270: step 1164, loss 0.160767, acc 0.953125
2017-09-09T15:26:33.298630: step 1165, loss 0.132657, acc 0.96875
2017-09-09T15:26:33.579111: step 1166, loss 0.199504, acc 0.9375
2017-09-09T15:26:33.863887: step 1167, loss 0.128271, acc 0.953125
2017-09-09T15:26:34.130588: step 1168, loss 0.121478, acc 1
2017-09-09T15:26:34.375461: step 1169, loss 0.161778, acc 0.9375
2017-09-09T15:26:34.672834: step 1170, loss 0.141808, acc 0.96875
2017-09-09T15:26:34.917048: step 1171, loss 0.187654, acc 0.90625
2017-09-09T15:26:35.182266: step 1172, loss 0.169215, acc 0.984375
2017-09-09T15:26:35.480323: step 1173, loss 0.19684, acc 0.96875
2017-09-09T15:26:35.771335: step 1174, loss 0.180341, acc 0.953125
2017-09-09T15:26:36.025611: step 1175, loss 0.213174, acc 0.921875
2017-09-09T15:26:36.313405: step 1176, loss 0.183241, acc 0.960784
2017-09-09T15:26:36.597001: step 1177, loss 0.126268, acc 0.96875
2017-09-09T15:26:36.856122: step 1178, loss 0.0518219, acc 1
2017-09-09T15:26:37.150236: step 1179, loss 0.111595, acc 1
2017-09-09T15:26:37.432861: step 1180, loss 0.175318, acc 0.9375
2017-09-09T15:26:37.693745: step 1181, loss 0.149021, acc 0.921875
2017-09-09T15:26:37.982110: step 1182, loss 0.143739, acc 0.9375
2017-09-09T15:26:38.265179: step 1183, loss 0.149052, acc 0.96875
2017-09-09T15:26:38.516575: step 1184, loss 0.0939022, acc 0.96875
2017-09-09T15:26:38.810442: step 1185, loss 0.215748, acc 0.9375
2017-09-09T15:26:39.090308: step 1186, loss 0.124528, acc 0.953125
2017-09-09T15:26:39.359099: step 1187, loss 0.190926, acc 0.9375
2017-09-09T15:26:39.626250: step 1188, loss 0.0592547, acc 1
2017-09-09T15:26:39.884247: step 1189, loss 0.0459746, acc 1
2017-09-09T15:26:40.199282: step 1190, loss 0.11424, acc 0.96875
2017-09-09T15:26:40.488643: step 1191, loss 0.121282, acc 0.984375
2017-09-09T15:26:40.745164: step 1192, loss 0.105852, acc 0.96875
2017-09-09T15:26:41.054729: step 1193, loss 0.142712, acc 0.96875
2017-09-09T15:26:41.354657: step 1194, loss 0.14248, acc 0.96875
2017-09-09T15:26:41.623113: step 1195, loss 0.140302, acc 0.96875
2017-09-09T15:26:41.902729: step 1196, loss 0.19214, acc 0.921875
2017-09-09T15:26:42.202720: step 1197, loss 0.0856095, acc 1
2017-09-09T15:26:42.496084: step 1198, loss 0.105779, acc 0.96875
2017-09-09T15:26:42.766643: step 1199, loss 0.182942, acc 0.90625
2017-09-09T15:26:43.067277: step 1200, loss 0.124753, acc 0.96875

Evaluation:
2017-09-09T15:26:43.126903: step 1200, loss 1.55503, acc 0.330935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1200

2017-09-09T15:26:45.456585: step 1201, loss 0.157457, acc 0.9375
2017-09-09T15:26:45.724250: step 1202, loss 0.186312, acc 0.953125
2017-09-09T15:26:45.981421: step 1203, loss 0.136614, acc 0.984375
2017-09-09T15:26:46.290603: step 1204, loss 0.11465, acc 0.96875
2017-09-09T15:26:46.569162: step 1205, loss 0.109765, acc 0.96875
2017-09-09T15:26:46.818105: step 1206, loss 0.108519, acc 0.953125
2017-09-09T15:26:47.105611: step 1207, loss 0.134782, acc 0.96875
2017-09-09T15:26:47.354844: step 1208, loss 0.140158, acc 0.984375
2017-09-09T15:26:47.629639: step 1209, loss 0.127914, acc 0.96875
2017-09-09T15:26:47.910106: step 1210, loss 0.114906, acc 0.96875
2017-09-09T15:26:48.167828: step 1211, loss 0.138958, acc 0.9375
2017-09-09T15:26:48.539416: step 1212, loss 0.129427, acc 0.96875
2017-09-09T15:26:48.776381: step 1213, loss 0.121195, acc 0.96875
2017-09-09T15:26:49.008703: step 1214, loss 0.109759, acc 0.96875
2017-09-09T15:26:49.243135: step 1215, loss 0.151187, acc 0.953125
2017-09-09T15:26:49.467642: step 1216, loss 0.12959, acc 0.9375
2017-09-09T15:26:49.710935: step 1217, loss 0.172464, acc 0.9375
2017-09-09T15:26:49.936706: step 1218, loss 0.0495065, acc 1
2017-09-09T15:26:50.161660: step 1219, loss 0.0864983, acc 0.984375
2017-09-09T15:26:50.404625: step 1220, loss 0.157559, acc 0.953125
2017-09-09T15:26:50.649824: step 1221, loss 0.0989562, acc 0.96875
2017-09-09T15:26:50.943622: step 1222, loss 0.112013, acc 0.96875
2017-09-09T15:26:51.223917: step 1223, loss 0.102486, acc 0.96875
2017-09-09T15:26:51.505647: step 1224, loss 0.153212, acc 0.984375
2017-09-09T15:26:51.790303: step 1225, loss 0.11657, acc 0.984375
2017-09-09T15:26:52.058841: step 1226, loss 0.157707, acc 0.953125
2017-09-09T15:26:52.333704: step 1227, loss 0.123327, acc 0.96875
2017-09-09T15:26:52.630449: step 1228, loss 0.167014, acc 0.953125
2017-09-09T15:26:52.944051: step 1229, loss 0.147846, acc 0.984375
2017-09-09T15:26:53.222350: step 1230, loss 0.114539, acc 0.984375
2017-09-09T15:26:53.507788: step 1231, loss 0.0701091, acc 0.984375
2017-09-09T15:26:53.792954: step 1232, loss 0.117735, acc 0.96875
2017-09-09T15:26:54.054983: step 1233, loss 0.11822, acc 0.984375
2017-09-09T15:26:54.347369: step 1234, loss 0.161574, acc 0.953125
2017-09-09T15:26:54.619922: step 1235, loss 0.0817296, acc 0.984375
2017-09-09T15:26:54.912431: step 1236, loss 0.0775056, acc 1
2017-09-09T15:26:55.215998: step 1237, loss 0.0913916, acc 0.96875
2017-09-09T15:26:55.511306: step 1238, loss 0.0981734, acc 0.984375
2017-09-09T15:26:55.780807: step 1239, loss 0.168131, acc 0.96875
2017-09-09T15:26:56.047131: step 1240, loss 0.121862, acc 0.984375
2017-09-09T15:26:56.328839: step 1241, loss 0.0831476, acc 0.984375
2017-09-09T15:26:56.602020: step 1242, loss 0.128212, acc 0.96875
2017-09-09T15:26:56.863928: step 1243, loss 0.10543, acc 0.953125
2017-09-09T15:26:57.133204: step 1244, loss 0.117904, acc 0.96875
2017-09-09T15:26:57.370835: step 1245, loss 0.0805463, acc 1
2017-09-09T15:26:57.634817: step 1246, loss 0.139426, acc 0.96875
2017-09-09T15:26:57.952533: step 1247, loss 0.121104, acc 0.953125
2017-09-09T15:26:58.206610: step 1248, loss 0.15526, acc 0.953125
2017-09-09T15:26:58.471455: step 1249, loss 0.134377, acc 0.96875
2017-09-09T15:26:58.766872: step 1250, loss 0.150808, acc 0.96875

Evaluation:
2017-09-09T15:26:58.828928: step 1250, loss 1.58044, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1250

2017-09-09T15:27:01.370352: step 1251, loss 0.158415, acc 0.953125
2017-09-09T15:27:01.668043: step 1252, loss 0.0795405, acc 1
2017-09-09T15:27:02.013573: step 1253, loss 0.14045, acc 0.96875
2017-09-09T15:27:02.342680: step 1254, loss 0.244197, acc 0.921875
2017-09-09T15:27:02.650660: step 1255, loss 0.161171, acc 0.96875
2017-09-09T15:27:02.952648: step 1256, loss 0.179166, acc 0.921875
2017-09-09T15:27:03.257036: step 1257, loss 0.107242, acc 0.984375
2017-09-09T15:27:03.528456: step 1258, loss 0.136874, acc 0.953125
2017-09-09T15:27:03.821479: step 1259, loss 0.095495, acc 0.96875
2017-09-09T15:27:04.077340: step 1260, loss 0.165296, acc 0.96875
2017-09-09T15:27:04.376554: step 1261, loss 0.185801, acc 0.9375
2017-09-09T15:27:04.669254: step 1262, loss 0.0778891, acc 1
2017-09-09T15:27:04.932823: step 1263, loss 0.138062, acc 0.953125
2017-09-09T15:27:05.232612: step 1264, loss 0.0896967, acc 0.984375
2017-09-09T15:27:05.534780: step 1265, loss 0.110464, acc 0.984375
2017-09-09T15:27:05.815215: step 1266, loss 0.0970481, acc 0.96875
2017-09-09T15:27:06.064478: step 1267, loss 0.110193, acc 0.96875
2017-09-09T15:27:06.364901: step 1268, loss 0.156558, acc 0.921875
2017-09-09T15:27:06.644884: step 1269, loss 0.137894, acc 0.953125
2017-09-09T15:27:06.889721: step 1270, loss 0.16587, acc 0.953125
2017-09-09T15:27:07.189134: step 1271, loss 0.178285, acc 0.953125
2017-09-09T15:27:07.480517: step 1272, loss 0.156075, acc 0.984375
2017-09-09T15:27:07.733890: step 1273, loss 0.0781766, acc 0.96875
2017-09-09T15:27:08.006192: step 1274, loss 0.185947, acc 0.960784
2017-09-09T15:27:08.281537: step 1275, loss 0.0722189, acc 1
2017-09-09T15:27:08.552204: step 1276, loss 0.0729252, acc 1
2017-09-09T15:27:08.849319: step 1277, loss 0.0672951, acc 0.984375
2017-09-09T15:27:09.116498: step 1278, loss 0.156301, acc 0.984375
2017-09-09T15:27:09.396204: step 1279, loss 0.0846586, acc 0.984375
2017-09-09T15:27:09.660877: step 1280, loss 0.141458, acc 0.953125
2017-09-09T15:27:09.964685: step 1281, loss 0.171924, acc 0.953125
2017-09-09T15:27:10.262448: step 1282, loss 0.115905, acc 0.953125
2017-09-09T15:27:10.532046: step 1283, loss 0.110432, acc 0.984375
2017-09-09T15:27:10.864935: step 1284, loss 0.22836, acc 0.921875
2017-09-09T15:27:11.168402: step 1285, loss 0.144347, acc 0.953125
2017-09-09T15:27:11.514980: step 1286, loss 0.084717, acc 0.984375
2017-09-09T15:27:11.758296: step 1287, loss 0.111981, acc 0.984375
2017-09-09T15:27:11.997994: step 1288, loss 0.135426, acc 0.9375
2017-09-09T15:27:12.240935: step 1289, loss 0.086732, acc 0.984375
2017-09-09T15:27:12.489027: step 1290, loss 0.0992186, acc 0.984375
2017-09-09T15:27:12.745205: step 1291, loss 0.213866, acc 0.921875
2017-09-09T15:27:13.047611: step 1292, loss 0.125332, acc 0.96875
2017-09-09T15:27:13.307971: step 1293, loss 0.0740163, acc 0.984375
2017-09-09T15:27:13.554356: step 1294, loss 0.092805, acc 1
2017-09-09T15:27:13.869303: step 1295, loss 0.0989689, acc 0.96875
2017-09-09T15:27:14.179926: step 1296, loss 0.20681, acc 0.9375
2017-09-09T15:27:14.430999: step 1297, loss 0.12812, acc 0.953125
2017-09-09T15:27:14.716411: step 1298, loss 0.0863084, acc 0.984375
2017-09-09T15:27:15.007209: step 1299, loss 0.0943492, acc 0.984375
2017-09-09T15:27:15.251877: step 1300, loss 0.0977943, acc 0.984375

Evaluation:
2017-09-09T15:27:15.367250: step 1300, loss 1.64725, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1300

2017-09-09T15:27:17.543076: step 1301, loss 0.169584, acc 0.96875
2017-09-09T15:27:17.854033: step 1302, loss 0.119581, acc 0.953125
2017-09-09T15:27:18.145284: step 1303, loss 0.0725393, acc 0.984375
2017-09-09T15:27:18.421958: step 1304, loss 0.0558712, acc 0.984375
2017-09-09T15:27:18.694031: step 1305, loss 0.0876094, acc 0.953125
2017-09-09T15:27:18.982720: step 1306, loss 0.045158, acc 1
2017-09-09T15:27:19.240628: step 1307, loss 0.0982892, acc 0.96875
2017-09-09T15:27:19.566157: step 1308, loss 0.0723321, acc 0.984375
2017-09-09T15:27:19.875231: step 1309, loss 0.0740889, acc 1
2017-09-09T15:27:20.152102: step 1310, loss 0.0733738, acc 0.984375
2017-09-09T15:27:20.430075: step 1311, loss 0.107853, acc 0.953125
2017-09-09T15:27:20.723398: step 1312, loss 0.155967, acc 0.9375
2017-09-09T15:27:21.008153: step 1313, loss 0.0939446, acc 1
2017-09-09T15:27:21.268131: step 1314, loss 0.133716, acc 0.96875
2017-09-09T15:27:21.553103: step 1315, loss 0.118453, acc 0.96875
2017-09-09T15:27:21.850785: step 1316, loss 0.0808831, acc 0.984375
2017-09-09T15:27:22.104214: step 1317, loss 0.131342, acc 0.9375
2017-09-09T15:27:22.399407: step 1318, loss 0.100519, acc 1
2017-09-09T15:27:22.704266: step 1319, loss 0.0746008, acc 1
2017-09-09T15:27:22.943479: step 1320, loss 0.0609964, acc 0.984375
2017-09-09T15:27:23.200335: step 1321, loss 0.0489988, acc 1
2017-09-09T15:27:23.450984: step 1322, loss 0.155179, acc 0.9375
2017-09-09T15:27:23.704824: step 1323, loss 0.0719617, acc 0.984375
2017-09-09T15:27:23.985998: step 1324, loss 0.110083, acc 0.953125
2017-09-09T15:27:24.274468: step 1325, loss 0.0934146, acc 1
2017-09-09T15:27:24.546849: step 1326, loss 0.114885, acc 0.953125
2017-09-09T15:27:24.879133: step 1327, loss 0.0445129, acc 1
2017-09-09T15:27:25.169219: step 1328, loss 0.066116, acc 0.984375
2017-09-09T15:27:25.426307: step 1329, loss 0.0924297, acc 1
2017-09-09T15:27:25.703239: step 1330, loss 0.123472, acc 0.984375
2017-09-09T15:27:25.965048: step 1331, loss 0.105818, acc 0.96875
2017-09-09T15:27:26.227220: step 1332, loss 0.11483, acc 0.96875
2017-09-09T15:27:27.091438: step 1333, loss 0.0815772, acc 0.984375
2017-09-09T15:27:27.359370: step 1334, loss 0.0923106, acc 0.96875
2017-09-09T15:27:27.609906: step 1335, loss 0.099808, acc 0.96875
2017-09-09T15:27:27.944441: step 1336, loss 0.0917155, acc 1
2017-09-09T15:27:28.239056: step 1337, loss 0.136682, acc 0.953125
2017-09-09T15:27:28.543328: step 1338, loss 0.070003, acc 0.984375
2017-09-09T15:27:28.805479: step 1339, loss 0.0740545, acc 1
2017-09-09T15:27:29.113130: step 1340, loss 0.142951, acc 0.96875
2017-09-09T15:27:29.390668: step 1341, loss 0.121463, acc 0.984375
2017-09-09T15:27:29.660396: step 1342, loss 0.127379, acc 0.953125
2017-09-09T15:27:29.980738: step 1343, loss 0.0800137, acc 0.96875
2017-09-09T15:27:30.249531: step 1344, loss 0.071434, acc 0.984375
2017-09-09T15:27:30.514459: step 1345, loss 0.133346, acc 0.96875
2017-09-09T15:27:30.817540: step 1346, loss 0.124355, acc 0.96875
2017-09-09T15:27:31.117389: step 1347, loss 0.101778, acc 0.96875
2017-09-09T15:27:31.386600: step 1348, loss 0.0716187, acc 0.984375
2017-09-09T15:27:31.674045: step 1349, loss 0.213435, acc 0.921875
2017-09-09T15:27:31.951827: step 1350, loss 0.108521, acc 0.96875

Evaluation:
2017-09-09T15:27:32.008482: step 1350, loss 1.61115, acc 0.330935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1350

2017-09-09T15:27:34.601464: step 1351, loss 0.15042, acc 0.9375
2017-09-09T15:27:34.853497: step 1352, loss 0.0623511, acc 0.984375
2017-09-09T15:27:35.100097: step 1353, loss 0.129649, acc 0.96875
2017-09-09T15:27:35.341607: step 1354, loss 0.148559, acc 0.953125
2017-09-09T15:27:35.581017: step 1355, loss 0.152345, acc 0.9375
2017-09-09T15:27:35.812992: step 1356, loss 0.111289, acc 0.953125
2017-09-09T15:27:36.112917: step 1357, loss 0.223599, acc 0.9375
2017-09-09T15:27:36.353930: step 1358, loss 0.0557375, acc 0.96875
2017-09-09T15:27:36.620512: step 1359, loss 0.146225, acc 0.96875
2017-09-09T15:27:36.882109: step 1360, loss 0.131111, acc 0.9375
2017-09-09T15:27:37.151086: step 1361, loss 0.0674416, acc 0.984375
2017-09-09T15:27:37.418689: step 1362, loss 0.0588751, acc 0.984375
2017-09-09T15:27:37.673792: step 1363, loss 0.106882, acc 0.953125
2017-09-09T15:27:37.918033: step 1364, loss 0.10308, acc 0.96875
2017-09-09T15:27:38.209851: step 1365, loss 0.0993958, acc 0.984375
2017-09-09T15:27:38.508959: step 1366, loss 0.190807, acc 0.9375
2017-09-09T15:27:38.769130: step 1367, loss 0.156303, acc 0.9375
2017-09-09T15:27:39.032494: step 1368, loss 0.102635, acc 0.953125
2017-09-09T15:27:39.310287: step 1369, loss 0.0749962, acc 0.984375
2017-09-09T15:27:39.570858: step 1370, loss 0.0730536, acc 1
2017-09-09T15:27:39.866545: step 1371, loss 0.130686, acc 0.96875
2017-09-09T15:27:40.117065: step 1372, loss 0.136591, acc 0.980392
2017-09-09T15:27:40.396905: step 1373, loss 0.0805954, acc 0.984375
2017-09-09T15:27:40.663920: step 1374, loss 0.117024, acc 0.953125
2017-09-09T15:27:40.966424: step 1375, loss 0.106238, acc 0.953125
2017-09-09T15:27:41.223931: step 1376, loss 0.0746819, acc 0.984375
2017-09-09T15:27:41.512342: step 1377, loss 0.170604, acc 0.953125
2017-09-09T15:27:41.796161: step 1378, loss 0.142801, acc 0.9375
2017-09-09T15:27:42.064778: step 1379, loss 0.124546, acc 0.9375
2017-09-09T15:27:42.346332: step 1380, loss 0.0910252, acc 0.984375
2017-09-09T15:27:42.611208: step 1381, loss 0.0983624, acc 0.96875
2017-09-09T15:27:42.902008: step 1382, loss 0.122236, acc 0.96875
2017-09-09T15:27:43.186184: step 1383, loss 0.154178, acc 0.921875
2017-09-09T15:27:43.436762: step 1384, loss 0.114592, acc 0.96875
2017-09-09T15:27:43.721459: step 1385, loss 0.0429247, acc 1
2017-09-09T15:27:43.991496: step 1386, loss 0.0982368, acc 0.96875
2017-09-09T15:27:44.266251: step 1387, loss 0.0897508, acc 0.96875
2017-09-09T15:27:44.533820: step 1388, loss 0.101915, acc 0.953125
2017-09-09T15:27:44.812450: step 1389, loss 0.0836431, acc 0.984375
2017-09-09T15:27:45.070471: step 1390, loss 0.0859638, acc 1
2017-09-09T15:27:45.370390: step 1391, loss 0.0992368, acc 0.96875
2017-09-09T15:27:45.633391: step 1392, loss 0.0946623, acc 0.96875
2017-09-09T15:27:45.893906: step 1393, loss 0.0512247, acc 1
2017-09-09T15:27:46.205493: step 1394, loss 0.0599873, acc 1
2017-09-09T15:27:46.503144: step 1395, loss 0.120487, acc 0.96875
2017-09-09T15:27:46.822049: step 1396, loss 0.0822805, acc 1
2017-09-09T15:27:47.092758: step 1397, loss 0.105863, acc 0.953125
2017-09-09T15:27:47.393952: step 1398, loss 0.0546212, acc 1
2017-09-09T15:27:47.664883: step 1399, loss 0.106958, acc 0.984375
2017-09-09T15:27:47.933125: step 1400, loss 0.0975166, acc 0.96875

Evaluation:
2017-09-09T15:27:48.006459: step 1400, loss 1.62692, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1400

2017-09-09T15:27:50.001512: step 1401, loss 0.0717838, acc 0.96875
2017-09-09T15:27:50.265012: step 1402, loss 0.107777, acc 0.984375
2017-09-09T15:27:50.541489: step 1403, loss 0.102525, acc 0.953125
2017-09-09T15:27:50.800422: step 1404, loss 0.185236, acc 0.921875
2017-09-09T15:27:51.064802: step 1405, loss 0.0901587, acc 0.984375
2017-09-09T15:27:51.353274: step 1406, loss 0.0659978, acc 0.96875
2017-09-09T15:27:51.625607: step 1407, loss 0.0850214, acc 0.984375
2017-09-09T15:27:51.953537: step 1408, loss 0.132387, acc 0.953125
2017-09-09T15:27:52.253423: step 1409, loss 0.0834665, acc 0.984375
2017-09-09T15:27:52.529065: step 1410, loss 0.0936553, acc 1
2017-09-09T15:27:52.790481: step 1411, loss 0.132113, acc 0.953125
2017-09-09T15:27:53.063050: step 1412, loss 0.168362, acc 0.921875
2017-09-09T15:27:53.364890: step 1413, loss 0.134623, acc 0.953125
2017-09-09T15:27:53.624190: step 1414, loss 0.115243, acc 0.984375
2017-09-09T15:27:53.929802: step 1415, loss 0.11806, acc 1
2017-09-09T15:27:54.218776: step 1416, loss 0.0734152, acc 0.984375
2017-09-09T15:27:54.496160: step 1417, loss 0.11905, acc 0.984375
2017-09-09T15:27:54.775332: step 1418, loss 0.135377, acc 0.921875
2017-09-09T15:27:55.044238: step 1419, loss 0.144663, acc 0.9375
2017-09-09T15:27:55.368113: step 1420, loss 0.148879, acc 0.921875
2017-09-09T15:27:55.638977: step 1421, loss 0.0880197, acc 0.96875
2017-09-09T15:27:56.005963: step 1422, loss 0.146327, acc 0.953125
2017-09-09T15:27:56.293358: step 1423, loss 0.091417, acc 0.96875
2017-09-09T15:27:56.561795: step 1424, loss 0.163077, acc 0.9375
2017-09-09T15:27:56.906168: step 1425, loss 0.0982973, acc 0.984375
2017-09-09T15:27:57.210902: step 1426, loss 0.0982881, acc 0.96875
2017-09-09T15:27:57.557608: step 1427, loss 0.101828, acc 0.984375
2017-09-09T15:27:57.912715: step 1428, loss 0.0800964, acc 0.96875
2017-09-09T15:27:58.172886: step 1429, loss 0.110179, acc 0.984375
2017-09-09T15:27:58.538872: step 1430, loss 0.144329, acc 0.953125
2017-09-09T15:27:58.836011: step 1431, loss 0.0676433, acc 1
2017-09-09T15:27:59.126760: step 1432, loss 0.0882422, acc 0.96875
2017-09-09T15:27:59.380935: step 1433, loss 0.0454999, acc 1
2017-09-09T15:27:59.646774: step 1434, loss 0.0698497, acc 1
2017-09-09T15:27:59.944193: step 1435, loss 0.0494072, acc 1
2017-09-09T15:28:00.214882: step 1436, loss 0.0797078, acc 0.984375
2017-09-09T15:28:00.478529: step 1437, loss 0.0672625, acc 1
2017-09-09T15:28:00.781737: step 1438, loss 0.0948706, acc 0.953125
2017-09-09T15:28:01.112650: step 1439, loss 0.095333, acc 0.96875
2017-09-09T15:28:01.415003: step 1440, loss 0.121462, acc 0.96875
2017-09-09T15:28:01.737093: step 1441, loss 0.128742, acc 0.953125
2017-09-09T15:28:02.071615: step 1442, loss 0.0627827, acc 0.984375
2017-09-09T15:28:02.381547: step 1443, loss 0.0632283, acc 0.984375
2017-09-09T15:28:02.671919: step 1444, loss 0.107995, acc 0.984375
2017-09-09T15:28:02.943283: step 1445, loss 0.0939958, acc 0.96875
2017-09-09T15:28:03.235971: step 1446, loss 0.174645, acc 0.921875
2017-09-09T15:28:03.537544: step 1447, loss 0.0644419, acc 0.984375
2017-09-09T15:28:03.825700: step 1448, loss 0.0923291, acc 0.96875
2017-09-09T15:28:04.152743: step 1449, loss 0.102055, acc 0.96875
2017-09-09T15:28:04.470656: step 1450, loss 0.0599783, acc 0.96875

Evaluation:
2017-09-09T15:28:04.574830: step 1450, loss 1.6638, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1450

2017-09-09T15:28:07.231581: step 1451, loss 0.0982792, acc 0.953125
2017-09-09T15:28:07.541329: step 1452, loss 0.0853361, acc 0.984375
2017-09-09T15:28:07.835905: step 1453, loss 0.105231, acc 0.96875
2017-09-09T15:28:08.217746: step 1454, loss 0.0977992, acc 0.96875
2017-09-09T15:28:08.531237: step 1455, loss 0.0859559, acc 0.984375
2017-09-09T15:28:08.849913: step 1456, loss 0.0580846, acc 0.984375
2017-09-09T15:28:09.164479: step 1457, loss 0.0578329, acc 0.984375
2017-09-09T15:28:09.478759: step 1458, loss 0.0811527, acc 0.984375
2017-09-09T15:28:09.793536: step 1459, loss 0.126499, acc 0.953125
2017-09-09T15:28:10.095170: step 1460, loss 0.123965, acc 0.96875
2017-09-09T15:28:10.456639: step 1461, loss 0.0736647, acc 0.984375
2017-09-09T15:28:10.761665: step 1462, loss 0.0729383, acc 0.984375
2017-09-09T15:28:11.027587: step 1463, loss 0.152589, acc 0.921875
2017-09-09T15:28:11.347716: step 1464, loss 0.114649, acc 0.96875
2017-09-09T15:28:11.668034: step 1465, loss 0.0829543, acc 1
2017-09-09T15:28:12.034735: step 1466, loss 0.0979389, acc 0.953125
2017-09-09T15:28:12.334469: step 1467, loss 0.0717392, acc 0.984375
2017-09-09T15:28:12.631604: step 1468, loss 0.0751107, acc 0.96875
2017-09-09T15:28:13.045556: step 1469, loss 0.0991922, acc 0.984375
2017-09-09T15:28:13.373639: step 1470, loss 0.0605824, acc 1
2017-09-09T15:28:13.654475: step 1471, loss 0.0482888, acc 1
2017-09-09T15:28:13.966610: step 1472, loss 0.0755544, acc 0.984375
2017-09-09T15:28:14.220483: step 1473, loss 0.054891, acc 0.984375
2017-09-09T15:28:14.497610: step 1474, loss 0.0554452, acc 0.984375
2017-09-09T15:28:14.789949: step 1475, loss 0.078177, acc 0.984375
2017-09-09T15:28:15.070904: step 1476, loss 0.0836052, acc 0.984375
2017-09-09T15:28:15.350560: step 1477, loss 0.145327, acc 0.96875
2017-09-09T15:28:15.657842: step 1478, loss 0.0725547, acc 0.984375
2017-09-09T15:28:15.953394: step 1479, loss 0.0780953, acc 0.984375
2017-09-09T15:28:16.265962: step 1480, loss 0.106542, acc 0.984375
2017-09-09T15:28:16.546580: step 1481, loss 0.107525, acc 0.953125
2017-09-09T15:28:16.841620: step 1482, loss 0.0719969, acc 0.984375
2017-09-09T15:28:17.174941: step 1483, loss 0.0615747, acc 0.984375
2017-09-09T15:28:17.451521: step 1484, loss 0.141642, acc 0.96875
2017-09-09T15:28:17.764675: step 1485, loss 0.0716902, acc 1
2017-09-09T15:28:18.064093: step 1486, loss 0.102178, acc 0.984375
2017-09-09T15:28:18.394187: step 1487, loss 0.1223, acc 0.984375
2017-09-09T15:28:18.720251: step 1488, loss 0.0949762, acc 0.984375
2017-09-09T15:28:19.010635: step 1489, loss 0.0740354, acc 0.96875
2017-09-09T15:28:19.364788: step 1490, loss 0.0486277, acc 0.984375
2017-09-09T15:28:19.634635: step 1491, loss 0.0818301, acc 0.96875
2017-09-09T15:28:19.945551: step 1492, loss 0.145172, acc 0.953125
2017-09-09T15:28:20.223335: step 1493, loss 0.140831, acc 0.96875
2017-09-09T15:28:20.510928: step 1494, loss 0.174613, acc 0.9375
2017-09-09T15:28:20.891401: step 1495, loss 0.130192, acc 0.953125
2017-09-09T15:28:21.165818: step 1496, loss 0.0687465, acc 0.984375
2017-09-09T15:28:21.497052: step 1497, loss 0.0902026, acc 0.96875
2017-09-09T15:28:21.826720: step 1498, loss 0.072523, acc 0.984375
2017-09-09T15:28:22.113758: step 1499, loss 0.0638521, acc 0.984375
2017-09-09T15:28:22.497745: step 1500, loss 0.0842063, acc 0.984375

Evaluation:
2017-09-09T15:28:22.553429: step 1500, loss 1.568, acc 0.330935

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1500

2017-09-09T15:28:26.037256: step 1501, loss 0.111208, acc 0.96875
2017-09-09T15:28:26.345003: step 1502, loss 0.0567482, acc 1
2017-09-09T15:28:26.608610: step 1503, loss 0.0749212, acc 0.984375
2017-09-09T15:28:26.858604: step 1504, loss 0.0493496, acc 1
2017-09-09T15:28:27.132190: step 1505, loss 0.0870759, acc 0.984375
2017-09-09T15:28:27.457747: step 1506, loss 0.0778134, acc 0.984375
2017-09-09T15:28:27.800495: step 1507, loss 0.0265349, acc 1
2017-09-09T15:28:28.098329: step 1508, loss 0.0608505, acc 1
2017-09-09T15:28:28.462096: step 1509, loss 0.0651676, acc 0.984375
2017-09-09T15:28:28.783589: step 1510, loss 0.0896506, acc 0.96875
2017-09-09T15:28:29.060756: step 1511, loss 0.170125, acc 0.921875
2017-09-09T15:28:29.343643: step 1512, loss 0.0739225, acc 0.96875
2017-09-09T15:28:29.621469: step 1513, loss 0.0329458, acc 1
2017-09-09T15:28:29.948190: step 1514, loss 0.159994, acc 0.953125
2017-09-09T15:28:30.226235: step 1515, loss 0.0862121, acc 0.96875
2017-09-09T15:28:30.571959: step 1516, loss 0.0756316, acc 0.96875
2017-09-09T15:28:30.864028: step 1517, loss 0.105385, acc 0.984375
2017-09-09T15:28:31.161681: step 1518, loss 0.0618919, acc 0.984375
2017-09-09T15:28:31.438680: step 1519, loss 0.0558862, acc 1
2017-09-09T15:28:31.707944: step 1520, loss 0.0883127, acc 0.96875
2017-09-09T15:28:32.003506: step 1521, loss 0.160583, acc 0.921875
2017-09-09T15:28:32.275520: step 1522, loss 0.100505, acc 0.96875
2017-09-09T15:28:32.626285: step 1523, loss 0.0420955, acc 1
2017-09-09T15:28:32.942188: step 1524, loss 0.0551027, acc 1
2017-09-09T15:28:33.265978: step 1525, loss 0.0374005, acc 1
2017-09-09T15:28:33.632404: step 1526, loss 0.0769023, acc 0.984375
2017-09-09T15:28:33.906590: step 1527, loss 0.0843872, acc 0.984375
2017-09-09T15:28:34.223772: step 1528, loss 0.0456098, acc 0.984375
2017-09-09T15:28:34.510309: step 1529, loss 0.092547, acc 0.96875
2017-09-09T15:28:34.792201: step 1530, loss 0.0753346, acc 0.984375
2017-09-09T15:28:35.102381: step 1531, loss 0.106314, acc 0.953125
2017-09-09T15:28:35.377836: step 1532, loss 0.127359, acc 0.984375
2017-09-09T15:28:35.694819: step 1533, loss 0.185853, acc 0.9375
2017-09-09T15:28:35.997421: step 1534, loss 0.155656, acc 0.921875
2017-09-09T15:28:36.309558: step 1535, loss 0.104093, acc 0.9375
2017-09-09T15:28:36.627367: step 1536, loss 0.066107, acc 0.96875
2017-09-09T15:28:36.910832: step 1537, loss 0.117277, acc 0.96875
2017-09-09T15:28:37.203405: step 1538, loss 0.0451621, acc 1
2017-09-09T15:28:37.491715: step 1539, loss 0.0738005, acc 1
2017-09-09T15:28:37.800014: step 1540, loss 0.0745889, acc 0.984375
2017-09-09T15:28:38.059014: step 1541, loss 0.0921898, acc 0.984375
2017-09-09T15:28:38.376599: step 1542, loss 0.103394, acc 0.96875
2017-09-09T15:28:38.680355: step 1543, loss 0.119367, acc 0.96875
2017-09-09T15:28:38.973719: step 1544, loss 0.061147, acc 1
2017-09-09T15:28:39.288162: step 1545, loss 0.0987039, acc 0.984375
2017-09-09T15:28:39.572644: step 1546, loss 0.040699, acc 1
2017-09-09T15:28:39.893573: step 1547, loss 0.09588, acc 0.96875
2017-09-09T15:28:40.172893: step 1548, loss 0.0302896, acc 1
2017-09-09T15:28:40.555820: step 1549, loss 0.0761656, acc 0.96875
2017-09-09T15:28:40.866628: step 1550, loss 0.127575, acc 0.9375

Evaluation:
2017-09-09T15:28:40.998939: step 1550, loss 1.6388, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1550

2017-09-09T15:28:42.887949: step 1551, loss 0.0772019, acc 0.96875
2017-09-09T15:28:43.190607: step 1552, loss 0.0974617, acc 0.96875
2017-09-09T15:28:43.506622: step 1553, loss 0.0907598, acc 0.96875
2017-09-09T15:28:43.834963: step 1554, loss 0.156454, acc 0.953125
2017-09-09T15:28:44.148493: step 1555, loss 0.0992152, acc 0.96875
2017-09-09T15:28:44.455698: step 1556, loss 0.0904215, acc 0.96875
2017-09-09T15:28:44.788756: step 1557, loss 0.0997143, acc 0.953125
2017-09-09T15:28:45.097924: step 1558, loss 0.0714762, acc 0.96875
2017-09-09T15:28:45.372604: step 1559, loss 0.0833599, acc 0.96875
2017-09-09T15:28:45.689209: step 1560, loss 0.121126, acc 0.96875
2017-09-09T15:28:46.056760: step 1561, loss 0.0789639, acc 0.984375
2017-09-09T15:28:46.352693: step 1562, loss 0.0742141, acc 0.984375
2017-09-09T15:28:46.631547: step 1563, loss 0.0848586, acc 1
2017-09-09T15:28:46.948393: step 1564, loss 0.0370819, acc 1
2017-09-09T15:28:47.225410: step 1565, loss 0.100484, acc 0.96875
2017-09-09T15:28:47.551019: step 1566, loss 0.103657, acc 0.984375
2017-09-09T15:28:47.862123: step 1567, loss 0.0715387, acc 0.984375
2017-09-09T15:28:48.207740: step 1568, loss 0.0588085, acc 1
2017-09-09T15:28:48.511617: step 1569, loss 0.104016, acc 0.953125
2017-09-09T15:28:48.789804: step 1570, loss 0.167917, acc 0.953125
2017-09-09T15:28:49.131607: step 1571, loss 0.0539206, acc 1
2017-09-09T15:28:49.409433: step 1572, loss 0.068584, acc 0.984375
2017-09-09T15:28:49.702655: step 1573, loss 0.0855382, acc 0.984375
2017-09-09T15:28:50.020710: step 1574, loss 0.103827, acc 0.96875
2017-09-09T15:28:50.290497: step 1575, loss 0.182574, acc 0.96875
2017-09-09T15:28:50.649038: step 1576, loss 0.0947472, acc 0.953125
2017-09-09T15:28:50.974731: step 1577, loss 0.0810151, acc 0.96875
2017-09-09T15:28:51.276027: step 1578, loss 0.135614, acc 0.953125
2017-09-09T15:28:51.617784: step 1579, loss 0.0583158, acc 1
2017-09-09T15:28:51.920819: step 1580, loss 0.114265, acc 0.953125
2017-09-09T15:28:52.267353: step 1581, loss 0.0922738, acc 0.96875
2017-09-09T15:28:52.561616: step 1582, loss 0.0623992, acc 0.96875
2017-09-09T15:28:52.957892: step 1583, loss 0.0674177, acc 0.984375
2017-09-09T15:28:53.265781: step 1584, loss 0.0639603, acc 0.96875
2017-09-09T15:28:53.539479: step 1585, loss 0.124341, acc 0.953125
2017-09-09T15:28:53.823884: step 1586, loss 0.0745126, acc 0.984375
2017-09-09T15:28:54.086787: step 1587, loss 0.141109, acc 0.953125
2017-09-09T15:28:54.412559: step 1588, loss 0.0518602, acc 1
2017-09-09T15:28:54.712900: step 1589, loss 0.0522835, acc 1
2017-09-09T15:28:55.015986: step 1590, loss 0.121173, acc 0.953125
2017-09-09T15:28:55.261562: step 1591, loss 0.0755514, acc 0.984375
2017-09-09T15:28:55.557875: step 1592, loss 0.0369877, acc 1
2017-09-09T15:28:55.853037: step 1593, loss 0.144417, acc 0.9375
2017-09-09T15:28:56.190931: step 1594, loss 0.0348703, acc 0.984375
2017-09-09T15:28:56.508278: step 1595, loss 0.0948472, acc 0.984375
2017-09-09T15:28:56.769300: step 1596, loss 0.0554906, acc 1
2017-09-09T15:28:57.125589: step 1597, loss 0.0889796, acc 0.953125
2017-09-09T15:28:57.407768: step 1598, loss 0.047856, acc 0.984375
2017-09-09T15:28:57.734252: step 1599, loss 0.142476, acc 0.953125
2017-09-09T15:28:58.326780: step 1600, loss 0.041456, acc 1

Evaluation:
2017-09-09T15:28:58.389861: step 1600, loss 1.7325, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1600

2017-09-09T15:29:01.373407: step 1601, loss 0.0787383, acc 0.96875
2017-09-09T15:29:01.669607: step 1602, loss 0.0734806, acc 0.984375
2017-09-09T15:29:01.930456: step 1603, loss 0.0983312, acc 0.96875
2017-09-09T15:29:02.253117: step 1604, loss 0.0645691, acc 0.96875
2017-09-09T15:29:02.554877: step 1605, loss 0.126272, acc 0.953125
2017-09-09T15:29:02.823173: step 1606, loss 0.0822572, acc 0.953125
2017-09-09T15:29:03.129607: step 1607, loss 0.123757, acc 0.96875
2017-09-09T15:29:03.423303: step 1608, loss 0.100213, acc 1
2017-09-09T15:29:03.732173: step 1609, loss 0.060293, acc 0.984375
2017-09-09T15:29:04.016256: step 1610, loss 0.137467, acc 0.96875
2017-09-09T15:29:04.302911: step 1611, loss 0.064548, acc 1
2017-09-09T15:29:04.582638: step 1612, loss 0.0623337, acc 0.984375
2017-09-09T15:29:04.891249: step 1613, loss 0.0363416, acc 1
2017-09-09T15:29:05.231540: step 1614, loss 0.109338, acc 0.984375
2017-09-09T15:29:05.518942: step 1615, loss 0.119076, acc 0.96875
2017-09-09T15:29:05.833140: step 1616, loss 0.0862195, acc 0.984375
2017-09-09T15:29:06.129188: step 1617, loss 0.0590135, acc 0.96875
2017-09-09T15:29:06.408378: step 1618, loss 0.0601039, acc 0.984375
2017-09-09T15:29:06.731991: step 1619, loss 0.0666438, acc 0.96875
2017-09-09T15:29:07.035134: step 1620, loss 0.0831058, acc 0.96875
2017-09-09T15:29:07.387911: step 1621, loss 0.117487, acc 0.96875
2017-09-09T15:29:07.798521: step 1622, loss 0.0636916, acc 0.984375
2017-09-09T15:29:08.118669: step 1623, loss 0.0923594, acc 0.96875
2017-09-09T15:29:08.421006: step 1624, loss 0.0595092, acc 1
2017-09-09T15:29:08.731071: step 1625, loss 0.109753, acc 0.984375
2017-09-09T15:29:09.045520: step 1626, loss 0.122192, acc 0.953125
2017-09-09T15:29:09.316710: step 1627, loss 0.0124476, acc 1
2017-09-09T15:29:09.649261: step 1628, loss 0.0642119, acc 0.984375
2017-09-09T15:29:09.935188: step 1629, loss 0.0700319, acc 0.984375
2017-09-09T15:29:10.228892: step 1630, loss 0.0771109, acc 0.96875
2017-09-09T15:29:10.539230: step 1631, loss 0.0299663, acc 1
2017-09-09T15:29:10.800081: step 1632, loss 0.0963504, acc 0.984375
2017-09-09T15:29:11.109257: step 1633, loss 0.0565891, acc 1
2017-09-09T15:29:11.387913: step 1634, loss 0.100952, acc 0.953125
2017-09-09T15:29:11.703426: step 1635, loss 0.0552181, acc 0.984375
2017-09-09T15:29:11.963624: step 1636, loss 0.0792981, acc 0.984375
2017-09-09T15:29:12.288615: step 1637, loss 0.1045, acc 0.96875
2017-09-09T15:29:12.584615: step 1638, loss 0.0782096, acc 0.96875
2017-09-09T15:29:12.884475: step 1639, loss 0.0457621, acc 0.984375
2017-09-09T15:29:13.218276: step 1640, loss 0.160488, acc 0.9375
2017-09-09T15:29:13.498715: step 1641, loss 0.118781, acc 0.953125
2017-09-09T15:29:13.824931: step 1642, loss 0.0483809, acc 0.984375
2017-09-09T15:29:14.132848: step 1643, loss 0.0467925, acc 1
2017-09-09T15:29:14.419259: step 1644, loss 0.0415976, acc 1
2017-09-09T15:29:14.764622: step 1645, loss 0.111416, acc 0.9375
2017-09-09T15:29:15.042969: step 1646, loss 0.0205175, acc 1
2017-09-09T15:29:15.390232: step 1647, loss 0.0773942, acc 0.984375
2017-09-09T15:29:15.662542: step 1648, loss 0.0803347, acc 0.96875
2017-09-09T15:29:15.973802: step 1649, loss 0.113591, acc 0.9375
2017-09-09T15:29:16.322428: step 1650, loss 0.0451118, acc 0.984375

Evaluation:
2017-09-09T15:29:16.391725: step 1650, loss 1.7822, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1650

2017-09-09T15:29:18.746404: step 1651, loss 0.0911436, acc 0.96875
2017-09-09T15:29:19.025481: step 1652, loss 0.0335433, acc 1
2017-09-09T15:29:19.301929: step 1653, loss 0.0391751, acc 0.984375
2017-09-09T15:29:19.607335: step 1654, loss 0.0745503, acc 0.984375
2017-09-09T15:29:19.884031: step 1655, loss 0.04112, acc 1
2017-09-09T15:29:20.211851: step 1656, loss 0.102158, acc 0.96875
2017-09-09T15:29:20.475562: step 1657, loss 0.114656, acc 0.953125
2017-09-09T15:29:20.829136: step 1658, loss 0.0475612, acc 0.984375
2017-09-09T15:29:21.127089: step 1659, loss 0.115732, acc 0.96875
2017-09-09T15:29:21.523379: step 1660, loss 0.0742848, acc 0.953125
2017-09-09T15:29:21.828351: step 1661, loss 0.10466, acc 0.96875
2017-09-09T15:29:22.114048: step 1662, loss 0.041424, acc 1
2017-09-09T15:29:22.417168: step 1663, loss 0.0820852, acc 0.96875
2017-09-09T15:29:22.699918: step 1664, loss 0.0241386, acc 1
2017-09-09T15:29:23.015965: step 1665, loss 0.0939696, acc 0.984375
2017-09-09T15:29:23.297805: step 1666, loss 0.114971, acc 0.941176
2017-09-09T15:29:23.596653: step 1667, loss 0.10224, acc 0.953125
2017-09-09T15:29:23.927687: step 1668, loss 0.0898922, acc 0.96875
2017-09-09T15:29:24.202005: step 1669, loss 0.0860036, acc 0.96875
2017-09-09T15:29:24.542168: step 1670, loss 0.171411, acc 0.9375
2017-09-09T15:29:24.861608: step 1671, loss 0.0258107, acc 1
2017-09-09T15:29:25.195004: step 1672, loss 0.0733059, acc 0.984375
2017-09-09T15:29:25.509375: step 1673, loss 0.0732359, acc 0.984375
2017-09-09T15:29:25.798906: step 1674, loss 0.0458925, acc 1
2017-09-09T15:29:26.136208: step 1675, loss 0.0784131, acc 0.984375
2017-09-09T15:29:26.455392: step 1676, loss 0.0929513, acc 0.984375
2017-09-09T15:29:26.740446: step 1677, loss 0.051622, acc 1
2017-09-09T15:29:27.020912: step 1678, loss 0.0534958, acc 0.984375
2017-09-09T15:29:27.325476: step 1679, loss 0.02402, acc 1
2017-09-09T15:29:27.646625: step 1680, loss 0.0940583, acc 0.953125
2017-09-09T15:29:27.946573: step 1681, loss 0.0680549, acc 0.96875
2017-09-09T15:29:28.274987: step 1682, loss 0.0366339, acc 1
2017-09-09T15:29:28.538459: step 1683, loss 0.0418433, acc 0.984375
2017-09-09T15:29:28.882691: step 1684, loss 0.0337414, acc 0.984375
2017-09-09T15:29:29.166799: step 1685, loss 0.0884404, acc 0.96875
2017-09-09T15:29:29.479547: step 1686, loss 0.0681335, acc 0.984375
2017-09-09T15:29:29.778537: step 1687, loss 0.0258758, acc 0.984375
2017-09-09T15:29:30.044094: step 1688, loss 0.0879729, acc 0.96875
2017-09-09T15:29:30.420930: step 1689, loss 0.0613994, acc 0.984375
2017-09-09T15:29:30.715866: step 1690, loss 0.0482304, acc 1
2017-09-09T15:29:31.048350: step 1691, loss 0.0346717, acc 1
2017-09-09T15:29:31.364566: step 1692, loss 0.0281426, acc 0.984375
2017-09-09T15:29:31.672725: step 1693, loss 0.0337921, acc 1
2017-09-09T15:29:31.975162: step 1694, loss 0.0334107, acc 1
2017-09-09T15:29:32.249865: step 1695, loss 0.0428366, acc 0.984375
2017-09-09T15:29:32.631503: step 1696, loss 0.0302209, acc 1
2017-09-09T15:29:32.942220: step 1697, loss 0.0880288, acc 0.953125
2017-09-09T15:29:33.225619: step 1698, loss 0.0334891, acc 1
2017-09-09T15:29:33.619455: step 1699, loss 0.049152, acc 1
2017-09-09T15:29:33.917372: step 1700, loss 0.0976985, acc 0.9375

Evaluation:
2017-09-09T15:29:34.000642: step 1700, loss 1.71348, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1700

2017-09-09T15:29:36.783445: step 1701, loss 0.0657415, acc 0.96875
2017-09-09T15:29:37.094972: step 1702, loss 0.0395836, acc 0.984375
2017-09-09T15:29:37.361937: step 1703, loss 0.0859149, acc 0.984375
2017-09-09T15:29:37.666895: step 1704, loss 0.0400702, acc 1
2017-09-09T15:29:37.923136: step 1705, loss 0.0965649, acc 0.96875
2017-09-09T15:29:38.248139: step 1706, loss 0.0758534, acc 0.953125
2017-09-09T15:29:38.609677: step 1707, loss 0.119811, acc 0.9375
2017-09-09T15:29:38.983520: step 1708, loss 0.0648711, acc 0.96875
2017-09-09T15:29:39.281782: step 1709, loss 0.0464839, acc 0.984375
2017-09-09T15:29:39.643674: step 1710, loss 0.0409589, acc 1
2017-09-09T15:29:39.936594: step 1711, loss 0.111284, acc 0.984375
2017-09-09T15:29:40.227222: step 1712, loss 0.0670754, acc 0.96875
2017-09-09T15:29:40.546559: step 1713, loss 0.0712192, acc 0.96875
2017-09-09T15:29:40.839199: step 1714, loss 0.0490035, acc 0.984375
2017-09-09T15:29:41.223418: step 1715, loss 0.0531753, acc 0.984375
2017-09-09T15:29:41.522656: step 1716, loss 0.0137137, acc 1
2017-09-09T15:29:41.786221: step 1717, loss 0.0508568, acc 0.984375
2017-09-09T15:29:42.092832: step 1718, loss 0.0822457, acc 0.96875
2017-09-09T15:29:42.390477: step 1719, loss 0.107016, acc 0.953125
2017-09-09T15:29:42.688340: step 1720, loss 0.0685447, acc 1
2017-09-09T15:29:42.998369: step 1721, loss 0.0855675, acc 0.96875
2017-09-09T15:29:43.326271: step 1722, loss 0.109372, acc 0.984375
2017-09-09T15:29:43.652027: step 1723, loss 0.0439315, acc 1
2017-09-09T15:29:43.959352: step 1724, loss 0.0953235, acc 0.96875
2017-09-09T15:29:44.261302: step 1725, loss 0.116936, acc 0.953125
2017-09-09T15:29:44.558132: step 1726, loss 0.0570133, acc 0.984375
2017-09-09T15:29:44.889866: step 1727, loss 0.0888425, acc 0.96875
2017-09-09T15:29:45.173172: step 1728, loss 0.078267, acc 0.984375
2017-09-09T15:29:45.504804: step 1729, loss 0.0381928, acc 0.984375
2017-09-09T15:29:45.798250: step 1730, loss 0.0796847, acc 0.984375
2017-09-09T15:29:46.071142: step 1731, loss 0.0422401, acc 0.984375
2017-09-09T15:29:46.349900: step 1732, loss 0.0994972, acc 0.96875
2017-09-09T15:29:46.635641: step 1733, loss 0.0458118, acc 0.984375
2017-09-09T15:29:46.900770: step 1734, loss 0.0783598, acc 0.96875
2017-09-09T15:29:47.179887: step 1735, loss 0.124321, acc 0.953125
2017-09-09T15:29:47.488020: step 1736, loss 0.0880086, acc 0.984375
2017-09-09T15:29:47.749534: step 1737, loss 0.0233363, acc 1
2017-09-09T15:29:48.113611: step 1738, loss 0.0595055, acc 0.984375
2017-09-09T15:29:48.385328: step 1739, loss 0.133957, acc 0.953125
2017-09-09T15:29:48.697025: step 1740, loss 0.0946755, acc 0.96875
2017-09-09T15:29:49.039829: step 1741, loss 0.0605654, acc 1
2017-09-09T15:29:49.407289: step 1742, loss 0.0742159, acc 0.96875
2017-09-09T15:29:49.711451: step 1743, loss 0.108505, acc 0.953125
2017-09-09T15:29:50.015277: step 1744, loss 0.0893316, acc 0.984375
2017-09-09T15:29:50.315588: step 1745, loss 0.106215, acc 0.953125
2017-09-09T15:29:50.598658: step 1746, loss 0.0879759, acc 0.953125
2017-09-09T15:29:50.876333: step 1747, loss 0.120464, acc 0.96875
2017-09-09T15:29:51.152768: step 1748, loss 0.0518719, acc 0.984375
2017-09-09T15:29:51.403331: step 1749, loss 0.0867192, acc 0.953125
2017-09-09T15:29:51.715182: step 1750, loss 0.0724237, acc 0.96875

Evaluation:
2017-09-09T15:29:51.780094: step 1750, loss 1.65862, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1750

2017-09-09T15:29:54.920065: step 1751, loss 0.0924203, acc 0.96875
2017-09-09T15:29:55.249079: step 1752, loss 0.0739388, acc 0.96875
2017-09-09T15:29:55.528967: step 1753, loss 0.0510568, acc 0.96875
2017-09-09T15:29:55.837911: step 1754, loss 0.135171, acc 0.9375
2017-09-09T15:29:56.149338: step 1755, loss 0.118233, acc 0.953125
2017-09-09T15:29:56.425732: step 1756, loss 0.100084, acc 0.984375
2017-09-09T15:29:56.751932: step 1757, loss 0.156366, acc 0.953125
2017-09-09T15:29:57.052181: step 1758, loss 0.0380119, acc 0.984375
2017-09-09T15:29:57.375903: step 1759, loss 0.0802641, acc 1
2017-09-09T15:29:57.642402: step 1760, loss 0.0799847, acc 0.984375
2017-09-09T15:29:57.925886: step 1761, loss 0.107705, acc 0.96875
2017-09-09T15:29:58.224746: step 1762, loss 0.052319, acc 0.984375
2017-09-09T15:29:58.477884: step 1763, loss 0.066688, acc 0.984375
2017-09-09T15:29:58.765655: step 1764, loss 0.109437, acc 0.960784
2017-09-09T15:29:59.046700: step 1765, loss 0.083737, acc 0.984375
2017-09-09T15:29:59.363565: step 1766, loss 0.0698204, acc 0.984375
2017-09-09T15:29:59.645920: step 1767, loss 0.0568408, acc 0.984375
2017-09-09T15:29:59.970760: step 1768, loss 0.0654961, acc 1
2017-09-09T15:30:00.271926: step 1769, loss 0.0926561, acc 0.984375
2017-09-09T15:30:00.548506: step 1770, loss 0.0627171, acc 0.984375
2017-09-09T15:30:00.836840: step 1771, loss 0.078605, acc 0.96875
2017-09-09T15:30:01.131098: step 1772, loss 0.102102, acc 0.96875
2017-09-09T15:30:01.460494: step 1773, loss 0.11932, acc 0.953125
2017-09-09T15:30:01.758546: step 1774, loss 0.0589925, acc 0.984375
2017-09-09T15:30:02.085826: step 1775, loss 0.0703012, acc 0.96875
2017-09-09T15:30:02.421573: step 1776, loss 0.0800858, acc 0.984375
2017-09-09T15:30:02.685642: step 1777, loss 0.146988, acc 0.96875
2017-09-09T15:30:02.993815: step 1778, loss 0.111233, acc 0.9375
2017-09-09T15:30:03.299444: step 1779, loss 0.119679, acc 0.984375
2017-09-09T15:30:03.624299: step 1780, loss 0.0739336, acc 0.96875
2017-09-09T15:30:04.009413: step 1781, loss 0.0783873, acc 0.96875
2017-09-09T15:30:04.339414: step 1782, loss 0.0704503, acc 0.984375
2017-09-09T15:30:04.626430: step 1783, loss 0.0603224, acc 0.984375
2017-09-09T15:30:04.885324: step 1784, loss 0.0581518, acc 0.96875
2017-09-09T15:30:05.173152: step 1785, loss 0.11341, acc 0.984375
2017-09-09T15:30:05.456594: step 1786, loss 0.0916954, acc 0.96875
2017-09-09T15:30:05.727896: step 1787, loss 0.0406058, acc 0.984375
2017-09-09T15:30:06.012982: step 1788, loss 0.035009, acc 1
2017-09-09T15:30:06.381864: step 1789, loss 0.0598486, acc 1
2017-09-09T15:30:06.667890: step 1790, loss 0.0420341, acc 1
2017-09-09T15:30:06.961879: step 1791, loss 0.0952223, acc 0.984375
2017-09-09T15:30:07.231753: step 1792, loss 0.114524, acc 0.953125
2017-09-09T15:30:07.539152: step 1793, loss 0.123736, acc 0.96875
2017-09-09T15:30:07.828282: step 1794, loss 0.0815196, acc 0.984375
2017-09-09T15:30:08.109303: step 1795, loss 0.0370181, acc 1
2017-09-09T15:30:08.413304: step 1796, loss 0.175663, acc 0.921875
2017-09-09T15:30:08.666010: step 1797, loss 0.0396933, acc 1
2017-09-09T15:30:09.027165: step 1798, loss 0.054738, acc 0.96875
2017-09-09T15:30:09.362386: step 1799, loss 0.102345, acc 0.96875
2017-09-09T15:30:09.654094: step 1800, loss 0.101632, acc 0.953125

Evaluation:
2017-09-09T15:30:09.736720: step 1800, loss 1.73998, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1800

2017-09-09T15:30:11.995935: step 1801, loss 0.0370547, acc 1
2017-09-09T15:30:12.290489: step 1802, loss 0.0521297, acc 0.984375
2017-09-09T15:30:12.565827: step 1803, loss 0.0629466, acc 1
2017-09-09T15:30:12.841828: step 1804, loss 0.0340039, acc 1
2017-09-09T15:30:13.120249: step 1805, loss 0.070947, acc 0.96875
2017-09-09T15:30:13.459078: step 1806, loss 0.0221547, acc 1
2017-09-09T15:30:13.724095: step 1807, loss 0.159526, acc 0.953125
2017-09-09T15:30:14.042071: step 1808, loss 0.0780103, acc 0.984375
2017-09-09T15:30:14.361517: step 1809, loss 0.061737, acc 1
2017-09-09T15:30:14.633477: step 1810, loss 0.0346912, acc 1
2017-09-09T15:30:14.953169: step 1811, loss 0.222767, acc 0.96875
2017-09-09T15:30:15.232610: step 1812, loss 0.0820601, acc 0.96875
2017-09-09T15:30:15.551560: step 1813, loss 0.0345322, acc 1
2017-09-09T15:30:15.853509: step 1814, loss 0.0477214, acc 1
2017-09-09T15:30:16.195931: step 1815, loss 0.126675, acc 0.953125
2017-09-09T15:30:16.502718: step 1816, loss 0.0477637, acc 1
2017-09-09T15:30:16.779472: step 1817, loss 0.101744, acc 0.953125
2017-09-09T15:30:17.230751: step 1818, loss 0.0624699, acc 0.984375
2017-09-09T15:30:17.513373: step 1819, loss 0.0762607, acc 0.984375
2017-09-09T15:30:17.762344: step 1820, loss 0.0668176, acc 0.96875
2017-09-09T15:30:18.087379: step 1821, loss 0.0516963, acc 1
2017-09-09T15:30:18.359989: step 1822, loss 0.161125, acc 0.9375
2017-09-09T15:30:18.621570: step 1823, loss 0.0439306, acc 1
2017-09-09T15:30:18.938721: step 1824, loss 0.0634661, acc 0.984375
2017-09-09T15:30:19.262605: step 1825, loss 0.0693869, acc 1
2017-09-09T15:30:19.544795: step 1826, loss 0.0787486, acc 0.984375
2017-09-09T15:30:19.855090: step 1827, loss 0.0306832, acc 0.984375
2017-09-09T15:30:20.192205: step 1828, loss 0.0470309, acc 1
2017-09-09T15:30:20.484232: step 1829, loss 0.110363, acc 0.953125
2017-09-09T15:30:20.812057: step 1830, loss 0.058575, acc 0.96875
2017-09-09T15:30:21.128769: step 1831, loss 0.130803, acc 0.953125
2017-09-09T15:30:21.417315: step 1832, loss 0.110715, acc 0.953125
2017-09-09T15:30:21.806854: step 1833, loss 0.0825433, acc 1
2017-09-09T15:30:22.111286: step 1834, loss 0.0499891, acc 1
2017-09-09T15:30:22.402148: step 1835, loss 0.0632357, acc 0.984375
2017-09-09T15:30:22.701709: step 1836, loss 0.0623424, acc 0.96875
2017-09-09T15:30:22.980417: step 1837, loss 0.0991909, acc 0.96875
2017-09-09T15:30:23.357343: step 1838, loss 0.0446111, acc 1
2017-09-09T15:30:23.641746: step 1839, loss 0.0606734, acc 0.984375
2017-09-09T15:30:23.933995: step 1840, loss 0.0529148, acc 0.984375
2017-09-09T15:30:24.250304: step 1841, loss 0.0628794, acc 0.984375
2017-09-09T15:30:24.522078: step 1842, loss 0.0773096, acc 0.96875
2017-09-09T15:30:24.879759: step 1843, loss 0.0577826, acc 1
2017-09-09T15:30:25.150373: step 1844, loss 0.0496355, acc 0.984375
2017-09-09T15:30:25.442374: step 1845, loss 0.110561, acc 0.953125
2017-09-09T15:30:25.746681: step 1846, loss 0.0843228, acc 0.96875
2017-09-09T15:30:26.028354: step 1847, loss 0.112813, acc 0.9375
2017-09-09T15:30:26.353310: step 1848, loss 0.100118, acc 0.96875
2017-09-09T15:30:26.652923: step 1849, loss 0.221928, acc 0.90625
2017-09-09T15:30:26.960660: step 1850, loss 0.116076, acc 0.9375

Evaluation:
2017-09-09T15:30:27.020612: step 1850, loss 1.95461, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1850

2017-09-09T15:30:30.162862: step 1851, loss 0.0331319, acc 1
2017-09-09T15:30:30.443555: step 1852, loss 0.0654032, acc 0.984375
2017-09-09T15:30:30.777028: step 1853, loss 0.0494454, acc 0.984375
2017-09-09T15:30:31.052103: step 1854, loss 0.0457996, acc 1
2017-09-09T15:30:31.414075: step 1855, loss 0.0412518, acc 1
2017-09-09T15:30:31.711398: step 1856, loss 0.0752021, acc 0.96875
2017-09-09T15:30:31.964224: step 1857, loss 0.0357642, acc 1
2017-09-09T15:30:32.281551: step 1858, loss 0.0536968, acc 0.984375
2017-09-09T15:30:32.537051: step 1859, loss 0.0563792, acc 1
2017-09-09T15:30:32.828864: step 1860, loss 0.06102, acc 0.984375
2017-09-09T15:30:33.127403: step 1861, loss 0.0428863, acc 0.984375
2017-09-09T15:30:33.386142: step 1862, loss 0.0421294, acc 1
2017-09-09T15:30:33.778477: step 1863, loss 0.0198365, acc 1
2017-09-09T15:30:34.089370: step 1864, loss 0.115135, acc 0.96875
2017-09-09T15:30:34.425177: step 1865, loss 0.0267533, acc 1
2017-09-09T15:30:34.767951: step 1866, loss 0.0742658, acc 0.96875
2017-09-09T15:30:35.091589: step 1867, loss 0.111548, acc 0.953125
2017-09-09T15:30:35.406308: step 1868, loss 0.0559109, acc 0.984375
2017-09-09T15:30:35.770651: step 1869, loss 0.0338843, acc 1
2017-09-09T15:30:36.044703: step 1870, loss 0.0918883, acc 0.953125
2017-09-09T15:30:36.447669: step 1871, loss 0.053326, acc 0.984375
2017-09-09T15:30:36.757186: step 1872, loss 0.0889984, acc 0.96875
2017-09-09T15:30:37.040159: step 1873, loss 0.0443058, acc 0.96875
2017-09-09T15:30:37.375875: step 1874, loss 0.105896, acc 0.96875
2017-09-09T15:30:37.657775: step 1875, loss 0.0356757, acc 1
2017-09-09T15:30:38.014762: step 1876, loss 0.0278869, acc 1
2017-09-09T15:30:38.286772: step 1877, loss 0.10361, acc 0.9375
2017-09-09T15:30:38.612532: step 1878, loss 0.0151061, acc 1
2017-09-09T15:30:38.957375: step 1879, loss 0.0547841, acc 0.984375
2017-09-09T15:30:39.240431: step 1880, loss 0.060783, acc 0.984375
2017-09-09T15:30:39.564167: step 1881, loss 0.0976035, acc 0.953125
2017-09-09T15:30:39.862176: step 1882, loss 0.0221274, acc 1
2017-09-09T15:30:40.224011: step 1883, loss 0.0596984, acc 0.96875
2017-09-09T15:30:40.501741: step 1884, loss 0.0432633, acc 1
2017-09-09T15:30:40.786656: step 1885, loss 0.0704094, acc 0.96875
2017-09-09T15:30:41.126574: step 1886, loss 0.0944503, acc 0.984375
2017-09-09T15:30:41.394109: step 1887, loss 0.0732979, acc 0.984375
2017-09-09T15:30:41.747616: step 1888, loss 0.0546675, acc 0.984375
2017-09-09T15:30:42.020079: step 1889, loss 0.131662, acc 0.921875
2017-09-09T15:30:42.318982: step 1890, loss 0.117857, acc 0.9375
2017-09-09T15:30:42.601196: step 1891, loss 0.14215, acc 0.953125
2017-09-09T15:30:42.883409: step 1892, loss 0.0313165, acc 1
2017-09-09T15:30:43.179865: step 1893, loss 0.0454989, acc 0.984375
2017-09-09T15:30:43.456030: step 1894, loss 0.0390475, acc 1
2017-09-09T15:30:43.802257: step 1895, loss 0.094536, acc 1
2017-09-09T15:30:44.083566: step 1896, loss 0.024844, acc 1
2017-09-09T15:30:44.384007: step 1897, loss 0.0554625, acc 0.984375
2017-09-09T15:30:44.689605: step 1898, loss 0.0517596, acc 1
2017-09-09T15:30:44.972097: step 1899, loss 0.0675202, acc 0.984375
2017-09-09T15:30:45.267540: step 1900, loss 0.0684347, acc 0.96875

Evaluation:
2017-09-09T15:30:45.344764: step 1900, loss 1.75314, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1900

2017-09-09T15:30:47.709324: step 1901, loss 0.0420008, acc 1
2017-09-09T15:30:48.009294: step 1902, loss 0.13136, acc 0.9375
2017-09-09T15:30:48.359100: step 1903, loss 0.0495312, acc 0.984375
2017-09-09T15:30:48.640557: step 1904, loss 0.0962923, acc 0.984375
2017-09-09T15:30:48.984987: step 1905, loss 0.0576277, acc 0.984375
2017-09-09T15:30:49.255657: step 1906, loss 0.0259254, acc 1
2017-09-09T15:30:49.596613: step 1907, loss 0.0468812, acc 0.984375
2017-09-09T15:30:49.907532: step 1908, loss 0.0251077, acc 1
2017-09-09T15:30:50.185129: step 1909, loss 0.100443, acc 0.96875
2017-09-09T15:30:50.487299: step 1910, loss 0.170215, acc 0.921875
2017-09-09T15:30:50.779414: step 1911, loss 0.103597, acc 0.96875
2017-09-09T15:30:51.097657: step 1912, loss 0.0207306, acc 1
2017-09-09T15:30:51.344911: step 1913, loss 0.0534223, acc 0.984375
2017-09-09T15:30:51.695876: step 1914, loss 0.0435164, acc 0.984375
2017-09-09T15:30:52.002672: step 1915, loss 0.149321, acc 0.9375
2017-09-09T15:30:52.314046: step 1916, loss 0.0476513, acc 0.984375
2017-09-09T15:30:52.681675: step 1917, loss 0.067011, acc 1
2017-09-09T15:30:52.957079: step 1918, loss 0.0776886, acc 0.984375
2017-09-09T15:30:53.251947: step 1919, loss 0.0899165, acc 0.96875
2017-09-09T15:30:53.616087: step 1920, loss 0.0390892, acc 0.984375
2017-09-09T15:30:53.949698: step 1921, loss 0.0444951, acc 1
2017-09-09T15:30:54.236692: step 1922, loss 0.0877549, acc 0.953125
2017-09-09T15:30:54.514373: step 1923, loss 0.0317612, acc 1
2017-09-09T15:30:54.830846: step 1924, loss 0.053227, acc 0.984375
2017-09-09T15:30:55.216436: step 1925, loss 0.115053, acc 0.9375
2017-09-09T15:30:55.532315: step 1926, loss 0.0236129, acc 1
2017-09-09T15:30:55.840634: step 1927, loss 0.0382475, acc 1
2017-09-09T15:30:56.196618: step 1928, loss 0.0116916, acc 1
2017-09-09T15:30:56.481225: step 1929, loss 0.042443, acc 0.984375
2017-09-09T15:30:56.817289: step 1930, loss 0.0781786, acc 0.953125
2017-09-09T15:30:57.114957: step 1931, loss 0.0215315, acc 1
2017-09-09T15:30:57.408367: step 1932, loss 0.0348977, acc 1
2017-09-09T15:30:57.745375: step 1933, loss 0.0307957, acc 1
2017-09-09T15:30:58.029251: step 1934, loss 0.0744454, acc 0.984375
2017-09-09T15:30:58.370119: step 1935, loss 0.159144, acc 0.953125
2017-09-09T15:30:58.641391: step 1936, loss 0.0753727, acc 0.984375
2017-09-09T15:30:59.027261: step 1937, loss 0.0998246, acc 0.96875
2017-09-09T15:30:59.331096: step 1938, loss 0.0646039, acc 0.984375
2017-09-09T15:30:59.651828: step 1939, loss 0.102394, acc 0.953125
2017-09-09T15:30:59.909015: step 1940, loss 0.0441494, acc 0.984375
2017-09-09T15:31:00.191626: step 1941, loss 0.0638756, acc 0.96875
2017-09-09T15:31:00.438291: step 1942, loss 0.0480485, acc 1
2017-09-09T15:31:00.696200: step 1943, loss 0.0502535, acc 0.984375
2017-09-09T15:31:00.967264: step 1944, loss 0.067695, acc 0.984375
2017-09-09T15:31:01.236731: step 1945, loss 0.0469606, acc 0.984375
2017-09-09T15:31:01.508859: step 1946, loss 0.0902257, acc 0.96875
2017-09-09T15:31:01.805943: step 1947, loss 0.145775, acc 0.953125
2017-09-09T15:31:02.069479: step 1948, loss 0.0470262, acc 0.984375
2017-09-09T15:31:02.385197: step 1949, loss 0.0807374, acc 0.953125
2017-09-09T15:31:02.653688: step 1950, loss 0.0334672, acc 1

Evaluation:
2017-09-09T15:31:02.752290: step 1950, loss 1.88711, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-1950

2017-09-09T15:31:05.538936: step 1951, loss 0.0770787, acc 0.984375
2017-09-09T15:31:05.833448: step 1952, loss 0.198969, acc 0.90625
2017-09-09T15:31:06.132681: step 1953, loss 0.0655082, acc 0.984375
2017-09-09T15:31:06.442368: step 1954, loss 0.0400766, acc 0.984375
2017-09-09T15:31:06.797743: step 1955, loss 0.0732807, acc 0.984375
2017-09-09T15:31:07.065005: step 1956, loss 0.0303715, acc 1
2017-09-09T15:31:07.410754: step 1957, loss 0.0371635, acc 1
2017-09-09T15:31:07.748852: step 1958, loss 0.0416728, acc 0.984375
2017-09-09T15:31:08.064629: step 1959, loss 0.148173, acc 0.921875
2017-09-09T15:31:08.447603: step 1960, loss 0.0238717, acc 1
2017-09-09T15:31:08.730450: step 1961, loss 0.0653796, acc 0.96875
2017-09-09T15:31:09.032979: step 1962, loss 0.108914, acc 0.96875
2017-09-09T15:31:09.335678: step 1963, loss 0.204386, acc 0.90625
2017-09-09T15:31:09.616568: step 1964, loss 0.0368587, acc 1
2017-09-09T15:31:09.927595: step 1965, loss 0.041628, acc 1
2017-09-09T15:31:10.203686: step 1966, loss 0.0617332, acc 0.984375
2017-09-09T15:31:10.546561: step 1967, loss 0.0797223, acc 0.984375
2017-09-09T15:31:10.855891: step 1968, loss 0.043961, acc 1
2017-09-09T15:31:11.182915: step 1969, loss 0.0924119, acc 0.953125
2017-09-09T15:31:11.459234: step 1970, loss 0.0372144, acc 1
2017-09-09T15:31:11.801960: step 1971, loss 0.0290189, acc 1
2017-09-09T15:31:12.127542: step 1972, loss 0.109021, acc 0.953125
2017-09-09T15:31:12.422349: step 1973, loss 0.0269474, acc 1
2017-09-09T15:31:12.769920: step 1974, loss 0.0722907, acc 0.984375
2017-09-09T15:31:13.093791: step 1975, loss 0.0560466, acc 0.984375
2017-09-09T15:31:13.479505: step 1976, loss 0.0751804, acc 0.96875
2017-09-09T15:31:13.778086: step 1977, loss 0.0553085, acc 0.984375
2017-09-09T15:31:14.085069: step 1978, loss 0.0891192, acc 0.953125
2017-09-09T15:31:14.388292: step 1979, loss 0.0987042, acc 0.953125
2017-09-09T15:31:14.659198: step 1980, loss 0.0596555, acc 0.984375
2017-09-09T15:31:14.979439: step 1981, loss 0.054824, acc 1
2017-09-09T15:31:15.831816: step 1982, loss 0.0524207, acc 0.984375
2017-09-09T15:31:16.187784: step 1983, loss 0.0824203, acc 0.953125
2017-09-09T15:31:16.465913: step 1984, loss 0.0705252, acc 0.96875
2017-09-09T15:31:16.788389: step 1985, loss 0.0307243, acc 0.984375
2017-09-09T15:31:17.069282: step 1986, loss 0.0931438, acc 0.953125
2017-09-09T15:31:17.382635: step 1987, loss 0.0859223, acc 0.96875
2017-09-09T15:31:17.716712: step 1988, loss 0.0506137, acc 0.96875
2017-09-09T15:31:18.007137: step 1989, loss 0.0776145, acc 0.953125
2017-09-09T15:31:18.370348: step 1990, loss 0.0614164, acc 1
2017-09-09T15:31:18.675379: step 1991, loss 0.0571217, acc 0.96875
2017-09-09T15:31:18.987426: step 1992, loss 0.0943252, acc 0.96875
2017-09-09T15:31:19.334960: step 1993, loss 0.0366757, acc 1
2017-09-09T15:31:19.629363: step 1994, loss 0.0922598, acc 0.984375
2017-09-09T15:31:19.913610: step 1995, loss 0.0909406, acc 0.96875
2017-09-09T15:31:20.265279: step 1996, loss 0.102797, acc 0.96875
2017-09-09T15:31:20.563626: step 1997, loss 0.0433362, acc 1
2017-09-09T15:31:20.934972: step 1998, loss 0.0568144, acc 1
2017-09-09T15:31:21.210335: step 1999, loss 0.0463963, acc 1
2017-09-09T15:31:21.490351: step 2000, loss 0.0138398, acc 1

Evaluation:
2017-09-09T15:31:21.568163: step 2000, loss 2.1184, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2000

2017-09-09T15:31:24.729005: step 2001, loss 0.0313382, acc 0.984375
2017-09-09T15:31:25.013926: step 2002, loss 0.080456, acc 0.96875
2017-09-09T15:31:25.330824: step 2003, loss 0.0393906, acc 0.984375
2017-09-09T15:31:25.614882: step 2004, loss 0.0141424, acc 1
2017-09-09T15:31:25.959625: step 2005, loss 0.0285129, acc 1
2017-09-09T15:31:26.269979: step 2006, loss 0.0137425, acc 1
2017-09-09T15:31:26.589784: step 2007, loss 0.0652273, acc 0.984375
2017-09-09T15:31:26.906528: step 2008, loss 0.0277424, acc 0.984375
2017-09-09T15:31:27.337557: step 2009, loss 0.0739272, acc 0.984375
2017-09-09T15:31:27.621797: step 2010, loss 0.0235222, acc 1
2017-09-09T15:31:27.936376: step 2011, loss 0.0811925, acc 0.96875
2017-09-09T15:31:28.248282: step 2012, loss 0.0925521, acc 0.953125
2017-09-09T15:31:28.525869: step 2013, loss 0.0733974, acc 0.96875
2017-09-09T15:31:28.816866: step 2014, loss 0.118994, acc 0.921875
2017-09-09T15:31:29.095346: step 2015, loss 0.0622626, acc 0.96875
2017-09-09T15:31:29.375374: step 2016, loss 0.0426218, acc 0.984375
2017-09-09T15:31:29.679189: step 2017, loss 0.0294006, acc 0.984375
2017-09-09T15:31:29.980568: step 2018, loss 0.0817919, acc 0.984375
2017-09-09T15:31:30.300204: step 2019, loss 0.0591862, acc 0.984375
2017-09-09T15:31:30.592382: step 2020, loss 0.0553235, acc 0.984375
2017-09-09T15:31:30.902182: step 2021, loss 0.0367296, acc 1
2017-09-09T15:31:31.173085: step 2022, loss 0.102178, acc 0.984375
2017-09-09T15:31:31.500451: step 2023, loss 0.0335292, acc 0.984375
2017-09-09T15:31:31.836049: step 2024, loss 0.0834484, acc 0.984375
2017-09-09T15:31:32.122561: step 2025, loss 0.0992601, acc 0.96875
2017-09-09T15:31:32.441104: step 2026, loss 0.0966233, acc 0.953125
2017-09-09T15:31:32.772145: step 2027, loss 0.0938137, acc 0.96875
2017-09-09T15:31:33.112917: step 2028, loss 0.0449486, acc 0.96875
2017-09-09T15:31:33.443894: step 2029, loss 0.0351819, acc 1
2017-09-09T15:31:33.749303: step 2030, loss 0.0447872, acc 1
2017-09-09T15:31:34.032573: step 2031, loss 0.0564462, acc 1
2017-09-09T15:31:34.319361: step 2032, loss 0.047116, acc 0.984375
2017-09-09T15:31:34.602286: step 2033, loss 0.0511661, acc 0.984375
2017-09-09T15:31:34.963174: step 2034, loss 0.0877826, acc 0.953125
2017-09-09T15:31:35.229333: step 2035, loss 0.113999, acc 0.96875
2017-09-09T15:31:35.623166: step 2036, loss 0.077413, acc 0.953125
2017-09-09T15:31:35.901150: step 2037, loss 0.0324645, acc 1
2017-09-09T15:31:36.216922: step 2038, loss 0.0621059, acc 0.984375
2017-09-09T15:31:36.526194: step 2039, loss 0.0646026, acc 0.984375
2017-09-09T15:31:36.785979: step 2040, loss 0.0281225, acc 0.984375
2017-09-09T15:31:37.089714: step 2041, loss 0.0635166, acc 0.984375
2017-09-09T15:31:37.359105: step 2042, loss 0.131894, acc 0.953125
2017-09-09T15:31:37.653608: step 2043, loss 0.0874841, acc 0.96875
2017-09-09T15:31:37.945289: step 2044, loss 0.0785424, acc 0.96875
2017-09-09T15:31:38.229387: step 2045, loss 0.0824997, acc 0.96875
2017-09-09T15:31:38.495247: step 2046, loss 0.0365359, acc 0.984375
2017-09-09T15:31:38.788973: step 2047, loss 0.075173, acc 0.96875
2017-09-09T15:31:39.080384: step 2048, loss 0.0553216, acc 0.984375
2017-09-09T15:31:39.368013: step 2049, loss 0.023707, acc 1
2017-09-09T15:31:39.724360: step 2050, loss 0.0634637, acc 0.984375

Evaluation:
2017-09-09T15:31:39.789318: step 2050, loss 2.08948, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2050

2017-09-09T15:31:42.142178: step 2051, loss 0.0583375, acc 0.984375
2017-09-09T15:31:42.476471: step 2052, loss 0.0305426, acc 1
2017-09-09T15:31:42.726378: step 2053, loss 0.163437, acc 0.953125
2017-09-09T15:31:43.034252: step 2054, loss 0.0242526, acc 1
2017-09-09T15:31:43.373194: step 2055, loss 0.0457884, acc 0.984375
2017-09-09T15:31:43.663921: step 2056, loss 0.070231, acc 0.96875
2017-09-09T15:31:44.061419: step 2057, loss 0.0656864, acc 0.984375
2017-09-09T15:31:44.359480: step 2058, loss 0.0468037, acc 0.980392
2017-09-09T15:31:44.688912: step 2059, loss 0.130921, acc 0.953125
2017-09-09T15:31:45.046930: step 2060, loss 0.0293178, acc 1
2017-09-09T15:31:45.367997: step 2061, loss 0.0777869, acc 0.984375
2017-09-09T15:31:45.700190: step 2062, loss 0.0881526, acc 0.984375
2017-09-09T15:31:46.033965: step 2063, loss 0.0941071, acc 0.9375
2017-09-09T15:31:46.341326: step 2064, loss 0.0922635, acc 0.96875
2017-09-09T15:31:46.644876: step 2065, loss 0.049346, acc 0.984375
2017-09-09T15:31:47.024673: step 2066, loss 0.0724068, acc 0.984375
2017-09-09T15:31:47.375612: step 2067, loss 0.0821036, acc 0.953125
2017-09-09T15:31:47.696771: step 2068, loss 0.0539934, acc 0.984375
2017-09-09T15:31:48.035819: step 2069, loss 0.0450679, acc 0.984375
2017-09-09T15:31:48.329040: step 2070, loss 0.0406497, acc 0.984375
2017-09-09T15:31:48.718958: step 2071, loss 0.0384647, acc 1
2017-09-09T15:31:49.035282: step 2072, loss 0.0712835, acc 0.984375
2017-09-09T15:31:49.355623: step 2073, loss 0.128779, acc 0.9375
2017-09-09T15:31:49.679256: step 2074, loss 0.0561708, acc 0.96875
2017-09-09T15:31:49.948669: step 2075, loss 0.0448207, acc 0.984375
2017-09-09T15:31:50.273927: step 2076, loss 0.0632488, acc 0.96875
2017-09-09T15:31:50.548469: step 2077, loss 0.0561627, acc 0.984375
2017-09-09T15:31:50.913965: step 2078, loss 0.146388, acc 0.96875
2017-09-09T15:31:51.189742: step 2079, loss 0.0608236, acc 0.984375
2017-09-09T15:31:51.457576: step 2080, loss 0.0474422, acc 0.984375
2017-09-09T15:31:51.832520: step 2081, loss 0.0334953, acc 0.984375
2017-09-09T15:31:52.200451: step 2082, loss 0.0580888, acc 0.96875
2017-09-09T15:31:52.493635: step 2083, loss 0.034191, acc 1
2017-09-09T15:31:52.821864: step 2084, loss 0.0450581, acc 0.984375
2017-09-09T15:31:53.137115: step 2085, loss 0.107102, acc 0.953125
2017-09-09T15:31:53.515128: step 2086, loss 0.0374918, acc 0.984375
2017-09-09T15:31:53.829443: step 2087, loss 0.161568, acc 0.9375
2017-09-09T15:31:54.094528: step 2088, loss 0.0396792, acc 1
2017-09-09T15:31:54.388358: step 2089, loss 0.0405095, acc 0.984375
2017-09-09T15:31:54.646731: step 2090, loss 0.0229651, acc 1
2017-09-09T15:31:54.980224: step 2091, loss 0.0353366, acc 0.984375
2017-09-09T15:31:55.275977: step 2092, loss 0.0397181, acc 1
2017-09-09T15:31:55.567358: step 2093, loss 0.0819501, acc 0.96875
2017-09-09T15:31:55.830480: step 2094, loss 0.0205527, acc 1
2017-09-09T15:31:56.144383: step 2095, loss 0.0461358, acc 0.984375
2017-09-09T15:31:56.423775: step 2096, loss 0.0713591, acc 0.984375
2017-09-09T15:31:56.676734: step 2097, loss 0.0435767, acc 0.984375
2017-09-09T15:31:56.989668: step 2098, loss 0.0319076, acc 0.984375
2017-09-09T15:31:57.279380: step 2099, loss 0.0148414, acc 1
2017-09-09T15:31:57.542573: step 2100, loss 0.137592, acc 0.96875

Evaluation:
2017-09-09T15:31:57.678147: step 2100, loss 1.90136, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2100

2017-09-09T15:32:00.785065: step 2101, loss 0.0282871, acc 1
2017-09-09T15:32:01.072130: step 2102, loss 0.0650369, acc 0.96875
2017-09-09T15:32:01.415057: step 2103, loss 0.0156371, acc 1
2017-09-09T15:32:01.748940: step 2104, loss 0.0775917, acc 0.96875
2017-09-09T15:32:02.059688: step 2105, loss 0.0695045, acc 0.984375
2017-09-09T15:32:02.410836: step 2106, loss 0.0442199, acc 0.984375
2017-09-09T15:32:02.694133: step 2107, loss 0.0429747, acc 0.984375
2017-09-09T15:32:02.967725: step 2108, loss 0.0334885, acc 1
2017-09-09T15:32:03.269983: step 2109, loss 0.0322267, acc 1
2017-09-09T15:32:03.576381: step 2110, loss 0.0392978, acc 1
2017-09-09T15:32:03.952706: step 2111, loss 0.0773205, acc 0.96875
2017-09-09T15:32:04.218039: step 2112, loss 0.0218037, acc 1
2017-09-09T15:32:04.565537: step 2113, loss 0.0738658, acc 0.96875
2017-09-09T15:32:04.876291: step 2114, loss 0.116193, acc 0.953125
2017-09-09T15:32:05.142450: step 2115, loss 0.0416681, acc 0.96875
2017-09-09T15:32:05.491349: step 2116, loss 0.0922263, acc 0.953125
2017-09-09T15:32:05.783111: step 2117, loss 0.042515, acc 0.984375
2017-09-09T15:32:06.140038: step 2118, loss 0.0251787, acc 1
2017-09-09T15:32:06.430834: step 2119, loss 0.0827794, acc 0.953125
2017-09-09T15:32:06.733763: step 2120, loss 0.0245996, acc 1
2017-09-09T15:32:07.020474: step 2121, loss 0.0683706, acc 0.984375
2017-09-09T15:32:07.300455: step 2122, loss 0.0120484, acc 1
2017-09-09T15:32:07.621876: step 2123, loss 0.0305221, acc 1
2017-09-09T15:32:07.906191: step 2124, loss 0.0528869, acc 1
2017-09-09T15:32:08.253512: step 2125, loss 0.0496664, acc 1
2017-09-09T15:32:08.540228: step 2126, loss 0.0652929, acc 0.984375
2017-09-09T15:32:08.839812: step 2127, loss 0.0157435, acc 1
2017-09-09T15:32:09.105407: step 2128, loss 0.0333941, acc 1
2017-09-09T15:32:09.446459: step 2129, loss 0.0587344, acc 0.984375
2017-09-09T15:32:09.746292: step 2130, loss 0.0655201, acc 0.96875
2017-09-09T15:32:10.040910: step 2131, loss 0.0485963, acc 0.984375
2017-09-09T15:32:10.307636: step 2132, loss 0.020528, acc 1
2017-09-09T15:32:10.607582: step 2133, loss 0.037911, acc 0.984375
2017-09-09T15:32:10.906841: step 2134, loss 0.0341065, acc 1
2017-09-09T15:32:11.197970: step 2135, loss 0.165678, acc 0.9375
2017-09-09T15:32:11.499305: step 2136, loss 0.107227, acc 0.96875
2017-09-09T15:32:11.771789: step 2137, loss 0.020449, acc 1
2017-09-09T15:32:12.047138: step 2138, loss 0.0851319, acc 0.984375
2017-09-09T15:32:12.327929: step 2139, loss 0.0736792, acc 0.984375
2017-09-09T15:32:12.674944: step 2140, loss 0.0641041, acc 0.984375
2017-09-09T15:32:12.982043: step 2141, loss 0.0925435, acc 0.953125
2017-09-09T15:32:13.311073: step 2142, loss 0.0585908, acc 0.984375
2017-09-09T15:32:13.675045: step 2143, loss 0.0505576, acc 0.984375
2017-09-09T15:32:13.986138: step 2144, loss 0.0632242, acc 1
2017-09-09T15:32:14.342940: step 2145, loss 0.0644546, acc 0.96875
2017-09-09T15:32:14.661983: step 2146, loss 0.0943046, acc 0.953125
2017-09-09T15:32:14.971473: step 2147, loss 0.0432214, acc 1
2017-09-09T15:32:15.352105: step 2148, loss 0.0540606, acc 0.984375
2017-09-09T15:32:15.643166: step 2149, loss 0.0994586, acc 0.96875
2017-09-09T15:32:16.360162: step 2150, loss 0.035877, acc 1

Evaluation:
2017-09-09T15:32:16.444591: step 2150, loss 1.89396, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2150

2017-09-09T15:32:19.041343: step 2151, loss 0.0212194, acc 1
2017-09-09T15:32:19.415186: step 2152, loss 0.075715, acc 0.96875
2017-09-09T15:32:19.746033: step 2153, loss 0.0824074, acc 0.984375
2017-09-09T15:32:20.053022: step 2154, loss 0.0772259, acc 0.96875
2017-09-09T15:32:20.383015: step 2155, loss 0.0740285, acc 0.984375
2017-09-09T15:32:20.660763: step 2156, loss 0.0687713, acc 0.980392
2017-09-09T15:32:21.076628: step 2157, loss 0.0441959, acc 0.984375
2017-09-09T15:32:21.380543: step 2158, loss 0.0531947, acc 0.96875
2017-09-09T15:32:21.665833: step 2159, loss 0.069738, acc 0.984375
2017-09-09T15:32:22.002142: step 2160, loss 0.096789, acc 0.953125
2017-09-09T15:32:22.332673: step 2161, loss 0.041752, acc 0.984375
2017-09-09T15:32:22.622078: step 2162, loss 0.0713829, acc 1
2017-09-09T15:32:22.905019: step 2163, loss 0.0510621, acc 0.96875
2017-09-09T15:32:23.219844: step 2164, loss 0.0285418, acc 1
2017-09-09T15:32:23.526158: step 2165, loss 0.0791456, acc 0.984375
2017-09-09T15:32:23.926462: step 2166, loss 0.073647, acc 0.96875
2017-09-09T15:32:24.216087: step 2167, loss 0.0732031, acc 0.96875
2017-09-09T15:32:24.516809: step 2168, loss 0.0478673, acc 0.984375
2017-09-09T15:32:24.788657: step 2169, loss 0.0155991, acc 1
2017-09-09T15:32:25.116539: step 2170, loss 0.0528615, acc 1
2017-09-09T15:32:25.465107: step 2171, loss 0.0810267, acc 0.984375
2017-09-09T15:32:25.748814: step 2172, loss 0.0293659, acc 1
2017-09-09T15:32:26.026039: step 2173, loss 0.151289, acc 0.9375
2017-09-09T15:32:26.342809: step 2174, loss 0.0740824, acc 0.984375
2017-09-09T15:32:26.603087: step 2175, loss 0.0547733, acc 0.96875
2017-09-09T15:32:26.868547: step 2176, loss 0.0610038, acc 0.96875
2017-09-09T15:32:27.176667: step 2177, loss 0.0279329, acc 1
2017-09-09T15:32:27.456305: step 2178, loss 0.0239845, acc 1
2017-09-09T15:32:27.764036: step 2179, loss 0.0428478, acc 1
2017-09-09T15:32:28.062799: step 2180, loss 0.044321, acc 0.984375
2017-09-09T15:32:28.363079: step 2181, loss 0.0764214, acc 0.953125
2017-09-09T15:32:28.671581: step 2182, loss 0.069973, acc 0.96875
2017-09-09T15:32:28.953414: step 2183, loss 0.0267641, acc 1
2017-09-09T15:32:29.278786: step 2184, loss 0.0818182, acc 0.953125
2017-09-09T15:32:29.577923: step 2185, loss 0.0256188, acc 1
2017-09-09T15:32:29.894062: step 2186, loss 0.049357, acc 1
2017-09-09T15:32:30.169975: step 2187, loss 0.0324221, acc 1
2017-09-09T15:32:30.524263: step 2188, loss 0.0522966, acc 0.984375
2017-09-09T15:32:30.838954: step 2189, loss 0.0140627, acc 1
2017-09-09T15:32:31.114514: step 2190, loss 0.130398, acc 0.9375
2017-09-09T15:32:31.473348: step 2191, loss 0.0876833, acc 0.953125
2017-09-09T15:32:31.730354: step 2192, loss 0.0438397, acc 1
2017-09-09T15:32:32.028024: step 2193, loss 0.0876999, acc 0.96875
2017-09-09T15:32:32.314698: step 2194, loss 0.0484282, acc 0.984375
2017-09-09T15:32:32.574902: step 2195, loss 0.0519972, acc 0.96875
2017-09-09T15:32:32.864808: step 2196, loss 0.0478892, acc 1
2017-09-09T15:32:33.159573: step 2197, loss 0.0370682, acc 0.984375
2017-09-09T15:32:33.489925: step 2198, loss 0.0317976, acc 0.984375
2017-09-09T15:32:33.766845: step 2199, loss 0.047241, acc 0.984375
2017-09-09T15:32:34.151820: step 2200, loss 0.0757347, acc 0.984375

Evaluation:
2017-09-09T15:32:34.217529: step 2200, loss 1.75311, acc 0.351079

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2200

2017-09-09T15:32:36.955610: step 2201, loss 0.0579599, acc 1
2017-09-09T15:32:37.237643: step 2202, loss 0.0277504, acc 1
2017-09-09T15:32:37.550248: step 2203, loss 0.0976469, acc 0.96875
2017-09-09T15:32:37.909198: step 2204, loss 0.0476574, acc 1
2017-09-09T15:32:38.240511: step 2205, loss 0.0789282, acc 0.953125
2017-09-09T15:32:38.516420: step 2206, loss 0.0575967, acc 0.96875
2017-09-09T15:32:38.783981: step 2207, loss 0.0261882, acc 1
2017-09-09T15:32:39.106548: step 2208, loss 0.0630289, acc 0.96875
2017-09-09T15:32:39.360604: step 2209, loss 0.0529128, acc 0.984375
2017-09-09T15:32:39.651642: step 2210, loss 0.0221197, acc 1
2017-09-09T15:32:39.926001: step 2211, loss 0.0413464, acc 0.984375
2017-09-09T15:32:40.239030: step 2212, loss 0.0302229, acc 1
2017-09-09T15:32:40.504187: step 2213, loss 0.0238334, acc 1
2017-09-09T15:32:40.798593: step 2214, loss 0.0149883, acc 1
2017-09-09T15:32:41.098712: step 2215, loss 0.0731046, acc 0.96875
2017-09-09T15:32:41.378280: step 2216, loss 0.0209131, acc 1
2017-09-09T15:32:41.716393: step 2217, loss 0.0421936, acc 0.984375
2017-09-09T15:32:42.013077: step 2218, loss 0.0117525, acc 1
2017-09-09T15:32:42.321728: step 2219, loss 0.0429581, acc 0.984375
2017-09-09T15:32:42.626049: step 2220, loss 0.0225652, acc 1
2017-09-09T15:32:42.928134: step 2221, loss 0.0811252, acc 0.96875
2017-09-09T15:32:43.301360: step 2222, loss 0.0544925, acc 0.984375
2017-09-09T15:32:43.568088: step 2223, loss 0.0287375, acc 1
2017-09-09T15:32:43.929982: step 2224, loss 0.0546559, acc 0.984375
2017-09-09T15:32:44.280583: step 2225, loss 0.0618968, acc 0.984375
2017-09-09T15:32:44.605807: step 2226, loss 0.065953, acc 0.984375
2017-09-09T15:32:44.942028: step 2227, loss 0.0780311, acc 0.953125
2017-09-09T15:32:45.270492: step 2228, loss 0.0304924, acc 1
2017-09-09T15:32:45.550786: step 2229, loss 0.0491716, acc 0.984375
2017-09-09T15:32:45.903071: step 2230, loss 0.0324336, acc 0.984375
2017-09-09T15:32:46.205529: step 2231, loss 0.0717249, acc 0.96875
2017-09-09T15:32:46.519564: step 2232, loss 0.0711686, acc 0.984375
2017-09-09T15:32:46.806291: step 2233, loss 0.0605249, acc 0.984375
2017-09-09T15:32:47.785689: step 2234, loss 0.0250905, acc 1
2017-09-09T15:32:48.095553: step 2235, loss 0.0623292, acc 0.984375
2017-09-09T15:32:48.426871: step 2236, loss 0.0703848, acc 0.96875
2017-09-09T15:32:48.791115: step 2237, loss 0.0314769, acc 0.984375
2017-09-09T15:32:49.072256: step 2238, loss 0.0620751, acc 0.96875
2017-09-09T15:32:49.441493: step 2239, loss 0.112993, acc 0.953125
2017-09-09T15:32:49.727497: step 2240, loss 0.0434955, acc 0.984375
2017-09-09T15:32:50.007376: step 2241, loss 0.0477561, acc 0.984375
2017-09-09T15:32:50.313745: step 2242, loss 0.0691647, acc 0.96875
2017-09-09T15:32:50.579386: step 2243, loss 0.042577, acc 0.984375
2017-09-09T15:32:50.901163: step 2244, loss 0.0351047, acc 0.984375
2017-09-09T15:32:51.188896: step 2245, loss 0.0633324, acc 0.96875
2017-09-09T15:32:51.526777: step 2246, loss 0.0853243, acc 0.953125
2017-09-09T15:32:51.865117: step 2247, loss 0.110409, acc 0.953125
2017-09-09T15:32:52.181248: step 2248, loss 0.0935003, acc 0.984375
2017-09-09T15:32:52.502483: step 2249, loss 0.0243254, acc 1
2017-09-09T15:32:52.788404: step 2250, loss 0.0181379, acc 1

Evaluation:
2017-09-09T15:32:52.892012: step 2250, loss 2.05925, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2250

2017-09-09T15:32:56.200948: step 2251, loss 0.0531951, acc 0.984375
2017-09-09T15:32:56.493922: step 2252, loss 0.02105, acc 1
2017-09-09T15:32:56.779121: step 2253, loss 0.0996268, acc 0.953125
2017-09-09T15:32:57.112005: step 2254, loss 0.0783895, acc 0.980392
2017-09-09T15:32:57.396151: step 2255, loss 0.043527, acc 0.984375
2017-09-09T15:32:57.748794: step 2256, loss 0.0159355, acc 1
2017-09-09T15:32:58.038470: step 2257, loss 0.0519192, acc 0.984375
2017-09-09T15:32:58.342361: step 2258, loss 0.0503524, acc 1
2017-09-09T15:32:58.610429: step 2259, loss 0.0438697, acc 0.984375
2017-09-09T15:32:58.917772: step 2260, loss 0.102752, acc 0.96875
2017-09-09T15:32:59.266219: step 2261, loss 0.0508713, acc 0.96875
2017-09-09T15:32:59.555258: step 2262, loss 0.070864, acc 0.984375
2017-09-09T15:32:59.871741: step 2263, loss 0.0266708, acc 1
2017-09-09T15:33:00.177426: step 2264, loss 0.0164966, acc 1
2017-09-09T15:33:00.445765: step 2265, loss 0.0774058, acc 0.96875
2017-09-09T15:33:00.727779: step 2266, loss 0.0814488, acc 0.984375
2017-09-09T15:33:01.037570: step 2267, loss 0.0243083, acc 0.984375
2017-09-09T15:33:01.344740: step 2268, loss 0.0599536, acc 0.984375
2017-09-09T15:33:01.642686: step 2269, loss 0.0879377, acc 0.984375
2017-09-09T15:33:01.940790: step 2270, loss 0.0503898, acc 0.984375
2017-09-09T15:33:02.218604: step 2271, loss 0.0698195, acc 0.984375
2017-09-09T15:33:02.542703: step 2272, loss 0.0626319, acc 0.984375
2017-09-09T15:33:02.831931: step 2273, loss 0.0515279, acc 0.984375
2017-09-09T15:33:03.174402: step 2274, loss 0.0820577, acc 0.953125
2017-09-09T15:33:03.466098: step 2275, loss 0.0646666, acc 0.96875
2017-09-09T15:33:03.758567: step 2276, loss 0.0655835, acc 1
2017-09-09T15:33:04.103899: step 2277, loss 0.0481933, acc 1
2017-09-09T15:33:04.372330: step 2278, loss 0.0231393, acc 0.984375
2017-09-09T15:33:04.656965: step 2279, loss 0.0328505, acc 1
2017-09-09T15:33:04.967390: step 2280, loss 0.0445586, acc 0.984375
2017-09-09T15:33:05.283002: step 2281, loss 0.0581201, acc 0.96875
2017-09-09T15:33:05.572326: step 2282, loss 0.049279, acc 0.984375
2017-09-09T15:33:05.854703: step 2283, loss 0.0879466, acc 0.984375
2017-09-09T15:33:06.145661: step 2284, loss 0.117528, acc 0.953125
2017-09-09T15:33:06.401287: step 2285, loss 0.0332787, acc 0.984375
2017-09-09T15:33:06.721701: step 2286, loss 0.0257939, acc 1
2017-09-09T15:33:07.016337: step 2287, loss 0.0157975, acc 1
2017-09-09T15:33:07.391477: step 2288, loss 0.0522303, acc 0.984375
2017-09-09T15:33:07.706968: step 2289, loss 0.0244895, acc 0.984375
2017-09-09T15:33:07.978195: step 2290, loss 0.0440394, acc 0.984375
2017-09-09T15:33:08.297978: step 2291, loss 0.0355991, acc 1
2017-09-09T15:33:08.561616: step 2292, loss 0.0421494, acc 1
2017-09-09T15:33:08.825788: step 2293, loss 0.0616646, acc 0.96875
2017-09-09T15:33:09.146608: step 2294, loss 0.0333469, acc 1
2017-09-09T15:33:09.451862: step 2295, loss 0.081488, acc 0.984375
2017-09-09T15:33:09.759185: step 2296, loss 0.0806796, acc 0.96875
2017-09-09T15:33:10.066406: step 2297, loss 0.05039, acc 0.984375
2017-09-09T15:33:10.354174: step 2298, loss 0.0275786, acc 1
2017-09-09T15:33:10.687163: step 2299, loss 0.159063, acc 0.921875
2017-09-09T15:33:10.961066: step 2300, loss 0.0390669, acc 1

Evaluation:
2017-09-09T15:33:11.095463: step 2300, loss 1.93478, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2300

2017-09-09T15:33:13.338206: step 2301, loss 0.0643047, acc 0.96875
2017-09-09T15:33:13.696753: step 2302, loss 0.0288843, acc 0.984375
2017-09-09T15:33:13.994713: step 2303, loss 0.0421083, acc 1
2017-09-09T15:33:14.364789: step 2304, loss 0.0142813, acc 1
2017-09-09T15:33:14.719336: step 2305, loss 0.0360652, acc 0.984375
2017-09-09T15:33:15.023768: step 2306, loss 0.0895169, acc 0.953125
2017-09-09T15:33:15.356977: step 2307, loss 0.0165581, acc 1
2017-09-09T15:33:15.691276: step 2308, loss 0.0606042, acc 0.96875
2017-09-09T15:33:15.975509: step 2309, loss 0.0418018, acc 0.984375
2017-09-09T15:33:16.328783: step 2310, loss 0.0180271, acc 1
2017-09-09T15:33:16.651447: step 2311, loss 0.145687, acc 0.9375
2017-09-09T15:33:16.980747: step 2312, loss 0.0434704, acc 0.96875
2017-09-09T15:33:17.348995: step 2313, loss 0.0756245, acc 0.96875
2017-09-09T15:33:17.641346: step 2314, loss 0.0367764, acc 1
2017-09-09T15:33:17.956673: step 2315, loss 0.0767147, acc 0.953125
2017-09-09T15:33:18.245519: step 2316, loss 0.04866, acc 1
2017-09-09T15:33:18.574290: step 2317, loss 0.111111, acc 0.96875
2017-09-09T15:33:18.912991: step 2318, loss 0.0135218, acc 1
2017-09-09T15:33:19.231874: step 2319, loss 0.0344581, acc 1
2017-09-09T15:33:19.528281: step 2320, loss 0.111226, acc 0.96875
2017-09-09T15:33:19.840895: step 2321, loss 0.035423, acc 0.984375
2017-09-09T15:33:20.129089: step 2322, loss 0.0275742, acc 1
2017-09-09T15:33:20.420242: step 2323, loss 0.0269383, acc 0.984375
2017-09-09T15:33:20.713251: step 2324, loss 0.0366221, acc 0.984375
2017-09-09T15:33:21.059362: step 2325, loss 0.0912275, acc 0.96875
2017-09-09T15:33:21.324922: step 2326, loss 0.0668884, acc 0.96875
2017-09-09T15:33:21.622204: step 2327, loss 0.0448972, acc 0.984375
2017-09-09T15:33:22.034451: step 2328, loss 0.0484564, acc 0.984375
2017-09-09T15:33:22.442261: step 2329, loss 0.0881305, acc 0.984375
2017-09-09T15:33:22.724285: step 2330, loss 0.00963838, acc 1
2017-09-09T15:33:22.996895: step 2331, loss 0.0353773, acc 1
2017-09-09T15:33:23.318104: step 2332, loss 0.17318, acc 0.9375
2017-09-09T15:33:23.580002: step 2333, loss 0.082235, acc 0.953125
2017-09-09T15:33:23.862144: step 2334, loss 0.0391102, acc 0.984375
2017-09-09T15:33:24.110227: step 2335, loss 0.022259, acc 0.984375
2017-09-09T15:33:24.445806: step 2336, loss 0.0189827, acc 1
2017-09-09T15:33:24.779738: step 2337, loss 0.0350438, acc 0.984375
2017-09-09T15:33:25.041934: step 2338, loss 0.0671659, acc 0.984375
2017-09-09T15:33:25.330241: step 2339, loss 0.0965063, acc 0.96875
2017-09-09T15:33:25.640986: step 2340, loss 0.024455, acc 1
2017-09-09T15:33:25.998585: step 2341, loss 0.0332457, acc 0.984375
2017-09-09T15:33:26.294857: step 2342, loss 0.0556684, acc 0.984375
2017-09-09T15:33:26.614378: step 2343, loss 0.0310246, acc 1
2017-09-09T15:33:26.986212: step 2344, loss 0.0796581, acc 0.984375
2017-09-09T15:33:27.354180: step 2345, loss 0.0451499, acc 1
2017-09-09T15:33:27.655269: step 2346, loss 0.0617926, acc 1
2017-09-09T15:33:28.008003: step 2347, loss 0.0930872, acc 0.953125
2017-09-09T15:33:28.376198: step 2348, loss 0.0712424, acc 0.984375
2017-09-09T15:33:28.706543: step 2349, loss 0.0470106, acc 1
2017-09-09T15:33:29.036991: step 2350, loss 0.0477712, acc 0.984375

Evaluation:
2017-09-09T15:33:29.091700: step 2350, loss 1.87778, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2350

2017-09-09T15:33:32.610300: step 2351, loss 0.0269835, acc 1
2017-09-09T15:33:32.914598: step 2352, loss 0.0484186, acc 0.980392
2017-09-09T15:33:33.224390: step 2353, loss 0.0888231, acc 0.96875
2017-09-09T15:33:33.548937: step 2354, loss 0.0838704, acc 0.984375
2017-09-09T15:33:33.834973: step 2355, loss 0.019072, acc 1
2017-09-09T15:33:34.271905: step 2356, loss 0.0320685, acc 1
2017-09-09T15:33:34.592496: step 2357, loss 0.0565858, acc 0.96875
2017-09-09T15:33:34.937999: step 2358, loss 0.035703, acc 0.984375
2017-09-09T15:33:35.233504: step 2359, loss 0.0844463, acc 0.96875
2017-09-09T15:33:35.538614: step 2360, loss 0.0611305, acc 0.96875
2017-09-09T15:33:35.829043: step 2361, loss 0.0702729, acc 0.953125
2017-09-09T15:33:36.137252: step 2362, loss 0.0376327, acc 0.984375
2017-09-09T15:33:36.421814: step 2363, loss 0.00855529, acc 1
2017-09-09T15:33:36.690489: step 2364, loss 0.0334224, acc 1
2017-09-09T15:33:36.959973: step 2365, loss 0.0431398, acc 1
2017-09-09T15:33:37.273843: step 2366, loss 0.0400704, acc 1
2017-09-09T15:33:37.577156: step 2367, loss 0.0740344, acc 0.96875
2017-09-09T15:33:37.841777: step 2368, loss 0.0572427, acc 0.984375
2017-09-09T15:33:38.226582: step 2369, loss 0.0143065, acc 1
2017-09-09T15:33:38.501159: step 2370, loss 0.0610338, acc 0.984375
2017-09-09T15:33:38.823127: step 2371, loss 0.08558, acc 0.984375
2017-09-09T15:33:39.134023: step 2372, loss 0.0877754, acc 0.984375
2017-09-09T15:33:39.445092: step 2373, loss 0.0486933, acc 0.984375
2017-09-09T15:33:39.802999: step 2374, loss 0.0489028, acc 0.96875
2017-09-09T15:33:40.124230: step 2375, loss 0.0503682, acc 0.984375
2017-09-09T15:33:40.452769: step 2376, loss 0.0613018, acc 0.96875
2017-09-09T15:33:40.761827: step 2377, loss 0.0305245, acc 1
2017-09-09T15:33:41.057374: step 2378, loss 0.0704334, acc 0.96875
2017-09-09T15:33:41.339483: step 2379, loss 0.019833, acc 1
2017-09-09T15:33:41.642246: step 2380, loss 0.0380838, acc 1
2017-09-09T15:33:41.975651: step 2381, loss 0.0149062, acc 1
2017-09-09T15:33:42.276852: step 2382, loss 0.0473197, acc 1
2017-09-09T15:33:42.585685: step 2383, loss 0.0426714, acc 0.984375
2017-09-09T15:33:42.870326: step 2384, loss 0.181856, acc 0.90625
2017-09-09T15:33:43.161944: step 2385, loss 0.109396, acc 0.984375
2017-09-09T15:33:43.500353: step 2386, loss 0.0216125, acc 1
2017-09-09T15:33:43.771411: step 2387, loss 0.0359898, acc 1
2017-09-09T15:33:44.139242: step 2388, loss 0.0706737, acc 0.96875
2017-09-09T15:33:44.450018: step 2389, loss 0.0666742, acc 0.96875
2017-09-09T15:33:44.717422: step 2390, loss 0.0164085, acc 1
2017-09-09T15:33:45.067152: step 2391, loss 0.018578, acc 1
2017-09-09T15:33:45.364024: step 2392, loss 0.0664134, acc 0.984375
2017-09-09T15:33:45.665483: step 2393, loss 0.0767952, acc 0.96875
2017-09-09T15:33:45.961503: step 2394, loss 0.0249255, acc 1
2017-09-09T15:33:46.290291: step 2395, loss 0.0257847, acc 1
2017-09-09T15:33:46.585338: step 2396, loss 0.090459, acc 0.984375
2017-09-09T15:33:46.911751: step 2397, loss 0.0307709, acc 1
2017-09-09T15:33:47.238571: step 2398, loss 0.0382226, acc 0.984375
2017-09-09T15:33:47.567289: step 2399, loss 0.0379124, acc 0.984375
2017-09-09T15:33:47.872051: step 2400, loss 0.0479375, acc 0.984375

Evaluation:
2017-09-09T15:33:47.938513: step 2400, loss 1.94762, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2400

2017-09-09T15:33:50.606404: step 2401, loss 0.12069, acc 0.953125
2017-09-09T15:33:50.854151: step 2402, loss 0.0360512, acc 0.984375
2017-09-09T15:33:51.178643: step 2403, loss 0.0613618, acc 0.984375
2017-09-09T15:33:51.465318: step 2404, loss 0.0669274, acc 0.96875
2017-09-09T15:33:51.741607: step 2405, loss 0.0298651, acc 1
2017-09-09T15:33:52.043617: step 2406, loss 0.0247046, acc 1
2017-09-09T15:33:52.325917: step 2407, loss 0.0572823, acc 0.96875
2017-09-09T15:33:52.595999: step 2408, loss 0.0495166, acc 0.984375
2017-09-09T15:33:52.978324: step 2409, loss 0.0379349, acc 1
2017-09-09T15:33:53.278880: step 2410, loss 0.0584829, acc 0.984375
2017-09-09T15:33:53.614964: step 2411, loss 0.043944, acc 1
2017-09-09T15:33:53.890064: step 2412, loss 0.121585, acc 0.9375
2017-09-09T15:33:54.163080: step 2413, loss 0.101428, acc 0.96875
2017-09-09T15:33:54.480281: step 2414, loss 0.0533353, acc 0.984375
2017-09-09T15:33:54.753183: step 2415, loss 0.047529, acc 0.984375
2017-09-09T15:33:55.150285: step 2416, loss 0.0131375, acc 1
2017-09-09T15:33:55.438398: step 2417, loss 0.040852, acc 0.984375
2017-09-09T15:33:55.763274: step 2418, loss 0.0969228, acc 0.953125
2017-09-09T15:33:56.079645: step 2419, loss 0.0490245, acc 0.984375
2017-09-09T15:33:56.374483: step 2420, loss 0.0440769, acc 0.984375
2017-09-09T15:33:56.698355: step 2421, loss 0.0492749, acc 0.984375
2017-09-09T15:33:56.990292: step 2422, loss 0.140144, acc 0.9375
2017-09-09T15:33:57.332681: step 2423, loss 0.0612877, acc 0.96875
2017-09-09T15:33:57.642010: step 2424, loss 0.0191052, acc 1
2017-09-09T15:33:57.973351: step 2425, loss 0.0324602, acc 0.984375
2017-09-09T15:33:58.314007: step 2426, loss 0.0626498, acc 0.984375
2017-09-09T15:33:58.967834: step 2427, loss 0.0214663, acc 1
2017-09-09T15:33:59.271362: step 2428, loss 0.165806, acc 0.921875
2017-09-09T15:33:59.604189: step 2429, loss 0.0362453, acc 1
2017-09-09T15:33:59.863564: step 2430, loss 0.0948358, acc 0.953125
2017-09-09T15:34:00.190687: step 2431, loss 0.0541896, acc 0.984375
2017-09-09T15:34:00.471777: step 2432, loss 0.0587702, acc 0.96875
2017-09-09T15:34:00.742115: step 2433, loss 0.0599942, acc 0.984375
2017-09-09T15:34:01.038521: step 2434, loss 0.0549481, acc 0.96875
2017-09-09T15:34:01.326439: step 2435, loss 0.0526746, acc 0.984375
2017-09-09T15:34:01.652437: step 2436, loss 0.075094, acc 0.984375
2017-09-09T15:34:01.951394: step 2437, loss 0.0261862, acc 0.984375
2017-09-09T15:34:02.218148: step 2438, loss 0.0334372, acc 0.984375
2017-09-09T15:34:02.570486: step 2439, loss 0.0239738, acc 1
2017-09-09T15:34:02.843978: step 2440, loss 0.0321598, acc 1
2017-09-09T15:34:03.182532: step 2441, loss 0.0138007, acc 1
2017-09-09T15:34:03.541327: step 2442, loss 0.0721128, acc 0.984375
2017-09-09T15:34:03.830354: step 2443, loss 0.0782854, acc 0.984375
2017-09-09T15:34:04.256537: step 2444, loss 0.106752, acc 0.96875
2017-09-09T15:34:04.603189: step 2445, loss 0.0525485, acc 0.984375
2017-09-09T15:34:04.866665: step 2446, loss 0.0825606, acc 0.96875
2017-09-09T15:34:05.168685: step 2447, loss 0.0464911, acc 0.984375
2017-09-09T15:34:05.481050: step 2448, loss 0.0291425, acc 0.984375
2017-09-09T15:34:05.768020: step 2449, loss 0.0242868, acc 1
2017-09-09T15:34:06.057576: step 2450, loss 0.022505, acc 1

Evaluation:
2017-09-09T15:34:06.145095: step 2450, loss 1.95453, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2450

2017-09-09T15:34:08.792392: step 2451, loss 0.127142, acc 0.953125
2017-09-09T15:34:09.117936: step 2452, loss 0.0730079, acc 0.953125
2017-09-09T15:34:09.490554: step 2453, loss 0.024508, acc 1
2017-09-09T15:34:09.773757: step 2454, loss 0.0492325, acc 0.984375
2017-09-09T15:34:10.098261: step 2455, loss 0.0338982, acc 1
2017-09-09T15:34:10.413244: step 2456, loss 0.0283856, acc 1
2017-09-09T15:34:10.696264: step 2457, loss 0.0863452, acc 0.953125
2017-09-09T15:34:11.008172: step 2458, loss 0.0169539, acc 1
2017-09-09T15:34:11.276325: step 2459, loss 0.0403229, acc 0.984375
2017-09-09T15:34:11.594866: step 2460, loss 0.0401649, acc 0.984375
2017-09-09T15:34:11.857272: step 2461, loss 0.032502, acc 0.984375
2017-09-09T15:34:12.231769: step 2462, loss 0.0106427, acc 1
2017-09-09T15:34:12.555127: step 2463, loss 0.10947, acc 0.96875
2017-09-09T15:34:12.826433: step 2464, loss 0.0230776, acc 1
2017-09-09T15:34:13.093512: step 2465, loss 0.00806548, acc 1
2017-09-09T15:34:13.359063: step 2466, loss 0.0933044, acc 0.953125
2017-09-09T15:34:13.697272: step 2467, loss 0.0376135, acc 0.984375
2017-09-09T15:34:13.984002: step 2468, loss 0.0626963, acc 0.984375
2017-09-09T15:34:14.330966: step 2469, loss 0.017721, acc 1
2017-09-09T15:34:14.602774: step 2470, loss 0.039892, acc 1
2017-09-09T15:34:14.920005: step 2471, loss 0.0465712, acc 1
2017-09-09T15:34:15.227841: step 2472, loss 0.159694, acc 0.953125
2017-09-09T15:34:15.516227: step 2473, loss 0.0425613, acc 0.984375
2017-09-09T15:34:15.870203: step 2474, loss 0.057701, acc 0.96875
2017-09-09T15:34:16.132373: step 2475, loss 0.0253587, acc 0.984375
2017-09-09T15:34:16.512653: step 2476, loss 0.0712437, acc 0.96875
2017-09-09T15:34:16.829914: step 2477, loss 0.0549015, acc 0.984375
2017-09-09T15:34:17.141801: step 2478, loss 0.105191, acc 0.96875
2017-09-09T15:34:17.465791: step 2479, loss 0.0759493, acc 0.953125
2017-09-09T15:34:17.738010: step 2480, loss 0.079123, acc 0.984375
2017-09-09T15:34:18.147987: step 2481, loss 0.0543883, acc 0.984375
2017-09-09T15:34:18.483113: step 2482, loss 0.0186288, acc 1
2017-09-09T15:34:18.747362: step 2483, loss 0.0473893, acc 0.984375
2017-09-09T15:34:19.047724: step 2484, loss 0.108855, acc 0.953125
2017-09-09T15:34:19.383258: step 2485, loss 0.0623647, acc 0.984375
2017-09-09T15:34:19.695784: step 2486, loss 0.0282736, acc 0.984375
2017-09-09T15:34:19.984228: step 2487, loss 0.045387, acc 0.984375
2017-09-09T15:34:20.242799: step 2488, loss 0.0542147, acc 1
2017-09-09T15:34:20.540316: step 2489, loss 0.0906454, acc 0.96875
2017-09-09T15:34:20.833776: step 2490, loss 0.0448003, acc 0.984375
2017-09-09T15:34:21.102499: step 2491, loss 0.0639845, acc 0.96875
2017-09-09T15:34:21.454631: step 2492, loss 0.00790686, acc 1
2017-09-09T15:34:21.745442: step 2493, loss 0.0490713, acc 0.96875
2017-09-09T15:34:22.028652: step 2494, loss 0.0810166, acc 0.953125
2017-09-09T15:34:22.296254: step 2495, loss 0.0472008, acc 0.984375
2017-09-09T15:34:22.611391: step 2496, loss 0.0561527, acc 1
2017-09-09T15:34:22.934399: step 2497, loss 0.0549452, acc 1
2017-09-09T15:34:23.272169: step 2498, loss 0.0759467, acc 0.96875
2017-09-09T15:34:23.659322: step 2499, loss 0.0813214, acc 0.96875
2017-09-09T15:34:23.919209: step 2500, loss 0.072197, acc 0.96875

Evaluation:
2017-09-09T15:34:24.068888: step 2500, loss 2.09756, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2500

2017-09-09T15:34:27.025765: step 2501, loss 0.0314731, acc 0.984375
2017-09-09T15:34:27.371912: step 2502, loss 0.0316377, acc 0.984375
2017-09-09T15:34:27.696162: step 2503, loss 0.056075, acc 0.984375
2017-09-09T15:34:27.967178: step 2504, loss 0.0570373, acc 1
2017-09-09T15:34:28.291816: step 2505, loss 0.0522559, acc 0.984375
2017-09-09T15:34:28.609229: step 2506, loss 0.040968, acc 1
2017-09-09T15:34:28.948238: step 2507, loss 0.0451785, acc 0.984375
2017-09-09T15:34:29.275907: step 2508, loss 0.0772982, acc 0.984375
2017-09-09T15:34:29.558362: step 2509, loss 0.0683659, acc 0.96875
2017-09-09T15:34:29.917630: step 2510, loss 0.0653494, acc 0.984375
2017-09-09T15:34:30.208902: step 2511, loss 0.0657477, acc 0.953125
2017-09-09T15:34:30.533885: step 2512, loss 0.0336725, acc 0.984375
2017-09-09T15:34:30.817865: step 2513, loss 0.048203, acc 0.984375
2017-09-09T15:34:31.111891: step 2514, loss 0.117392, acc 0.921875
2017-09-09T15:34:31.399884: step 2515, loss 0.0317671, acc 0.984375
2017-09-09T15:34:31.698012: step 2516, loss 0.0180378, acc 1
2017-09-09T15:34:32.001219: step 2517, loss 0.056065, acc 0.96875
2017-09-09T15:34:32.286663: step 2518, loss 0.0315173, acc 0.984375
2017-09-09T15:34:32.713546: step 2519, loss 0.0377261, acc 0.984375
2017-09-09T15:34:33.078344: step 2520, loss 0.107759, acc 0.953125
2017-09-09T15:34:33.340361: step 2521, loss 0.0402828, acc 0.984375
2017-09-09T15:34:33.625020: step 2522, loss 0.0484577, acc 0.984375
2017-09-09T15:34:33.963354: step 2523, loss 0.0657173, acc 0.953125
2017-09-09T15:34:34.252106: step 2524, loss 0.0250181, acc 1
2017-09-09T15:34:34.540154: step 2525, loss 0.102144, acc 0.953125
2017-09-09T15:34:34.864868: step 2526, loss 0.05125, acc 0.984375
2017-09-09T15:34:35.208078: step 2527, loss 0.0804004, acc 0.9375
2017-09-09T15:34:35.477879: step 2528, loss 0.0455539, acc 1
2017-09-09T15:34:35.817515: step 2529, loss 0.00631854, acc 1
2017-09-09T15:34:36.115890: step 2530, loss 0.0233245, acc 0.984375
2017-09-09T15:34:36.390088: step 2531, loss 0.0547695, acc 0.984375
2017-09-09T15:34:36.700381: step 2532, loss 0.0419426, acc 1
2017-09-09T15:34:36.981107: step 2533, loss 0.0343293, acc 0.984375
2017-09-09T15:34:37.331432: step 2534, loss 0.0591869, acc 0.984375
2017-09-09T15:34:37.620807: step 2535, loss 0.0657192, acc 0.984375
2017-09-09T15:34:37.907995: step 2536, loss 0.0858561, acc 0.96875
2017-09-09T15:34:38.248216: step 2537, loss 0.0357534, acc 0.984375
2017-09-09T15:34:38.516892: step 2538, loss 0.0127308, acc 1
2017-09-09T15:34:38.868178: step 2539, loss 0.0359711, acc 1
2017-09-09T15:34:39.149383: step 2540, loss 0.0526031, acc 0.984375
2017-09-09T15:34:39.481643: step 2541, loss 0.0900458, acc 0.96875
2017-09-09T15:34:39.789921: step 2542, loss 0.0815981, acc 0.96875
2017-09-09T15:34:40.087604: step 2543, loss 0.0651577, acc 0.96875
2017-09-09T15:34:40.414245: step 2544, loss 0.032429, acc 0.984375
2017-09-09T15:34:40.732359: step 2545, loss 0.01231, acc 1
2017-09-09T15:34:41.051162: step 2546, loss 0.0762675, acc 0.953125
2017-09-09T15:34:41.358151: step 2547, loss 0.0154533, acc 1
2017-09-09T15:34:41.670105: step 2548, loss 0.0150439, acc 1
2017-09-09T15:34:42.022878: step 2549, loss 0.0196214, acc 1
2017-09-09T15:34:42.305595: step 2550, loss 0.0429501, acc 0.984375

Evaluation:
2017-09-09T15:34:42.425197: step 2550, loss 2.02375, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2550

2017-09-09T15:34:44.710214: step 2551, loss 0.0883811, acc 0.953125
2017-09-09T15:34:45.040387: step 2552, loss 0.0516608, acc 1
2017-09-09T15:34:45.327877: step 2553, loss 0.0315409, acc 1
2017-09-09T15:34:45.682919: step 2554, loss 0.0593001, acc 0.96875
2017-09-09T15:34:45.961327: step 2555, loss 0.0547159, acc 0.984375
2017-09-09T15:34:46.337575: step 2556, loss 0.0134702, acc 1
2017-09-09T15:34:46.632630: step 2557, loss 0.0196323, acc 1
2017-09-09T15:34:46.921970: step 2558, loss 0.0299498, acc 1
2017-09-09T15:34:47.235957: step 2559, loss 0.0210849, acc 1
2017-09-09T15:34:47.526879: step 2560, loss 0.0111395, acc 1
2017-09-09T15:34:47.829178: step 2561, loss 0.0224687, acc 1
2017-09-09T15:34:48.100997: step 2562, loss 0.0364071, acc 1
2017-09-09T15:34:48.406601: step 2563, loss 0.0212835, acc 1
2017-09-09T15:34:48.732246: step 2564, loss 0.0731543, acc 0.953125
2017-09-09T15:34:49.014595: step 2565, loss 0.0125741, acc 1
2017-09-09T15:34:49.277422: step 2566, loss 0.0676634, acc 0.984375
2017-09-09T15:34:49.628328: step 2567, loss 0.038418, acc 0.984375
2017-09-09T15:34:49.918511: step 2568, loss 0.0908139, acc 0.953125
2017-09-09T15:34:50.198378: step 2569, loss 0.0227848, acc 1
2017-09-09T15:34:50.568596: step 2570, loss 0.068207, acc 0.984375
2017-09-09T15:34:50.836706: step 2571, loss 0.0803685, acc 0.984375
2017-09-09T15:34:51.172519: step 2572, loss 0.0319109, acc 1
2017-09-09T15:34:51.479370: step 2573, loss 0.0304192, acc 0.984375
2017-09-09T15:34:51.758545: step 2574, loss 0.0493488, acc 0.984375
2017-09-09T15:34:52.064776: step 2575, loss 0.0462399, acc 1
2017-09-09T15:34:52.363441: step 2576, loss 0.0391062, acc 0.984375
2017-09-09T15:34:52.638700: step 2577, loss 0.0502338, acc 0.984375
2017-09-09T15:34:52.941723: step 2578, loss 0.0418897, acc 0.984375
2017-09-09T15:34:53.266055: step 2579, loss 0.030133, acc 0.984375
2017-09-09T15:34:53.560048: step 2580, loss 0.0660104, acc 0.96875
2017-09-09T15:34:53.878290: step 2581, loss 0.07167, acc 0.984375
2017-09-09T15:34:54.143646: step 2582, loss 0.0246041, acc 0.984375
2017-09-09T15:34:54.441358: step 2583, loss 0.0120889, acc 1
2017-09-09T15:34:54.752313: step 2584, loss 0.0565588, acc 0.984375
2017-09-09T15:34:55.037010: step 2585, loss 0.0375466, acc 1
2017-09-09T15:34:55.405974: step 2586, loss 0.0315392, acc 0.984375
2017-09-09T15:34:55.717597: step 2587, loss 0.0254717, acc 1
2017-09-09T15:34:55.983779: step 2588, loss 0.0559451, acc 0.984375
2017-09-09T15:34:56.359633: step 2589, loss 0.0246613, acc 1
2017-09-09T15:34:56.620132: step 2590, loss 0.0784247, acc 0.96875
2017-09-09T15:34:56.977230: step 2591, loss 0.0133477, acc 1
2017-09-09T15:34:57.291966: step 2592, loss 0.0340289, acc 0.984375
2017-09-09T15:34:57.566378: step 2593, loss 0.0926183, acc 0.953125
2017-09-09T15:34:57.900607: step 2594, loss 0.0158077, acc 1
2017-09-09T15:34:58.202140: step 2595, loss 0.0826923, acc 0.984375
2017-09-09T15:34:58.539456: step 2596, loss 0.0580636, acc 0.984375
2017-09-09T15:34:58.898486: step 2597, loss 0.0770394, acc 0.96875
2017-09-09T15:34:59.172593: step 2598, loss 0.046724, acc 0.96875
2017-09-09T15:34:59.499835: step 2599, loss 0.0354583, acc 0.984375
2017-09-09T15:34:59.795098: step 2600, loss 0.0795117, acc 0.96875

Evaluation:
2017-09-09T15:34:59.875197: step 2600, loss 1.91973, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2600

2017-09-09T15:35:02.675780: step 2601, loss 0.0422033, acc 0.984375
2017-09-09T15:35:02.974596: step 2602, loss 0.0685159, acc 0.984375
2017-09-09T15:35:03.273623: step 2603, loss 0.0481922, acc 0.984375
2017-09-09T15:35:03.579408: step 2604, loss 0.111394, acc 0.96875
2017-09-09T15:35:03.887478: step 2605, loss 0.0103994, acc 1
2017-09-09T15:35:04.149677: step 2606, loss 0.0391568, acc 0.984375
2017-09-09T15:35:04.598896: step 2607, loss 0.0499746, acc 0.984375
2017-09-09T15:35:04.871830: step 2608, loss 0.0643919, acc 0.984375
2017-09-09T15:35:05.227128: step 2609, loss 0.0201272, acc 1
2017-09-09T15:35:05.533807: step 2610, loss 0.0348743, acc 1
2017-09-09T15:35:05.847368: step 2611, loss 0.0275481, acc 1
2017-09-09T15:35:06.154633: step 2612, loss 0.0643567, acc 0.96875
2017-09-09T15:35:06.472607: step 2613, loss 0.0134195, acc 1
2017-09-09T15:35:06.796312: step 2614, loss 0.0581695, acc 0.984375
2017-09-09T15:35:07.097796: step 2615, loss 0.0873787, acc 0.953125
2017-09-09T15:35:07.455442: step 2616, loss 0.11843, acc 0.96875
2017-09-09T15:35:07.789142: step 2617, loss 0.0288458, acc 1
2017-09-09T15:35:08.059442: step 2618, loss 0.0119379, acc 1
2017-09-09T15:35:08.356441: step 2619, loss 0.0365358, acc 0.984375
2017-09-09T15:35:08.642577: step 2620, loss 0.0307949, acc 1
2017-09-09T15:35:08.986550: step 2621, loss 0.0266416, acc 1
2017-09-09T15:35:09.254049: step 2622, loss 0.0245204, acc 1
2017-09-09T15:35:09.591861: step 2623, loss 0.0968932, acc 0.953125
2017-09-09T15:35:09.896699: step 2624, loss 0.0373241, acc 0.96875
2017-09-09T15:35:10.216823: step 2625, loss 0.00400376, acc 1
2017-09-09T15:35:10.502201: step 2626, loss 0.0403657, acc 0.96875
2017-09-09T15:35:10.817354: step 2627, loss 0.0557867, acc 0.96875
2017-09-09T15:35:11.141297: step 2628, loss 0.0869404, acc 0.96875
2017-09-09T15:35:11.405915: step 2629, loss 0.108341, acc 0.9375
2017-09-09T15:35:11.807649: step 2630, loss 0.0290319, acc 1
2017-09-09T15:35:12.088470: step 2631, loss 0.0609971, acc 0.96875
2017-09-09T15:35:12.431541: step 2632, loss 0.0543396, acc 0.96875
2017-09-09T15:35:12.704644: step 2633, loss 0.0775736, acc 0.96875
2017-09-09T15:35:13.007544: step 2634, loss 0.0364336, acc 0.984375
2017-09-09T15:35:13.305037: step 2635, loss 0.0372281, acc 1
2017-09-09T15:35:13.567547: step 2636, loss 0.0162341, acc 1
2017-09-09T15:35:13.925244: step 2637, loss 0.103028, acc 0.953125
2017-09-09T15:35:14.222826: step 2638, loss 0.0794472, acc 0.96875
2017-09-09T15:35:14.504103: step 2639, loss 0.0105888, acc 1
2017-09-09T15:35:14.797308: step 2640, loss 0.0332586, acc 1
2017-09-09T15:35:15.076068: step 2641, loss 0.0275565, acc 1
2017-09-09T15:35:15.432338: step 2642, loss 0.0344763, acc 1
2017-09-09T15:35:15.744267: step 2643, loss 0.0239382, acc 0.984375
2017-09-09T15:35:16.137496: step 2644, loss 0.0442566, acc 0.96875
2017-09-09T15:35:16.466605: step 2645, loss 0.0184138, acc 0.984375
2017-09-09T15:35:16.747468: step 2646, loss 0.0173125, acc 1
2017-09-09T15:35:17.019316: step 2647, loss 0.0262736, acc 1
2017-09-09T15:35:17.347570: step 2648, loss 0.0432065, acc 0.984375
2017-09-09T15:35:17.696468: step 2649, loss 0.0338058, acc 0.984375
2017-09-09T15:35:18.007959: step 2650, loss 0.0289776, acc 0.984375

Evaluation:
2017-09-09T15:35:18.073626: step 2650, loss 1.75952, acc 0.34964

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2650

2017-09-09T15:35:20.337007: step 2651, loss 0.0256734, acc 1
2017-09-09T15:35:20.633575: step 2652, loss 0.0172251, acc 1
2017-09-09T15:35:20.948477: step 2653, loss 0.0973963, acc 0.953125
2017-09-09T15:35:21.232555: step 2654, loss 0.0284962, acc 1
2017-09-09T15:35:21.581934: step 2655, loss 0.0168834, acc 1
2017-09-09T15:35:21.886297: step 2656, loss 0.1287, acc 0.90625
2017-09-09T15:35:22.205075: step 2657, loss 0.0313989, acc 0.984375
2017-09-09T15:35:22.536619: step 2658, loss 0.0463405, acc 0.96875
2017-09-09T15:35:22.817913: step 2659, loss 0.104118, acc 0.953125
2017-09-09T15:35:23.188378: step 2660, loss 0.0450844, acc 0.984375
2017-09-09T15:35:23.539129: step 2661, loss 0.0622343, acc 0.96875
2017-09-09T15:35:23.826742: step 2662, loss 0.0206111, acc 1
2017-09-09T15:35:24.209826: step 2663, loss 0.0789732, acc 0.96875
2017-09-09T15:35:24.549023: step 2664, loss 0.055191, acc 0.984375
2017-09-09T15:35:24.810179: step 2665, loss 0.0285587, acc 1
2017-09-09T15:35:25.154185: step 2666, loss 0.0131967, acc 1
2017-09-09T15:35:25.419003: step 2667, loss 0.026796, acc 1
2017-09-09T15:35:25.765725: step 2668, loss 0.0336912, acc 0.984375
2017-09-09T15:35:26.114094: step 2669, loss 0.00793179, acc 1
2017-09-09T15:35:26.388201: step 2670, loss 0.0499176, acc 0.984375
2017-09-09T15:35:26.710349: step 2671, loss 0.0210163, acc 0.984375
2017-09-09T15:35:27.007870: step 2672, loss 0.027254, acc 0.984375
2017-09-09T15:35:27.350047: step 2673, loss 0.0426969, acc 0.96875
2017-09-09T15:35:27.652810: step 2674, loss 0.0341058, acc 1
2017-09-09T15:35:27.962748: step 2675, loss 0.0627907, acc 0.96875
2017-09-09T15:35:28.243441: step 2676, loss 0.0567437, acc 0.984375
2017-09-09T15:35:28.536673: step 2677, loss 0.0307764, acc 1
2017-09-09T15:35:28.830814: step 2678, loss 0.0841702, acc 0.953125
2017-09-09T15:35:29.120080: step 2679, loss 0.0464063, acc 0.96875
2017-09-09T15:35:29.440749: step 2680, loss 0.0635988, acc 0.984375
2017-09-09T15:35:29.832236: step 2681, loss 0.0569363, acc 0.96875
2017-09-09T15:35:30.243383: step 2682, loss 0.0263264, acc 1
2017-09-09T15:35:30.546419: step 2683, loss 0.0182114, acc 1
2017-09-09T15:35:30.826241: step 2684, loss 0.02314, acc 1
2017-09-09T15:35:31.115112: step 2685, loss 0.0213458, acc 1
2017-09-09T15:35:31.414219: step 2686, loss 0.0266836, acc 1
2017-09-09T15:35:31.697897: step 2687, loss 0.0989342, acc 0.96875
2017-09-09T15:35:31.981845: step 2688, loss 0.132865, acc 0.953125
2017-09-09T15:35:32.301159: step 2689, loss 0.10092, acc 0.9375
2017-09-09T15:35:32.637219: step 2690, loss 0.0486651, acc 0.984375
2017-09-09T15:35:32.939031: step 2691, loss 0.0304923, acc 1
2017-09-09T15:35:33.317034: step 2692, loss 0.0528204, acc 0.984375
2017-09-09T15:35:33.636868: step 2693, loss 0.089471, acc 0.96875
2017-09-09T15:35:33.914556: step 2694, loss 0.0225955, acc 1
2017-09-09T15:35:34.240586: step 2695, loss 0.0202041, acc 1
2017-09-09T15:35:34.562914: step 2696, loss 0.0504329, acc 0.984375
2017-09-09T15:35:34.904342: step 2697, loss 0.0450864, acc 1
2017-09-09T15:35:35.310418: step 2698, loss 0.0945594, acc 0.96875
2017-09-09T15:35:35.606142: step 2699, loss 0.0152909, acc 1
2017-09-09T15:35:35.962773: step 2700, loss 0.0375317, acc 0.96875

Evaluation:
2017-09-09T15:35:36.022753: step 2700, loss 1.96468, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2700

2017-09-09T15:35:38.939082: step 2701, loss 0.0317562, acc 1
2017-09-09T15:35:39.230829: step 2702, loss 0.0234797, acc 1
2017-09-09T15:35:39.539661: step 2703, loss 0.0541411, acc 0.96875
2017-09-09T15:35:39.884879: step 2704, loss 0.0889653, acc 0.96875
2017-09-09T15:35:40.153691: step 2705, loss 0.10063, acc 0.96875
2017-09-09T15:35:40.520585: step 2706, loss 0.114433, acc 0.96875
2017-09-09T15:35:40.809617: step 2707, loss 0.0533285, acc 0.96875
2017-09-09T15:35:41.112582: step 2708, loss 0.0143686, acc 1
2017-09-09T15:35:41.452376: step 2709, loss 0.143558, acc 0.96875
2017-09-09T15:35:41.742326: step 2710, loss 0.0210947, acc 1
2017-09-09T15:35:42.076590: step 2711, loss 0.0349692, acc 0.984375
2017-09-09T15:35:42.383798: step 2712, loss 0.0104714, acc 1
2017-09-09T15:35:42.692256: step 2713, loss 0.171655, acc 0.953125
2017-09-09T15:35:42.995517: step 2714, loss 0.0582736, acc 0.984375
2017-09-09T15:35:43.278932: step 2715, loss 0.0327686, acc 0.984375
2017-09-09T15:35:43.608983: step 2716, loss 0.0629544, acc 0.96875
2017-09-09T15:35:43.904594: step 2717, loss 0.0369918, acc 0.984375
2017-09-09T15:35:44.231577: step 2718, loss 0.0550769, acc 0.984375
2017-09-09T15:35:44.636655: step 2719, loss 0.0759634, acc 0.96875
2017-09-09T15:35:45.018672: step 2720, loss 0.0436673, acc 1
2017-09-09T15:35:45.310409: step 2721, loss 0.037738, acc 0.984375
2017-09-09T15:35:45.647952: step 2722, loss 0.0553024, acc 1
2017-09-09T15:35:45.925054: step 2723, loss 0.0407458, acc 1
2017-09-09T15:35:46.207506: step 2724, loss 0.0434167, acc 0.984375
2017-09-09T15:35:46.608652: step 2725, loss 0.0145393, acc 1
2017-09-09T15:35:46.889769: step 2726, loss 0.0301696, acc 1
2017-09-09T15:35:47.217281: step 2727, loss 0.0239882, acc 1
2017-09-09T15:35:47.519250: step 2728, loss 0.0248946, acc 1
2017-09-09T15:35:47.837927: step 2729, loss 0.0361352, acc 1
2017-09-09T15:35:48.198121: step 2730, loss 0.0677722, acc 0.984375
2017-09-09T15:35:48.477465: step 2731, loss 0.00310189, acc 1
2017-09-09T15:35:48.796146: step 2732, loss 0.0272606, acc 1
2017-09-09T15:35:49.063169: step 2733, loss 0.0841591, acc 0.96875
2017-09-09T15:35:49.381872: step 2734, loss 0.0698077, acc 0.96875
2017-09-09T15:35:49.759016: step 2735, loss 0.00991576, acc 1
2017-09-09T15:35:50.103355: step 2736, loss 0.0491114, acc 0.984375
2017-09-09T15:35:50.378113: step 2737, loss 0.0369088, acc 0.984375
2017-09-09T15:35:50.686872: step 2738, loss 0.0362638, acc 0.984375
2017-09-09T15:35:50.962651: step 2739, loss 0.0161976, acc 1
2017-09-09T15:35:51.320530: step 2740, loss 0.0490043, acc 0.984375
2017-09-09T15:35:51.595370: step 2741, loss 0.0786783, acc 0.953125
2017-09-09T15:35:51.932334: step 2742, loss 0.0250397, acc 1
2017-09-09T15:35:52.214364: step 2743, loss 0.0512761, acc 0.984375
2017-09-09T15:35:52.537980: step 2744, loss 0.110377, acc 0.960784
2017-09-09T15:35:52.871983: step 2745, loss 0.0520003, acc 0.96875
2017-09-09T15:35:53.132218: step 2746, loss 0.039873, acc 0.984375
2017-09-09T15:35:53.523020: step 2747, loss 0.0244942, acc 1
2017-09-09T15:35:53.819767: step 2748, loss 0.0288577, acc 0.984375
2017-09-09T15:35:54.108094: step 2749, loss 0.0247709, acc 1
2017-09-09T15:35:54.434492: step 2750, loss 0.0908505, acc 0.96875

Evaluation:
2017-09-09T15:35:54.519577: step 2750, loss 2.41414, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2750

2017-09-09T15:35:57.741003: step 2751, loss 0.0750654, acc 0.984375
2017-09-09T15:35:58.036711: step 2752, loss 0.0227815, acc 1
2017-09-09T15:35:58.350153: step 2753, loss 0.0148151, acc 1
2017-09-09T15:35:58.663494: step 2754, loss 0.0208194, acc 1
2017-09-09T15:35:58.984267: step 2755, loss 0.0422829, acc 0.984375
2017-09-09T15:35:59.283207: step 2756, loss 0.0595522, acc 0.984375
2017-09-09T15:35:59.571161: step 2757, loss 0.0137684, acc 1
2017-09-09T15:35:59.890767: step 2758, loss 0.073608, acc 0.96875
2017-09-09T15:36:00.177643: step 2759, loss 0.0409628, acc 0.984375
2017-09-09T15:36:00.479895: step 2760, loss 0.0794205, acc 0.96875
2017-09-09T15:36:00.770111: step 2761, loss 0.0217336, acc 0.984375
2017-09-09T15:36:01.117791: step 2762, loss 0.0699796, acc 0.984375
2017-09-09T15:36:01.416630: step 2763, loss 0.0275331, acc 1
2017-09-09T15:36:01.761274: step 2764, loss 0.0255811, acc 0.984375
2017-09-09T15:36:02.098515: step 2765, loss 0.0265265, acc 1
2017-09-09T15:36:02.378883: step 2766, loss 0.0670386, acc 0.96875
2017-09-09T15:36:02.701849: step 2767, loss 0.042967, acc 0.984375
2017-09-09T15:36:03.024099: step 2768, loss 0.0522764, acc 0.984375
2017-09-09T15:36:03.352832: step 2769, loss 0.0529224, acc 0.984375
2017-09-09T15:36:03.676834: step 2770, loss 0.0411862, acc 1
2017-09-09T15:36:03.985221: step 2771, loss 0.0647749, acc 0.984375
2017-09-09T15:36:04.281921: step 2772, loss 0.0174224, acc 0.984375
2017-09-09T15:36:04.572546: step 2773, loss 0.00504041, acc 1
2017-09-09T15:36:04.906634: step 2774, loss 0.0261135, acc 0.984375
2017-09-09T15:36:05.213873: step 2775, loss 0.0354641, acc 1
2017-09-09T15:36:05.521663: step 2776, loss 0.0962845, acc 0.953125
2017-09-09T15:36:05.894321: step 2777, loss 0.0125461, acc 1
2017-09-09T15:36:06.203740: step 2778, loss 0.0202704, acc 1
2017-09-09T15:36:06.518335: step 2779, loss 0.0252214, acc 0.984375
2017-09-09T15:36:06.811870: step 2780, loss 0.0360657, acc 0.984375
2017-09-09T15:36:07.097101: step 2781, loss 0.0272674, acc 0.984375
2017-09-09T15:36:07.421923: step 2782, loss 0.037598, acc 0.984375
2017-09-09T15:36:07.695236: step 2783, loss 0.0464117, acc 0.96875
2017-09-09T15:36:07.969811: step 2784, loss 0.0659582, acc 0.953125
2017-09-09T15:36:08.265036: step 2785, loss 0.0755506, acc 0.96875
2017-09-09T15:36:08.542166: step 2786, loss 0.00960134, acc 1
2017-09-09T15:36:08.869714: step 2787, loss 0.0180563, acc 1
2017-09-09T15:36:09.180144: step 2788, loss 0.0423699, acc 0.96875
2017-09-09T15:36:09.509779: step 2789, loss 0.019197, acc 1
2017-09-09T15:36:09.767677: step 2790, loss 0.0222604, acc 1
2017-09-09T15:36:10.102419: step 2791, loss 0.0372434, acc 0.984375
2017-09-09T15:36:10.413316: step 2792, loss 0.0236444, acc 1
2017-09-09T15:36:10.683942: step 2793, loss 0.0314466, acc 1
2017-09-09T15:36:11.019824: step 2794, loss 0.0710062, acc 0.96875
2017-09-09T15:36:11.289703: step 2795, loss 0.0162874, acc 1
2017-09-09T15:36:11.635124: step 2796, loss 0.0627755, acc 1
2017-09-09T15:36:11.945803: step 2797, loss 0.0511035, acc 0.96875
2017-09-09T15:36:12.283709: step 2798, loss 0.0250067, acc 0.984375
2017-09-09T15:36:12.601540: step 2799, loss 0.0404408, acc 0.984375
2017-09-09T15:36:12.912107: step 2800, loss 0.103313, acc 0.921875

Evaluation:
2017-09-09T15:36:13.012797: step 2800, loss 1.86136, acc 0.351079

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2800

2017-09-09T15:36:14.935336: step 2801, loss 0.0054373, acc 1
2017-09-09T15:36:15.309824: step 2802, loss 0.0764664, acc 0.96875
2017-09-09T15:36:15.627482: step 2803, loss 0.146938, acc 0.953125
2017-09-09T15:36:15.979419: step 2804, loss 0.00550826, acc 1
2017-09-09T15:36:16.334377: step 2805, loss 0.0518205, acc 0.96875
2017-09-09T15:36:16.670496: step 2806, loss 0.0689468, acc 0.96875
2017-09-09T15:36:17.032609: step 2807, loss 0.0503296, acc 0.984375
2017-09-09T15:36:17.340654: step 2808, loss 0.0417491, acc 0.984375
2017-09-09T15:36:17.679425: step 2809, loss 0.0645437, acc 1
2017-09-09T15:36:17.961643: step 2810, loss 0.0712743, acc 0.984375
2017-09-09T15:36:18.229650: step 2811, loss 0.0456708, acc 1
2017-09-09T15:36:18.601382: step 2812, loss 0.0202773, acc 1
2017-09-09T15:36:18.889870: step 2813, loss 0.0668218, acc 0.984375
2017-09-09T15:36:19.242606: step 2814, loss 0.0197229, acc 1
2017-09-09T15:36:19.517469: step 2815, loss 0.0581638, acc 0.984375
2017-09-09T15:36:19.814445: step 2816, loss 0.0164793, acc 1
2017-09-09T15:36:20.105080: step 2817, loss 0.116161, acc 0.953125
2017-09-09T15:36:20.406930: step 2818, loss 0.0257807, acc 1
2017-09-09T15:36:20.752528: step 2819, loss 0.0598614, acc 1
2017-09-09T15:36:21.023576: step 2820, loss 0.0490237, acc 1
2017-09-09T15:36:21.377403: step 2821, loss 0.0363148, acc 1
2017-09-09T15:36:21.714324: step 2822, loss 0.0310484, acc 1
2017-09-09T15:36:22.008006: step 2823, loss 0.10529, acc 0.96875
2017-09-09T15:36:22.353963: step 2824, loss 0.0964989, acc 0.96875
2017-09-09T15:36:22.680160: step 2825, loss 0.0416186, acc 0.984375
2017-09-09T15:36:22.984384: step 2826, loss 0.0204543, acc 1
2017-09-09T15:36:23.311051: step 2827, loss 0.0405785, acc 0.984375
2017-09-09T15:36:23.623771: step 2828, loss 0.0332447, acc 0.984375
2017-09-09T15:36:23.938788: step 2829, loss 0.0267456, acc 1
2017-09-09T15:36:24.272730: step 2830, loss 0.0310705, acc 1
2017-09-09T15:36:24.545123: step 2831, loss 0.0872746, acc 0.96875
2017-09-09T15:36:24.859668: step 2832, loss 0.00892295, acc 1
2017-09-09T15:36:25.145250: step 2833, loss 0.0152932, acc 1
2017-09-09T15:36:25.492490: step 2834, loss 0.00810201, acc 1
2017-09-09T15:36:25.801775: step 2835, loss 0.0514124, acc 1
2017-09-09T15:36:26.087062: step 2836, loss 0.0448818, acc 0.984375
2017-09-09T15:36:26.405967: step 2837, loss 0.0323315, acc 1
2017-09-09T15:36:26.709767: step 2838, loss 0.0497501, acc 0.984375
2017-09-09T15:36:27.128849: step 2839, loss 0.0148083, acc 1
2017-09-09T15:36:27.428462: step 2840, loss 0.0129994, acc 1
2017-09-09T15:36:27.712674: step 2841, loss 0.0648612, acc 0.984375
2017-09-09T15:36:27.972176: step 2842, loss 0.0506902, acc 0.980392
2017-09-09T15:36:28.300451: step 2843, loss 0.00833824, acc 1
2017-09-09T15:36:28.619073: step 2844, loss 0.0708366, acc 0.984375
2017-09-09T15:36:28.940524: step 2845, loss 0.0766515, acc 0.96875
2017-09-09T15:36:29.259593: step 2846, loss 0.0318375, acc 1
2017-09-09T15:36:29.574319: step 2847, loss 0.0730942, acc 0.984375
2017-09-09T15:36:29.868557: step 2848, loss 0.023345, acc 1
2017-09-09T15:36:30.185611: step 2849, loss 0.0263328, acc 1
2017-09-09T15:36:30.492120: step 2850, loss 0.0735436, acc 0.953125

Evaluation:
2017-09-09T15:36:30.560214: step 2850, loss 2.37372, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2850

2017-09-09T15:36:34.090703: step 2851, loss 0.0582562, acc 0.96875
2017-09-09T15:36:34.367998: step 2852, loss 0.0377188, acc 0.984375
2017-09-09T15:36:34.702967: step 2853, loss 0.150197, acc 0.96875
2017-09-09T15:36:35.056084: step 2854, loss 0.0399144, acc 0.984375
2017-09-09T15:36:35.343581: step 2855, loss 0.041398, acc 0.984375
2017-09-09T15:36:35.688027: step 2856, loss 0.107356, acc 0.96875
2017-09-09T15:36:36.001040: step 2857, loss 0.0219444, acc 0.984375
2017-09-09T15:36:36.359474: step 2858, loss 0.0331957, acc 0.984375
2017-09-09T15:36:36.664147: step 2859, loss 0.0269729, acc 0.984375
2017-09-09T15:36:36.986421: step 2860, loss 0.0161256, acc 1
2017-09-09T15:36:37.337949: step 2861, loss 0.0509236, acc 1
2017-09-09T15:36:37.606091: step 2862, loss 0.0774837, acc 0.96875
2017-09-09T15:36:37.978705: step 2863, loss 0.0388376, acc 1
2017-09-09T15:36:38.291341: step 2864, loss 0.00931767, acc 1
2017-09-09T15:36:38.595621: step 2865, loss 0.024562, acc 1
2017-09-09T15:36:38.988481: step 2866, loss 0.0618627, acc 0.96875
2017-09-09T15:36:39.266711: step 2867, loss 0.0201825, acc 1
2017-09-09T15:36:39.600355: step 2868, loss 0.0299431, acc 1
2017-09-09T15:36:39.927226: step 2869, loss 0.047429, acc 0.984375
2017-09-09T15:36:40.246001: step 2870, loss 0.0190641, acc 0.984375
2017-09-09T15:36:40.503423: step 2871, loss 0.037475, acc 0.984375
2017-09-09T15:36:40.850943: step 2872, loss 0.103332, acc 0.953125
2017-09-09T15:36:41.132585: step 2873, loss 0.0619713, acc 0.96875
2017-09-09T15:36:41.433282: step 2874, loss 0.10453, acc 0.9375
2017-09-09T15:36:41.718862: step 2875, loss 0.0826384, acc 0.96875
2017-09-09T15:36:41.994227: step 2876, loss 0.0243207, acc 1
2017-09-09T15:36:42.337947: step 2877, loss 0.0304383, acc 1
2017-09-09T15:36:42.635839: step 2878, loss 0.0407446, acc 0.984375
2017-09-09T15:36:42.887431: step 2879, loss 0.0665973, acc 0.96875
2017-09-09T15:36:43.175420: step 2880, loss 0.0258616, acc 1
2017-09-09T15:36:43.485392: step 2881, loss 0.0225796, acc 0.984375
2017-09-09T15:36:43.787306: step 2882, loss 0.0951766, acc 0.9375
2017-09-09T15:36:44.110870: step 2883, loss 0.0644062, acc 0.96875
2017-09-09T15:36:44.392066: step 2884, loss 0.0509625, acc 0.984375
2017-09-09T15:36:44.698130: step 2885, loss 0.0237548, acc 1
2017-09-09T15:36:45.029022: step 2886, loss 0.0432725, acc 0.984375
2017-09-09T15:36:45.304486: step 2887, loss 0.0281053, acc 0.984375
2017-09-09T15:36:45.684265: step 2888, loss 0.0126383, acc 1
2017-09-09T15:36:45.995114: step 2889, loss 0.0448917, acc 0.96875
2017-09-09T15:36:46.307873: step 2890, loss 0.0744255, acc 0.96875
2017-09-09T15:36:46.681876: step 2891, loss 0.0433729, acc 0.984375
2017-09-09T15:36:46.986949: step 2892, loss 0.0672184, acc 0.96875
2017-09-09T15:36:47.316209: step 2893, loss 0.0236993, acc 1
2017-09-09T15:36:47.646475: step 2894, loss 0.059609, acc 0.984375
2017-09-09T15:36:47.918632: step 2895, loss 0.024407, acc 1
2017-09-09T15:36:48.270023: step 2896, loss 0.0806416, acc 0.984375
2017-09-09T15:36:48.556831: step 2897, loss 0.0212274, acc 1
2017-09-09T15:36:48.883785: step 2898, loss 0.0442188, acc 0.984375
2017-09-09T15:36:49.195700: step 2899, loss 0.08025, acc 0.953125
2017-09-09T15:36:49.493439: step 2900, loss 0.089026, acc 0.96875

Evaluation:
2017-09-09T15:36:49.555964: step 2900, loss 1.71872, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2900

2017-09-09T15:36:51.763102: step 2901, loss 0.00719534, acc 1
2017-09-09T15:36:52.051084: step 2902, loss 0.0265182, acc 0.984375
2017-09-09T15:36:52.354086: step 2903, loss 0.0312556, acc 0.984375
2017-09-09T15:36:52.625698: step 2904, loss 0.12438, acc 0.953125
2017-09-09T15:36:52.944654: step 2905, loss 0.0515011, acc 0.984375
2017-09-09T15:36:53.219106: step 2906, loss 0.0596285, acc 0.984375
2017-09-09T15:36:53.563604: step 2907, loss 0.0273407, acc 1
2017-09-09T15:36:53.838494: step 2908, loss 0.0231786, acc 1
2017-09-09T15:36:54.201377: step 2909, loss 0.0243114, acc 1
2017-09-09T15:36:54.499687: step 2910, loss 0.0484707, acc 0.984375
2017-09-09T15:36:54.806562: step 2911, loss 0.0165792, acc 1
2017-09-09T15:36:55.074171: step 2912, loss 0.0750778, acc 0.953125
2017-09-09T15:36:55.517308: step 2913, loss 0.0635792, acc 0.96875
2017-09-09T15:36:55.793038: step 2914, loss 0.0482674, acc 0.96875
2017-09-09T15:36:56.138958: step 2915, loss 0.0549103, acc 1
2017-09-09T15:36:56.457454: step 2916, loss 0.0454086, acc 0.984375
2017-09-09T15:36:56.783931: step 2917, loss 0.0350988, acc 0.984375
2017-09-09T15:36:57.069347: step 2918, loss 0.0329471, acc 0.984375
2017-09-09T15:36:57.369118: step 2919, loss 0.0158714, acc 1
2017-09-09T15:36:57.675989: step 2920, loss 0.0265134, acc 1
2017-09-09T15:36:58.036965: step 2921, loss 0.0729206, acc 0.953125
2017-09-09T15:36:58.359175: step 2922, loss 0.11775, acc 0.984375
2017-09-09T15:36:58.665560: step 2923, loss 0.0223731, acc 1
2017-09-09T15:36:58.941490: step 2924, loss 0.0448779, acc 1
2017-09-09T15:36:59.299509: step 2925, loss 0.0344445, acc 0.984375
2017-09-09T15:36:59.579563: step 2926, loss 0.0242096, acc 1
2017-09-09T15:36:59.896340: step 2927, loss 0.010734, acc 1
2017-09-09T15:37:00.196365: step 2928, loss 0.019796, acc 0.984375
2017-09-09T15:37:00.466093: step 2929, loss 0.0434809, acc 0.984375
2017-09-09T15:37:00.742371: step 2930, loss 0.0444883, acc 0.984375
2017-09-09T15:37:01.016810: step 2931, loss 0.0538049, acc 0.96875
2017-09-09T15:37:01.350258: step 2932, loss 0.0576811, acc 0.984375
2017-09-09T15:37:01.632507: step 2933, loss 0.0526099, acc 0.96875
2017-09-09T15:37:01.975171: step 2934, loss 0.0418371, acc 0.984375
2017-09-09T15:37:02.286464: step 2935, loss 0.0448324, acc 0.984375
2017-09-09T15:37:02.581200: step 2936, loss 0.0618385, acc 0.984375
2017-09-09T15:37:02.910781: step 2937, loss 0.0458868, acc 0.984375
2017-09-09T15:37:03.265836: step 2938, loss 0.05547, acc 0.984375
2017-09-09T15:37:03.600068: step 2939, loss 0.0132382, acc 1
2017-09-09T15:37:03.897485: step 2940, loss 0.0149716, acc 1
2017-09-09T15:37:04.185631: step 2941, loss 0.0282131, acc 0.984375
2017-09-09T15:37:04.498646: step 2942, loss 0.0437429, acc 1
2017-09-09T15:37:04.813480: step 2943, loss 0.0460187, acc 0.96875
2017-09-09T15:37:05.089457: step 2944, loss 0.0627605, acc 0.984375
2017-09-09T15:37:05.470209: step 2945, loss 0.0354097, acc 1
2017-09-09T15:37:05.801620: step 2946, loss 0.00467722, acc 1
2017-09-09T15:37:06.166980: step 2947, loss 0.0614504, acc 0.984375
2017-09-09T15:37:06.537964: step 2948, loss 0.032111, acc 1
2017-09-09T15:37:06.804393: step 2949, loss 0.0438281, acc 1
2017-09-09T15:37:07.166127: step 2950, loss 0.00803408, acc 1

Evaluation:
2017-09-09T15:37:07.279970: step 2950, loss 1.96341, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-2950

2017-09-09T15:37:09.902917: step 2951, loss 0.0429689, acc 0.984375
2017-09-09T15:37:10.187075: step 2952, loss 0.0108848, acc 1
2017-09-09T15:37:10.480155: step 2953, loss 0.0112868, acc 1
2017-09-09T15:37:10.772425: step 2954, loss 0.0252561, acc 0.984375
2017-09-09T15:37:11.079967: step 2955, loss 0.0828186, acc 0.96875
2017-09-09T15:37:11.404613: step 2956, loss 0.00503534, acc 1
2017-09-09T15:37:11.754265: step 2957, loss 0.0490821, acc 0.96875
2017-09-09T15:37:12.038936: step 2958, loss 0.0196043, acc 1
2017-09-09T15:37:12.414755: step 2959, loss 0.058872, acc 0.984375
2017-09-09T15:37:12.705334: step 2960, loss 0.0320344, acc 0.984375
2017-09-09T15:37:12.983115: step 2961, loss 0.0410842, acc 1
2017-09-09T15:37:13.301798: step 2962, loss 0.0409388, acc 0.984375
2017-09-09T15:37:13.570237: step 2963, loss 0.0242666, acc 1
2017-09-09T15:37:13.882146: step 2964, loss 0.0601158, acc 0.984375
2017-09-09T15:37:14.144302: step 2965, loss 0.0363967, acc 0.984375
2017-09-09T15:37:14.450972: step 2966, loss 0.0232815, acc 1
2017-09-09T15:37:14.781062: step 2967, loss 0.096653, acc 0.953125
2017-09-09T15:37:15.118963: step 2968, loss 0.058531, acc 0.984375
2017-09-09T15:37:15.480177: step 2969, loss 0.06684, acc 0.984375
2017-09-09T15:37:15.760782: step 2970, loss 0.0148516, acc 1
2017-09-09T15:37:16.099091: step 2971, loss 0.018821, acc 0.984375
2017-09-09T15:37:16.408169: step 2972, loss 0.0295658, acc 1
2017-09-09T15:37:16.717814: step 2973, loss 0.0194438, acc 1
2017-09-09T15:37:17.022833: step 2974, loss 0.0110441, acc 1
2017-09-09T15:37:17.305953: step 2975, loss 0.0231713, acc 1
2017-09-09T15:37:17.631633: step 2976, loss 0.0432306, acc 0.96875
2017-09-09T15:37:17.913855: step 2977, loss 0.0214357, acc 1
2017-09-09T15:37:18.248965: step 2978, loss 0.0171751, acc 1
2017-09-09T15:37:18.533996: step 2979, loss 0.0214185, acc 1
2017-09-09T15:37:18.848588: step 2980, loss 0.00653798, acc 1
2017-09-09T15:37:19.139109: step 2981, loss 0.0643379, acc 0.96875
2017-09-09T15:37:19.407402: step 2982, loss 0.0905946, acc 0.984375
2017-09-09T15:37:19.767075: step 2983, loss 0.0749714, acc 0.953125
2017-09-09T15:37:20.036333: step 2984, loss 0.047266, acc 0.984375
2017-09-09T15:37:20.355022: step 2985, loss 0.0508084, acc 0.984375
2017-09-09T15:37:20.685498: step 2986, loss 0.0536754, acc 0.96875
2017-09-09T15:37:20.972715: step 2987, loss 0.114134, acc 0.9375
2017-09-09T15:37:21.320697: step 2988, loss 0.0545989, acc 0.984375
2017-09-09T15:37:21.599155: step 2989, loss 0.0162007, acc 1
2017-09-09T15:37:21.952039: step 2990, loss 0.0131466, acc 1
2017-09-09T15:37:22.276909: step 2991, loss 0.0592931, acc 0.96875
2017-09-09T15:37:22.598266: step 2992, loss 0.0122302, acc 1
2017-09-09T15:37:22.922403: step 2993, loss 0.0470032, acc 0.984375
2017-09-09T15:37:23.221334: step 2994, loss 0.0075915, acc 1
2017-09-09T15:37:23.552940: step 2995, loss 0.0190053, acc 1
2017-09-09T15:37:23.811900: step 2996, loss 0.0483579, acc 0.984375
2017-09-09T15:37:24.192988: step 2997, loss 0.0114741, acc 1
2017-09-09T15:37:24.488795: step 2998, loss 0.0644911, acc 0.984375
2017-09-09T15:37:24.791223: step 2999, loss 0.00732144, acc 1
2017-09-09T15:37:25.068867: step 3000, loss 0.0519525, acc 0.984375

Evaluation:
2017-09-09T15:37:25.163167: step 3000, loss 2.24627, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3000

2017-09-09T15:37:28.337707: step 3001, loss 0.0556792, acc 0.984375
2017-09-09T15:37:28.704129: step 3002, loss 0.0411208, acc 0.984375
2017-09-09T15:37:29.029361: step 3003, loss 0.0369117, acc 0.96875
2017-09-09T15:37:29.393612: step 3004, loss 0.0808294, acc 0.96875
2017-09-09T15:37:29.753725: step 3005, loss 0.0109153, acc 1
2017-09-09T15:37:30.058071: step 3006, loss 0.0138752, acc 1
2017-09-09T15:37:30.417357: step 3007, loss 0.0552104, acc 0.984375
2017-09-09T15:37:30.720553: step 3008, loss 0.0159079, acc 1
2017-09-09T15:37:31.065013: step 3009, loss 0.046485, acc 0.96875
2017-09-09T15:37:31.375041: step 3010, loss 0.0282731, acc 0.984375
2017-09-09T15:37:31.725601: step 3011, loss 0.0257403, acc 1
2017-09-09T15:37:32.066941: step 3012, loss 0.0432064, acc 0.96875
2017-09-09T15:37:32.399329: step 3013, loss 0.0556232, acc 0.96875
2017-09-09T15:37:32.696700: step 3014, loss 0.14349, acc 0.9375
2017-09-09T15:37:33.063129: step 3015, loss 0.0277331, acc 1
2017-09-09T15:37:33.332470: step 3016, loss 0.00668843, acc 1
2017-09-09T15:37:33.664757: step 3017, loss 0.0618566, acc 0.96875
2017-09-09T15:37:33.942654: step 3018, loss 0.0130545, acc 1
2017-09-09T15:37:34.284776: step 3019, loss 0.0354145, acc 1
2017-09-09T15:37:34.623443: step 3020, loss 0.0428603, acc 0.984375
2017-09-09T15:37:34.950984: step 3021, loss 0.0189217, acc 1
2017-09-09T15:37:35.357953: step 3022, loss 0.00551767, acc 1
2017-09-09T15:37:35.675686: step 3023, loss 0.0657847, acc 0.96875
2017-09-09T15:37:35.942199: step 3024, loss 0.0570805, acc 0.984375
2017-09-09T15:37:36.271696: step 3025, loss 0.0261035, acc 0.984375
2017-09-09T15:37:36.569455: step 3026, loss 0.0746513, acc 0.953125
2017-09-09T15:37:36.968015: step 3027, loss 0.0398067, acc 0.984375
2017-09-09T15:37:37.271457: step 3028, loss 0.134288, acc 0.90625
2017-09-09T15:37:37.518018: step 3029, loss 0.0807072, acc 0.953125
2017-09-09T15:37:37.775886: step 3030, loss 0.00941914, acc 1
2017-09-09T15:37:38.068751: step 3031, loss 0.0130566, acc 1
2017-09-09T15:37:38.378419: step 3032, loss 0.0887702, acc 0.984375
2017-09-09T15:37:38.653515: step 3033, loss 0.0453126, acc 1
2017-09-09T15:37:38.928036: step 3034, loss 0.0536595, acc 0.96875
2017-09-09T15:37:39.199975: step 3035, loss 0.0359053, acc 1
2017-09-09T15:37:39.543684: step 3036, loss 0.00301405, acc 1
2017-09-09T15:37:39.878851: step 3037, loss 0.0188258, acc 1
2017-09-09T15:37:40.186489: step 3038, loss 0.142989, acc 0.921569
2017-09-09T15:37:40.510201: step 3039, loss 0.00922529, acc 1
2017-09-09T15:37:40.787562: step 3040, loss 0.0479971, acc 0.96875
2017-09-09T15:37:41.089382: step 3041, loss 0.0333568, acc 1
2017-09-09T15:37:41.361294: step 3042, loss 0.043057, acc 0.984375
2017-09-09T15:37:41.664588: step 3043, loss 0.0202256, acc 1
2017-09-09T15:37:42.042388: step 3044, loss 0.059603, acc 0.96875
2017-09-09T15:37:42.314969: step 3045, loss 0.0201822, acc 0.984375
2017-09-09T15:37:42.646669: step 3046, loss 0.113178, acc 0.96875
2017-09-09T15:37:42.947260: step 3047, loss 0.024113, acc 0.984375
2017-09-09T15:37:43.280878: step 3048, loss 0.0803735, acc 0.984375
2017-09-09T15:37:43.661397: step 3049, loss 0.0378028, acc 0.984375
2017-09-09T15:37:43.968794: step 3050, loss 0.0215435, acc 1

Evaluation:
2017-09-09T15:37:44.080058: step 3050, loss 2.1677, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3050

2017-09-09T15:37:46.404776: step 3051, loss 0.0348151, acc 1
2017-09-09T15:37:46.722653: step 3052, loss 0.0586347, acc 0.984375
2017-09-09T15:37:47.021127: step 3053, loss 0.0551043, acc 0.984375
2017-09-09T15:37:47.312508: step 3054, loss 0.0182359, acc 1
2017-09-09T15:37:47.648944: step 3055, loss 0.0589187, acc 0.984375
2017-09-09T15:37:47.936533: step 3056, loss 0.0436308, acc 0.984375
2017-09-09T15:37:48.269296: step 3057, loss 0.0152219, acc 0.984375
2017-09-09T15:37:48.551435: step 3058, loss 0.0543239, acc 0.984375
2017-09-09T15:37:48.854475: step 3059, loss 0.0410989, acc 0.984375
2017-09-09T15:37:49.156091: step 3060, loss 0.0369366, acc 0.984375
2017-09-09T15:37:49.447921: step 3061, loss 0.021192, acc 1
2017-09-09T15:37:49.733075: step 3062, loss 0.0219972, acc 1
2017-09-09T15:37:50.023734: step 3063, loss 0.0397239, acc 0.96875
2017-09-09T15:37:50.363494: step 3064, loss 0.0637998, acc 0.96875
2017-09-09T15:37:50.648280: step 3065, loss 0.0450688, acc 0.984375
2017-09-09T15:37:51.026392: step 3066, loss 0.0901256, acc 0.96875
2017-09-09T15:37:51.312340: step 3067, loss 0.0476692, acc 1
2017-09-09T15:37:51.654319: step 3068, loss 0.00896427, acc 1
2017-09-09T15:37:51.966115: step 3069, loss 0.0249757, acc 0.984375
2017-09-09T15:37:52.259221: step 3070, loss 0.0725155, acc 0.953125
2017-09-09T15:37:52.599195: step 3071, loss 0.105704, acc 0.953125
2017-09-09T15:37:52.894014: step 3072, loss 0.00685398, acc 1
2017-09-09T15:37:53.310288: step 3073, loss 0.0578961, acc 0.96875
2017-09-09T15:37:53.629998: step 3074, loss 0.0319378, acc 1
2017-09-09T15:37:53.933479: step 3075, loss 0.0207433, acc 1
2017-09-09T15:37:54.190145: step 3076, loss 0.0286381, acc 0.984375
2017-09-09T15:37:54.498553: step 3077, loss 0.0205991, acc 1
2017-09-09T15:37:54.796552: step 3078, loss 0.0451212, acc 0.984375
2017-09-09T15:37:55.091339: step 3079, loss 0.0587743, acc 0.984375
2017-09-09T15:37:55.390938: step 3080, loss 0.12963, acc 0.9375
2017-09-09T15:37:55.727676: step 3081, loss 0.0690089, acc 0.96875
2017-09-09T15:37:56.011629: step 3082, loss 0.0186789, acc 1
2017-09-09T15:37:56.339623: step 3083, loss 0.0469467, acc 0.96875
2017-09-09T15:37:56.672859: step 3084, loss 0.0375569, acc 0.984375
2017-09-09T15:37:56.962311: step 3085, loss 0.0227163, acc 1
2017-09-09T15:37:57.574393: step 3086, loss 0.0238749, acc 0.984375
2017-09-09T15:37:57.861903: step 3087, loss 0.0304524, acc 0.984375
2017-09-09T15:37:58.189936: step 3088, loss 0.0652136, acc 0.984375
2017-09-09T15:37:58.490762: step 3089, loss 0.0263276, acc 1
2017-09-09T15:37:58.770346: step 3090, loss 0.0722355, acc 0.96875
2017-09-09T15:37:59.083681: step 3091, loss 0.0420316, acc 0.984375
2017-09-09T15:37:59.428724: step 3092, loss 0.0209807, acc 0.984375
2017-09-09T15:37:59.711162: step 3093, loss 0.0753942, acc 0.96875
2017-09-09T15:38:00.010817: step 3094, loss 0.0204803, acc 1
2017-09-09T15:38:00.342725: step 3095, loss 0.0618388, acc 0.96875
2017-09-09T15:38:00.638449: step 3096, loss 0.00603353, acc 1
2017-09-09T15:38:00.923273: step 3097, loss 0.00316022, acc 1
2017-09-09T15:38:01.231167: step 3098, loss 0.0499883, acc 0.984375
2017-09-09T15:38:01.550090: step 3099, loss 0.0346895, acc 0.984375
2017-09-09T15:38:01.910108: step 3100, loss 0.00224334, acc 1

Evaluation:
2017-09-09T15:38:01.967506: step 3100, loss 2.45359, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3100

2017-09-09T15:38:05.116328: step 3101, loss 0.0551911, acc 0.984375
2017-09-09T15:38:05.510242: step 3102, loss 0.0370274, acc 1
2017-09-09T15:38:05.834073: step 3103, loss 0.00448135, acc 1
2017-09-09T15:38:06.127877: step 3104, loss 0.0445786, acc 0.984375
2017-09-09T15:38:06.425148: step 3105, loss 0.0396618, acc 0.984375
2017-09-09T15:38:06.761423: step 3106, loss 0.0176355, acc 0.984375
2017-09-09T15:38:07.028465: step 3107, loss 0.0218502, acc 1
2017-09-09T15:38:07.339328: step 3108, loss 0.0643843, acc 0.953125
2017-09-09T15:38:07.743713: step 3109, loss 0.00907996, acc 1
2017-09-09T15:38:08.072389: step 3110, loss 0.0756138, acc 0.953125
2017-09-09T15:38:08.349272: step 3111, loss 0.0293135, acc 1
2017-09-09T15:38:08.669907: step 3112, loss 0.0419782, acc 0.96875
2017-09-09T15:38:08.949682: step 3113, loss 0.0237592, acc 0.984375
2017-09-09T15:38:09.255232: step 3114, loss 0.0738879, acc 0.953125
2017-09-09T15:38:09.523161: step 3115, loss 0.0129158, acc 1
2017-09-09T15:38:09.848067: step 3116, loss 0.0292296, acc 1
2017-09-09T15:38:10.129074: step 3117, loss 0.0150614, acc 1
2017-09-09T15:38:10.464395: step 3118, loss 0.0456592, acc 0.96875
2017-09-09T15:38:10.785501: step 3119, loss 0.0190453, acc 0.984375
2017-09-09T15:38:11.079797: step 3120, loss 0.0368837, acc 0.984375
2017-09-09T15:38:11.380814: step 3121, loss 0.0127417, acc 1
2017-09-09T15:38:11.692942: step 3122, loss 0.0358175, acc 1
2017-09-09T15:38:12.000437: step 3123, loss 0.0892712, acc 0.96875
2017-09-09T15:38:12.285341: step 3124, loss 0.0129916, acc 1
2017-09-09T15:38:12.651395: step 3125, loss 0.0139346, acc 1
2017-09-09T15:38:12.958141: step 3126, loss 0.0521496, acc 0.96875
2017-09-09T15:38:13.251660: step 3127, loss 0.0311203, acc 1
2017-09-09T15:38:13.557977: step 3128, loss 0.0699953, acc 0.953125
2017-09-09T15:38:13.900151: step 3129, loss 0.00999609, acc 1
2017-09-09T15:38:14.201009: step 3130, loss 0.10862, acc 0.9375
2017-09-09T15:38:14.509254: step 3131, loss 0.0202987, acc 0.984375
2017-09-09T15:38:14.819466: step 3132, loss 0.0137303, acc 1
2017-09-09T15:38:15.168558: step 3133, loss 0.0671536, acc 0.953125
2017-09-09T15:38:15.500362: step 3134, loss 0.0032077, acc 1
2017-09-09T15:38:15.847046: step 3135, loss 0.0549482, acc 0.984375
2017-09-09T15:38:16.139201: step 3136, loss 0.0185913, acc 0.980392
2017-09-09T15:38:16.485096: step 3137, loss 0.0272751, acc 1
2017-09-09T15:38:16.855208: step 3138, loss 0.0425, acc 0.984375
2017-09-09T15:38:17.166161: step 3139, loss 0.0744331, acc 0.953125
2017-09-09T15:38:17.513780: step 3140, loss 0.0391686, acc 1
2017-09-09T15:38:17.865091: step 3141, loss 0.0817673, acc 0.96875
2017-09-09T15:38:18.162748: step 3142, loss 0.0506371, acc 1
2017-09-09T15:38:18.503536: step 3143, loss 0.0401011, acc 0.984375
2017-09-09T15:38:18.783647: step 3144, loss 0.00442201, acc 1
2017-09-09T15:38:19.105446: step 3145, loss 0.0368401, acc 0.984375
2017-09-09T15:38:19.412361: step 3146, loss 0.0549045, acc 0.984375
2017-09-09T15:38:19.703819: step 3147, loss 0.0332165, acc 1
2017-09-09T15:38:20.033412: step 3148, loss 0.0840123, acc 0.96875
2017-09-09T15:38:20.323601: step 3149, loss 0.0135436, acc 1
2017-09-09T15:38:20.687711: step 3150, loss 0.03823, acc 1

Evaluation:
2017-09-09T15:38:20.756639: step 3150, loss 1.75312, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3150

2017-09-09T15:38:23.042050: step 3151, loss 0.086081, acc 0.953125
2017-09-09T15:38:23.349634: step 3152, loss 0.0275584, acc 1
2017-09-09T15:38:23.660464: step 3153, loss 0.0544741, acc 0.984375
2017-09-09T15:38:23.950359: step 3154, loss 0.0260185, acc 0.984375
2017-09-09T15:38:24.290053: step 3155, loss 0.00554608, acc 1
2017-09-09T15:38:24.597658: step 3156, loss 0.0197789, acc 1
2017-09-09T15:38:24.905818: step 3157, loss 0.0569671, acc 0.96875
2017-09-09T15:38:25.235810: step 3158, loss 0.0655623, acc 0.96875
2017-09-09T15:38:25.638843: step 3159, loss 0.054704, acc 0.96875
2017-09-09T15:38:25.911164: step 3160, loss 0.00629451, acc 1
2017-09-09T15:38:26.298797: step 3161, loss 0.0471619, acc 0.984375
2017-09-09T15:38:26.604894: step 3162, loss 0.0324978, acc 1
2017-09-09T15:38:26.890565: step 3163, loss 0.0636569, acc 0.984375
2017-09-09T15:38:27.194299: step 3164, loss 0.0380357, acc 0.984375
2017-09-09T15:38:27.485765: step 3165, loss 0.0181233, acc 1
2017-09-09T15:38:27.859659: step 3166, loss 0.0513848, acc 0.984375
2017-09-09T15:38:28.130076: step 3167, loss 0.021134, acc 1
2017-09-09T15:38:28.472144: step 3168, loss 0.0664439, acc 0.96875
2017-09-09T15:38:28.773654: step 3169, loss 0.0919782, acc 0.953125
2017-09-09T15:38:29.049569: step 3170, loss 0.00408437, acc 1
2017-09-09T15:38:29.335722: step 3171, loss 0.0647717, acc 0.96875
2017-09-09T15:38:29.611625: step 3172, loss 0.0167207, acc 1
2017-09-09T15:38:29.928019: step 3173, loss 0.0458681, acc 0.984375
2017-09-09T15:38:30.182786: step 3174, loss 0.0524617, acc 0.96875
2017-09-09T15:38:30.511236: step 3175, loss 0.0643071, acc 0.984375
2017-09-09T15:38:30.811343: step 3176, loss 0.0284538, acc 0.984375
2017-09-09T15:38:31.150859: step 3177, loss 0.0313015, acc 1
2017-09-09T15:38:31.459116: step 3178, loss 0.0675443, acc 0.96875
2017-09-09T15:38:31.747317: step 3179, loss 0.0258494, acc 1
2017-09-09T15:38:32.079464: step 3180, loss 0.0261663, acc 1
2017-09-09T15:38:32.353559: step 3181, loss 0.0474884, acc 0.984375
2017-09-09T15:38:32.617908: step 3182, loss 0.0308838, acc 1
2017-09-09T15:38:32.929941: step 3183, loss 0.0258368, acc 1
2017-09-09T15:38:33.195005: step 3184, loss 0.0205421, acc 0.984375
2017-09-09T15:38:33.568225: step 3185, loss 0.0262731, acc 0.984375
2017-09-09T15:38:33.879496: step 3186, loss 0.0199201, acc 0.984375
2017-09-09T15:38:34.170826: step 3187, loss 0.0246837, acc 0.984375
2017-09-09T15:38:34.480308: step 3188, loss 0.0204011, acc 0.984375
2017-09-09T15:38:34.767946: step 3189, loss 0.0159139, acc 1
2017-09-09T15:38:35.063448: step 3190, loss 0.0368331, acc 0.984375
2017-09-09T15:38:35.367035: step 3191, loss 0.0166701, acc 1
2017-09-09T15:38:35.723367: step 3192, loss 0.0876064, acc 0.96875
2017-09-09T15:38:36.002437: step 3193, loss 0.0104585, acc 1
2017-09-09T15:38:36.292249: step 3194, loss 0.0372826, acc 0.984375
2017-09-09T15:38:36.632067: step 3195, loss 0.0179567, acc 1
2017-09-09T15:38:36.935086: step 3196, loss 0.0191439, acc 1
2017-09-09T15:38:37.236987: step 3197, loss 0.0262971, acc 0.984375
2017-09-09T15:38:37.547938: step 3198, loss 0.0288751, acc 0.984375
2017-09-09T15:38:37.892357: step 3199, loss 0.0528611, acc 0.96875
2017-09-09T15:38:38.243795: step 3200, loss 0.00926184, acc 1

Evaluation:
2017-09-09T15:38:38.334321: step 3200, loss 2.09718, acc 0.34964

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3200

2017-09-09T15:38:41.247558: step 3201, loss 0.0248257, acc 1
2017-09-09T15:38:41.546412: step 3202, loss 0.0388913, acc 0.96875
2017-09-09T15:38:41.880942: step 3203, loss 0.0420257, acc 0.984375
2017-09-09T15:38:42.194255: step 3204, loss 0.0647307, acc 0.953125
2017-09-09T15:38:42.525505: step 3205, loss 0.0706852, acc 0.953125
2017-09-09T15:38:42.821615: step 3206, loss 0.154014, acc 0.953125
2017-09-09T15:38:43.144500: step 3207, loss 0.0449124, acc 0.984375
2017-09-09T15:38:43.809428: step 3208, loss 0.0107586, acc 1
2017-09-09T15:38:44.119293: step 3209, loss 0.0188304, acc 1
2017-09-09T15:38:44.439333: step 3210, loss 0.082721, acc 0.96875
2017-09-09T15:38:44.715611: step 3211, loss 0.0527935, acc 0.984375
2017-09-09T15:38:44.996734: step 3212, loss 0.0530463, acc 0.984375
2017-09-09T15:38:45.323102: step 3213, loss 0.0165868, acc 1
2017-09-09T15:38:45.609823: step 3214, loss 0.0246534, acc 0.984375
2017-09-09T15:38:45.938630: step 3215, loss 0.0549595, acc 0.953125
2017-09-09T15:38:46.236221: step 3216, loss 0.0299763, acc 0.984375
2017-09-09T15:38:46.583040: step 3217, loss 0.0884133, acc 0.96875
2017-09-09T15:38:46.919282: step 3218, loss 0.0313155, acc 1
2017-09-09T15:38:47.193987: step 3219, loss 0.00728525, acc 1
2017-09-09T15:38:47.532413: step 3220, loss 0.0590945, acc 0.96875
2017-09-09T15:38:47.826011: step 3221, loss 0.0474875, acc 0.984375
2017-09-09T15:38:48.151866: step 3222, loss 0.0146469, acc 1
2017-09-09T15:38:48.429384: step 3223, loss 0.0476643, acc 0.96875
2017-09-09T15:38:48.743079: step 3224, loss 0.0706237, acc 0.984375
2017-09-09T15:38:49.023949: step 3225, loss 0.0127457, acc 1
2017-09-09T15:38:49.321502: step 3226, loss 0.0254726, acc 1
2017-09-09T15:38:49.684524: step 3227, loss 0.0249148, acc 1
2017-09-09T15:38:49.965783: step 3228, loss 0.0286066, acc 0.984375
2017-09-09T15:38:50.224095: step 3229, loss 0.0637068, acc 0.984375
2017-09-09T15:38:50.568672: step 3230, loss 0.0892828, acc 0.96875
2017-09-09T15:38:50.857878: step 3231, loss 0.0623323, acc 0.96875
2017-09-09T15:38:51.275825: step 3232, loss 0.0217034, acc 1
2017-09-09T15:38:51.588880: step 3233, loss 0.0349878, acc 0.984375
2017-09-09T15:38:51.868020: step 3234, loss 0.052156, acc 0.980392
2017-09-09T15:38:52.155740: step 3235, loss 0.0164847, acc 1
2017-09-09T15:38:52.450009: step 3236, loss 0.0289408, acc 0.984375
2017-09-09T15:38:52.708798: step 3237, loss 0.050139, acc 0.96875
2017-09-09T15:38:52.964326: step 3238, loss 0.0644972, acc 0.96875
2017-09-09T15:38:53.285754: step 3239, loss 0.080389, acc 0.953125
2017-09-09T15:38:53.605250: step 3240, loss 0.0355786, acc 1
2017-09-09T15:38:53.930685: step 3241, loss 0.0124495, acc 1
2017-09-09T15:38:54.182601: step 3242, loss 0.0257315, acc 0.984375
2017-09-09T15:38:54.510914: step 3243, loss 0.0129882, acc 1
2017-09-09T15:38:54.863336: step 3244, loss 0.00170917, acc 1
2017-09-09T15:38:55.163467: step 3245, loss 0.0311468, acc 0.984375
2017-09-09T15:38:55.448481: step 3246, loss 0.0440926, acc 0.984375
2017-09-09T15:38:55.781253: step 3247, loss 0.0124675, acc 1
2017-09-09T15:38:56.106213: step 3248, loss 0.0195396, acc 1
2017-09-09T15:38:56.385690: step 3249, loss 0.0286134, acc 0.984375
2017-09-09T15:38:56.743304: step 3250, loss 0.0176556, acc 1

Evaluation:
2017-09-09T15:38:56.817371: step 3250, loss 2.25184, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3250

2017-09-09T15:38:59.882924: step 3251, loss 0.0190277, acc 1
2017-09-09T15:39:00.157616: step 3252, loss 0.0513939, acc 0.984375
2017-09-09T15:39:00.452437: step 3253, loss 0.0021587, acc 1
2017-09-09T15:39:00.767919: step 3254, loss 0.0271991, acc 0.984375
2017-09-09T15:39:01.053505: step 3255, loss 0.0175758, acc 1
2017-09-09T15:39:01.395834: step 3256, loss 0.0414926, acc 1
2017-09-09T15:39:01.679589: step 3257, loss 0.0639385, acc 0.96875
2017-09-09T15:39:01.972404: step 3258, loss 0.0451098, acc 0.953125
2017-09-09T15:39:02.257823: step 3259, loss 0.0522114, acc 0.984375
2017-09-09T15:39:02.554091: step 3260, loss 0.0878598, acc 0.96875
2017-09-09T15:39:02.854893: step 3261, loss 0.0243769, acc 1
2017-09-09T15:39:03.272925: step 3262, loss 0.0472762, acc 0.96875
2017-09-09T15:39:03.578856: step 3263, loss 0.025874, acc 1
2017-09-09T15:39:03.901287: step 3264, loss 0.0281212, acc 0.984375
2017-09-09T15:39:04.217731: step 3265, loss 0.0431609, acc 0.96875
2017-09-09T15:39:04.521945: step 3266, loss 0.00388497, acc 1
2017-09-09T15:39:04.842589: step 3267, loss 0.0185001, acc 1
2017-09-09T15:39:05.171690: step 3268, loss 0.0586248, acc 0.96875
2017-09-09T15:39:05.518879: step 3269, loss 0.0637791, acc 0.984375
2017-09-09T15:39:05.908896: step 3270, loss 0.0470462, acc 0.984375
2017-09-09T15:39:06.224366: step 3271, loss 0.0125258, acc 1
2017-09-09T15:39:06.500064: step 3272, loss 0.0778658, acc 0.96875
2017-09-09T15:39:06.855361: step 3273, loss 0.0338923, acc 0.984375
2017-09-09T15:39:07.134373: step 3274, loss 0.0398873, acc 1
2017-09-09T15:39:07.408326: step 3275, loss 0.0635033, acc 0.984375
2017-09-09T15:39:07.768524: step 3276, loss 0.109684, acc 0.9375
2017-09-09T15:39:08.064473: step 3277, loss 0.0266708, acc 1
2017-09-09T15:39:08.427805: step 3278, loss 0.0461279, acc 0.96875
2017-09-09T15:39:08.752191: step 3279, loss 0.0138518, acc 0.984375
2017-09-09T15:39:09.099270: step 3280, loss 0.0315241, acc 0.984375
2017-09-09T15:39:09.423807: step 3281, loss 0.0175255, acc 1
2017-09-09T15:39:09.715335: step 3282, loss 0.0338552, acc 0.984375
2017-09-09T15:39:10.071034: step 3283, loss 0.0193091, acc 1
2017-09-09T15:39:10.381460: step 3284, loss 0.0214914, acc 1
2017-09-09T15:39:10.696362: step 3285, loss 0.0207183, acc 1
2017-09-09T15:39:11.033615: step 3286, loss 0.0131758, acc 1
2017-09-09T15:39:11.327199: step 3287, loss 0.0245438, acc 1
2017-09-09T15:39:11.671108: step 3288, loss 0.0611535, acc 0.96875
2017-09-09T15:39:11.944159: step 3289, loss 0.00557305, acc 1
2017-09-09T15:39:12.300541: step 3290, loss 0.0544444, acc 0.96875
2017-09-09T15:39:12.593772: step 3291, loss 0.0393377, acc 0.984375
2017-09-09T15:39:12.874630: step 3292, loss 0.0103931, acc 1
2017-09-09T15:39:13.251883: step 3293, loss 0.00512546, acc 1
2017-09-09T15:39:13.520871: step 3294, loss 0.027485, acc 0.984375
2017-09-09T15:39:13.902427: step 3295, loss 0.00845885, acc 1
2017-09-09T15:39:14.271715: step 3296, loss 0.0621524, acc 0.96875
2017-09-09T15:39:14.549082: step 3297, loss 0.0152892, acc 0.984375
2017-09-09T15:39:14.852219: step 3298, loss 0.0213375, acc 0.984375
2017-09-09T15:39:15.117974: step 3299, loss 0.0130223, acc 1
2017-09-09T15:39:15.480482: step 3300, loss 0.0322007, acc 0.984375

Evaluation:
2017-09-09T15:39:15.538888: step 3300, loss 1.86418, acc 0.34964

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3300

2017-09-09T15:39:17.837438: step 3301, loss 0.0715109, acc 0.984375
2017-09-09T15:39:18.163997: step 3302, loss 0.0597681, acc 0.96875
2017-09-09T15:39:18.509875: step 3303, loss 0.0781423, acc 0.984375
2017-09-09T15:39:18.796715: step 3304, loss 0.0521703, acc 0.984375
2017-09-09T15:39:19.191380: step 3305, loss 0.0729029, acc 0.96875
2017-09-09T15:39:19.503869: step 3306, loss 0.078409, acc 0.96875
2017-09-09T15:39:19.779387: step 3307, loss 0.0306609, acc 1
2017-09-09T15:39:20.090233: step 3308, loss 0.0646952, acc 0.984375
2017-09-09T15:39:20.384709: step 3309, loss 0.108167, acc 0.953125
2017-09-09T15:39:20.675550: step 3310, loss 0.0417733, acc 0.984375
2017-09-09T15:39:20.986954: step 3311, loss 0.0169359, acc 1
2017-09-09T15:39:21.267843: step 3312, loss 0.0565023, acc 0.984375
2017-09-09T15:39:21.634592: step 3313, loss 0.0166125, acc 1
2017-09-09T15:39:21.968930: step 3314, loss 0.0211666, acc 1
2017-09-09T15:39:22.287477: step 3315, loss 0.0192592, acc 1
2017-09-09T15:39:22.600284: step 3316, loss 0.107303, acc 0.953125
2017-09-09T15:39:22.958302: step 3317, loss 0.0241539, acc 1
2017-09-09T15:39:23.265579: step 3318, loss 0.041395, acc 0.984375
2017-09-09T15:39:23.634618: step 3319, loss 0.0668937, acc 0.984375
2017-09-09T15:39:24.062948: step 3320, loss 0.0556672, acc 0.984375
2017-09-09T15:39:24.367672: step 3321, loss 0.0434962, acc 0.984375
2017-09-09T15:39:24.701526: step 3322, loss 0.0482663, acc 0.984375
2017-09-09T15:39:24.990933: step 3323, loss 0.0265653, acc 1
2017-09-09T15:39:25.315818: step 3324, loss 0.0440463, acc 0.984375
2017-09-09T15:39:25.633949: step 3325, loss 0.0641816, acc 0.96875
2017-09-09T15:39:25.908454: step 3326, loss 0.0274044, acc 0.984375
2017-09-09T15:39:26.215849: step 3327, loss 0.0221061, acc 1
2017-09-09T15:39:26.503863: step 3328, loss 0.0318907, acc 1
2017-09-09T15:39:26.789787: step 3329, loss 0.0796626, acc 0.984375
2017-09-09T15:39:27.148322: step 3330, loss 0.0129443, acc 1
2017-09-09T15:39:27.448467: step 3331, loss 0.0647386, acc 0.96875
2017-09-09T15:39:27.814172: step 3332, loss 0.0577302, acc 0.980392
2017-09-09T15:39:28.135770: step 3333, loss 0.0161678, acc 1
2017-09-09T15:39:28.464879: step 3334, loss 0.0276263, acc 0.984375
2017-09-09T15:39:28.838972: step 3335, loss 0.0294635, acc 1
2017-09-09T15:39:29.378369: step 3336, loss 0.0145821, acc 1
2017-09-09T15:39:29.705013: step 3337, loss 0.0538897, acc 1
2017-09-09T15:39:30.041324: step 3338, loss 0.0281071, acc 1
2017-09-09T15:39:30.336770: step 3339, loss 0.00968664, acc 1
2017-09-09T15:39:30.664043: step 3340, loss 0.0193266, acc 0.984375
2017-09-09T15:39:30.964132: step 3341, loss 0.0279431, acc 1
2017-09-09T15:39:31.327152: step 3342, loss 0.071951, acc 0.984375
2017-09-09T15:39:31.637853: step 3343, loss 0.0394322, acc 0.984375
2017-09-09T15:39:31.932788: step 3344, loss 0.063078, acc 0.984375
2017-09-09T15:39:32.227170: step 3345, loss 0.121218, acc 0.9375
2017-09-09T15:39:32.514316: step 3346, loss 0.0566679, acc 0.984375
2017-09-09T15:39:32.853051: step 3347, loss 0.0339902, acc 1
2017-09-09T15:39:33.145714: step 3348, loss 0.0262201, acc 1
2017-09-09T15:39:33.490906: step 3349, loss 0.00876851, acc 1
2017-09-09T15:39:33.809174: step 3350, loss 0.0366272, acc 1

Evaluation:
2017-09-09T15:39:33.891691: step 3350, loss 1.79362, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3350

2017-09-09T15:39:37.623124: step 3351, loss 0.0832958, acc 0.96875
2017-09-09T15:39:37.950570: step 3352, loss 0.0243999, acc 1
2017-09-09T15:39:38.246985: step 3353, loss 0.0106633, acc 1
2017-09-09T15:39:38.613412: step 3354, loss 0.0164615, acc 1
2017-09-09T15:39:38.898271: step 3355, loss 0.0314366, acc 0.984375
2017-09-09T15:39:39.250277: step 3356, loss 0.0506693, acc 0.984375
2017-09-09T15:39:39.580009: step 3357, loss 0.0148658, acc 1
2017-09-09T15:39:39.858378: step 3358, loss 0.0109991, acc 1
2017-09-09T15:39:40.216474: step 3359, loss 0.0378908, acc 0.984375
2017-09-09T15:39:40.521385: step 3360, loss 0.0526369, acc 0.984375
2017-09-09T15:39:40.849330: step 3361, loss 0.0163994, acc 1
2017-09-09T15:39:41.135656: step 3362, loss 0.0105228, acc 1
2017-09-09T15:39:41.445069: step 3363, loss 0.0412768, acc 0.984375
2017-09-09T15:39:41.799244: step 3364, loss 0.0200609, acc 1
2017-09-09T15:39:42.070568: step 3365, loss 0.0230626, acc 0.984375
2017-09-09T15:39:42.447710: step 3366, loss 0.0244565, acc 1
2017-09-09T15:39:42.728253: step 3367, loss 0.0368716, acc 0.984375
2017-09-09T15:39:43.069356: step 3368, loss 0.0295474, acc 1
2017-09-09T15:39:43.370566: step 3369, loss 0.0257208, acc 1
2017-09-09T15:39:43.662412: step 3370, loss 0.0163858, acc 1
2017-09-09T15:39:43.984989: step 3371, loss 0.0226376, acc 1
2017-09-09T15:39:44.247693: step 3372, loss 0.0376722, acc 0.984375
2017-09-09T15:39:44.595856: step 3373, loss 0.0676888, acc 0.96875
2017-09-09T15:39:44.873554: step 3374, loss 0.0380304, acc 1
2017-09-09T15:39:45.167005: step 3375, loss 0.0244527, acc 1
2017-09-09T15:39:45.464634: step 3376, loss 0.0528363, acc 0.984375
2017-09-09T15:39:45.731604: step 3377, loss 0.072501, acc 0.953125
2017-09-09T15:39:46.056416: step 3378, loss 0.0110578, acc 1
2017-09-09T15:39:46.353383: step 3379, loss 0.0035709, acc 1
2017-09-09T15:39:46.665358: step 3380, loss 0.00995047, acc 1
2017-09-09T15:39:46.932873: step 3381, loss 0.00283499, acc 1
2017-09-09T15:39:47.252068: step 3382, loss 0.0514864, acc 0.96875
2017-09-09T15:39:47.543491: step 3383, loss 0.0245778, acc 1
2017-09-09T15:39:47.931687: step 3384, loss 0.0249923, acc 0.984375
2017-09-09T15:39:48.195884: step 3385, loss 0.023375, acc 0.984375
2017-09-09T15:39:48.473541: step 3386, loss 0.0164745, acc 1
2017-09-09T15:39:48.792238: step 3387, loss 0.0305302, acc 0.984375
2017-09-09T15:39:49.052195: step 3388, loss 0.0231153, acc 1
2017-09-09T15:39:49.873678: step 3389, loss 0.00613866, acc 1
2017-09-09T15:39:50.166905: step 3390, loss 0.0210314, acc 1
2017-09-09T15:39:50.437542: step 3391, loss 0.0451012, acc 0.96875
2017-09-09T15:39:50.810260: step 3392, loss 0.0759104, acc 0.96875
2017-09-09T15:39:51.109252: step 3393, loss 0.0454357, acc 0.96875
2017-09-09T15:39:51.487477: step 3394, loss 0.0374935, acc 0.984375
2017-09-09T15:39:51.819003: step 3395, loss 0.0478959, acc 0.96875
2017-09-09T15:39:52.132094: step 3396, loss 0.0698045, acc 0.96875
2017-09-09T15:39:52.472559: step 3397, loss 0.172255, acc 0.9375
2017-09-09T15:39:52.774364: step 3398, loss 0.0847617, acc 0.96875
2017-09-09T15:39:53.072920: step 3399, loss 0.0224029, acc 0.984375
2017-09-09T15:39:53.437367: step 3400, loss 0.0576234, acc 0.984375

Evaluation:
2017-09-09T15:39:53.527235: step 3400, loss 2.2598, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3400

2017-09-09T15:39:56.701384: step 3401, loss 0.0155578, acc 1
2017-09-09T15:39:57.055897: step 3402, loss 0.033324, acc 0.984375
2017-09-09T15:39:57.350784: step 3403, loss 0.0332531, acc 0.984375
2017-09-09T15:39:57.673317: step 3404, loss 0.0628708, acc 0.984375
2017-09-09T15:39:57.984511: step 3405, loss 0.0486054, acc 0.984375
2017-09-09T15:39:58.280295: step 3406, loss 0.0375533, acc 0.984375
2017-09-09T15:39:58.618769: step 3407, loss 0.00827468, acc 1
2017-09-09T15:39:58.889856: step 3408, loss 0.0529891, acc 0.984375
2017-09-09T15:39:59.199947: step 3409, loss 0.0316582, acc 1
2017-09-09T15:39:59.480309: step 3410, loss 0.0592039, acc 0.984375
2017-09-09T15:39:59.859912: step 3411, loss 0.0129528, acc 1
2017-09-09T15:40:00.158997: step 3412, loss 0.0419598, acc 1
2017-09-09T15:40:00.452811: step 3413, loss 0.00537484, acc 1
2017-09-09T15:40:00.739709: step 3414, loss 0.0391089, acc 0.984375
2017-09-09T15:40:01.079241: step 3415, loss 0.0745414, acc 0.96875
2017-09-09T15:40:01.412358: step 3416, loss 0.00766136, acc 1
2017-09-09T15:40:01.664267: step 3417, loss 0.0100221, acc 1
2017-09-09T15:40:01.954458: step 3418, loss 0.0144248, acc 1
2017-09-09T15:40:02.222849: step 3419, loss 0.0578475, acc 0.96875
2017-09-09T15:40:02.508326: step 3420, loss 0.0250032, acc 1
2017-09-09T15:40:02.770137: step 3421, loss 0.0553122, acc 0.984375
2017-09-09T15:40:03.084295: step 3422, loss 0.0110466, acc 1
2017-09-09T15:40:03.369488: step 3423, loss 0.00474187, acc 1
2017-09-09T15:40:03.683087: step 3424, loss 0.0530308, acc 0.96875
2017-09-09T15:40:03.981866: step 3425, loss 0.0386248, acc 0.984375
2017-09-09T15:40:04.267739: step 3426, loss 0.0277668, acc 0.984375
2017-09-09T15:40:04.603821: step 3427, loss 0.047079, acc 0.984375
2017-09-09T15:40:04.894636: step 3428, loss 0.0299247, acc 0.984375
2017-09-09T15:40:05.231082: step 3429, loss 0.0166628, acc 1
2017-09-09T15:40:05.580195: step 3430, loss 0.0692051, acc 0.980392
2017-09-09T15:40:05.849757: step 3431, loss 0.0604593, acc 0.984375
2017-09-09T15:40:06.221339: step 3432, loss 0.00767566, acc 1
2017-09-09T15:40:06.528959: step 3433, loss 0.057278, acc 0.984375
2017-09-09T15:40:06.826055: step 3434, loss 0.0184215, acc 0.984375
2017-09-09T15:40:07.170826: step 3435, loss 0.010018, acc 1
2017-09-09T15:40:07.444765: step 3436, loss 0.0523649, acc 0.984375
2017-09-09T15:40:07.792099: step 3437, loss 0.00482894, acc 1
2017-09-09T15:40:08.072748: step 3438, loss 0.0171513, acc 1
2017-09-09T15:40:08.402717: step 3439, loss 0.0774708, acc 0.96875
2017-09-09T15:40:08.767741: step 3440, loss 0.0468357, acc 0.984375
2017-09-09T15:40:09.043977: step 3441, loss 0.0222298, acc 0.984375
2017-09-09T15:40:09.408093: step 3442, loss 0.0125184, acc 1
2017-09-09T15:40:09.763768: step 3443, loss 0.0197828, acc 1
2017-09-09T15:40:10.065423: step 3444, loss 0.0529304, acc 0.984375
2017-09-09T15:40:10.451061: step 3445, loss 0.0412139, acc 0.984375
2017-09-09T15:40:10.760045: step 3446, loss 0.0199381, acc 1
2017-09-09T15:40:11.127425: step 3447, loss 0.0199665, acc 0.984375
2017-09-09T15:40:11.434466: step 3448, loss 0.0354348, acc 1
2017-09-09T15:40:11.739034: step 3449, loss 0.0227935, acc 0.984375
2017-09-09T15:40:12.082623: step 3450, loss 0.0537112, acc 0.984375

Evaluation:
2017-09-09T15:40:12.141905: step 3450, loss 1.78498, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3450

2017-09-09T15:40:14.779545: step 3451, loss 0.0408903, acc 0.984375
2017-09-09T15:40:15.187482: step 3452, loss 0.109865, acc 0.953125
2017-09-09T15:40:15.470662: step 3453, loss 0.0246543, acc 1
2017-09-09T15:40:15.778607: step 3454, loss 0.0674776, acc 0.96875
2017-09-09T15:40:16.105288: step 3455, loss 0.0725647, acc 0.96875
2017-09-09T15:40:16.384960: step 3456, loss 0.0346751, acc 0.984375
2017-09-09T15:40:16.812660: step 3457, loss 0.0690263, acc 0.96875
2017-09-09T15:40:17.084766: step 3458, loss 0.0530416, acc 0.984375
2017-09-09T15:40:17.477857: step 3459, loss 0.0103127, acc 1
2017-09-09T15:40:17.769987: step 3460, loss 0.0302454, acc 1
2017-09-09T15:40:18.033147: step 3461, loss 0.0369985, acc 1
2017-09-09T15:40:18.341983: step 3462, loss 0.00763325, acc 1
2017-09-09T15:40:18.620994: step 3463, loss 0.0635132, acc 0.96875
2017-09-09T15:40:18.886927: step 3464, loss 0.0300389, acc 0.984375
2017-09-09T15:40:19.239331: step 3465, loss 0.00519543, acc 1
2017-09-09T15:40:19.597354: step 3466, loss 0.0203094, acc 1
2017-09-09T15:40:19.961361: step 3467, loss 0.0500104, acc 0.984375
2017-09-09T15:40:20.236199: step 3468, loss 0.0207969, acc 0.984375
2017-09-09T15:40:20.562907: step 3469, loss 0.0204627, acc 1
2017-09-09T15:40:20.854318: step 3470, loss 0.0342378, acc 0.984375
2017-09-09T15:40:21.171158: step 3471, loss 0.0153611, acc 1
2017-09-09T15:40:21.478441: step 3472, loss 0.131944, acc 0.9375
2017-09-09T15:40:21.787851: step 3473, loss 0.0111211, acc 1
2017-09-09T15:40:22.129819: step 3474, loss 0.0431432, acc 0.984375
2017-09-09T15:40:22.463950: step 3475, loss 0.0116268, acc 1
2017-09-09T15:40:22.748837: step 3476, loss 0.0287802, acc 1
2017-09-09T15:40:23.057905: step 3477, loss 0.022168, acc 1
2017-09-09T15:40:23.341841: step 3478, loss 0.040298, acc 0.984375
2017-09-09T15:40:23.682528: step 3479, loss 0.00906627, acc 1
2017-09-09T15:40:23.970201: step 3480, loss 0.0204159, acc 1
2017-09-09T15:40:24.286960: step 3481, loss 0.031831, acc 1
2017-09-09T15:40:24.554292: step 3482, loss 0.0202636, acc 0.984375
2017-09-09T15:40:24.817722: step 3483, loss 0.0374532, acc 0.984375
2017-09-09T15:40:25.127892: step 3484, loss 0.0926546, acc 0.984375
2017-09-09T15:40:25.413673: step 3485, loss 0.0662586, acc 0.984375
2017-09-09T15:40:25.721492: step 3486, loss 0.0719633, acc 0.984375
2017-09-09T15:40:26.081921: step 3487, loss 0.0323986, acc 0.984375
2017-09-09T15:40:26.433982: step 3488, loss 0.00852092, acc 1
2017-09-09T15:40:26.734810: step 3489, loss 0.036368, acc 0.984375
2017-09-09T15:40:27.021848: step 3490, loss 0.0341646, acc 0.984375
2017-09-09T15:40:27.381260: step 3491, loss 0.0979379, acc 0.96875
2017-09-09T15:40:27.735725: step 3492, loss 0.0222228, acc 0.984375
2017-09-09T15:40:28.094487: step 3493, loss 0.0380877, acc 0.96875
2017-09-09T15:40:28.408024: step 3494, loss 0.0515859, acc 0.96875
2017-09-09T15:40:28.757529: step 3495, loss 0.00385436, acc 1
2017-09-09T15:40:29.134632: step 3496, loss 0.0748165, acc 0.96875
2017-09-09T15:40:29.429083: step 3497, loss 0.0244898, acc 0.984375
2017-09-09T15:40:29.758067: step 3498, loss 0.00762061, acc 1
2017-09-09T15:40:30.032430: step 3499, loss 0.0391447, acc 0.96875
2017-09-09T15:40:30.371116: step 3500, loss 0.103557, acc 0.953125

Evaluation:
2017-09-09T15:40:30.508190: step 3500, loss 2.29078, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3500

2017-09-09T15:40:33.707495: step 3501, loss 0.0162888, acc 1
2017-09-09T15:40:34.008315: step 3502, loss 0.00499287, acc 1
2017-09-09T15:40:34.315616: step 3503, loss 0.0241228, acc 1
2017-09-09T15:40:34.641815: step 3504, loss 0.0244953, acc 1
2017-09-09T15:40:34.906053: step 3505, loss 0.0902189, acc 0.96875
2017-09-09T15:40:35.204960: step 3506, loss 0.0399981, acc 0.984375
2017-09-09T15:40:35.482635: step 3507, loss 0.0472388, acc 0.984375
2017-09-09T15:40:35.840118: step 3508, loss 0.0147876, acc 1
2017-09-09T15:40:36.157456: step 3509, loss 0.031194, acc 1
2017-09-09T15:40:36.443304: step 3510, loss 0.0581415, acc 0.984375
2017-09-09T15:40:36.808052: step 3511, loss 0.0812632, acc 0.96875
2017-09-09T15:40:37.123498: step 3512, loss 0.0887503, acc 0.984375
2017-09-09T15:40:37.456849: step 3513, loss 0.0275364, acc 1
2017-09-09T15:40:37.789694: step 3514, loss 0.00685092, acc 1
2017-09-09T15:40:38.085724: step 3515, loss 0.101386, acc 0.953125
2017-09-09T15:40:38.475004: step 3516, loss 0.0416708, acc 0.984375
2017-09-09T15:40:38.756791: step 3517, loss 0.0395374, acc 1
2017-09-09T15:40:39.045977: step 3518, loss 0.0286085, acc 0.984375
2017-09-09T15:40:39.328995: step 3519, loss 0.225424, acc 0.9375
2017-09-09T15:40:39.658397: step 3520, loss 0.0250942, acc 0.984375
2017-09-09T15:40:39.975600: step 3521, loss 0.00447401, acc 1
2017-09-09T15:40:40.295850: step 3522, loss 0.0155617, acc 1
2017-09-09T15:40:40.575943: step 3523, loss 0.0436024, acc 0.984375
2017-09-09T15:40:40.916981: step 3524, loss 0.0579985, acc 0.984375
2017-09-09T15:40:41.235975: step 3525, loss 0.0424716, acc 0.984375
2017-09-09T15:40:41.531434: step 3526, loss 0.0199293, acc 0.984375
2017-09-09T15:40:41.854324: step 3527, loss 0.0646441, acc 0.96875
2017-09-09T15:40:42.125776: step 3528, loss 0.00953053, acc 1
2017-09-09T15:40:42.481631: step 3529, loss 0.0300158, acc 0.984375
2017-09-09T15:40:42.777295: step 3530, loss 0.0399118, acc 0.96875
2017-09-09T15:40:43.077547: step 3531, loss 0.0297154, acc 0.984375
2017-09-09T15:40:43.407062: step 3532, loss 0.0382598, acc 0.984375
2017-09-09T15:40:43.710995: step 3533, loss 0.0573333, acc 0.96875
2017-09-09T15:40:44.036138: step 3534, loss 0.0303182, acc 1
2017-09-09T15:40:44.361585: step 3535, loss 0.0079996, acc 1
2017-09-09T15:40:44.776992: step 3536, loss 0.013085, acc 1
2017-09-09T15:40:45.133610: step 3537, loss 0.0136585, acc 1
2017-09-09T15:40:45.472586: step 3538, loss 0.0242287, acc 1
2017-09-09T15:40:45.803816: step 3539, loss 0.0426714, acc 0.96875
2017-09-09T15:40:46.122135: step 3540, loss 0.0261754, acc 0.984375
2017-09-09T15:40:46.407963: step 3541, loss 0.0151119, acc 1
2017-09-09T15:40:46.684144: step 3542, loss 0.0426943, acc 0.984375
2017-09-09T15:40:46.966264: step 3543, loss 0.00946808, acc 1
2017-09-09T15:40:47.233992: step 3544, loss 0.0225363, acc 1
2017-09-09T15:40:47.523511: step 3545, loss 0.0373179, acc 0.984375
2017-09-09T15:40:47.851328: step 3546, loss 0.0618445, acc 0.953125
2017-09-09T15:40:48.181138: step 3547, loss 0.0505826, acc 0.984375
2017-09-09T15:40:48.494711: step 3548, loss 0.0199758, acc 1
2017-09-09T15:40:48.899802: step 3549, loss 0.0234375, acc 1
2017-09-09T15:40:49.187062: step 3550, loss 0.0133211, acc 0.984375

Evaluation:
2017-09-09T15:40:49.318702: step 3550, loss 1.92661, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3550

2017-09-09T15:40:51.626389: step 3551, loss 0.0689391, acc 0.953125
2017-09-09T15:40:51.985703: step 3552, loss 0.0486583, acc 0.984375
2017-09-09T15:40:52.314457: step 3553, loss 0.00521427, acc 1
2017-09-09T15:40:52.642066: step 3554, loss 0.0295965, acc 1
2017-09-09T15:40:52.979963: step 3555, loss 0.0151836, acc 1
2017-09-09T15:40:53.288405: step 3556, loss 0.0116324, acc 1
2017-09-09T15:40:53.654467: step 3557, loss 0.0560267, acc 0.96875
2017-09-09T15:40:54.013191: step 3558, loss 0.0157505, acc 1
2017-09-09T15:40:54.360078: step 3559, loss 0.0240333, acc 0.984375
2017-09-09T15:40:54.702339: step 3560, loss 0.0588263, acc 0.984375
2017-09-09T15:40:55.016653: step 3561, loss 0.00608377, acc 1
2017-09-09T15:40:55.293860: step 3562, loss 0.0241115, acc 1
2017-09-09T15:40:55.590873: step 3563, loss 0.0141394, acc 1
2017-09-09T15:40:55.880908: step 3564, loss 0.0337117, acc 0.984375
2017-09-09T15:40:56.235857: step 3565, loss 0.0131356, acc 1
2017-09-09T15:40:56.521919: step 3566, loss 0.045746, acc 0.984375
2017-09-09T15:40:56.817965: step 3567, loss 0.0614286, acc 0.984375
2017-09-09T15:40:57.116446: step 3568, loss 0.0282897, acc 0.984375
2017-09-09T15:40:57.391132: step 3569, loss 0.0127963, acc 1
2017-09-09T15:40:57.668353: step 3570, loss 0.012118, acc 1
2017-09-09T15:40:57.940265: step 3571, loss 0.0959198, acc 0.953125
2017-09-09T15:40:58.260782: step 3572, loss 0.0192606, acc 1
2017-09-09T15:40:58.650388: step 3573, loss 0.0508285, acc 0.984375
2017-09-09T15:40:58.910237: step 3574, loss 0.0287795, acc 0.984375
2017-09-09T15:40:59.219372: step 3575, loss 0.0414955, acc 0.984375
2017-09-09T15:40:59.511300: step 3576, loss 0.0113963, acc 1
2017-09-09T15:40:59.825203: step 3577, loss 0.0179156, acc 1
2017-09-09T15:41:00.110762: step 3578, loss 0.0884666, acc 0.96875
2017-09-09T15:41:00.423355: step 3579, loss 0.00267696, acc 1
2017-09-09T15:41:00.701853: step 3580, loss 0.0275783, acc 1
2017-09-09T15:41:01.066111: step 3581, loss 0.0337034, acc 1
2017-09-09T15:41:01.395608: step 3582, loss 0.00377225, acc 1
2017-09-09T15:41:01.741921: step 3583, loss 0.0959812, acc 0.953125
2017-09-09T15:41:02.058197: step 3584, loss 0.0179307, acc 1
2017-09-09T15:41:02.412976: step 3585, loss 0.0192791, acc 1
2017-09-09T15:41:02.797286: step 3586, loss 0.0467611, acc 0.984375
2017-09-09T15:41:03.067135: step 3587, loss 0.00450312, acc 1
2017-09-09T15:41:03.438610: step 3588, loss 0.0453832, acc 0.984375
2017-09-09T15:41:03.804508: step 3589, loss 0.00419679, acc 1
2017-09-09T15:41:04.085030: step 3590, loss 0.0923217, acc 0.96875
2017-09-09T15:41:04.408483: step 3591, loss 0.0167381, acc 0.984375
2017-09-09T15:41:04.771710: step 3592, loss 0.056068, acc 0.96875
2017-09-09T15:41:05.099152: step 3593, loss 0.110571, acc 0.953125
2017-09-09T15:41:05.452314: step 3594, loss 0.0211956, acc 1
2017-09-09T15:41:05.743770: step 3595, loss 0.0522656, acc 0.984375
2017-09-09T15:41:06.063522: step 3596, loss 0.0227149, acc 1
2017-09-09T15:41:06.388635: step 3597, loss 0.0414618, acc 0.984375
2017-09-09T15:41:06.679961: step 3598, loss 0.0172343, acc 1
2017-09-09T15:41:07.061112: step 3599, loss 0.025501, acc 1
2017-09-09T15:41:07.333256: step 3600, loss 0.0324875, acc 0.984375

Evaluation:
2017-09-09T15:41:07.417269: step 3600, loss 1.88162, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3600

2017-09-09T15:41:10.641400: step 3601, loss 0.103527, acc 0.9375
2017-09-09T15:41:10.946835: step 3602, loss 0.0070449, acc 1
2017-09-09T15:41:11.233369: step 3603, loss 0.0422965, acc 0.984375
2017-09-09T15:41:11.569149: step 3604, loss 0.0370508, acc 0.984375
2017-09-09T15:41:11.875815: step 3605, loss 0.0505125, acc 0.96875
2017-09-09T15:41:12.251003: step 3606, loss 0.0267968, acc 1
2017-09-09T15:41:12.570292: step 3607, loss 0.0130436, acc 1
2017-09-09T15:41:12.882535: step 3608, loss 0.0104841, acc 1
2017-09-09T15:41:13.163555: step 3609, loss 0.00424045, acc 1
2017-09-09T15:41:13.589787: step 3610, loss 0.0491552, acc 0.984375
2017-09-09T15:41:13.883456: step 3611, loss 0.0526087, acc 0.984375
2017-09-09T15:41:14.209810: step 3612, loss 0.0393176, acc 0.984375
2017-09-09T15:41:14.504493: step 3613, loss 0.0672703, acc 0.984375
2017-09-09T15:41:14.781149: step 3614, loss 0.0238335, acc 1
2017-09-09T15:41:15.066665: step 3615, loss 0.055811, acc 0.96875
2017-09-09T15:41:15.350728: step 3616, loss 0.0777409, acc 0.96875
2017-09-09T15:41:15.700460: step 3617, loss 0.0219546, acc 0.984375
2017-09-09T15:41:16.004222: step 3618, loss 0.0274292, acc 0.984375
2017-09-09T15:41:16.300366: step 3619, loss 0.00363113, acc 1
2017-09-09T15:41:16.581021: step 3620, loss 0.0117178, acc 1
2017-09-09T15:41:16.853536: step 3621, loss 0.0410957, acc 1
2017-09-09T15:41:17.244437: step 3622, loss 0.0439898, acc 0.984375
2017-09-09T15:41:17.557025: step 3623, loss 0.0741659, acc 0.984375
2017-09-09T15:41:17.841514: step 3624, loss 0.0213111, acc 1
2017-09-09T15:41:18.189677: step 3625, loss 0.0291512, acc 0.984375
2017-09-09T15:41:18.468482: step 3626, loss 0.0388031, acc 0.980392
2017-09-09T15:41:18.799319: step 3627, loss 0.0208584, acc 1
2017-09-09T15:41:19.128509: step 3628, loss 0.0579037, acc 0.984375
2017-09-09T15:41:19.394112: step 3629, loss 0.0448716, acc 0.96875
2017-09-09T15:41:19.746561: step 3630, loss 0.0353707, acc 0.984375
2017-09-09T15:41:20.021623: step 3631, loss 0.0306895, acc 0.984375
2017-09-09T15:41:20.359855: step 3632, loss 0.0385343, acc 0.984375
2017-09-09T15:41:20.679983: step 3633, loss 0.0383702, acc 0.984375
2017-09-09T15:41:20.972870: step 3634, loss 0.0109598, acc 1
2017-09-09T15:41:21.306933: step 3635, loss 0.0447539, acc 1
2017-09-09T15:41:21.590143: step 3636, loss 0.0027201, acc 1
2017-09-09T15:41:21.934003: step 3637, loss 0.04053, acc 0.984375
2017-09-09T15:41:22.249597: step 3638, loss 0.0588165, acc 0.984375
2017-09-09T15:41:22.528241: step 3639, loss 0.0746186, acc 0.96875
2017-09-09T15:41:22.849331: step 3640, loss 0.0722336, acc 0.953125
2017-09-09T15:41:23.139378: step 3641, loss 0.0266525, acc 1
2017-09-09T15:41:23.492628: step 3642, loss 0.0157006, acc 1
2017-09-09T15:41:23.783962: step 3643, loss 0.00978829, acc 1
2017-09-09T15:41:24.133364: step 3644, loss 0.0712348, acc 0.953125
2017-09-09T15:41:24.489855: step 3645, loss 0.0156572, acc 1
2017-09-09T15:41:24.792838: step 3646, loss 0.011345, acc 1
2017-09-09T15:41:25.153829: step 3647, loss 0.0917843, acc 0.96875
2017-09-09T15:41:25.465985: step 3648, loss 0.0287267, acc 0.984375
2017-09-09T15:41:25.783095: step 3649, loss 0.0079788, acc 1
2017-09-09T15:41:26.084106: step 3650, loss 0.0345903, acc 1

Evaluation:
2017-09-09T15:41:26.149358: step 3650, loss 1.80788, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3650

2017-09-09T15:41:28.436255: step 3651, loss 0.0169182, acc 1
2017-09-09T15:41:28.764119: step 3652, loss 0.0401848, acc 0.984375
2017-09-09T15:41:29.095588: step 3653, loss 0.0194483, acc 1
2017-09-09T15:41:29.408557: step 3654, loss 0.0413977, acc 0.984375
2017-09-09T15:41:29.803915: step 3655, loss 0.0310977, acc 0.984375
2017-09-09T15:41:30.094451: step 3656, loss 0.0250264, acc 1
2017-09-09T15:41:30.402559: step 3657, loss 0.0930776, acc 0.96875
2017-09-09T15:41:30.760305: step 3658, loss 0.0298878, acc 0.984375
2017-09-09T15:41:31.105773: step 3659, loss 0.0208016, acc 1
2017-09-09T15:41:31.493946: step 3660, loss 0.044241, acc 0.984375
2017-09-09T15:41:31.838307: step 3661, loss 0.0335407, acc 0.984375
2017-09-09T15:41:32.203951: step 3662, loss 0.0183771, acc 0.984375
2017-09-09T15:41:32.531799: step 3663, loss 0.0380792, acc 0.984375
2017-09-09T15:41:32.863672: step 3664, loss 0.0452182, acc 0.96875
2017-09-09T15:41:33.211939: step 3665, loss 0.0309645, acc 0.984375
2017-09-09T15:41:33.538584: step 3666, loss 0.0110346, acc 1
2017-09-09T15:41:33.853465: step 3667, loss 0.04778, acc 0.984375
2017-09-09T15:41:34.200114: step 3668, loss 0.0252197, acc 0.984375
2017-09-09T15:41:34.554287: step 3669, loss 0.0398069, acc 0.984375
2017-09-09T15:41:34.910364: step 3670, loss 0.0120942, acc 1
2017-09-09T15:41:35.275224: step 3671, loss 0.0237685, acc 1
2017-09-09T15:41:35.575085: step 3672, loss 0.0167592, acc 1
2017-09-09T15:41:35.883294: step 3673, loss 0.0120909, acc 1
2017-09-09T15:41:36.216665: step 3674, loss 0.0269017, acc 0.984375
2017-09-09T15:41:36.549757: step 3675, loss 0.0691374, acc 0.984375
2017-09-09T15:41:36.855366: step 3676, loss 0.0894798, acc 0.96875
2017-09-09T15:41:37.169304: step 3677, loss 0.00992567, acc 1
2017-09-09T15:41:37.536048: step 3678, loss 0.0178982, acc 1
2017-09-09T15:41:37.844131: step 3679, loss 0.0138994, acc 1
2017-09-09T15:41:38.125820: step 3680, loss 0.0216491, acc 1
2017-09-09T15:41:38.466391: step 3681, loss 0.00874856, acc 1
2017-09-09T15:41:38.785365: step 3682, loss 0.0177271, acc 1
2017-09-09T15:41:39.069682: step 3683, loss 0.0024503, acc 1
2017-09-09T15:41:39.410647: step 3684, loss 0.0238088, acc 0.984375
2017-09-09T15:41:39.756578: step 3685, loss 0.0106775, acc 1
2017-09-09T15:41:40.074814: step 3686, loss 0.0660795, acc 0.96875
2017-09-09T15:41:40.449708: step 3687, loss 0.0610087, acc 0.96875
2017-09-09T15:41:40.787883: step 3688, loss 0.015112, acc 1
2017-09-09T15:41:41.123109: step 3689, loss 0.00802412, acc 1
2017-09-09T15:41:41.440682: step 3690, loss 0.0338793, acc 0.984375
2017-09-09T15:41:41.748281: step 3691, loss 0.0282333, acc 0.984375
2017-09-09T15:41:42.034182: step 3692, loss 0.0157403, acc 0.984375
2017-09-09T15:41:42.356874: step 3693, loss 0.0518475, acc 0.96875
2017-09-09T15:41:42.686842: step 3694, loss 0.0311365, acc 0.984375
2017-09-09T15:41:42.949262: step 3695, loss 0.0420641, acc 0.984375
2017-09-09T15:41:43.305201: step 3696, loss 0.0392573, acc 0.984375
2017-09-09T15:41:43.641405: step 3697, loss 0.0432075, acc 0.984375
2017-09-09T15:41:43.955174: step 3698, loss 0.036587, acc 0.984375
2017-09-09T15:41:44.280066: step 3699, loss 0.0209767, acc 1
2017-09-09T15:41:44.612169: step 3700, loss 0.136761, acc 0.96875

Evaluation:
2017-09-09T15:41:44.746425: step 3700, loss 2.10637, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3700

2017-09-09T15:41:47.665640: step 3701, loss 0.119143, acc 0.9375
2017-09-09T15:41:47.992970: step 3702, loss 0.0821925, acc 0.96875
2017-09-09T15:41:48.309609: step 3703, loss 0.0135759, acc 1
2017-09-09T15:41:48.703032: step 3704, loss 0.016223, acc 1
2017-09-09T15:41:48.975380: step 3705, loss 0.0338747, acc 0.984375
2017-09-09T15:41:49.310885: step 3706, loss 0.0234647, acc 0.984375
2017-09-09T15:41:49.622240: step 3707, loss 0.0172867, acc 1
2017-09-09T15:41:49.924436: step 3708, loss 0.0566575, acc 0.96875
2017-09-09T15:41:50.271841: step 3709, loss 0.0748975, acc 0.984375
2017-09-09T15:41:50.550097: step 3710, loss 0.0492514, acc 0.984375
2017-09-09T15:41:50.917717: step 3711, loss 0.026143, acc 0.984375
2017-09-09T15:41:51.206080: step 3712, loss 0.0764618, acc 0.96875
2017-09-09T15:41:51.528020: step 3713, loss 0.0141601, acc 1
2017-09-09T15:41:51.806078: step 3714, loss 0.0151324, acc 1
2017-09-09T15:41:52.077190: step 3715, loss 0.0653391, acc 0.96875
2017-09-09T15:41:52.451449: step 3716, loss 0.0276049, acc 0.984375
2017-09-09T15:41:52.746559: step 3717, loss 0.0500606, acc 0.96875
2017-09-09T15:41:53.090425: step 3718, loss 0.0274059, acc 0.984375
2017-09-09T15:41:53.402244: step 3719, loss 0.0631268, acc 0.984375
2017-09-09T15:41:53.791350: step 3720, loss 0.00857563, acc 1
2017-09-09T15:41:54.093037: step 3721, loss 0.0275325, acc 1
2017-09-09T15:41:54.393850: step 3722, loss 0.024598, acc 1
2017-09-09T15:41:54.679662: step 3723, loss 0.0373392, acc 0.96875
2017-09-09T15:41:54.926787: step 3724, loss 0.0268795, acc 0.980392
2017-09-09T15:41:55.261794: step 3725, loss 0.0324957, acc 0.984375
2017-09-09T15:41:55.584899: step 3726, loss 0.053575, acc 0.96875
2017-09-09T15:41:55.861106: step 3727, loss 0.0976792, acc 0.953125
2017-09-09T15:41:56.150978: step 3728, loss 0.0213622, acc 1
2017-09-09T15:41:56.515370: step 3729, loss 0.00236386, acc 1
2017-09-09T15:41:56.815766: step 3730, loss 0.0426478, acc 1
2017-09-09T15:41:57.124675: step 3731, loss 0.0678992, acc 0.96875
2017-09-09T15:41:57.420323: step 3732, loss 0.00878508, acc 1
2017-09-09T15:41:57.753634: step 3733, loss 0.0347869, acc 1
2017-09-09T15:41:58.127924: step 3734, loss 0.00679787, acc 1
2017-09-09T15:41:58.410821: step 3735, loss 0.0289334, acc 0.984375
2017-09-09T15:41:58.812194: step 3736, loss 0.064858, acc 0.984375
2017-09-09T15:41:59.150008: step 3737, loss 0.00282391, acc 1
2017-09-09T15:41:59.433067: step 3738, loss 0.0485391, acc 0.984375
2017-09-09T15:41:59.837073: step 3739, loss 0.0356251, acc 0.984375
2017-09-09T15:42:00.149752: step 3740, loss 0.0720644, acc 0.984375
2017-09-09T15:42:00.470234: step 3741, loss 0.0830993, acc 0.96875
2017-09-09T15:42:00.823794: step 3742, loss 0.0803528, acc 0.953125
2017-09-09T15:42:01.096400: step 3743, loss 0.0100785, acc 1
2017-09-09T15:42:01.452485: step 3744, loss 0.0551044, acc 0.984375
2017-09-09T15:42:01.755744: step 3745, loss 0.0463186, acc 0.96875
2017-09-09T15:42:02.067666: step 3746, loss 0.0145576, acc 1
2017-09-09T15:42:02.391158: step 3747, loss 0.0343177, acc 1
2017-09-09T15:42:02.677383: step 3748, loss 0.00996042, acc 1
2017-09-09T15:42:03.018615: step 3749, loss 0.0233111, acc 1
2017-09-09T15:42:03.317181: step 3750, loss 0.0265978, acc 0.984375

Evaluation:
2017-09-09T15:42:03.440888: step 3750, loss 1.84249, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3750

2017-09-09T15:42:07.157520: step 3751, loss 0.0656546, acc 0.953125
2017-09-09T15:42:07.456471: step 3752, loss 0.00310469, acc 1
2017-09-09T15:42:07.783565: step 3753, loss 0.0214744, acc 1
2017-09-09T15:42:08.137675: step 3754, loss 0.0396788, acc 0.984375
2017-09-09T15:42:08.434374: step 3755, loss 0.0268142, acc 0.984375
2017-09-09T15:42:08.723496: step 3756, loss 0.0463992, acc 0.984375
2017-09-09T15:42:09.141378: step 3757, loss 0.034248, acc 0.96875
2017-09-09T15:42:09.423259: step 3758, loss 0.024686, acc 1
2017-09-09T15:42:09.740399: step 3759, loss 0.0755939, acc 0.96875
2017-09-09T15:42:10.030604: step 3760, loss 0.0436036, acc 0.984375
2017-09-09T15:42:10.334994: step 3761, loss 0.030071, acc 0.984375
2017-09-09T15:42:10.616040: step 3762, loss 0.0399067, acc 0.984375
2017-09-09T15:42:10.889602: step 3763, loss 0.0103342, acc 1
2017-09-09T15:42:11.188676: step 3764, loss 0.00304241, acc 1
2017-09-09T15:42:11.509500: step 3765, loss 0.0272493, acc 0.984375
2017-09-09T15:42:11.871148: step 3766, loss 0.0520135, acc 0.96875
2017-09-09T15:42:12.174849: step 3767, loss 0.0794323, acc 0.9375
2017-09-09T15:42:12.505758: step 3768, loss 0.0508207, acc 0.984375
2017-09-09T15:42:12.858004: step 3769, loss 0.00951035, acc 1
2017-09-09T15:42:13.122299: step 3770, loss 0.0783042, acc 0.96875
2017-09-09T15:42:13.441579: step 3771, loss 0.0330541, acc 0.984375
2017-09-09T15:42:13.739115: step 3772, loss 0.0650876, acc 0.96875
2017-09-09T15:42:14.013964: step 3773, loss 0.0210354, acc 0.984375
2017-09-09T15:42:14.312752: step 3774, loss 0.0169703, acc 1
2017-09-09T15:42:14.610458: step 3775, loss 0.0344458, acc 0.984375
2017-09-09T15:42:14.974629: step 3776, loss 0.0342699, acc 0.984375
2017-09-09T15:42:15.282588: step 3777, loss 0.0882236, acc 0.953125
2017-09-09T15:42:15.608304: step 3778, loss 0.0748032, acc 0.96875
2017-09-09T15:42:15.943317: step 3779, loss 0.0171263, acc 0.984375
2017-09-09T15:42:16.236102: step 3780, loss 0.00758332, acc 1
2017-09-09T15:42:16.559482: step 3781, loss 0.029674, acc 0.984375
2017-09-09T15:42:16.841197: step 3782, loss 0.0198859, acc 1
2017-09-09T15:42:17.186180: step 3783, loss 0.0557206, acc 0.984375
2017-09-09T15:42:17.499362: step 3784, loss 0.0139222, acc 1
2017-09-09T15:42:17.788063: step 3785, loss 0.0399605, acc 0.984375
2017-09-09T15:42:18.121025: step 3786, loss 0.0403722, acc 0.984375
2017-09-09T15:42:18.415764: step 3787, loss 0.00340531, acc 1
2017-09-09T15:42:18.784145: step 3788, loss 0.0221014, acc 1
2017-09-09T15:42:19.077069: step 3789, loss 0.0183397, acc 1
2017-09-09T15:42:19.419789: step 3790, loss 0.00299851, acc 1
2017-09-09T15:42:19.742623: step 3791, loss 0.0306696, acc 0.984375
2017-09-09T15:42:20.046880: step 3792, loss 0.0230478, acc 0.984375
2017-09-09T15:42:20.453180: step 3793, loss 0.0620501, acc 0.96875
2017-09-09T15:42:20.743187: step 3794, loss 0.0146597, acc 1
2017-09-09T15:42:21.081243: step 3795, loss 0.0350927, acc 0.984375
2017-09-09T15:42:21.352978: step 3796, loss 0.0336574, acc 0.984375
2017-09-09T15:42:21.689963: step 3797, loss 0.0190656, acc 1
2017-09-09T15:42:21.995241: step 3798, loss 0.0251138, acc 1
2017-09-09T15:42:22.314496: step 3799, loss 0.134752, acc 0.953125
2017-09-09T15:42:22.603771: step 3800, loss 0.0119561, acc 1

Evaluation:
2017-09-09T15:42:22.673901: step 3800, loss 1.97299, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3800

2017-09-09T15:42:24.711808: step 3801, loss 0.0354886, acc 0.984375
2017-09-09T15:42:24.972547: step 3802, loss 0.0445346, acc 0.984375
2017-09-09T15:42:25.287111: step 3803, loss 0.0618635, acc 0.984375
2017-09-09T15:42:25.541281: step 3804, loss 0.00766444, acc 1
2017-09-09T15:42:25.906025: step 3805, loss 0.0233478, acc 1
2017-09-09T15:42:26.224609: step 3806, loss 0.0078592, acc 1
2017-09-09T15:42:26.501213: step 3807, loss 0.0571707, acc 0.984375
2017-09-09T15:42:26.829798: step 3808, loss 0.00391117, acc 1
2017-09-09T15:42:27.105167: step 3809, loss 0.0671597, acc 0.96875
2017-09-09T15:42:27.490684: step 3810, loss 0.051096, acc 0.984375
2017-09-09T15:42:27.797287: step 3811, loss 0.0397772, acc 1
2017-09-09T15:42:28.108536: step 3812, loss 0.0500001, acc 0.984375
2017-09-09T15:42:28.418247: step 3813, loss 0.0325199, acc 1
2017-09-09T15:42:28.731560: step 3814, loss 0.011387, acc 1
2017-09-09T15:42:29.066866: step 3815, loss 0.0760159, acc 0.953125
2017-09-09T15:42:29.386068: step 3816, loss 0.0454811, acc 0.984375
2017-09-09T15:42:29.682197: step 3817, loss 0.0499229, acc 0.984375
2017-09-09T15:42:29.965130: step 3818, loss 0.0455918, acc 1
2017-09-09T15:42:30.237061: step 3819, loss 0.0509535, acc 0.984375
2017-09-09T15:42:30.532629: step 3820, loss 0.0645147, acc 0.96875
2017-09-09T15:42:30.820984: step 3821, loss 0.00504249, acc 1
2017-09-09T15:42:31.139346: step 3822, loss 0.02614, acc 1
2017-09-09T15:42:31.417802: step 3823, loss 0.0423994, acc 0.96875
2017-09-09T15:42:31.722313: step 3824, loss 0.0340194, acc 1
2017-09-09T15:42:32.012403: step 3825, loss 0.0248606, acc 0.984375
2017-09-09T15:42:32.323257: step 3826, loss 0.00961224, acc 1
2017-09-09T15:42:32.685271: step 3827, loss 0.0406464, acc 1
2017-09-09T15:42:32.968815: step 3828, loss 0.066229, acc 0.96875
2017-09-09T15:42:33.244600: step 3829, loss 0.0248259, acc 1
2017-09-09T15:42:33.555089: step 3830, loss 0.00634573, acc 1
2017-09-09T15:42:33.823683: step 3831, loss 0.0262088, acc 0.984375
2017-09-09T15:42:34.178446: step 3832, loss 0.0637781, acc 0.96875
2017-09-09T15:42:34.487093: step 3833, loss 0.0624444, acc 0.96875
2017-09-09T15:42:34.838374: step 3834, loss 0.0410784, acc 0.96875
2017-09-09T15:42:35.131384: step 3835, loss 0.0491216, acc 0.96875
2017-09-09T15:42:35.430321: step 3836, loss 0.0137048, acc 1
2017-09-09T15:42:35.790837: step 3837, loss 0.0178733, acc 1
2017-09-09T15:42:36.082392: step 3838, loss 0.00519195, acc 1
2017-09-09T15:42:36.449773: step 3839, loss 0.054695, acc 0.984375
2017-09-09T15:42:36.760013: step 3840, loss 0.0532623, acc 0.984375
2017-09-09T15:42:37.068372: step 3841, loss 0.0294612, acc 0.984375
2017-09-09T15:42:37.393494: step 3842, loss 0.0876135, acc 0.96875
2017-09-09T15:42:37.713241: step 3843, loss 0.0956083, acc 0.96875
2017-09-09T15:42:38.141918: step 3844, loss 0.0283947, acc 0.984375
2017-09-09T15:42:38.443711: step 3845, loss 0.0842761, acc 0.96875
2017-09-09T15:42:38.784135: step 3846, loss 0.0165082, acc 1
2017-09-09T15:42:39.096733: step 3847, loss 0.0188721, acc 1
2017-09-09T15:42:39.399945: step 3848, loss 0.0171681, acc 0.984375
2017-09-09T15:42:39.669705: step 3849, loss 0.0270772, acc 1
2017-09-09T15:42:40.016546: step 3850, loss 0.0392898, acc 0.984375

Evaluation:
2017-09-09T15:42:40.114751: step 3850, loss 2.09723, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3850

2017-09-09T15:42:43.088032: step 3851, loss 0.0143511, acc 1
2017-09-09T15:42:43.445834: step 3852, loss 0.0114635, acc 1
2017-09-09T15:42:43.746504: step 3853, loss 0.0254986, acc 1
2017-09-09T15:42:44.059087: step 3854, loss 0.0659115, acc 0.96875
2017-09-09T15:42:44.332211: step 3855, loss 0.0407021, acc 0.984375
2017-09-09T15:42:44.680419: step 3856, loss 0.0185339, acc 0.984375
2017-09-09T15:42:45.012272: step 3857, loss 0.0397323, acc 0.984375
2017-09-09T15:42:45.308084: step 3858, loss 0.0306168, acc 1
2017-09-09T15:42:45.624261: step 3859, loss 0.0118506, acc 1
2017-09-09T15:42:45.939496: step 3860, loss 0.0108745, acc 1
2017-09-09T15:42:46.310410: step 3861, loss 0.0455658, acc 0.96875
2017-09-09T15:42:46.588903: step 3862, loss 0.0412083, acc 0.96875
2017-09-09T15:42:46.862261: step 3863, loss 0.00298669, acc 1
2017-09-09T15:42:47.187276: step 3864, loss 0.0326001, acc 0.984375
2017-09-09T15:42:47.500279: step 3865, loss 0.0341841, acc 0.984375
2017-09-09T15:42:47.828014: step 3866, loss 0.0104681, acc 1
2017-09-09T15:42:48.158267: step 3867, loss 0.041389, acc 0.96875
2017-09-09T15:42:48.496134: step 3868, loss 0.0412014, acc 0.96875
2017-09-09T15:42:48.849861: step 3869, loss 0.0129523, acc 1
2017-09-09T15:42:49.161055: step 3870, loss 0.0689007, acc 0.96875
2017-09-09T15:42:49.461762: step 3871, loss 0.0140982, acc 1
2017-09-09T15:42:49.763475: step 3872, loss 0.00240894, acc 1
2017-09-09T15:42:50.082806: step 3873, loss 0.0123675, acc 1
2017-09-09T15:42:50.396663: step 3874, loss 0.0027026, acc 1
2017-09-09T15:42:50.674037: step 3875, loss 0.0585927, acc 0.953125
2017-09-09T15:42:51.011773: step 3876, loss 0.0152571, acc 1
2017-09-09T15:42:51.401154: step 3877, loss 0.0101885, acc 1
2017-09-09T15:42:51.682518: step 3878, loss 0.0209055, acc 1
2017-09-09T15:42:51.970808: step 3879, loss 0.0515162, acc 0.96875
2017-09-09T15:42:52.293880: step 3880, loss 0.0395946, acc 0.96875
2017-09-09T15:42:52.560511: step 3881, loss 0.0253976, acc 1
2017-09-09T15:42:52.875567: step 3882, loss 0.0184007, acc 0.984375
2017-09-09T15:42:53.210074: step 3883, loss 0.0359962, acc 0.96875
2017-09-09T15:42:53.522195: step 3884, loss 0.055704, acc 0.984375
2017-09-09T15:42:53.831864: step 3885, loss 0.0121062, acc 1
2017-09-09T15:42:54.233260: step 3886, loss 0.0113662, acc 1
2017-09-09T15:42:54.531104: step 3887, loss 0.0118256, acc 1
2017-09-09T15:42:54.877935: step 3888, loss 0.0216942, acc 1
2017-09-09T15:42:55.211261: step 3889, loss 0.0189331, acc 0.984375
2017-09-09T15:42:55.505593: step 3890, loss 0.00154647, acc 1
2017-09-09T15:42:55.853312: step 3891, loss 0.0222208, acc 0.984375
2017-09-09T15:42:56.125063: step 3892, loss 0.0403616, acc 0.984375
2017-09-09T15:42:56.489537: step 3893, loss 0.0395591, acc 0.984375
2017-09-09T15:42:56.786730: step 3894, loss 0.0110227, acc 1
2017-09-09T15:42:57.084378: step 3895, loss 0.0234369, acc 0.984375
2017-09-09T15:42:57.379168: step 3896, loss 0.042532, acc 0.984375
2017-09-09T15:42:57.677597: step 3897, loss 0.0416152, acc 0.984375
2017-09-09T15:42:58.028872: step 3898, loss 0.0119904, acc 1
2017-09-09T15:42:58.327983: step 3899, loss 0.0388601, acc 0.984375
2017-09-09T15:42:58.669226: step 3900, loss 0.0724261, acc 0.96875

Evaluation:
2017-09-09T15:42:58.725807: step 3900, loss 2.08252, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3900

2017-09-09T15:43:00.937900: step 3901, loss 0.00591071, acc 1
2017-09-09T15:43:01.278406: step 3902, loss 0.0187607, acc 0.984375
2017-09-09T15:43:01.585312: step 3903, loss 0.0114014, acc 1
2017-09-09T15:43:01.858753: step 3904, loss 0.0490591, acc 0.96875
2017-09-09T15:43:02.281134: step 3905, loss 0.069518, acc 0.96875
2017-09-09T15:43:02.586326: step 3906, loss 0.0256177, acc 0.984375
2017-09-09T15:43:02.909842: step 3907, loss 0.11067, acc 0.953125
2017-09-09T15:43:03.258166: step 3908, loss 0.0515984, acc 0.984375
2017-09-09T15:43:03.532810: step 3909, loss 0.0338464, acc 0.984375
2017-09-09T15:43:03.882430: step 3910, loss 0.0341253, acc 1
2017-09-09T15:43:04.194254: step 3911, loss 0.0339302, acc 0.984375
2017-09-09T15:43:04.521189: step 3912, loss 0.0511637, acc 0.984375
2017-09-09T15:43:04.869824: step 3913, loss 0.0484689, acc 0.984375
2017-09-09T15:43:05.187339: step 3914, loss 0.0148034, acc 1
2017-09-09T15:43:05.586825: step 3915, loss 0.00989957, acc 1
2017-09-09T15:43:05.881637: step 3916, loss 0.0397495, acc 0.984375
2017-09-09T15:43:06.226400: step 3917, loss 0.0331691, acc 1
2017-09-09T15:43:06.542950: step 3918, loss 0.014541, acc 1
2017-09-09T15:43:06.876413: step 3919, loss 0.0557329, acc 0.96875
2017-09-09T15:43:07.195735: step 3920, loss 0.018523, acc 1
2017-09-09T15:43:07.473762: step 3921, loss 0.0255877, acc 0.984375
2017-09-09T15:43:07.727109: step 3922, loss 0.056299, acc 0.984375
2017-09-09T15:43:07.995404: step 3923, loss 0.0248965, acc 0.984375
2017-09-09T15:43:08.316197: step 3924, loss 0.0480499, acc 0.984375
2017-09-09T15:43:08.572787: step 3925, loss 0.0541953, acc 0.96875
2017-09-09T15:43:08.855089: step 3926, loss 0.0418275, acc 0.96875
2017-09-09T15:43:09.156130: step 3927, loss 0.0243201, acc 1
2017-09-09T15:43:09.482987: step 3928, loss 0.0644125, acc 0.96875
2017-09-09T15:43:09.789311: step 3929, loss 0.0245844, acc 1
2017-09-09T15:43:10.076220: step 3930, loss 0.0394786, acc 0.984375
2017-09-09T15:43:10.399106: step 3931, loss 0.0708169, acc 0.96875
2017-09-09T15:43:10.707017: step 3932, loss 0.0166448, acc 1
2017-09-09T15:43:11.096893: step 3933, loss 0.0355429, acc 1
2017-09-09T15:43:11.426282: step 3934, loss 0.00592748, acc 1
2017-09-09T15:43:11.736202: step 3935, loss 0.0181593, acc 1
2017-09-09T15:43:12.135657: step 3936, loss 0.0412685, acc 1
2017-09-09T15:43:12.424509: step 3937, loss 0.040401, acc 0.984375
2017-09-09T15:43:12.794985: step 3938, loss 0.0227501, acc 0.984375
2017-09-09T15:43:13.423130: step 3939, loss 0.034543, acc 0.984375
2017-09-09T15:43:13.732231: step 3940, loss 0.0122668, acc 1
2017-09-09T15:43:14.031259: step 3941, loss 0.0117083, acc 1
2017-09-09T15:43:14.358959: step 3942, loss 0.0506523, acc 0.953125
2017-09-09T15:43:14.652610: step 3943, loss 0.0543967, acc 0.984375
2017-09-09T15:43:14.999616: step 3944, loss 0.053107, acc 0.96875
2017-09-09T15:43:15.284826: step 3945, loss 0.0465786, acc 0.984375
2017-09-09T15:43:15.647351: step 3946, loss 0.00436614, acc 1
2017-09-09T15:43:15.910428: step 3947, loss 0.0574196, acc 0.984375
2017-09-09T15:43:16.222212: step 3948, loss 0.056654, acc 0.984375
2017-09-09T15:43:16.503237: step 3949, loss 0.0125307, acc 1
2017-09-09T15:43:16.815113: step 3950, loss 0.0266212, acc 1

Evaluation:
2017-09-09T15:43:16.939310: step 3950, loss 2.24538, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-3950

2017-09-09T15:43:19.808747: step 3951, loss 0.034979, acc 0.984375
2017-09-09T15:43:20.107198: step 3952, loss 0.012868, acc 1
2017-09-09T15:43:20.385774: step 3953, loss 0.0585275, acc 0.96875
2017-09-09T15:43:20.665877: step 3954, loss 0.0128473, acc 1
2017-09-09T15:43:20.953855: step 3955, loss 0.0343233, acc 0.96875
2017-09-09T15:43:21.266085: step 3956, loss 0.0232081, acc 1
2017-09-09T15:43:21.569985: step 3957, loss 0.0416852, acc 0.96875
2017-09-09T15:43:21.891724: step 3958, loss 0.0408091, acc 0.984375
2017-09-09T15:43:22.173262: step 3959, loss 0.00445651, acc 1
2017-09-09T15:43:22.550817: step 3960, loss 0.00523178, acc 1
2017-09-09T15:43:22.868839: step 3961, loss 0.0561241, acc 0.984375
2017-09-09T15:43:23.178184: step 3962, loss 0.0841269, acc 0.96875
2017-09-09T15:43:23.508316: step 3963, loss 0.0235733, acc 1
2017-09-09T15:43:23.818985: step 3964, loss 0.0103504, acc 1
2017-09-09T15:43:24.170545: step 3965, loss 0.0385396, acc 1
2017-09-09T15:43:24.462990: step 3966, loss 0.018403, acc 1
2017-09-09T15:43:24.789381: step 3967, loss 0.00530217, acc 1
2017-09-09T15:43:25.174093: step 3968, loss 0.00936409, acc 1
2017-09-09T15:43:25.462670: step 3969, loss 0.00462478, acc 1
2017-09-09T15:43:25.800623: step 3970, loss 0.00500732, acc 1
2017-09-09T15:43:26.113801: step 3971, loss 0.0147646, acc 1
2017-09-09T15:43:26.415369: step 3972, loss 0.0579096, acc 0.96875
2017-09-09T15:43:26.731817: step 3973, loss 0.0207852, acc 0.984375
2017-09-09T15:43:27.020684: step 3974, loss 0.0158814, acc 1
2017-09-09T15:43:27.335796: step 3975, loss 0.0118974, acc 1
2017-09-09T15:43:27.631982: step 3976, loss 0.0388383, acc 0.984375
2017-09-09T15:43:27.936649: step 3977, loss 0.0669222, acc 0.984375
2017-09-09T15:43:28.306298: step 3978, loss 0.0408062, acc 0.984375
2017-09-09T15:43:28.640009: step 3979, loss 0.0157433, acc 1
2017-09-09T15:43:28.951167: step 3980, loss 0.00754258, acc 1
2017-09-09T15:43:29.234027: step 3981, loss 0.0554328, acc 0.984375
2017-09-09T15:43:29.555367: step 3982, loss 0.0172455, acc 1
2017-09-09T15:43:29.878937: step 3983, loss 0.0335778, acc 1
2017-09-09T15:43:30.160693: step 3984, loss 0.0604507, acc 0.984375
2017-09-09T15:43:30.495157: step 3985, loss 0.00683508, acc 1
2017-09-09T15:43:30.822164: step 3986, loss 0.0387021, acc 0.984375
2017-09-09T15:43:31.120708: step 3987, loss 0.116172, acc 0.9375
2017-09-09T15:43:31.449747: step 3988, loss 0.0260033, acc 0.984375
2017-09-09T15:43:31.742066: step 3989, loss 0.0172593, acc 1
2017-09-09T15:43:32.101156: step 3990, loss 0.030051, acc 1
2017-09-09T15:43:32.439630: step 3991, loss 0.0359321, acc 0.984375
2017-09-09T15:43:32.736128: step 3992, loss 0.0206599, acc 0.984375
2017-09-09T15:43:33.108571: step 3993, loss 0.0289979, acc 1
2017-09-09T15:43:33.409913: step 3994, loss 0.0172884, acc 1
2017-09-09T15:43:33.749071: step 3995, loss 0.00242169, acc 1
2017-09-09T15:43:34.052497: step 3996, loss 0.0227543, acc 0.984375
2017-09-09T15:43:34.337990: step 3997, loss 0.0381368, acc 0.984375
2017-09-09T15:43:34.652871: step 3998, loss 0.0318355, acc 0.984375
2017-09-09T15:43:34.941197: step 3999, loss 0.0439392, acc 0.984375
2017-09-09T15:43:35.287920: step 4000, loss 0.0374532, acc 0.984375

Evaluation:
2017-09-09T15:43:35.378426: step 4000, loss 2.50551, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4000

2017-09-09T15:43:38.575565: step 4001, loss 0.0297549, acc 0.984375
2017-09-09T15:43:38.870746: step 4002, loss 0.0221506, acc 1
2017-09-09T15:43:39.222115: step 4003, loss 0.00363587, acc 1
2017-09-09T15:43:39.489266: step 4004, loss 0.0269142, acc 0.984375
2017-09-09T15:43:39.805458: step 4005, loss 0.0287175, acc 0.984375
2017-09-09T15:43:40.139487: step 4006, loss 0.0638685, acc 0.96875
2017-09-09T15:43:40.460570: step 4007, loss 0.0582598, acc 0.96875
2017-09-09T15:43:40.803940: step 4008, loss 0.0348226, acc 0.984375
2017-09-09T15:43:41.092294: step 4009, loss 0.00435399, acc 1
2017-09-09T15:43:41.436057: step 4010, loss 0.00654692, acc 1
2017-09-09T15:43:41.761655: step 4011, loss 0.0168124, acc 0.984375
2017-09-09T15:43:42.040459: step 4012, loss 0.0719732, acc 0.96875
2017-09-09T15:43:42.334038: step 4013, loss 0.0119992, acc 1
2017-09-09T15:43:42.620630: step 4014, loss 0.00675663, acc 1
2017-09-09T15:43:42.899425: step 4015, loss 0.0214414, acc 0.984375
2017-09-09T15:43:43.177608: step 4016, loss 0.0286589, acc 0.984375
2017-09-09T15:43:43.477143: step 4017, loss 0.0450753, acc 0.984375
2017-09-09T15:43:43.765760: step 4018, loss 0.0102649, acc 1
2017-09-09T15:43:44.104521: step 4019, loss 0.0194435, acc 0.984375
2017-09-09T15:43:44.393315: step 4020, loss 0.0381729, acc 0.96875
2017-09-09T15:43:44.667681: step 4021, loss 0.0212528, acc 1
2017-09-09T15:43:44.960703: step 4022, loss 0.0428478, acc 0.96875
2017-09-09T15:43:45.244251: step 4023, loss 0.0450241, acc 0.984375
2017-09-09T15:43:45.565088: step 4024, loss 0.00592121, acc 1
2017-09-09T15:43:45.850405: step 4025, loss 0.0544345, acc 0.96875
2017-09-09T15:43:46.208893: step 4026, loss 0.0182728, acc 0.984375
2017-09-09T15:43:46.506929: step 4027, loss 0.0507417, acc 0.96875
2017-09-09T15:43:46.824340: step 4028, loss 0.0241287, acc 1
2017-09-09T15:43:47.097135: step 4029, loss 0.00800439, acc 1
2017-09-09T15:43:47.403920: step 4030, loss 0.045712, acc 0.96875
2017-09-09T15:43:47.745553: step 4031, loss 0.0162028, acc 1
2017-09-09T15:43:48.053263: step 4032, loss 0.0559868, acc 0.984375
2017-09-09T15:43:48.377357: step 4033, loss 0.0156648, acc 1
2017-09-09T15:43:48.650583: step 4034, loss 0.0287254, acc 0.984375
2017-09-09T15:43:48.932919: step 4035, loss 0.0155337, acc 1
2017-09-09T15:43:49.379514: step 4036, loss 0.00514604, acc 1
2017-09-09T15:43:49.651013: step 4037, loss 0.0160526, acc 1
2017-09-09T15:43:49.924897: step 4038, loss 0.0279845, acc 0.984375
2017-09-09T15:43:50.215977: step 4039, loss 0.0634729, acc 0.984375
2017-09-09T15:43:50.517001: step 4040, loss 0.0312074, acc 0.984375
2017-09-09T15:43:50.778395: step 4041, loss 0.0145135, acc 1
2017-09-09T15:43:51.086008: step 4042, loss 0.0215581, acc 1
2017-09-09T15:43:51.466771: step 4043, loss 0.0402141, acc 0.96875
2017-09-09T15:43:51.797605: step 4044, loss 0.0539233, acc 0.953125
2017-09-09T15:43:52.108215: step 4045, loss 0.0718099, acc 0.96875
2017-09-09T15:43:52.405622: step 4046, loss 0.0191326, acc 1
2017-09-09T15:43:52.689166: step 4047, loss 0.0372085, acc 0.984375
2017-09-09T15:43:53.025708: step 4048, loss 0.0445194, acc 0.984375
2017-09-09T15:43:53.331348: step 4049, loss 0.0203761, acc 1
2017-09-09T15:43:53.707080: step 4050, loss 0.0104379, acc 1

Evaluation:
2017-09-09T15:43:53.765622: step 4050, loss 1.99448, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4050

2017-09-09T15:43:56.105756: step 4051, loss 0.0279265, acc 0.984375
2017-09-09T15:43:56.421378: step 4052, loss 0.0338412, acc 0.96875
2017-09-09T15:43:56.747318: step 4053, loss 0.0236922, acc 1
2017-09-09T15:43:57.066477: step 4054, loss 0.0396813, acc 0.96875
2017-09-09T15:43:57.445890: step 4055, loss 0.0378309, acc 0.984375
2017-09-09T15:43:57.752161: step 4056, loss 0.0613903, acc 0.953125
2017-09-09T15:43:58.059118: step 4057, loss 0.11591, acc 0.921875
2017-09-09T15:43:58.439311: step 4058, loss 0.0135634, acc 1
2017-09-09T15:43:58.731282: step 4059, loss 0.00904525, acc 1
2017-09-09T15:43:59.085807: step 4060, loss 0.0137046, acc 1
2017-09-09T15:43:59.386065: step 4061, loss 0.0183289, acc 0.984375
2017-09-09T15:43:59.718385: step 4062, loss 0.00399999, acc 1
2017-09-09T15:44:00.056717: step 4063, loss 0.0134931, acc 1
2017-09-09T15:44:00.370995: step 4064, loss 0.0709573, acc 0.96875
2017-09-09T15:44:00.711800: step 4065, loss 0.0356608, acc 1
2017-09-09T15:44:00.984076: step 4066, loss 0.0730939, acc 0.953125
2017-09-09T15:44:01.286520: step 4067, loss 0.0395065, acc 0.984375
2017-09-09T15:44:01.603042: step 4068, loss 0.0240548, acc 1
2017-09-09T15:44:01.866260: step 4069, loss 0.0400051, acc 0.96875
2017-09-09T15:44:02.196941: step 4070, loss 0.00994746, acc 1
2017-09-09T15:44:02.501983: step 4071, loss 0.00920506, acc 1
2017-09-09T15:44:02.773556: step 4072, loss 0.0346064, acc 0.984375
2017-09-09T15:44:03.077236: step 4073, loss 0.0335591, acc 1
2017-09-09T15:44:03.358529: step 4074, loss 0.0134714, acc 1
2017-09-09T15:44:03.676623: step 4075, loss 0.0808261, acc 0.953125
2017-09-09T15:44:03.948568: step 4076, loss 0.0227774, acc 0.984375
2017-09-09T15:44:04.335162: step 4077, loss 0.0616697, acc 0.96875
2017-09-09T15:44:04.654875: step 4078, loss 0.0456965, acc 0.96875
2017-09-09T15:44:04.945171: step 4079, loss 0.0359559, acc 0.984375
2017-09-09T15:44:05.251797: step 4080, loss 0.0637441, acc 0.953125
2017-09-09T15:44:05.565173: step 4081, loss 0.0214874, acc 0.984375
2017-09-09T15:44:05.825510: step 4082, loss 0.0537001, acc 0.953125
2017-09-09T15:44:06.152478: step 4083, loss 0.0285287, acc 1
2017-09-09T15:44:06.510109: step 4084, loss 0.0141857, acc 1
2017-09-09T15:44:06.828992: step 4085, loss 0.0364468, acc 0.984375
2017-09-09T15:44:07.112585: step 4086, loss 0.0245157, acc 1
2017-09-09T15:44:07.492368: step 4087, loss 0.0632392, acc 0.984375
2017-09-09T15:44:07.815455: step 4088, loss 0.0106447, acc 1
2017-09-09T15:44:08.183214: step 4089, loss 0.018986, acc 1
2017-09-09T15:44:08.516022: step 4090, loss 0.0514861, acc 0.96875
2017-09-09T15:44:08.854429: step 4091, loss 0.0138509, acc 1
2017-09-09T15:44:09.209607: step 4092, loss 0.0209003, acc 1
2017-09-09T15:44:09.505318: step 4093, loss 0.0437368, acc 0.984375
2017-09-09T15:44:09.818332: step 4094, loss 0.00604877, acc 1
2017-09-09T15:44:10.150555: step 4095, loss 0.0686524, acc 0.96875
2017-09-09T15:44:10.503170: step 4096, loss 0.0135651, acc 0.984375
2017-09-09T15:44:10.812460: step 4097, loss 0.0176971, acc 0.984375
2017-09-09T15:44:11.116078: step 4098, loss 0.0174461, acc 1
2017-09-09T15:44:11.455090: step 4099, loss 0.102588, acc 0.96875
2017-09-09T15:44:11.741144: step 4100, loss 0.0232563, acc 0.984375

Evaluation:
2017-09-09T15:44:11.829586: step 4100, loss 2.41021, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4100

2017-09-09T15:44:14.864912: step 4101, loss 0.0243885, acc 1
2017-09-09T15:44:15.228720: step 4102, loss 0.0260682, acc 0.984375
2017-09-09T15:44:15.503816: step 4103, loss 0.0465004, acc 0.984375
2017-09-09T15:44:15.833860: step 4104, loss 0.0405363, acc 0.96875
2017-09-09T15:44:16.124346: step 4105, loss 0.040888, acc 0.984375
2017-09-09T15:44:16.441627: step 4106, loss 0.00216459, acc 1
2017-09-09T15:44:16.781738: step 4107, loss 0.0286227, acc 1
2017-09-09T15:44:17.067293: step 4108, loss 0.069757, acc 0.96875
2017-09-09T15:44:17.461717: step 4109, loss 0.0110409, acc 1
2017-09-09T15:44:17.746727: step 4110, loss 0.0357934, acc 0.96875
2017-09-09T15:44:18.045658: step 4111, loss 0.102896, acc 0.96875
2017-09-09T15:44:18.364007: step 4112, loss 0.0801841, acc 0.953125
2017-09-09T15:44:18.650835: step 4113, loss 0.0133801, acc 0.984375
2017-09-09T15:44:18.932532: step 4114, loss 0.084466, acc 0.984375
2017-09-09T15:44:19.218890: step 4115, loss 0.0636922, acc 0.96875
2017-09-09T15:44:19.517222: step 4116, loss 0.0245645, acc 0.980392
2017-09-09T15:44:19.846210: step 4117, loss 0.0149417, acc 1
2017-09-09T15:44:20.152349: step 4118, loss 0.0501736, acc 0.96875
2017-09-09T15:44:20.456010: step 4119, loss 0.0470696, acc 0.96875
2017-09-09T15:44:20.747097: step 4120, loss 0.00398717, acc 1
2017-09-09T15:44:21.090883: step 4121, loss 0.0423726, acc 0.984375
2017-09-09T15:44:21.398628: step 4122, loss 0.0145061, acc 1
2017-09-09T15:44:21.722577: step 4123, loss 0.0761383, acc 0.953125
2017-09-09T15:44:22.061710: step 4124, loss 0.0342228, acc 0.984375
2017-09-09T15:44:22.375822: step 4125, loss 0.0667994, acc 0.96875
2017-09-09T15:44:22.679400: step 4126, loss 0.036682, acc 0.984375
2017-09-09T15:44:23.029721: step 4127, loss 0.0282771, acc 0.984375
2017-09-09T15:44:23.338477: step 4128, loss 0.0506846, acc 0.984375
2017-09-09T15:44:23.740137: step 4129, loss 0.00719402, acc 1
2017-09-09T15:44:24.061984: step 4130, loss 0.0364483, acc 0.984375
2017-09-09T15:44:24.392461: step 4131, loss 0.0613203, acc 0.984375
2017-09-09T15:44:24.671402: step 4132, loss 0.029968, acc 1
2017-09-09T15:44:24.992488: step 4133, loss 0.0016217, acc 1
2017-09-09T15:44:25.313094: step 4134, loss 0.00409348, acc 1
2017-09-09T15:44:25.619395: step 4135, loss 0.0230629, acc 1
2017-09-09T15:44:25.966408: step 4136, loss 0.023605, acc 0.984375
2017-09-09T15:44:26.267814: step 4137, loss 0.0788659, acc 0.96875
2017-09-09T15:44:26.582309: step 4138, loss 0.00338372, acc 1
2017-09-09T15:44:26.899839: step 4139, loss 0.0163925, acc 1
2017-09-09T15:44:27.186899: step 4140, loss 0.0478199, acc 0.96875
2017-09-09T15:44:27.500503: step 4141, loss 0.14569, acc 0.96875
2017-09-09T15:44:27.784595: step 4142, loss 0.00749417, acc 1
2017-09-09T15:44:28.124046: step 4143, loss 0.0110688, acc 1
2017-09-09T15:44:28.409505: step 4144, loss 0.0388266, acc 0.984375
2017-09-09T15:44:28.717536: step 4145, loss 0.0292101, acc 0.984375
2017-09-09T15:44:29.054217: step 4146, loss 0.0317869, acc 1
2017-09-09T15:44:29.365900: step 4147, loss 0.00644764, acc 1
2017-09-09T15:44:30.215141: step 4148, loss 0.0592714, acc 0.984375
2017-09-09T15:44:30.545741: step 4149, loss 0.00749455, acc 1
2017-09-09T15:44:30.876873: step 4150, loss 0.0454972, acc 0.96875

Evaluation:
2017-09-09T15:44:30.943867: step 4150, loss 2.1064, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4150

2017-09-09T15:44:33.196679: step 4151, loss 0.0685406, acc 0.953125
2017-09-09T15:44:33.475368: step 4152, loss 0.0493588, acc 0.984375
2017-09-09T15:44:33.798277: step 4153, loss 0.0413264, acc 0.984375
2017-09-09T15:44:34.067893: step 4154, loss 0.0165054, acc 1
2017-09-09T15:44:34.344160: step 4155, loss 0.0211287, acc 0.984375
2017-09-09T15:44:34.662610: step 4156, loss 0.0781766, acc 0.96875
2017-09-09T15:44:34.925252: step 4157, loss 0.0277165, acc 1
2017-09-09T15:44:35.200985: step 4158, loss 0.0241742, acc 0.984375
2017-09-09T15:44:35.589852: step 4159, loss 0.0963845, acc 0.9375
2017-09-09T15:44:35.906148: step 4160, loss 0.0372403, acc 1
2017-09-09T15:44:36.232564: step 4161, loss 0.0188257, acc 1
2017-09-09T15:44:36.599662: step 4162, loss 0.0685172, acc 0.96875
2017-09-09T15:44:36.887249: step 4163, loss 0.0086129, acc 1
2017-09-09T15:44:37.252841: step 4164, loss 0.0632611, acc 0.984375
2017-09-09T15:44:37.538344: step 4165, loss 0.0337586, acc 1
2017-09-09T15:44:37.915725: step 4166, loss 0.0152315, acc 0.984375
2017-09-09T15:44:38.209989: step 4167, loss 0.0199754, acc 0.984375
2017-09-09T15:44:38.525743: step 4168, loss 0.0324896, acc 0.984375
2017-09-09T15:44:38.931058: step 4169, loss 0.0719602, acc 0.96875
2017-09-09T15:44:39.243134: step 4170, loss 0.0277631, acc 1
2017-09-09T15:44:39.549867: step 4171, loss 0.00485841, acc 1
2017-09-09T15:44:39.887492: step 4172, loss 0.0372231, acc 0.984375
2017-09-09T15:44:40.148093: step 4173, loss 0.025293, acc 0.984375
2017-09-09T15:44:40.518770: step 4174, loss 0.00769871, acc 1
2017-09-09T15:44:40.795473: step 4175, loss 0.0223457, acc 0.984375
2017-09-09T15:44:41.095374: step 4176, loss 0.00819809, acc 1
2017-09-09T15:44:41.408997: step 4177, loss 0.0773567, acc 0.953125
2017-09-09T15:44:41.696510: step 4178, loss 0.0237943, acc 1
2017-09-09T15:44:42.056855: step 4179, loss 0.0421527, acc 0.96875
2017-09-09T15:44:42.359731: step 4180, loss 0.0212991, acc 1
2017-09-09T15:44:42.667473: step 4181, loss 0.00352826, acc 1
2017-09-09T15:44:42.999422: step 4182, loss 0.0274498, acc 1
2017-09-09T15:44:43.296415: step 4183, loss 0.00362671, acc 1
2017-09-09T15:44:43.651241: step 4184, loss 0.0831722, acc 0.96875
2017-09-09T15:44:43.947586: step 4185, loss 0.0489844, acc 0.984375
2017-09-09T15:44:44.249475: step 4186, loss 0.0393498, acc 0.984375
2017-09-09T15:44:44.595303: step 4187, loss 0.0346584, acc 0.96875
2017-09-09T15:44:44.894419: step 4188, loss 0.028317, acc 0.984375
2017-09-09T15:44:45.291780: step 4189, loss 0.0113898, acc 1
2017-09-09T15:44:45.584958: step 4190, loss 0.0408259, acc 0.984375
2017-09-09T15:44:45.867942: step 4191, loss 0.00728074, acc 1
2017-09-09T15:44:46.219882: step 4192, loss 0.0666325, acc 0.96875
2017-09-09T15:44:46.557763: step 4193, loss 0.030673, acc 1
2017-09-09T15:44:46.879904: step 4194, loss 0.00152745, acc 1
2017-09-09T15:44:47.187976: step 4195, loss 0.013912, acc 1
2017-09-09T15:44:47.463492: step 4196, loss 0.00261196, acc 1
2017-09-09T15:44:47.762045: step 4197, loss 0.00170623, acc 1
2017-09-09T15:44:48.052129: step 4198, loss 0.0340749, acc 0.984375
2017-09-09T15:44:48.337339: step 4199, loss 0.0756196, acc 0.96875
2017-09-09T15:44:48.663556: step 4200, loss 0.0166283, acc 0.984375

Evaluation:
2017-09-09T15:44:48.750029: step 4200, loss 2.08093, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4200

2017-09-09T15:44:51.363169: step 4201, loss 0.00645584, acc 1
2017-09-09T15:44:51.689090: step 4202, loss 0.0375763, acc 0.984375
2017-09-09T15:44:51.996444: step 4203, loss 0.0620691, acc 0.984375
2017-09-09T15:44:52.285654: step 4204, loss 0.0209944, acc 0.984375
2017-09-09T15:44:52.637082: step 4205, loss 0.0090852, acc 1
2017-09-09T15:44:52.966373: step 4206, loss 0.00724452, acc 1
2017-09-09T15:44:53.308442: step 4207, loss 0.00660032, acc 1
2017-09-09T15:44:53.643750: step 4208, loss 0.0297711, acc 0.984375
2017-09-09T15:44:53.959001: step 4209, loss 0.0142442, acc 1
2017-09-09T15:44:54.297399: step 4210, loss 0.0556476, acc 0.984375
2017-09-09T15:44:54.628080: step 4211, loss 0.0114989, acc 1
2017-09-09T15:44:54.928491: step 4212, loss 0.0395875, acc 0.96875
2017-09-09T15:44:55.287354: step 4213, loss 0.0771595, acc 0.984375
2017-09-09T15:44:55.555462: step 4214, loss 0.0117798, acc 1
2017-09-09T15:44:55.875523: step 4215, loss 0.026136, acc 1
2017-09-09T15:44:56.171706: step 4216, loss 0.00785511, acc 1
2017-09-09T15:44:56.467187: step 4217, loss 0.0341826, acc 0.984375
2017-09-09T15:44:56.792020: step 4218, loss 0.00713225, acc 1
2017-09-09T15:44:57.076597: step 4219, loss 0.043309, acc 0.96875
2017-09-09T15:44:57.410459: step 4220, loss 0.0133191, acc 1
2017-09-09T15:44:57.766522: step 4221, loss 0.0278144, acc 1
2017-09-09T15:44:58.045853: step 4222, loss 0.0279338, acc 0.984375
2017-09-09T15:44:58.334044: step 4223, loss 0.0222888, acc 0.984375
2017-09-09T15:44:58.611840: step 4224, loss 0.0432852, acc 0.96875
2017-09-09T15:44:58.916650: step 4225, loss 0.0149391, acc 1
2017-09-09T15:44:59.214125: step 4226, loss 0.00722864, acc 1
2017-09-09T15:44:59.575935: step 4227, loss 0.0201897, acc 0.984375
2017-09-09T15:44:59.899507: step 4228, loss 0.0283444, acc 0.984375
2017-09-09T15:45:00.324117: step 4229, loss 0.0034437, acc 1
2017-09-09T15:45:00.633568: step 4230, loss 0.0315365, acc 0.984375
2017-09-09T15:45:00.946274: step 4231, loss 0.0455428, acc 1
2017-09-09T15:45:01.271193: step 4232, loss 0.00190093, acc 1
2017-09-09T15:45:01.612978: step 4233, loss 0.00857455, acc 1
2017-09-09T15:45:01.964133: step 4234, loss 0.0757141, acc 0.96875
2017-09-09T15:45:02.248382: step 4235, loss 0.023249, acc 0.984375
2017-09-09T15:45:02.583768: step 4236, loss 0.0207513, acc 0.984375
2017-09-09T15:45:02.885220: step 4237, loss 0.0200068, acc 1
2017-09-09T15:45:03.141261: step 4238, loss 0.0722681, acc 0.96875
2017-09-09T15:45:03.534626: step 4239, loss 0.0197192, acc 0.984375
2017-09-09T15:45:03.830444: step 4240, loss 0.0438203, acc 0.984375
2017-09-09T15:45:04.193955: step 4241, loss 0.0155624, acc 1
2017-09-09T15:45:04.530806: step 4242, loss 0.0425916, acc 0.984375
2017-09-09T15:45:04.872673: step 4243, loss 0.0102868, acc 1
2017-09-09T15:45:05.261695: step 4244, loss 0.0166229, acc 1
2017-09-09T15:45:05.547681: step 4245, loss 0.015957, acc 1
2017-09-09T15:45:05.921199: step 4246, loss 0.0028218, acc 1
2017-09-09T15:45:06.295724: step 4247, loss 0.0285363, acc 0.984375
2017-09-09T15:45:06.596835: step 4248, loss 0.0021915, acc 1
2017-09-09T15:45:06.994814: step 4249, loss 0.0346139, acc 0.984375
2017-09-09T15:45:07.368973: step 4250, loss 0.0341906, acc 0.984375

Evaluation:
2017-09-09T15:45:07.451228: step 4250, loss 2.47989, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4250

2017-09-09T15:45:11.057429: step 4251, loss 0.0397693, acc 0.984375
2017-09-09T15:45:11.347155: step 4252, loss 0.153461, acc 0.9375
2017-09-09T15:45:11.668010: step 4253, loss 0.0630149, acc 0.96875
2017-09-09T15:45:12.043653: step 4254, loss 0.0202088, acc 0.984375
2017-09-09T15:45:12.349618: step 4255, loss 0.0475718, acc 0.984375
2017-09-09T15:45:12.679103: step 4256, loss 0.0760431, acc 0.953125
2017-09-09T15:45:12.997058: step 4257, loss 0.0154593, acc 1
2017-09-09T15:45:13.282820: step 4258, loss 0.0197862, acc 0.984375
2017-09-09T15:45:13.599640: step 4259, loss 0.0510926, acc 0.984375
2017-09-09T15:45:13.903003: step 4260, loss 0.117348, acc 0.96875
2017-09-09T15:45:14.275649: step 4261, loss 0.0410498, acc 0.96875
2017-09-09T15:45:14.525005: step 4262, loss 0.0025027, acc 1
2017-09-09T15:45:14.923591: step 4263, loss 0.0218503, acc 1
2017-09-09T15:45:15.230280: step 4264, loss 0.0548143, acc 0.984375
2017-09-09T15:45:15.477599: step 4265, loss 0.0128578, acc 1
2017-09-09T15:45:15.772662: step 4266, loss 0.0455973, acc 0.984375
2017-09-09T15:45:16.019215: step 4267, loss 0.0346831, acc 1
2017-09-09T15:45:16.264351: step 4268, loss 0.0395742, acc 0.96875
2017-09-09T15:45:16.605678: step 4269, loss 0.020261, acc 0.984375
2017-09-09T15:45:16.926172: step 4270, loss 0.0669846, acc 0.96875
2017-09-09T15:45:17.221215: step 4271, loss 0.0323895, acc 0.984375
2017-09-09T15:45:17.559778: step 4272, loss 0.0292263, acc 0.984375
2017-09-09T15:45:17.905295: step 4273, loss 0.0425468, acc 0.984375
2017-09-09T15:45:18.197384: step 4274, loss 0.0267069, acc 0.984375
2017-09-09T15:45:18.508945: step 4275, loss 0.0049688, acc 1
2017-09-09T15:45:18.800472: step 4276, loss 0.0595079, acc 0.96875
2017-09-09T15:45:19.158552: step 4277, loss 0.0169244, acc 1
2017-09-09T15:45:19.531648: step 4278, loss 0.0683649, acc 0.96875
2017-09-09T15:45:19.823170: step 4279, loss 0.0266286, acc 0.984375
2017-09-09T15:45:20.208580: step 4280, loss 0.0345653, acc 1
2017-09-09T15:45:20.518237: step 4281, loss 0.0444238, acc 0.96875
2017-09-09T15:45:20.814053: step 4282, loss 0.0305538, acc 0.984375
2017-09-09T15:45:21.141423: step 4283, loss 0.0508362, acc 0.96875
2017-09-09T15:45:21.424501: step 4284, loss 0.0676237, acc 0.96875
2017-09-09T15:45:21.827208: step 4285, loss 0.0274586, acc 0.984375
2017-09-09T15:45:22.164497: step 4286, loss 0.00547026, acc 1
2017-09-09T15:45:22.455966: step 4287, loss 0.0569302, acc 0.984375
2017-09-09T15:45:22.804280: step 4288, loss 0.0482145, acc 0.96875
2017-09-09T15:45:23.088100: step 4289, loss 0.0180507, acc 1
2017-09-09T15:45:23.411614: step 4290, loss 0.0695141, acc 0.953125
2017-09-09T15:45:23.735272: step 4291, loss 0.00972441, acc 1
2017-09-09T15:45:24.021371: step 4292, loss 0.0658697, acc 0.96875
2017-09-09T15:45:24.366835: step 4293, loss 0.0269279, acc 0.984375
2017-09-09T15:45:24.640303: step 4294, loss 0.0170469, acc 0.984375
2017-09-09T15:45:25.024836: step 4295, loss 0.0160168, acc 1
2017-09-09T15:45:25.347308: step 4296, loss 0.0298545, acc 1
2017-09-09T15:45:25.663810: step 4297, loss 0.0206024, acc 0.984375
2017-09-09T15:45:25.994134: step 4298, loss 0.00928123, acc 1
2017-09-09T15:45:26.291237: step 4299, loss 0.0345126, acc 0.984375
2017-09-09T15:45:26.687128: step 4300, loss 0.0350547, acc 0.984375

Evaluation:
2017-09-09T15:45:26.760335: step 4300, loss 2.14652, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4300

2017-09-09T15:45:29.212158: step 4301, loss 0.0250835, acc 0.984375
2017-09-09T15:45:29.532801: step 4302, loss 0.0335034, acc 0.984375
2017-09-09T15:45:29.905622: step 4303, loss 0.0168563, acc 1
2017-09-09T15:45:30.219214: step 4304, loss 0.0182028, acc 1
2017-09-09T15:45:30.524361: step 4305, loss 0.0116617, acc 1
2017-09-09T15:45:30.826999: step 4306, loss 0.0468581, acc 0.984375
2017-09-09T15:45:31.157373: step 4307, loss 0.021264, acc 1
2017-09-09T15:45:31.499108: step 4308, loss 0.0270571, acc 0.984375
2017-09-09T15:45:31.783953: step 4309, loss 0.00690283, acc 1
2017-09-09T15:45:32.078913: step 4310, loss 0.0101066, acc 1
2017-09-09T15:45:32.388089: step 4311, loss 0.0260336, acc 1
2017-09-09T15:45:32.700237: step 4312, loss 0.0393016, acc 0.980392
2017-09-09T15:45:33.102921: step 4313, loss 0.0119165, acc 1
2017-09-09T15:45:33.380814: step 4314, loss 0.0157083, acc 1
2017-09-09T15:45:33.715846: step 4315, loss 0.0127194, acc 1
2017-09-09T15:45:34.065662: step 4316, loss 0.0204323, acc 1
2017-09-09T15:45:34.366835: step 4317, loss 0.0729963, acc 0.96875
2017-09-09T15:45:34.684109: step 4318, loss 0.0625076, acc 0.96875
2017-09-09T15:45:34.957670: step 4319, loss 0.00648623, acc 1
2017-09-09T15:45:35.269952: step 4320, loss 0.0197792, acc 1
2017-09-09T15:45:35.536124: step 4321, loss 0.00630683, acc 1
2017-09-09T15:45:35.905618: step 4322, loss 0.0262771, acc 0.984375
2017-09-09T15:45:36.171742: step 4323, loss 0.0346954, acc 0.984375
2017-09-09T15:45:36.453144: step 4324, loss 0.0463619, acc 0.984375
2017-09-09T15:45:36.785215: step 4325, loss 0.0223222, acc 0.984375
2017-09-09T15:45:37.099140: step 4326, loss 0.0266729, acc 0.984375
2017-09-09T15:45:37.435305: step 4327, loss 0.0329851, acc 0.984375
2017-09-09T15:45:37.736329: step 4328, loss 0.0216132, acc 1
2017-09-09T15:45:38.030561: step 4329, loss 0.155929, acc 0.9375
2017-09-09T15:45:38.364095: step 4330, loss 0.0187378, acc 1
2017-09-09T15:45:38.651493: step 4331, loss 0.030871, acc 0.984375
2017-09-09T15:45:38.951052: step 4332, loss 0.0208573, acc 1
2017-09-09T15:45:39.251105: step 4333, loss 0.0412384, acc 0.96875
2017-09-09T15:45:39.641638: step 4334, loss 0.00508638, acc 1
2017-09-09T15:45:39.941109: step 4335, loss 0.0382292, acc 0.984375
2017-09-09T15:45:40.239307: step 4336, loss 0.0177042, acc 1
2017-09-09T15:45:40.558210: step 4337, loss 0.0528769, acc 0.96875
2017-09-09T15:45:40.868637: step 4338, loss 0.033172, acc 0.984375
2017-09-09T15:45:41.227136: step 4339, loss 0.0502114, acc 0.96875
2017-09-09T15:45:41.517187: step 4340, loss 0.0029727, acc 1
2017-09-09T15:45:41.818849: step 4341, loss 0.0612054, acc 0.96875
2017-09-09T15:45:42.149128: step 4342, loss 0.0216162, acc 1
2017-09-09T15:45:42.450111: step 4343, loss 0.0459862, acc 0.984375
2017-09-09T15:45:42.847857: step 4344, loss 0.114932, acc 0.9375
2017-09-09T15:45:43.234545: step 4345, loss 0.00771747, acc 1
2017-09-09T15:45:43.518433: step 4346, loss 0.0252573, acc 0.984375
2017-09-09T15:45:43.812722: step 4347, loss 0.0227816, acc 0.984375
2017-09-09T15:45:44.121122: step 4348, loss 0.0487184, acc 0.984375
2017-09-09T15:45:44.399159: step 4349, loss 0.105551, acc 0.953125
2017-09-09T15:45:44.702050: step 4350, loss 0.0377905, acc 0.984375

Evaluation:
2017-09-09T15:45:44.761058: step 4350, loss 2.0677, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4350

2017-09-09T15:45:48.208297: step 4351, loss 0.0120021, acc 1
2017-09-09T15:45:48.506589: step 4352, loss 0.0172576, acc 1
2017-09-09T15:45:48.865641: step 4353, loss 0.0782345, acc 0.96875
2017-09-09T15:45:49.155608: step 4354, loss 0.00211285, acc 1
2017-09-09T15:45:49.465374: step 4355, loss 0.0499471, acc 0.984375
2017-09-09T15:45:49.785705: step 4356, loss 0.0284016, acc 0.984375
2017-09-09T15:45:50.115114: step 4357, loss 0.0195197, acc 1
2017-09-09T15:45:50.426996: step 4358, loss 0.043842, acc 0.984375
2017-09-09T15:45:50.726450: step 4359, loss 0.0474375, acc 0.984375
2017-09-09T15:45:51.067641: step 4360, loss 0.0341574, acc 0.96875
2017-09-09T15:45:51.395029: step 4361, loss 0.0226095, acc 1
2017-09-09T15:45:51.734578: step 4362, loss 0.0139992, acc 1
2017-09-09T15:45:52.092056: step 4363, loss 0.0221407, acc 1
2017-09-09T15:45:52.378463: step 4364, loss 0.0513706, acc 0.96875
2017-09-09T15:45:52.729480: step 4365, loss 0.00561437, acc 1
2017-09-09T15:45:52.990917: step 4366, loss 0.051113, acc 0.96875
2017-09-09T15:45:53.341672: step 4367, loss 0.0391841, acc 0.984375
2017-09-09T15:45:53.630749: step 4368, loss 0.0228446, acc 1
2017-09-09T15:45:53.931332: step 4369, loss 0.0763553, acc 0.96875
2017-09-09T15:45:54.268229: step 4370, loss 0.0431539, acc 0.984375
2017-09-09T15:45:54.538808: step 4371, loss 0.0466304, acc 0.984375
2017-09-09T15:45:54.820226: step 4372, loss 0.0171212, acc 1
2017-09-09T15:45:55.101160: step 4373, loss 0.0090173, acc 1
2017-09-09T15:45:55.403652: step 4374, loss 0.0220559, acc 0.984375
2017-09-09T15:45:55.718460: step 4375, loss 0.0114108, acc 1
2017-09-09T15:45:55.999104: step 4376, loss 0.0573364, acc 0.984375
2017-09-09T15:45:56.338314: step 4377, loss 0.0402046, acc 0.984375
2017-09-09T15:45:56.634671: step 4378, loss 0.00607186, acc 1
2017-09-09T15:45:56.986623: step 4379, loss 0.0140004, acc 1
2017-09-09T15:45:57.299621: step 4380, loss 0.0146548, acc 1
2017-09-09T15:45:57.601100: step 4381, loss 0.04028, acc 1
2017-09-09T15:45:57.927285: step 4382, loss 0.0598965, acc 0.984375
2017-09-09T15:45:58.194012: step 4383, loss 0.0287707, acc 0.984375
2017-09-09T15:45:58.566453: step 4384, loss 0.0189009, acc 1
2017-09-09T15:45:58.914767: step 4385, loss 0.0323812, acc 0.984375
2017-09-09T15:45:59.321541: step 4386, loss 0.0742304, acc 0.96875
2017-09-09T15:45:59.614894: step 4387, loss 0.017158, acc 1
2017-09-09T15:45:59.926857: step 4388, loss 0.0748895, acc 0.984375
2017-09-09T15:46:00.265102: step 4389, loss 0.00185924, acc 1
2017-09-09T15:46:00.548336: step 4390, loss 0.0616846, acc 0.96875
2017-09-09T15:46:00.893924: step 4391, loss 0.0169361, acc 1
2017-09-09T15:46:01.220099: step 4392, loss 0.00574927, acc 1
2017-09-09T15:46:01.508554: step 4393, loss 0.0152738, acc 1
2017-09-09T15:46:01.801762: step 4394, loss 0.0388132, acc 0.984375
2017-09-09T15:46:02.106215: step 4395, loss 0.0237923, acc 0.984375
2017-09-09T15:46:02.459702: step 4396, loss 0.00510684, acc 1
2017-09-09T15:46:02.776049: step 4397, loss 0.0316728, acc 0.984375
2017-09-09T15:46:03.080499: step 4398, loss 0.0185808, acc 1
2017-09-09T15:46:03.413080: step 4399, loss 0.0268611, acc 0.984375
2017-09-09T15:46:03.713813: step 4400, loss 0.00392801, acc 1

Evaluation:
2017-09-09T15:46:03.819809: step 4400, loss 1.82082, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4400

2017-09-09T15:46:06.139497: step 4401, loss 0.0154562, acc 1
2017-09-09T15:46:06.391139: step 4402, loss 0.0668871, acc 0.96875
2017-09-09T15:46:06.729442: step 4403, loss 0.0113362, acc 1
2017-09-09T15:46:07.010135: step 4404, loss 0.047572, acc 0.984375
2017-09-09T15:46:07.337933: step 4405, loss 0.0505857, acc 0.96875
2017-09-09T15:46:07.692868: step 4406, loss 0.0207867, acc 0.984375
2017-09-09T15:46:07.961115: step 4407, loss 0.0156744, acc 1
2017-09-09T15:46:08.320453: step 4408, loss 0.0400137, acc 0.984375
2017-09-09T15:46:08.607737: step 4409, loss 0.0232066, acc 0.984375
2017-09-09T15:46:08.937853: step 4410, loss 0.00471578, acc 1
2017-09-09T15:46:09.255447: step 4411, loss 0.0469801, acc 0.96875
2017-09-09T15:46:09.563835: step 4412, loss 0.0559651, acc 0.96875
2017-09-09T15:46:09.882342: step 4413, loss 0.0104759, acc 1
2017-09-09T15:46:10.176269: step 4414, loss 0.00529033, acc 1
2017-09-09T15:46:10.500789: step 4415, loss 0.0296787, acc 0.984375
2017-09-09T15:46:10.782593: step 4416, loss 0.0261982, acc 1
2017-09-09T15:46:11.119213: step 4417, loss 0.0113893, acc 1
2017-09-09T15:46:11.377646: step 4418, loss 0.0268458, acc 0.984375
2017-09-09T15:46:11.867111: step 4419, loss 0.0012193, acc 1
2017-09-09T15:46:12.224605: step 4420, loss 0.00313659, acc 1
2017-09-09T15:46:12.554061: step 4421, loss 0.00058796, acc 1
2017-09-09T15:46:12.826233: step 4422, loss 0.0271681, acc 1
2017-09-09T15:46:13.144121: step 4423, loss 0.00748334, acc 1
2017-09-09T15:46:13.437800: step 4424, loss 0.0218323, acc 1
2017-09-09T15:46:13.694426: step 4425, loss 0.0431658, acc 0.96875
2017-09-09T15:46:13.958839: step 4426, loss 0.00726656, acc 1
2017-09-09T15:46:14.234231: step 4427, loss 0.0294178, acc 0.984375
2017-09-09T15:46:14.500943: step 4428, loss 0.00246945, acc 1
2017-09-09T15:46:14.837996: step 4429, loss 0.0263861, acc 1
2017-09-09T15:46:15.181619: step 4430, loss 0.0195705, acc 1
2017-09-09T15:46:15.485252: step 4431, loss 0.00118651, acc 1
2017-09-09T15:46:15.775449: step 4432, loss 0.0415257, acc 0.984375
2017-09-09T15:46:16.092008: step 4433, loss 0.00915022, acc 1
2017-09-09T15:46:16.421716: step 4434, loss 0.00351391, acc 1
2017-09-09T15:46:16.731484: step 4435, loss 0.0216179, acc 0.984375
2017-09-09T15:46:16.989362: step 4436, loss 0.0522983, acc 0.96875
2017-09-09T15:46:17.306697: step 4437, loss 0.109829, acc 0.953125
2017-09-09T15:46:17.589294: step 4438, loss 0.0876921, acc 0.953125
2017-09-09T15:46:17.923818: step 4439, loss 0.00554087, acc 1
2017-09-09T15:46:18.224758: step 4440, loss 0.0219972, acc 1
2017-09-09T15:46:18.534397: step 4441, loss 0.0206243, acc 1
2017-09-09T15:46:18.892520: step 4442, loss 0.0173483, acc 1
2017-09-09T15:46:19.195102: step 4443, loss 0.0416497, acc 0.984375
2017-09-09T15:46:19.536087: step 4444, loss 0.005249, acc 1
2017-09-09T15:46:19.841489: step 4445, loss 0.0040268, acc 1
2017-09-09T15:46:20.140008: step 4446, loss 0.0311411, acc 0.984375
2017-09-09T15:46:20.504411: step 4447, loss 0.00161687, acc 1
2017-09-09T15:46:20.806075: step 4448, loss 0.00743567, acc 1
2017-09-09T15:46:21.113981: step 4449, loss 0.0175863, acc 1
2017-09-09T15:46:21.407799: step 4450, loss 0.0376659, acc 0.984375

Evaluation:
2017-09-09T15:46:21.519577: step 4450, loss 3.20569, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4450

2017-09-09T15:46:24.188202: step 4451, loss 0.0189693, acc 1
2017-09-09T15:46:24.499492: step 4452, loss 0.0340177, acc 1
2017-09-09T15:46:24.845294: step 4453, loss 0.0169369, acc 1
2017-09-09T15:46:25.178059: step 4454, loss 0.00576799, acc 1
2017-09-09T15:46:25.461611: step 4455, loss 0.0703148, acc 0.96875
2017-09-09T15:46:25.773948: step 4456, loss 0.02849, acc 0.984375
2017-09-09T15:46:26.055330: step 4457, loss 0.0404839, acc 0.984375
2017-09-09T15:46:26.434239: step 4458, loss 0.0348112, acc 0.984375
2017-09-09T15:46:26.726264: step 4459, loss 0.0373911, acc 1
2017-09-09T15:46:27.036410: step 4460, loss 0.0401506, acc 0.984375
2017-09-09T15:46:27.470358: step 4461, loss 0.00922168, acc 1
2017-09-09T15:46:27.842328: step 4462, loss 0.0472424, acc 0.984375
2017-09-09T15:46:28.126175: step 4463, loss 0.00438453, acc 1
2017-09-09T15:46:28.444591: step 4464, loss 0.0398954, acc 0.984375
2017-09-09T15:46:28.722199: step 4465, loss 0.0571558, acc 0.96875
2017-09-09T15:46:29.013025: step 4466, loss 0.00218321, acc 1
2017-09-09T15:46:29.336825: step 4467, loss 0.00530119, acc 1
2017-09-09T15:46:29.624751: step 4468, loss 0.0906488, acc 0.96875
2017-09-09T15:46:29.899955: step 4469, loss 0.0410209, acc 0.984375
2017-09-09T15:46:30.289344: step 4470, loss 0.0712981, acc 0.984375
2017-09-09T15:46:30.562957: step 4471, loss 0.0804681, acc 0.96875
2017-09-09T15:46:30.891662: step 4472, loss 0.00671129, acc 1
2017-09-09T15:46:31.174321: step 4473, loss 0.0353349, acc 0.984375
2017-09-09T15:46:31.456363: step 4474, loss 0.0411165, acc 0.984375
2017-09-09T15:46:31.807587: step 4475, loss 0.116773, acc 0.953125
2017-09-09T15:46:32.117045: step 4476, loss 0.0154543, acc 1
2017-09-09T15:46:32.414608: step 4477, loss 0.078987, acc 0.96875
2017-09-09T15:46:32.764881: step 4478, loss 0.0372615, acc 0.984375
2017-09-09T15:46:33.069141: step 4479, loss 0.0249905, acc 0.984375
2017-09-09T15:46:33.404604: step 4480, loss 0.0301836, acc 0.984375
2017-09-09T15:46:33.692553: step 4481, loss 0.023934, acc 0.984375
2017-09-09T15:46:34.001423: step 4482, loss 0.0537558, acc 0.96875
2017-09-09T15:46:34.342746: step 4483, loss 0.0355752, acc 0.984375
2017-09-09T15:46:34.640678: step 4484, loss 0.0688312, acc 0.953125
2017-09-09T15:46:35.022146: step 4485, loss 0.00480996, acc 1
2017-09-09T15:46:35.355522: step 4486, loss 0.0234079, acc 0.984375
2017-09-09T15:46:35.665812: step 4487, loss 0.0889188, acc 0.953125
2017-09-09T15:46:36.043439: step 4488, loss 0.00543125, acc 1
2017-09-09T15:46:36.309718: step 4489, loss 0.0285818, acc 1
2017-09-09T15:46:36.646377: step 4490, loss 0.0413692, acc 0.984375
2017-09-09T15:46:36.952228: step 4491, loss 0.0273736, acc 0.984375
2017-09-09T15:46:37.235999: step 4492, loss 0.045834, acc 0.984375
2017-09-09T15:46:37.578457: step 4493, loss 0.0112482, acc 1
2017-09-09T15:46:37.881101: step 4494, loss 0.0513432, acc 0.96875
2017-09-09T15:46:38.289262: step 4495, loss 0.00492234, acc 1
2017-09-09T15:46:38.592264: step 4496, loss 0.0237877, acc 1
2017-09-09T15:46:38.882772: step 4497, loss 0.0255826, acc 1
2017-09-09T15:46:39.199713: step 4498, loss 0.0681596, acc 0.96875
2017-09-09T15:46:39.495646: step 4499, loss 0.0289492, acc 0.984375
2017-09-09T15:46:39.859268: step 4500, loss 0.0378983, acc 1

Evaluation:
2017-09-09T15:46:39.938634: step 4500, loss 2.2846, acc 0.307914

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4500

2017-09-09T15:46:43.459582: step 4501, loss 0.0565879, acc 0.984375
2017-09-09T15:46:43.748263: step 4502, loss 0.0012276, acc 1
2017-09-09T15:46:44.090886: step 4503, loss 0.041325, acc 0.984375
2017-09-09T15:46:44.392564: step 4504, loss 0.019529, acc 1
2017-09-09T15:46:44.670973: step 4505, loss 0.0316977, acc 0.984375
2017-09-09T15:46:45.026145: step 4506, loss 0.0150701, acc 1
2017-09-09T15:46:45.295606: step 4507, loss 0.0388381, acc 0.96875
2017-09-09T15:46:45.645842: step 4508, loss 0.0117239, acc 1
2017-09-09T15:46:46.006525: step 4509, loss 0.00752201, acc 1
2017-09-09T15:46:46.342033: step 4510, loss 0.0557288, acc 0.96875
2017-09-09T15:46:46.666623: step 4511, loss 0.0134457, acc 1
2017-09-09T15:46:46.949358: step 4512, loss 0.0724979, acc 0.953125
2017-09-09T15:46:47.293708: step 4513, loss 0.0415735, acc 0.984375
2017-09-09T15:46:47.583414: step 4514, loss 0.086013, acc 0.9375
2017-09-09T15:46:47.920691: step 4515, loss 0.00728366, acc 1
2017-09-09T15:46:48.201214: step 4516, loss 0.0226028, acc 1
2017-09-09T15:46:48.513956: step 4517, loss 0.0277894, acc 1
2017-09-09T15:46:48.809440: step 4518, loss 0.0495408, acc 0.984375
2017-09-09T15:46:49.098757: step 4519, loss 0.0220904, acc 0.984375
2017-09-09T15:46:49.453055: step 4520, loss 0.017217, acc 1
2017-09-09T15:46:49.724087: step 4521, loss 0.00713398, acc 1
2017-09-09T15:46:50.032395: step 4522, loss 0.0719819, acc 0.953125
2017-09-09T15:46:50.387359: step 4523, loss 0.0252901, acc 0.984375
2017-09-09T15:46:50.687793: step 4524, loss 0.0401267, acc 0.984375
2017-09-09T15:46:51.052210: step 4525, loss 0.0420184, acc 0.96875
2017-09-09T15:46:51.329733: step 4526, loss 0.0450957, acc 0.96875
2017-09-09T15:46:51.648670: step 4527, loss 0.0371568, acc 0.984375
2017-09-09T15:46:51.942681: step 4528, loss 0.0367865, acc 0.984375
2017-09-09T15:46:52.287931: step 4529, loss 0.0616371, acc 0.96875
2017-09-09T15:46:52.641089: step 4530, loss 0.0453278, acc 0.984375
2017-09-09T15:46:52.935274: step 4531, loss 0.0417202, acc 0.96875
2017-09-09T15:46:53.309233: step 4532, loss 0.0209639, acc 0.984375
2017-09-09T15:46:53.596684: step 4533, loss 0.0206753, acc 0.984375
2017-09-09T15:46:53.937234: step 4534, loss 0.00154493, acc 1
2017-09-09T15:46:54.258423: step 4535, loss 0.0671891, acc 0.953125
2017-09-09T15:46:54.530308: step 4536, loss 0.100548, acc 0.96875
2017-09-09T15:46:54.839827: step 4537, loss 0.0809014, acc 0.96875
2017-09-09T15:46:55.124929: step 4538, loss 0.0315082, acc 0.984375
2017-09-09T15:46:55.434668: step 4539, loss 0.0288183, acc 0.984375
2017-09-09T15:46:55.754805: step 4540, loss 0.0383848, acc 0.96875
2017-09-09T15:46:56.048473: step 4541, loss 0.027217, acc 1
2017-09-09T15:46:56.452067: step 4542, loss 0.0832386, acc 0.953125
2017-09-09T15:46:56.766245: step 4543, loss 0.00972341, acc 1
2017-09-09T15:46:57.086639: step 4544, loss 0.0333789, acc 0.96875
2017-09-09T15:46:57.427783: step 4545, loss 0.0335651, acc 1
2017-09-09T15:46:57.716584: step 4546, loss 0.00653192, acc 1
2017-09-09T15:46:57.992566: step 4547, loss 0.0190405, acc 0.984375
2017-09-09T15:46:58.327649: step 4548, loss 0.0561995, acc 0.96875
2017-09-09T15:46:58.661862: step 4549, loss 0.0248571, acc 1
2017-09-09T15:46:58.994485: step 4550, loss 0.00745388, acc 1

Evaluation:
2017-09-09T15:46:59.081017: step 4550, loss 2.46776, acc 0.310791

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4550

2017-09-09T15:47:01.206439: step 4551, loss 0.0474041, acc 0.96875
2017-09-09T15:47:01.544647: step 4552, loss 0.026029, acc 0.984375
2017-09-09T15:47:01.839892: step 4553, loss 0.0104668, acc 1
2017-09-09T15:47:02.137103: step 4554, loss 0.0192493, acc 1
2017-09-09T15:47:02.445105: step 4555, loss 0.0193425, acc 0.984375
2017-09-09T15:47:02.756438: step 4556, loss 0.0985281, acc 0.9375
2017-09-09T15:47:03.059131: step 4557, loss 0.016339, acc 1
2017-09-09T15:47:03.327529: step 4558, loss 0.0117407, acc 1
2017-09-09T15:47:03.663496: step 4559, loss 0.00322025, acc 1
2017-09-09T15:47:03.965114: step 4560, loss 0.0878373, acc 0.96875
2017-09-09T15:47:04.339953: step 4561, loss 0.0332485, acc 0.984375
2017-09-09T15:47:04.666789: step 4562, loss 0.0280415, acc 0.984375
2017-09-09T15:47:04.948900: step 4563, loss 0.0162179, acc 1
2017-09-09T15:47:05.327032: step 4564, loss 0.0066326, acc 1
2017-09-09T15:47:05.660139: step 4565, loss 0.0309217, acc 0.96875
2017-09-09T15:47:05.982574: step 4566, loss 0.00222582, acc 1
2017-09-09T15:47:06.323130: step 4567, loss 0.0205102, acc 0.984375
2017-09-09T15:47:06.607608: step 4568, loss 0.067387, acc 0.96875
2017-09-09T15:47:06.944889: step 4569, loss 0.0399102, acc 1
2017-09-09T15:47:07.242676: step 4570, loss 0.0608868, acc 0.984375
2017-09-09T15:47:07.526940: step 4571, loss 0.0676149, acc 0.984375
2017-09-09T15:47:07.895959: step 4572, loss 0.0206142, acc 1
2017-09-09T15:47:08.155744: step 4573, loss 0.0206763, acc 1
2017-09-09T15:47:08.473495: step 4574, loss 0.0297198, acc 0.984375
2017-09-09T15:47:08.768496: step 4575, loss 0.000965337, acc 1
2017-09-09T15:47:09.065790: step 4576, loss 0.0417142, acc 0.984375
2017-09-09T15:47:09.408575: step 4577, loss 0.0600569, acc 0.96875
2017-09-09T15:47:09.703271: step 4578, loss 0.00587319, acc 1
2017-09-09T15:47:10.119002: step 4579, loss 0.000755249, acc 1
2017-09-09T15:47:10.399320: step 4580, loss 0.0837644, acc 0.953125
2017-09-09T15:47:10.709667: step 4581, loss 0.00287785, acc 1
2017-09-09T15:47:11.043592: step 4582, loss 0.0103003, acc 1
2017-09-09T15:47:11.347227: step 4583, loss 0.0387274, acc 0.984375
2017-09-09T15:47:11.624645: step 4584, loss 0.00543061, acc 1
2017-09-09T15:47:11.924042: step 4585, loss 0.0216186, acc 1
2017-09-09T15:47:12.232465: step 4586, loss 0.0165848, acc 1
2017-09-09T15:47:12.483718: step 4587, loss 0.021728, acc 0.984375
2017-09-09T15:47:12.970388: step 4588, loss 0.0539991, acc 0.96875
2017-09-09T15:47:13.273245: step 4589, loss 0.0262121, acc 0.984375
2017-09-09T15:47:13.610765: step 4590, loss 0.0817774, acc 0.96875
2017-09-09T15:47:13.951582: step 4591, loss 0.0685613, acc 0.953125
2017-09-09T15:47:14.266973: step 4592, loss 0.0243954, acc 1
2017-09-09T15:47:14.610393: step 4593, loss 0.00734825, acc 1
2017-09-09T15:47:14.921151: step 4594, loss 0.0425313, acc 0.96875
2017-09-09T15:47:15.326033: step 4595, loss 0.00454495, acc 1
2017-09-09T15:47:15.719251: step 4596, loss 0.0440879, acc 0.984375
2017-09-09T15:47:16.028126: step 4597, loss 0.0187412, acc 1
2017-09-09T15:47:16.362641: step 4598, loss 0.00952512, acc 1
2017-09-09T15:47:16.658246: step 4599, loss 0.00417248, acc 1
2017-09-09T15:47:16.975771: step 4600, loss 0.0411021, acc 0.984375

Evaluation:
2017-09-09T15:47:17.086870: step 4600, loss 2.65196, acc 0.346763

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4600

2017-09-09T15:47:20.348510: step 4601, loss 0.0457263, acc 0.96875
2017-09-09T15:47:20.632276: step 4602, loss 0.0188648, acc 1
2017-09-09T15:47:20.976318: step 4603, loss 0.0858701, acc 0.953125
2017-09-09T15:47:21.273511: step 4604, loss 0.0114412, acc 1
2017-09-09T15:47:21.572083: step 4605, loss 0.0395632, acc 0.984375
2017-09-09T15:47:21.872239: step 4606, loss 0.0115915, acc 1
2017-09-09T15:47:22.169307: step 4607, loss 0.0283237, acc 0.984375
2017-09-09T15:47:22.499900: step 4608, loss 0.050719, acc 0.984375
2017-09-09T15:47:22.817590: step 4609, loss 0.043864, acc 0.96875
2017-09-09T15:47:23.109426: step 4610, loss 0.00928418, acc 1
2017-09-09T15:47:23.423559: step 4611, loss 0.0315912, acc 0.984375
2017-09-09T15:47:23.709071: step 4612, loss 0.148033, acc 0.921875
2017-09-09T15:47:24.041358: step 4613, loss 0.0191874, acc 1
2017-09-09T15:47:24.313280: step 4614, loss 0.0578816, acc 0.984375
2017-09-09T15:47:24.669763: step 4615, loss 0.021326, acc 0.984375
2017-09-09T15:47:24.980610: step 4616, loss 0.0158319, acc 0.984375
2017-09-09T15:47:25.289895: step 4617, loss 0.00779268, acc 1
2017-09-09T15:47:25.609697: step 4618, loss 0.0555593, acc 0.984375
2017-09-09T15:47:25.903761: step 4619, loss 0.0305085, acc 0.984375
2017-09-09T15:47:26.285105: step 4620, loss 0.0288904, acc 1
2017-09-09T15:47:26.563671: step 4621, loss 0.0126221, acc 1
2017-09-09T15:47:26.854398: step 4622, loss 0.00250626, acc 1
2017-09-09T15:47:27.202029: step 4623, loss 0.0232032, acc 0.984375
2017-09-09T15:47:27.563363: step 4624, loss 0.00470176, acc 1
2017-09-09T15:47:27.929332: step 4625, loss 0.0750368, acc 0.953125
2017-09-09T15:47:28.236538: step 4626, loss 0.0256857, acc 1
2017-09-09T15:47:28.546392: step 4627, loss 0.0178378, acc 1
2017-09-09T15:47:28.833488: step 4628, loss 0.0106293, acc 1
2017-09-09T15:47:29.196310: step 4629, loss 0.0280846, acc 0.984375
2017-09-09T15:47:29.485845: step 4630, loss 0.0296674, acc 0.984375
2017-09-09T15:47:29.791914: step 4631, loss 0.00644368, acc 1
2017-09-09T15:47:30.082942: step 4632, loss 0.0339479, acc 0.984375
2017-09-09T15:47:30.383169: step 4633, loss 0.0141268, acc 1
2017-09-09T15:47:30.716359: step 4634, loss 0.014863, acc 1
2017-09-09T15:47:30.994341: step 4635, loss 0.0196018, acc 1
2017-09-09T15:47:31.354641: step 4636, loss 0.0308614, acc 0.984375
2017-09-09T15:47:31.685093: step 4637, loss 0.0453339, acc 0.984375
2017-09-09T15:47:31.964372: step 4638, loss 0.00317035, acc 1
2017-09-09T15:47:32.268968: step 4639, loss 0.0767104, acc 0.984375
2017-09-09T15:47:32.553430: step 4640, loss 0.0334988, acc 1
2017-09-09T15:47:32.901060: step 4641, loss 0.141449, acc 0.9375
2017-09-09T15:47:33.188043: step 4642, loss 0.0164585, acc 1
2017-09-09T15:47:33.535987: step 4643, loss 0.0592939, acc 0.984375
2017-09-09T15:47:33.825896: step 4644, loss 0.0297835, acc 0.984375
2017-09-09T15:47:34.175398: step 4645, loss 0.0468365, acc 0.984375
2017-09-09T15:47:34.488494: step 4646, loss 0.00690058, acc 1
2017-09-09T15:47:34.784704: step 4647, loss 0.0673522, acc 0.984375
2017-09-09T15:47:35.098971: step 4648, loss 0.014156, acc 1
2017-09-09T15:47:35.392876: step 4649, loss 0.0165978, acc 1
2017-09-09T15:47:35.738403: step 4650, loss 0.0431531, acc 0.984375

Evaluation:
2017-09-09T15:47:35.832078: step 4650, loss 1.99023, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4650

2017-09-09T15:47:38.063061: step 4651, loss 0.0607969, acc 0.96875
2017-09-09T15:47:38.362401: step 4652, loss 0.0661762, acc 0.96875
2017-09-09T15:47:38.632764: step 4653, loss 0.000898536, acc 1
2017-09-09T15:47:38.961298: step 4654, loss 0.0575437, acc 0.96875
2017-09-09T15:47:39.344393: step 4655, loss 0.0496483, acc 0.96875
2017-09-09T15:47:39.751894: step 4656, loss 0.0753681, acc 0.921875
2017-09-09T15:47:40.078137: step 4657, loss 0.0268223, acc 0.984375
2017-09-09T15:47:40.369562: step 4658, loss 0.0108625, acc 1
2017-09-09T15:47:40.645395: step 4659, loss 0.0229902, acc 0.984375
2017-09-09T15:47:40.977253: step 4660, loss 0.0175062, acc 1
2017-09-09T15:47:41.328614: step 4661, loss 0.045288, acc 0.984375
2017-09-09T15:47:41.676234: step 4662, loss 0.0229972, acc 1
2017-09-09T15:47:41.997459: step 4663, loss 0.018033, acc 0.984375
2017-09-09T15:47:42.372131: step 4664, loss 0.026668, acc 1
2017-09-09T15:47:42.688732: step 4665, loss 0.0240326, acc 0.984375
2017-09-09T15:47:42.979608: step 4666, loss 0.00443276, acc 1
2017-09-09T15:47:43.388159: step 4667, loss 0.0600917, acc 0.953125
2017-09-09T15:47:43.672804: step 4668, loss 0.0212189, acc 0.984375
2017-09-09T15:47:44.011920: step 4669, loss 0.0144191, acc 0.984375
2017-09-09T15:47:44.396429: step 4670, loss 0.0281961, acc 0.984375
2017-09-09T15:47:44.688097: step 4671, loss 0.0429651, acc 0.984375
2017-09-09T15:47:45.008884: step 4672, loss 0.200659, acc 0.96875
2017-09-09T15:47:45.317579: step 4673, loss 0.0439158, acc 0.96875
2017-09-09T15:47:45.595885: step 4674, loss 0.0240776, acc 1
2017-09-09T15:47:45.934321: step 4675, loss 0.0407442, acc 0.96875
2017-09-09T15:47:46.214820: step 4676, loss 0.0124355, acc 1
2017-09-09T15:47:46.534466: step 4677, loss 0.0195971, acc 0.984375
2017-09-09T15:47:46.832041: step 4678, loss 0.019535, acc 1
2017-09-09T15:47:47.128195: step 4679, loss 0.00315029, acc 1
2017-09-09T15:47:47.414582: step 4680, loss 0.0552762, acc 0.984375
2017-09-09T15:47:47.728423: step 4681, loss 0.0116479, acc 1
2017-09-09T15:47:48.077043: step 4682, loss 0.047956, acc 0.96875
2017-09-09T15:47:48.349859: step 4683, loss 0.00317637, acc 1
2017-09-09T15:47:48.681469: step 4684, loss 0.0159608, acc 1
2017-09-09T15:47:48.988835: step 4685, loss 0.0980715, acc 0.96875
2017-09-09T15:47:49.308640: step 4686, loss 0.00992885, acc 1
2017-09-09T15:47:49.630274: step 4687, loss 0.00205638, acc 1
2017-09-09T15:47:49.937504: step 4688, loss 0.00981974, acc 1
2017-09-09T15:47:50.292652: step 4689, loss 0.0297443, acc 0.984375
2017-09-09T15:47:50.644636: step 4690, loss 0.0203294, acc 1
2017-09-09T15:47:50.965198: step 4691, loss 0.0807486, acc 0.96875
2017-09-09T15:47:51.328224: step 4692, loss 0.0065807, acc 1
2017-09-09T15:47:51.644152: step 4693, loss 0.060823, acc 0.96875
2017-09-09T15:47:52.013841: step 4694, loss 0.0250226, acc 0.984375
2017-09-09T15:47:52.336338: step 4695, loss 0.00560982, acc 1
2017-09-09T15:47:52.623309: step 4696, loss 0.00228065, acc 1
2017-09-09T15:47:52.987284: step 4697, loss 0.00272491, acc 1
2017-09-09T15:47:53.286330: step 4698, loss 0.0764061, acc 0.984375
2017-09-09T15:47:53.662335: step 4699, loss 0.0248288, acc 0.984375
2017-09-09T15:47:53.987570: step 4700, loss 0.04522, acc 0.96875

Evaluation:
2017-09-09T15:47:54.075744: step 4700, loss 2.4657, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4700

2017-09-09T15:47:56.929862: step 4701, loss 0.0222627, acc 1
2017-09-09T15:47:57.298829: step 4702, loss 0.0175615, acc 1
2017-09-09T15:47:57.666034: step 4703, loss 0.0193449, acc 1
2017-09-09T15:47:58.021082: step 4704, loss 0.0120786, acc 1
2017-09-09T15:47:58.352221: step 4705, loss 0.00280678, acc 1
2017-09-09T15:47:58.694727: step 4706, loss 0.0360002, acc 0.984375
2017-09-09T15:47:59.020184: step 4707, loss 0.0173691, acc 1
2017-09-09T15:47:59.325939: step 4708, loss 0.0165422, acc 1
2017-09-09T15:47:59.641103: step 4709, loss 0.0198936, acc 1
2017-09-09T15:47:59.933323: step 4710, loss 0.0177947, acc 1
2017-09-09T15:48:00.258358: step 4711, loss 0.000746348, acc 1
2017-09-09T15:48:00.541728: step 4712, loss 0.00229406, acc 1
2017-09-09T15:48:00.903534: step 4713, loss 0.00438113, acc 1
2017-09-09T15:48:01.192102: step 4714, loss 0.00604254, acc 1
2017-09-09T15:48:01.515107: step 4715, loss 0.0157783, acc 0.984375
2017-09-09T15:48:01.813601: step 4716, loss 0.0060282, acc 1
2017-09-09T15:48:02.082591: step 4717, loss 0.00195298, acc 1
2017-09-09T15:48:02.385586: step 4718, loss 0.0347125, acc 0.984375
2017-09-09T15:48:02.671813: step 4719, loss 0.00468666, acc 1
2017-09-09T15:48:02.989964: step 4720, loss 0.00399275, acc 1
2017-09-09T15:48:03.299252: step 4721, loss 0.0408916, acc 0.984375
2017-09-09T15:48:03.567218: step 4722, loss 0.0294635, acc 0.984375
2017-09-09T15:48:03.945427: step 4723, loss 0.0401347, acc 0.984375
2017-09-09T15:48:04.289127: step 4724, loss 0.0309507, acc 0.984375
2017-09-09T15:48:04.575650: step 4725, loss 0.00370734, acc 1
2017-09-09T15:48:04.881401: step 4726, loss 0.0128998, acc 1
2017-09-09T15:48:05.181368: step 4727, loss 0.0491071, acc 0.96875
2017-09-09T15:48:05.486877: step 4728, loss 0.0386649, acc 0.984375
2017-09-09T15:48:05.770466: step 4729, loss 0.0445672, acc 0.984375
2017-09-09T15:48:06.131011: step 4730, loss 0.0133331, acc 1
2017-09-09T15:48:06.427228: step 4731, loss 0.00507612, acc 1
2017-09-09T15:48:06.739573: step 4732, loss 0.0091983, acc 1
2017-09-09T15:48:07.056005: step 4733, loss 0.0125228, acc 1
2017-09-09T15:48:07.349220: step 4734, loss 0.0007704, acc 1
2017-09-09T15:48:07.746124: step 4735, loss 0.0250419, acc 0.984375
2017-09-09T15:48:08.147181: step 4736, loss 0.0123138, acc 1
2017-09-09T15:48:08.472081: step 4737, loss 0.00813426, acc 1
2017-09-09T15:48:08.795367: step 4738, loss 0.0210542, acc 1
2017-09-09T15:48:09.150277: step 4739, loss 0.00016568, acc 1
2017-09-09T15:48:09.448412: step 4740, loss 0.0163853, acc 0.984375
2017-09-09T15:48:09.748251: step 4741, loss 0.051725, acc 0.984375
2017-09-09T15:48:10.053286: step 4742, loss 0.0386072, acc 0.984375
2017-09-09T15:48:10.350731: step 4743, loss 0.0370215, acc 0.984375
2017-09-09T15:48:10.645646: step 4744, loss 0.0286689, acc 1
2017-09-09T15:48:10.979383: step 4745, loss 0.0229112, acc 1
2017-09-09T15:48:11.313306: step 4746, loss 0.0603675, acc 0.9375
2017-09-09T15:48:11.673030: step 4747, loss 0.0102038, acc 1
2017-09-09T15:48:11.966634: step 4748, loss 0.0019108, acc 1
2017-09-09T15:48:12.319464: step 4749, loss 0.0208057, acc 0.984375
2017-09-09T15:48:12.662079: step 4750, loss 0.0145934, acc 1

Evaluation:
2017-09-09T15:48:12.770312: step 4750, loss 1.83595, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4750

2017-09-09T15:48:16.094311: step 4751, loss 0.0607849, acc 0.96875
2017-09-09T15:48:16.411160: step 4752, loss 0.0527185, acc 0.96875
2017-09-09T15:48:16.697860: step 4753, loss 0.0109897, acc 1
2017-09-09T15:48:17.051502: step 4754, loss 0.0325163, acc 1
2017-09-09T15:48:17.324850: step 4755, loss 0.0311579, acc 0.984375
2017-09-09T15:48:17.659659: step 4756, loss 0.0412864, acc 0.984375
2017-09-09T15:48:18.010515: step 4757, loss 0.0996212, acc 0.953125
2017-09-09T15:48:18.297253: step 4758, loss 0.0390824, acc 0.984375
2017-09-09T15:48:18.705502: step 4759, loss 0.0441838, acc 0.984375
2017-09-09T15:48:19.018578: step 4760, loss 0.0584073, acc 0.984375
2017-09-09T15:48:19.321622: step 4761, loss 0.00794738, acc 1
2017-09-09T15:48:19.632712: step 4762, loss 0.0378245, acc 0.96875
2017-09-09T15:48:19.902611: step 4763, loss 0.0500994, acc 0.96875
2017-09-09T15:48:20.230058: step 4764, loss 0.00267175, acc 1
2017-09-09T15:48:20.525494: step 4765, loss 0.00340793, acc 1
2017-09-09T15:48:20.829579: step 4766, loss 0.0172883, acc 1
2017-09-09T15:48:21.167926: step 4767, loss 0.00255598, acc 1
2017-09-09T15:48:21.443749: step 4768, loss 0.0105411, acc 1
2017-09-09T15:48:21.780487: step 4769, loss 0.0141688, acc 0.984375
2017-09-09T15:48:22.081602: step 4770, loss 0.0593393, acc 0.96875
2017-09-09T15:48:22.404566: step 4771, loss 0.0431944, acc 0.96875
2017-09-09T15:48:22.763511: step 4772, loss 0.0117263, acc 1
2017-09-09T15:48:23.039344: step 4773, loss 0.0494675, acc 0.96875
2017-09-09T15:48:23.386860: step 4774, loss 0.048321, acc 0.96875
2017-09-09T15:48:23.748231: step 4775, loss 0.00869716, acc 1
2017-09-09T15:48:24.024961: step 4776, loss 0.0452867, acc 0.984375
2017-09-09T15:48:24.377147: step 4777, loss 0.00923405, acc 1
2017-09-09T15:48:24.660614: step 4778, loss 0.00463823, acc 1
2017-09-09T15:48:24.947339: step 4779, loss 0.0239515, acc 0.984375
2017-09-09T15:48:25.302025: step 4780, loss 0.0143183, acc 1
2017-09-09T15:48:25.602854: step 4781, loss 0.0283687, acc 0.984375
2017-09-09T15:48:25.935179: step 4782, loss 0.0840577, acc 0.9375
2017-09-09T15:48:26.292417: step 4783, loss 0.0310082, acc 0.984375
2017-09-09T15:48:26.594121: step 4784, loss 0.00197664, acc 1
2017-09-09T15:48:26.942195: step 4785, loss 0.0241867, acc 0.984375
2017-09-09T15:48:27.283162: step 4786, loss 0.0077684, acc 1
2017-09-09T15:48:27.626062: step 4787, loss 0.0191086, acc 0.984375
2017-09-09T15:48:27.958329: step 4788, loss 0.0445015, acc 0.96875
2017-09-09T15:48:28.257664: step 4789, loss 0.00629535, acc 1
2017-09-09T15:48:28.586358: step 4790, loss 0.0350828, acc 0.984375
2017-09-09T15:48:28.894121: step 4791, loss 0.00412145, acc 1
2017-09-09T15:48:29.222550: step 4792, loss 0.0502174, acc 0.96875
2017-09-09T15:48:29.533806: step 4793, loss 0.010592, acc 1
2017-09-09T15:48:29.826508: step 4794, loss 0.00241737, acc 1
2017-09-09T15:48:30.146357: step 4795, loss 0.0953748, acc 0.953125
2017-09-09T15:48:30.449257: step 4796, loss 0.0855211, acc 0.96875
2017-09-09T15:48:30.802983: step 4797, loss 0.0833966, acc 0.953125
2017-09-09T15:48:31.132501: step 4798, loss 0.028221, acc 0.984375
2017-09-09T15:48:31.444400: step 4799, loss 0.0267886, acc 0.984375
2017-09-09T15:48:31.777787: step 4800, loss 0.0963053, acc 0.953125

Evaluation:
2017-09-09T15:48:31.856455: step 4800, loss 2.24632, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4800

2017-09-09T15:48:34.187395: step 4801, loss 0.0396112, acc 0.984375
2017-09-09T15:48:34.486472: step 4802, loss 0.0142863, acc 1
2017-09-09T15:48:34.825918: step 4803, loss 0.0448878, acc 0.984375
2017-09-09T15:48:35.113859: step 4804, loss 0.0194293, acc 0.984375
2017-09-09T15:48:35.451263: step 4805, loss 0.00851217, acc 1
2017-09-09T15:48:35.754499: step 4806, loss 0.045317, acc 1
2017-09-09T15:48:36.095131: step 4807, loss 0.0495892, acc 0.984375
2017-09-09T15:48:36.419428: step 4808, loss 0.042595, acc 1
2017-09-09T15:48:36.726928: step 4809, loss 0.0127059, acc 1
2017-09-09T15:48:37.048833: step 4810, loss 0.0331292, acc 1
2017-09-09T15:48:37.347441: step 4811, loss 0.00812774, acc 1
2017-09-09T15:48:37.825896: step 4812, loss 0.0017984, acc 1
2017-09-09T15:48:38.115337: step 4813, loss 0.00515946, acc 1
2017-09-09T15:48:38.376417: step 4814, loss 0.0349186, acc 0.984375
2017-09-09T15:48:38.703796: step 4815, loss 0.0472479, acc 0.96875
2017-09-09T15:48:38.989319: step 4816, loss 0.0459782, acc 0.96875
2017-09-09T15:48:39.266805: step 4817, loss 0.0189671, acc 1
2017-09-09T15:48:39.634648: step 4818, loss 0.00428361, acc 1
2017-09-09T15:48:39.898391: step 4819, loss 0.021668, acc 1
2017-09-09T15:48:40.196101: step 4820, loss 0.0534132, acc 0.953125
2017-09-09T15:48:40.477349: step 4821, loss 0.0241739, acc 0.984375
2017-09-09T15:48:40.750984: step 4822, loss 0.0109517, acc 1
2017-09-09T15:48:41.080787: step 4823, loss 0.023979, acc 1
2017-09-09T15:48:41.353663: step 4824, loss 0.0454332, acc 0.96875
2017-09-09T15:48:41.676374: step 4825, loss 0.000586647, acc 1
2017-09-09T15:48:41.991059: step 4826, loss 0.0767202, acc 0.953125
2017-09-09T15:48:42.321315: step 4827, loss 0.0342758, acc 0.96875
2017-09-09T15:48:42.634778: step 4828, loss 0.118658, acc 0.921875
2017-09-09T15:48:42.934185: step 4829, loss 0.0406406, acc 0.96875
2017-09-09T15:48:43.286989: step 4830, loss 0.0352987, acc 0.96875
2017-09-09T15:48:43.599127: step 4831, loss 0.0396966, acc 0.984375
2017-09-09T15:48:43.914296: step 4832, loss 0.0248185, acc 0.984375
2017-09-09T15:48:44.254514: step 4833, loss 0.0232023, acc 0.984375
2017-09-09T15:48:44.569977: step 4834, loss 0.0715919, acc 0.984375
2017-09-09T15:48:44.914897: step 4835, loss 0.0528858, acc 0.984375
2017-09-09T15:48:45.263546: step 4836, loss 0.00388819, acc 1
2017-09-09T15:48:45.606315: step 4837, loss 0.00659949, acc 1
2017-09-09T15:48:45.939812: step 4838, loss 0.00753426, acc 1
2017-09-09T15:48:46.237578: step 4839, loss 0.011226, acc 1
2017-09-09T15:48:46.564080: step 4840, loss 0.0186389, acc 0.984375
2017-09-09T15:48:46.869559: step 4841, loss 0.0503775, acc 0.96875
2017-09-09T15:48:47.287858: step 4842, loss 0.0178379, acc 0.984375
2017-09-09T15:48:47.559554: step 4843, loss 0.0230183, acc 0.984375
2017-09-09T15:48:47.876194: step 4844, loss 0.00873563, acc 1
2017-09-09T15:48:48.148908: step 4845, loss 0.0189176, acc 1
2017-09-09T15:48:48.499760: step 4846, loss 0.0498451, acc 0.96875
2017-09-09T15:48:48.870839: step 4847, loss 0.0577381, acc 0.96875
2017-09-09T15:48:49.175911: step 4848, loss 0.00706449, acc 1
2017-09-09T15:48:49.536174: step 4849, loss 0.0193687, acc 1
2017-09-09T15:48:49.808046: step 4850, loss 0.00290426, acc 1

Evaluation:
2017-09-09T15:48:49.916830: step 4850, loss 1.94401, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4850

2017-09-09T15:48:53.275301: step 4851, loss 0.057886, acc 0.984375
2017-09-09T15:48:53.577018: step 4852, loss 0.165899, acc 0.96875
2017-09-09T15:48:53.885045: step 4853, loss 0.021408, acc 0.984375
2017-09-09T15:48:54.177172: step 4854, loss 0.0462089, acc 0.984375
2017-09-09T15:48:54.436683: step 4855, loss 0.0825257, acc 0.953125
2017-09-09T15:48:54.683872: step 4856, loss 0.0270885, acc 0.984375
2017-09-09T15:48:55.122858: step 4857, loss 0.0843802, acc 0.953125
2017-09-09T15:48:55.526934: step 4858, loss 0.00273026, acc 1
2017-09-09T15:48:55.861866: step 4859, loss 0.00427819, acc 1
2017-09-09T15:48:56.243231: step 4860, loss 0.0834004, acc 0.96875
2017-09-09T15:48:56.548769: step 4861, loss 0.0232932, acc 0.984375
2017-09-09T15:48:56.891148: step 4862, loss 0.0268753, acc 1
2017-09-09T15:48:57.211381: step 4863, loss 0.0210708, acc 0.984375
2017-09-09T15:48:57.478675: step 4864, loss 0.00371527, acc 1
2017-09-09T15:48:57.797562: step 4865, loss 0.0153976, acc 1
2017-09-09T15:48:58.076899: step 4866, loss 0.00283901, acc 1
2017-09-09T15:48:58.385389: step 4867, loss 0.0321267, acc 0.984375
2017-09-09T15:48:58.683183: step 4868, loss 0.0285181, acc 0.984375
2017-09-09T15:48:58.971917: step 4869, loss 0.0114167, acc 1
2017-09-09T15:48:59.327220: step 4870, loss 0.000721636, acc 1
2017-09-09T15:48:59.625330: step 4871, loss 0.0395101, acc 0.984375
2017-09-09T15:48:59.960042: step 4872, loss 0.00591119, acc 1
2017-09-09T15:49:00.288291: step 4873, loss 0.0268827, acc 0.984375
2017-09-09T15:49:00.585611: step 4874, loss 0.0204778, acc 0.984375
2017-09-09T15:49:00.942485: step 4875, loss 0.00973979, acc 1
2017-09-09T15:49:01.253997: step 4876, loss 0.0413493, acc 0.96875
2017-09-09T15:49:01.607701: step 4877, loss 0.0240181, acc 0.984375
2017-09-09T15:49:01.947449: step 4878, loss 0.0359586, acc 0.984375
2017-09-09T15:49:02.245164: step 4879, loss 0.0342016, acc 1
2017-09-09T15:49:02.584124: step 4880, loss 0.0235877, acc 1
2017-09-09T15:49:02.869357: step 4881, loss 0.0185596, acc 1
2017-09-09T15:49:03.244217: step 4882, loss 0.0202456, acc 1
2017-09-09T15:49:03.544588: step 4883, loss 0.0457466, acc 0.984375
2017-09-09T15:49:03.870473: step 4884, loss 0.0236163, acc 0.984375
2017-09-09T15:49:04.201675: step 4885, loss 0.0438115, acc 0.984375
2017-09-09T15:49:04.518840: step 4886, loss 0.0547232, acc 0.96875
2017-09-09T15:49:04.915871: step 4887, loss 0.0804302, acc 0.953125
2017-09-09T15:49:05.230853: step 4888, loss 0.00870884, acc 1
2017-09-09T15:49:05.559455: step 4889, loss 0.0245688, acc 0.984375
2017-09-09T15:49:05.886265: step 4890, loss 0.0458322, acc 1
2017-09-09T15:49:06.173610: step 4891, loss 0.0230128, acc 1
2017-09-09T15:49:06.544829: step 4892, loss 0.0858115, acc 0.96875
2017-09-09T15:49:06.947875: step 4893, loss 0.00811067, acc 1
2017-09-09T15:49:07.228521: step 4894, loss 0.00397873, acc 1
2017-09-09T15:49:07.514530: step 4895, loss 0.0472277, acc 0.984375
2017-09-09T15:49:07.847537: step 4896, loss 0.0368472, acc 0.984375
2017-09-09T15:49:08.153532: step 4897, loss 0.00446314, acc 1
2017-09-09T15:49:08.449865: step 4898, loss 0.0124111, acc 1
2017-09-09T15:49:08.741373: step 4899, loss 0.034171, acc 1
2017-09-09T15:49:09.013953: step 4900, loss 0.0329721, acc 0.980392

Evaluation:
2017-09-09T15:49:09.140070: step 4900, loss 3.3886, acc 0.333813

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4900

2017-09-09T15:49:11.351067: step 4901, loss 0.0244497, acc 0.984375
2017-09-09T15:49:11.642084: step 4902, loss 0.0256482, acc 0.984375
2017-09-09T15:49:11.932246: step 4903, loss 0.0345061, acc 0.984375
2017-09-09T15:49:12.268879: step 4904, loss 0.0666921, acc 0.953125
2017-09-09T15:49:12.594203: step 4905, loss 0.0670851, acc 0.96875
2017-09-09T15:49:12.950058: step 4906, loss 0.0671878, acc 0.96875
2017-09-09T15:49:13.226384: step 4907, loss 0.0286656, acc 0.984375
2017-09-09T15:49:13.577947: step 4908, loss 0.0022481, acc 1
2017-09-09T15:49:13.940461: step 4909, loss 0.0904641, acc 0.953125
2017-09-09T15:49:14.246607: step 4910, loss 0.0248798, acc 0.984375
2017-09-09T15:49:14.620839: step 4911, loss 0.053847, acc 0.984375
2017-09-09T15:49:14.943778: step 4912, loss 0.0122469, acc 1
2017-09-09T15:49:15.241501: step 4913, loss 0.0510373, acc 0.984375
2017-09-09T15:49:15.559493: step 4914, loss 0.0520805, acc 0.96875
2017-09-09T15:49:15.859003: step 4915, loss 0.0444364, acc 0.984375
2017-09-09T15:49:16.179941: step 4916, loss 0.0257662, acc 1
2017-09-09T15:49:16.481421: step 4917, loss 0.0401417, acc 1
2017-09-09T15:49:16.764539: step 4918, loss 0.0346042, acc 0.984375
2017-09-09T15:49:17.104197: step 4919, loss 0.0324035, acc 0.984375
2017-09-09T15:49:17.363876: step 4920, loss 0.0263569, acc 0.984375
2017-09-09T15:49:17.712149: step 4921, loss 0.0375014, acc 0.984375
2017-09-09T15:49:18.042073: step 4922, loss 0.0527846, acc 0.984375
2017-09-09T15:49:18.354006: step 4923, loss 0.0162967, acc 1
2017-09-09T15:49:18.705207: step 4924, loss 0.0130364, acc 1
2017-09-09T15:49:19.011554: step 4925, loss 0.0564922, acc 0.96875
2017-09-09T15:49:19.330761: step 4926, loss 0.00509893, acc 1
2017-09-09T15:49:19.699615: step 4927, loss 0.0355131, acc 0.984375
2017-09-09T15:49:20.727349: step 4928, loss 0.0401946, acc 0.984375
2017-09-09T15:49:21.029063: step 4929, loss 0.0187899, acc 0.984375
2017-09-09T15:49:21.384507: step 4930, loss 0.0280665, acc 1
2017-09-09T15:49:21.707697: step 4931, loss 0.016668, acc 1
2017-09-09T15:49:22.085926: step 4932, loss 0.0427866, acc 0.96875
2017-09-09T15:49:22.388126: step 4933, loss 0.00594168, acc 1
2017-09-09T15:49:22.660921: step 4934, loss 0.0126783, acc 1
2017-09-09T15:49:22.958729: step 4935, loss 0.0194013, acc 1
2017-09-09T15:49:23.302965: step 4936, loss 0.0145603, acc 1
2017-09-09T15:49:23.604027: step 4937, loss 0.0221285, acc 1
2017-09-09T15:49:23.901913: step 4938, loss 0.00138951, acc 1
2017-09-09T15:49:24.178938: step 4939, loss 0.0835738, acc 0.96875
2017-09-09T15:49:24.465464: step 4940, loss 0.00228879, acc 1
2017-09-09T15:49:24.825001: step 4941, loss 0.0518414, acc 0.96875
2017-09-09T15:49:25.174888: step 4942, loss 0.0192669, acc 0.984375
2017-09-09T15:49:25.519379: step 4943, loss 0.00640624, acc 1
2017-09-09T15:49:25.859208: step 4944, loss 0.00310736, acc 1
2017-09-09T15:49:26.151068: step 4945, loss 0.00766996, acc 1
2017-09-09T15:49:26.543322: step 4946, loss 0.00785549, acc 1
2017-09-09T15:49:26.865835: step 4947, loss 0.0194284, acc 0.984375
2017-09-09T15:49:27.196719: step 4948, loss 0.0254365, acc 0.984375
2017-09-09T15:49:27.550388: step 4949, loss 0.0298187, acc 0.984375
2017-09-09T15:49:27.897548: step 4950, loss 0.0286703, acc 0.984375

Evaluation:
2017-09-09T15:49:28.011741: step 4950, loss 1.69606, acc 0.329496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-4950

2017-09-09T15:49:30.998613: step 4951, loss 0.0528329, acc 0.96875
2017-09-09T15:49:31.326918: step 4952, loss 0.00830464, acc 1
2017-09-09T15:49:31.628098: step 4953, loss 0.0224917, acc 1
2017-09-09T15:49:31.951535: step 4954, loss 0.00149897, acc 1
2017-09-09T15:49:32.302169: step 4955, loss 0.0520269, acc 0.96875
2017-09-09T15:49:32.598715: step 4956, loss 0.0531314, acc 0.96875
2017-09-09T15:49:32.913900: step 4957, loss 0.0663371, acc 0.96875
2017-09-09T15:49:33.235889: step 4958, loss 0.029684, acc 0.984375
2017-09-09T15:49:33.611941: step 4959, loss 0.0243758, acc 0.984375
2017-09-09T15:49:33.909098: step 4960, loss 0.0815445, acc 0.96875
2017-09-09T15:49:34.275217: step 4961, loss 0.0112877, acc 1
2017-09-09T15:49:34.598666: step 4962, loss 0.0156012, acc 1
2017-09-09T15:49:34.904863: step 4963, loss 0.0148469, acc 1
2017-09-09T15:49:35.249009: step 4964, loss 0.042151, acc 0.96875
2017-09-09T15:49:35.561321: step 4965, loss 0.00443406, acc 1
2017-09-09T15:49:35.941443: step 4966, loss 0.0439023, acc 0.984375
2017-09-09T15:49:36.226702: step 4967, loss 0.011272, acc 1
2017-09-09T15:49:36.487858: step 4968, loss 0.0357168, acc 0.984375
2017-09-09T15:49:36.781304: step 4969, loss 0.0269222, acc 1
2017-09-09T15:49:37.112360: step 4970, loss 0.0420651, acc 0.984375
2017-09-09T15:49:37.411380: step 4971, loss 0.00645065, acc 1
2017-09-09T15:49:37.688123: step 4972, loss 0.0648714, acc 0.96875
2017-09-09T15:49:38.004154: step 4973, loss 0.0197084, acc 1
2017-09-09T15:49:38.340038: step 4974, loss 0.00625327, acc 1
2017-09-09T15:49:38.655855: step 4975, loss 0.00352325, acc 1
2017-09-09T15:49:38.955046: step 4976, loss 0.0683164, acc 0.953125
2017-09-09T15:49:39.265221: step 4977, loss 0.0445915, acc 0.96875
2017-09-09T15:49:39.620751: step 4978, loss 0.086777, acc 0.96875
2017-09-09T15:49:39.897810: step 4979, loss 0.0334961, acc 0.984375
2017-09-09T15:49:40.233359: step 4980, loss 0.00787966, acc 1
2017-09-09T15:49:40.507805: step 4981, loss 0.0046707, acc 1
2017-09-09T15:49:40.821758: step 4982, loss 0.0321409, acc 0.984375
2017-09-09T15:49:41.187200: step 4983, loss 0.0178371, acc 1
2017-09-09T15:49:41.500068: step 4984, loss 0.00362149, acc 1
2017-09-09T15:49:41.878765: step 4985, loss 0.0501178, acc 0.984375
2017-09-09T15:49:42.212918: step 4986, loss 0.0125331, acc 1
2017-09-09T15:49:42.526163: step 4987, loss 0.0330401, acc 1
2017-09-09T15:49:42.853516: step 4988, loss 0.0434663, acc 1
2017-09-09T15:49:43.145231: step 4989, loss 0.0288642, acc 0.984375
2017-09-09T15:49:43.476487: step 4990, loss 0.0415456, acc 0.96875
2017-09-09T15:49:43.748997: step 4991, loss 0.0202292, acc 1
2017-09-09T15:49:44.150013: step 4992, loss 0.00383754, acc 1
2017-09-09T15:49:44.475561: step 4993, loss 0.0267928, acc 1
2017-09-09T15:49:44.853732: step 4994, loss 0.02108, acc 1
2017-09-09T15:49:45.215487: step 4995, loss 0.0236538, acc 0.984375
2017-09-09T15:49:45.516569: step 4996, loss 0.0258657, acc 0.984375
2017-09-09T15:49:45.817055: step 4997, loss 0.00140532, acc 1
2017-09-09T15:49:46.101288: step 4998, loss 0.0216811, acc 1
2017-09-09T15:49:46.410822: step 4999, loss 0.0333742, acc 0.984375
2017-09-09T15:49:46.702598: step 5000, loss 0.00591009, acc 1

Evaluation:
2017-09-09T15:49:46.796152: step 5000, loss 1.9467, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5000

2017-09-09T15:49:49.975801: step 5001, loss 0.042152, acc 1
2017-09-09T15:49:50.282032: step 5002, loss 0.0996502, acc 0.953125
2017-09-09T15:49:50.631021: step 5003, loss 0.0590391, acc 0.96875
2017-09-09T15:49:51.050964: step 5004, loss 0.0613506, acc 0.96875
2017-09-09T15:49:51.367214: step 5005, loss 0.00174541, acc 1
2017-09-09T15:49:51.675516: step 5006, loss 0.00992384, acc 1
2017-09-09T15:49:51.972823: step 5007, loss 0.00196035, acc 1
2017-09-09T15:49:52.223387: step 5008, loss 0.0816341, acc 0.96875
2017-09-09T15:49:52.519148: step 5009, loss 0.0213584, acc 1
2017-09-09T15:49:52.780646: step 5010, loss 0.00256624, acc 1
2017-09-09T15:49:53.063061: step 5011, loss 0.0587909, acc 0.984375
2017-09-09T15:49:53.375306: step 5012, loss 0.0152029, acc 1
2017-09-09T15:49:53.654229: step 5013, loss 0.00533935, acc 1
2017-09-09T15:49:53.925017: step 5014, loss 0.0226363, acc 0.984375
2017-09-09T15:49:54.249601: step 5015, loss 0.029108, acc 1
2017-09-09T15:49:54.572298: step 5016, loss 0.0304649, acc 0.984375
2017-09-09T15:49:54.896098: step 5017, loss 0.0291274, acc 0.984375
2017-09-09T15:49:55.187474: step 5018, loss 0.0024783, acc 1
2017-09-09T15:49:55.527124: step 5019, loss 0.00895669, acc 1
2017-09-09T15:49:55.865876: step 5020, loss 0.021633, acc 1
2017-09-09T15:49:56.158574: step 5021, loss 0.0494003, acc 0.984375
2017-09-09T15:49:56.489919: step 5022, loss 0.0181843, acc 0.984375
2017-09-09T15:49:56.784493: step 5023, loss 0.00177174, acc 1
2017-09-09T15:49:57.114445: step 5024, loss 0.00231923, acc 1
2017-09-09T15:49:57.415448: step 5025, loss 0.0304805, acc 0.984375
2017-09-09T15:49:57.731739: step 5026, loss 0.0308148, acc 0.984375
2017-09-09T15:49:58.026322: step 5027, loss 0.00151291, acc 1
2017-09-09T15:49:58.328280: step 5028, loss 0.0159352, acc 1
2017-09-09T15:49:58.638767: step 5029, loss 0.0465729, acc 0.96875
2017-09-09T15:49:58.921625: step 5030, loss 0.00137959, acc 1
2017-09-09T15:49:59.249639: step 5031, loss 0.00061722, acc 1
2017-09-09T15:49:59.515416: step 5032, loss 0.00792204, acc 1
2017-09-09T15:49:59.878322: step 5033, loss 0.000391212, acc 1
2017-09-09T15:50:00.188910: step 5034, loss 0.00375717, acc 1
2017-09-09T15:50:00.481521: step 5035, loss 0.0245562, acc 0.984375
2017-09-09T15:50:00.800848: step 5036, loss 0.0155199, acc 1
2017-09-09T15:50:01.123377: step 5037, loss 0.0139135, acc 0.984375
2017-09-09T15:50:01.460652: step 5038, loss 0.0157635, acc 1
2017-09-09T15:50:01.785713: step 5039, loss 0.0226769, acc 0.984375
2017-09-09T15:50:02.087500: step 5040, loss 0.0204188, acc 0.984375
2017-09-09T15:50:02.414460: step 5041, loss 0.0211926, acc 1
2017-09-09T15:50:02.721454: step 5042, loss 0.0195862, acc 0.984375
2017-09-09T15:50:03.054122: step 5043, loss 0.0121091, acc 1
2017-09-09T15:50:03.342381: step 5044, loss 0.00377556, acc 1
2017-09-09T15:50:03.685356: step 5045, loss 0.0038148, acc 1
2017-09-09T15:50:03.974424: step 5046, loss 0.0832623, acc 0.96875
2017-09-09T15:50:04.275240: step 5047, loss 0.037293, acc 0.984375
2017-09-09T15:50:04.580665: step 5048, loss 0.0134286, acc 1
2017-09-09T15:50:04.883501: step 5049, loss 0.000173768, acc 1
2017-09-09T15:50:05.165856: step 5050, loss 0.0109513, acc 1

Evaluation:
2017-09-09T15:50:05.268068: step 5050, loss 2.21266, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5050

2017-09-09T15:50:07.708177: step 5051, loss 0.06492, acc 0.96875
2017-09-09T15:50:08.055661: step 5052, loss 0.0688424, acc 0.96875
2017-09-09T15:50:08.357039: step 5053, loss 0.0331268, acc 0.984375
2017-09-09T15:50:08.761049: step 5054, loss 0.0352912, acc 0.984375
2017-09-09T15:50:09.078229: step 5055, loss 0.00729172, acc 1
2017-09-09T15:50:09.422433: step 5056, loss 0.0438065, acc 0.984375
2017-09-09T15:50:09.736712: step 5057, loss 0.00643675, acc 1
2017-09-09T15:50:10.028735: step 5058, loss 0.00839232, acc 1
2017-09-09T15:50:10.362151: step 5059, loss 0.0205514, acc 0.984375
2017-09-09T15:50:10.624548: step 5060, loss 0.034007, acc 0.96875
2017-09-09T15:50:10.923543: step 5061, loss 0.0285696, acc 1
2017-09-09T15:50:11.195028: step 5062, loss 0.0323585, acc 0.984375
2017-09-09T15:50:11.496368: step 5063, loss 0.0689223, acc 0.96875
2017-09-09T15:50:11.833871: step 5064, loss 0.054547, acc 0.96875
2017-09-09T15:50:12.105034: step 5065, loss 0.0624965, acc 0.953125
2017-09-09T15:50:12.391293: step 5066, loss 0.00232386, acc 1
2017-09-09T15:50:12.683619: step 5067, loss 0.0505128, acc 0.953125
2017-09-09T15:50:12.980236: step 5068, loss 0.0295734, acc 0.984375
2017-09-09T15:50:13.297844: step 5069, loss 0.0618125, acc 0.984375
2017-09-09T15:50:13.601740: step 5070, loss 0.0489137, acc 0.96875
2017-09-09T15:50:13.920397: step 5071, loss 0.0374762, acc 0.96875
2017-09-09T15:50:14.196187: step 5072, loss 0.0240032, acc 0.984375
2017-09-09T15:50:14.556188: step 5073, loss 0.044338, acc 0.984375
2017-09-09T15:50:14.877074: step 5074, loss 0.0495379, acc 0.953125
2017-09-09T15:50:15.172889: step 5075, loss 0.016684, acc 0.984375
2017-09-09T15:50:15.512937: step 5076, loss 0.00361627, acc 1
2017-09-09T15:50:15.904904: step 5077, loss 0.0198709, acc 0.984375
2017-09-09T15:50:16.224165: step 5078, loss 0.0135593, acc 1
2017-09-09T15:50:16.575713: step 5079, loss 0.0633972, acc 0.984375
2017-09-09T15:50:16.869626: step 5080, loss 0.052847, acc 0.984375
2017-09-09T15:50:17.214766: step 5081, loss 0.00639302, acc 1
2017-09-09T15:50:17.479847: step 5082, loss 0.0246303, acc 0.984375
2017-09-09T15:50:17.809925: step 5083, loss 0.0696475, acc 0.96875
2017-09-09T15:50:18.172085: step 5084, loss 0.0331567, acc 0.984375
2017-09-09T15:50:18.465576: step 5085, loss 0.00427377, acc 1
2017-09-09T15:50:18.843669: step 5086, loss 0.00647151, acc 1
2017-09-09T15:50:19.142182: step 5087, loss 0.00249985, acc 1
2017-09-09T15:50:19.464019: step 5088, loss 0.0358997, acc 0.984375
2017-09-09T15:50:19.749878: step 5089, loss 0.0385127, acc 0.984375
2017-09-09T15:50:20.129099: step 5090, loss 0.0359996, acc 1
2017-09-09T15:50:20.404352: step 5091, loss 0.0230112, acc 1
2017-09-09T15:50:20.697100: step 5092, loss 0.0295072, acc 0.984375
2017-09-09T15:50:20.967812: step 5093, loss 0.0238874, acc 1
2017-09-09T15:50:21.271571: step 5094, loss 0.00352687, acc 1
2017-09-09T15:50:21.590813: step 5095, loss 0.0197644, acc 1
2017-09-09T15:50:21.884005: step 5096, loss 0.00413944, acc 1
2017-09-09T15:50:22.200175: step 5097, loss 0.000637311, acc 1
2017-09-09T15:50:22.531108: step 5098, loss 0.0467323, acc 0.984375
2017-09-09T15:50:22.827346: step 5099, loss 0.0838186, acc 0.96875
2017-09-09T15:50:23.119402: step 5100, loss 0.0248973, acc 1

Evaluation:
2017-09-09T15:50:23.201623: step 5100, loss 2.15101, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5100

2017-09-09T15:50:26.543003: step 5101, loss 0.00318537, acc 1
2017-09-09T15:50:26.830006: step 5102, loss 0.00338113, acc 1
2017-09-09T15:50:27.186194: step 5103, loss 0.0400339, acc 0.96875
2017-09-09T15:50:27.482250: step 5104, loss 0.0385086, acc 0.984375
2017-09-09T15:50:27.815513: step 5105, loss 0.00042104, acc 1
2017-09-09T15:50:28.135243: step 5106, loss 0.0370465, acc 0.984375
2017-09-09T15:50:28.478801: step 5107, loss 0.00445867, acc 1
2017-09-09T15:50:28.853130: step 5108, loss 0.0608462, acc 0.953125
2017-09-09T15:50:29.138855: step 5109, loss 0.0779241, acc 0.953125
2017-09-09T15:50:29.490041: step 5110, loss 0.0494512, acc 0.96875
2017-09-09T15:50:29.807922: step 5111, loss 0.0315948, acc 0.984375
2017-09-09T15:50:30.085963: step 5112, loss 0.0243942, acc 0.984375
2017-09-09T15:50:30.380194: step 5113, loss 0.0308945, acc 1
2017-09-09T15:50:30.670848: step 5114, loss 0.0193246, acc 0.984375
2017-09-09T15:50:30.968037: step 5115, loss 0.000364161, acc 1
2017-09-09T15:50:31.291255: step 5116, loss 0.0458362, acc 0.96875
2017-09-09T15:50:31.602209: step 5117, loss 0.000355264, acc 1
2017-09-09T15:50:31.946450: step 5118, loss 0.0476566, acc 0.96875
2017-09-09T15:50:32.228045: step 5119, loss 0.0511055, acc 0.984375
2017-09-09T15:50:32.626976: step 5120, loss 0.00476633, acc 1
2017-09-09T15:50:32.958085: step 5121, loss 0.0153766, acc 1
2017-09-09T15:50:33.246932: step 5122, loss 0.00125462, acc 1
2017-09-09T15:50:33.639508: step 5123, loss 0.00479822, acc 1
2017-09-09T15:50:34.013633: step 5124, loss 0.0654988, acc 0.96875
2017-09-09T15:50:34.307388: step 5125, loss 0.0381297, acc 0.984375
2017-09-09T15:50:34.639125: step 5126, loss 0.0420924, acc 0.96875
2017-09-09T15:50:34.924105: step 5127, loss 0.0452391, acc 0.984375
2017-09-09T15:50:35.216232: step 5128, loss 0.0172537, acc 1
2017-09-09T15:50:35.534337: step 5129, loss 0.00303436, acc 1
2017-09-09T15:50:35.816691: step 5130, loss 0.00591753, acc 1
2017-09-09T15:50:36.088531: step 5131, loss 0.064536, acc 0.96875
2017-09-09T15:50:36.405552: step 5132, loss 0.0585831, acc 0.96875
2017-09-09T15:50:36.717118: step 5133, loss 0.016929, acc 1
2017-09-09T15:50:36.997596: step 5134, loss 0.0880691, acc 0.953125
2017-09-09T15:50:37.328040: step 5135, loss 0.03445, acc 1
2017-09-09T15:50:37.615254: step 5136, loss 0.0359726, acc 0.96875
2017-09-09T15:50:37.958041: step 5137, loss 0.0424258, acc 0.984375
2017-09-09T15:50:38.250346: step 5138, loss 0.00487246, acc 1
2017-09-09T15:50:38.601196: step 5139, loss 0.00971981, acc 1
2017-09-09T15:50:38.909850: step 5140, loss 0.0327491, acc 0.984375
2017-09-09T15:50:39.212302: step 5141, loss 0.0213611, acc 0.984375
2017-09-09T15:50:39.592163: step 5142, loss 0.0255381, acc 0.984375
2017-09-09T15:50:39.877510: step 5143, loss 0.0328385, acc 0.984375
2017-09-09T15:50:40.203396: step 5144, loss 0.0203252, acc 0.984375
2017-09-09T15:50:40.495634: step 5145, loss 0.038372, acc 0.984375
2017-09-09T15:50:40.791876: step 5146, loss 0.0213245, acc 1
2017-09-09T15:50:41.078487: step 5147, loss 0.0723021, acc 0.984375
2017-09-09T15:50:41.390140: step 5148, loss 0.115772, acc 0.96875
2017-09-09T15:50:41.728953: step 5149, loss 0.0248708, acc 0.984375
2017-09-09T15:50:42.016957: step 5150, loss 0.0153745, acc 1

Evaluation:
2017-09-09T15:50:42.149779: step 5150, loss 1.86106, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5150

2017-09-09T15:50:44.635930: step 5151, loss 0.013128, acc 1
2017-09-09T15:50:44.957133: step 5152, loss 0.00167793, acc 1
2017-09-09T15:50:45.246474: step 5153, loss 0.0202319, acc 0.984375
2017-09-09T15:50:45.550650: step 5154, loss 0.0357071, acc 0.984375
2017-09-09T15:50:45.928018: step 5155, loss 0.0122841, acc 1
2017-09-09T15:50:46.197746: step 5156, loss 0.0211998, acc 1
2017-09-09T15:50:46.544853: step 5157, loss 0.124503, acc 0.921875
2017-09-09T15:50:46.887010: step 5158, loss 0.00345599, acc 1
2017-09-09T15:50:47.191575: step 5159, loss 0.0102931, acc 1
2017-09-09T15:50:47.535924: step 5160, loss 0.00295504, acc 1
2017-09-09T15:50:47.830631: step 5161, loss 0.0215725, acc 1
2017-09-09T15:50:48.164405: step 5162, loss 0.0419404, acc 0.984375
2017-09-09T15:50:48.479868: step 5163, loss 0.0512365, acc 0.984375
2017-09-09T15:50:48.771613: step 5164, loss 0.0727627, acc 0.984375
2017-09-09T15:50:49.100735: step 5165, loss 0.0434658, acc 0.96875
2017-09-09T15:50:49.397167: step 5166, loss 0.0214175, acc 0.984375
2017-09-09T15:50:49.826915: step 5167, loss 0.0469266, acc 0.96875
2017-09-09T15:50:50.125131: step 5168, loss 0.0742698, acc 0.96875
2017-09-09T15:50:50.414825: step 5169, loss 0.0582124, acc 0.984375
2017-09-09T15:50:50.701956: step 5170, loss 0.0226649, acc 0.984375
2017-09-09T15:50:50.977077: step 5171, loss 0.0235466, acc 0.984375
2017-09-09T15:50:51.300819: step 5172, loss 0.0085808, acc 1
2017-09-09T15:50:51.640349: step 5173, loss 0.0643416, acc 0.96875
2017-09-09T15:50:51.991428: step 5174, loss 0.00803533, acc 1
2017-09-09T15:50:52.370435: step 5175, loss 0.0249754, acc 0.984375
2017-09-09T15:50:52.697484: step 5176, loss 0.0181925, acc 0.984375
2017-09-09T15:50:52.973488: step 5177, loss 0.0303208, acc 0.984375
2017-09-09T15:50:53.378417: step 5178, loss 0.0105179, acc 1
2017-09-09T15:50:53.788676: step 5179, loss 0.00926967, acc 1
2017-09-09T15:50:54.156121: step 5180, loss 0.0316199, acc 0.984375
2017-09-09T15:50:54.461914: step 5181, loss 0.0188801, acc 1
2017-09-09T15:50:54.808262: step 5182, loss 0.0704661, acc 0.953125
2017-09-09T15:50:55.162769: step 5183, loss 0.00332147, acc 1
2017-09-09T15:50:55.527383: step 5184, loss 0.0346486, acc 0.984375
2017-09-09T15:50:55.832588: step 5185, loss 0.00170379, acc 1
2017-09-09T15:50:56.165834: step 5186, loss 0.0221031, acc 0.984375
2017-09-09T15:50:56.515989: step 5187, loss 0.028841, acc 0.984375
2017-09-09T15:50:56.794963: step 5188, loss 0.100036, acc 0.96875
2017-09-09T15:50:57.147626: step 5189, loss 0.0136617, acc 1
2017-09-09T15:50:57.439757: step 5190, loss 0.0229337, acc 0.984375
2017-09-09T15:50:57.770622: step 5191, loss 0.0193534, acc 0.984375
2017-09-09T15:50:58.067717: step 5192, loss 0.014202, acc 0.984375
2017-09-09T15:50:58.377069: step 5193, loss 0.0328621, acc 0.984375
2017-09-09T15:50:58.665925: step 5194, loss 0.00117007, acc 1
2017-09-09T15:50:58.952468: step 5195, loss 0.0319921, acc 1
2017-09-09T15:50:59.272286: step 5196, loss 0.0195234, acc 0.984375
2017-09-09T15:50:59.546714: step 5197, loss 0.0358103, acc 0.984375
2017-09-09T15:50:59.864942: step 5198, loss 0.0039047, acc 1
2017-09-09T15:51:00.142965: step 5199, loss 0.0290914, acc 1
2017-09-09T15:51:00.530283: step 5200, loss 0.00452122, acc 1

Evaluation:
2017-09-09T15:51:00.604036: step 5200, loss 2.25287, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5200

2017-09-09T15:51:03.330412: step 5201, loss 0.0526429, acc 0.96875
2017-09-09T15:51:03.630533: step 5202, loss 0.0141656, acc 1
2017-09-09T15:51:03.958149: step 5203, loss 0.000987045, acc 1
2017-09-09T15:51:04.271902: step 5204, loss 0.0113532, acc 1
2017-09-09T15:51:04.535549: step 5205, loss 0.0148119, acc 1
2017-09-09T15:51:04.788550: step 5206, loss 0.0248423, acc 0.984375
2017-09-09T15:51:05.144938: step 5207, loss 0.0636165, acc 0.96875
2017-09-09T15:51:05.526459: step 5208, loss 0.0280839, acc 1
2017-09-09T15:51:05.886352: step 5209, loss 0.0689609, acc 0.96875
2017-09-09T15:51:06.247233: step 5210, loss 0.0472809, acc 0.96875
2017-09-09T15:51:06.625966: step 5211, loss 0.00276004, acc 1
2017-09-09T15:51:06.900968: step 5212, loss 0.0322854, acc 0.984375
2017-09-09T15:51:07.213169: step 5213, loss 0.00136285, acc 1
2017-09-09T15:51:07.533075: step 5214, loss 0.0414699, acc 0.984375
2017-09-09T15:51:07.807716: step 5215, loss 0.00311892, acc 1
2017-09-09T15:51:08.165547: step 5216, loss 0.0529589, acc 0.96875
2017-09-09T15:51:08.493758: step 5217, loss 0.0250899, acc 1
2017-09-09T15:51:08.786906: step 5218, loss 0.00185, acc 1
2017-09-09T15:51:09.172960: step 5219, loss 0.00901705, acc 1
2017-09-09T15:51:09.489618: step 5220, loss 0.0143723, acc 1
2017-09-09T15:51:09.826598: step 5221, loss 0.0106719, acc 1
2017-09-09T15:51:10.124656: step 5222, loss 0.0214744, acc 1
2017-09-09T15:51:10.456323: step 5223, loss 0.0330822, acc 0.984375
2017-09-09T15:51:10.815125: step 5224, loss 0.0638633, acc 0.953125
2017-09-09T15:51:11.105996: step 5225, loss 0.0136258, acc 1
2017-09-09T15:51:11.442986: step 5226, loss 0.0300284, acc 0.984375
2017-09-09T15:51:11.791586: step 5227, loss 0.000960361, acc 1
2017-09-09T15:51:12.091903: step 5228, loss 0.00811783, acc 1
2017-09-09T15:51:12.397423: step 5229, loss 0.0149347, acc 1
2017-09-09T15:51:12.690849: step 5230, loss 0.0710045, acc 0.953125
2017-09-09T15:51:13.107230: step 5231, loss 0.0435628, acc 0.984375
2017-09-09T15:51:13.430673: step 5232, loss 0.0400678, acc 0.984375
2017-09-09T15:51:13.748265: step 5233, loss 0.00312953, acc 1
2017-09-09T15:51:14.026806: step 5234, loss 0.0151333, acc 1
2017-09-09T15:51:14.307325: step 5235, loss 0.0194064, acc 1
2017-09-09T15:51:14.587269: step 5236, loss 0.0417604, acc 0.96875
2017-09-09T15:51:14.875411: step 5237, loss 0.0396516, acc 0.984375
2017-09-09T15:51:15.190036: step 5238, loss 0.0144183, acc 1
2017-09-09T15:51:15.458240: step 5239, loss 0.0955387, acc 0.96875
2017-09-09T15:51:15.759920: step 5240, loss 0.0327317, acc 0.984375
2017-09-09T15:51:16.040706: step 5241, loss 0.0462953, acc 0.984375
2017-09-09T15:51:16.429150: step 5242, loss 0.0580271, acc 0.96875
2017-09-09T15:51:16.731664: step 5243, loss 0.00677737, acc 1
2017-09-09T15:51:17.054102: step 5244, loss 0.0302197, acc 0.984375
2017-09-09T15:51:17.415482: step 5245, loss 0.00325828, acc 1
2017-09-09T15:51:17.739598: step 5246, loss 0.0387602, acc 0.984375
2017-09-09T15:51:18.078211: step 5247, loss 0.00598633, acc 1
2017-09-09T15:51:18.421064: step 5248, loss 0.0819948, acc 0.96875
2017-09-09T15:51:18.817185: step 5249, loss 0.00494147, acc 1
2017-09-09T15:51:19.138483: step 5250, loss 0.00490644, acc 1

Evaluation:
2017-09-09T15:51:19.202453: step 5250, loss 1.92927, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5250

2017-09-09T15:51:22.193242: step 5251, loss 0.00651845, acc 1
2017-09-09T15:51:22.582187: step 5252, loss 0.0158642, acc 1
2017-09-09T15:51:22.914409: step 5253, loss 0.0429234, acc 0.96875
2017-09-09T15:51:23.291203: step 5254, loss 0.015407, acc 0.984375
2017-09-09T15:51:23.606630: step 5255, loss 0.0116107, acc 1
2017-09-09T15:51:23.916900: step 5256, loss 0.000940425, acc 1
2017-09-09T15:51:24.312606: step 5257, loss 0.0288991, acc 1
2017-09-09T15:51:24.668631: step 5258, loss 0.0143142, acc 0.984375
2017-09-09T15:51:24.976087: step 5259, loss 0.0508121, acc 0.96875
2017-09-09T15:51:25.374609: step 5260, loss 0.0759199, acc 0.96875
2017-09-09T15:51:25.783612: step 5261, loss 0.0766972, acc 0.96875
2017-09-09T15:51:26.097183: step 5262, loss 0.0158099, acc 1
2017-09-09T15:51:26.460235: step 5263, loss 0.046031, acc 0.96875
2017-09-09T15:51:26.810927: step 5264, loss 0.00267666, acc 1
2017-09-09T15:51:27.092844: step 5265, loss 0.0415519, acc 0.984375
2017-09-09T15:51:27.450211: step 5266, loss 0.0158709, acc 0.984375
2017-09-09T15:51:27.771729: step 5267, loss 0.0405668, acc 0.984375
2017-09-09T15:51:28.069911: step 5268, loss 0.0128589, acc 1
2017-09-09T15:51:28.432597: step 5269, loss 0.0466517, acc 0.984375
2017-09-09T15:51:28.768733: step 5270, loss 0.00260244, acc 1
2017-09-09T15:51:29.100867: step 5271, loss 0.00919069, acc 1
2017-09-09T15:51:29.413384: step 5272, loss 0.0112292, acc 1
2017-09-09T15:51:29.811565: step 5273, loss 0.00528321, acc 1
2017-09-09T15:51:30.117081: step 5274, loss 0.0223275, acc 0.984375
2017-09-09T15:51:30.495630: step 5275, loss 0.0536449, acc 0.96875
2017-09-09T15:51:30.831346: step 5276, loss 0.052883, acc 0.96875
2017-09-09T15:51:31.116849: step 5277, loss 0.0229489, acc 1
2017-09-09T15:51:31.509763: step 5278, loss 0.0814025, acc 0.953125
2017-09-09T15:51:31.840431: step 5279, loss 0.00807856, acc 1
2017-09-09T15:51:32.091599: step 5280, loss 0.0437217, acc 0.96875
2017-09-09T15:51:32.472640: step 5281, loss 0.00503215, acc 1
2017-09-09T15:51:32.826408: step 5282, loss 0.00386513, acc 1
2017-09-09T15:51:33.111654: step 5283, loss 0.0411738, acc 0.96875
2017-09-09T15:51:33.386968: step 5284, loss 0.0233413, acc 0.984375
2017-09-09T15:51:33.708012: step 5285, loss 0.0355739, acc 1
2017-09-09T15:51:34.034751: step 5286, loss 0.00237757, acc 1
2017-09-09T15:51:34.427365: step 5287, loss 0.0111666, acc 1
2017-09-09T15:51:34.761936: step 5288, loss 0.0208319, acc 0.984375
2017-09-09T15:51:35.087016: step 5289, loss 0.00225958, acc 1
2017-09-09T15:51:35.481516: step 5290, loss 0.00663505, acc 1
2017-09-09T15:51:35.827065: step 5291, loss 0.0411475, acc 1
2017-09-09T15:51:36.169353: step 5292, loss 0.00709365, acc 1
2017-09-09T15:51:36.502743: step 5293, loss 0.0150397, acc 0.984375
2017-09-09T15:51:36.812881: step 5294, loss 0.0365731, acc 0.984375
2017-09-09T15:51:37.184331: step 5295, loss 0.00242451, acc 1
2017-09-09T15:51:37.487075: step 5296, loss 0.0441184, acc 0.984375
2017-09-09T15:51:37.881313: step 5297, loss 0.0315543, acc 0.984375
2017-09-09T15:51:38.269741: step 5298, loss 0.0126969, acc 1
2017-09-09T15:51:38.631903: step 5299, loss 0.0422471, acc 0.984375
2017-09-09T15:51:38.971396: step 5300, loss 0.00127944, acc 1

Evaluation:
2017-09-09T15:51:39.035817: step 5300, loss 2.01891, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5300

2017-09-09T15:51:41.535211: step 5301, loss 0.0495903, acc 0.984375
2017-09-09T15:51:41.851647: step 5302, loss 0.0241023, acc 1
2017-09-09T15:51:42.165250: step 5303, loss 0.00825193, acc 1
2017-09-09T15:51:42.423475: step 5304, loss 0.0154002, acc 1
2017-09-09T15:51:42.759500: step 5305, loss 0.0304844, acc 0.984375
2017-09-09T15:51:43.086615: step 5306, loss 0.00635045, acc 1
2017-09-09T15:51:43.380037: step 5307, loss 0.0439527, acc 0.984375
2017-09-09T15:51:43.732167: step 5308, loss 0.00641508, acc 1
2017-09-09T15:51:44.018083: step 5309, loss 0.0127532, acc 1
2017-09-09T15:51:44.382017: step 5310, loss 0.0194788, acc 1
2017-09-09T15:51:44.646501: step 5311, loss 0.0143011, acc 1
2017-09-09T15:51:44.978980: step 5312, loss 0.00808414, acc 1
2017-09-09T15:51:45.329596: step 5313, loss 0.0153712, acc 1
2017-09-09T15:51:45.606810: step 5314, loss 0.0323748, acc 0.984375
2017-09-09T15:51:45.944877: step 5315, loss 0.0265324, acc 0.984375
2017-09-09T15:51:46.222748: step 5316, loss 0.0300802, acc 0.984375
2017-09-09T15:51:46.600281: step 5317, loss 0.0283173, acc 0.96875
2017-09-09T15:51:46.907259: step 5318, loss 0.00589552, acc 1
2017-09-09T15:51:47.184495: step 5319, loss 0.0237835, acc 0.984375
2017-09-09T15:51:47.450605: step 5320, loss 0.0161185, acc 1
2017-09-09T15:51:48.128622: step 5321, loss 0.0541434, acc 0.96875
2017-09-09T15:51:48.443743: step 5322, loss 0.0253909, acc 1
2017-09-09T15:51:48.734274: step 5323, loss 0.0172947, acc 1
2017-09-09T15:51:49.060701: step 5324, loss 0.0244641, acc 0.984375
2017-09-09T15:51:49.404573: step 5325, loss 0.0312205, acc 0.984375
2017-09-09T15:51:49.707015: step 5326, loss 0.0465521, acc 0.96875
2017-09-09T15:51:50.082779: step 5327, loss 0.0999436, acc 0.953125
2017-09-09T15:51:50.433191: step 5328, loss 0.00148854, acc 1
2017-09-09T15:51:50.743406: step 5329, loss 0.0022788, acc 1
2017-09-09T15:51:51.056414: step 5330, loss 0.045536, acc 0.984375
2017-09-09T15:51:51.339088: step 5331, loss 0.00461419, acc 1
2017-09-09T15:51:51.698674: step 5332, loss 0.0259509, acc 0.984375
2017-09-09T15:51:52.017111: step 5333, loss 0.0247893, acc 1
2017-09-09T15:51:52.324981: step 5334, loss 0.00818623, acc 1
2017-09-09T15:51:52.629701: step 5335, loss 0.0580013, acc 0.96875
2017-09-09T15:51:52.920117: step 5336, loss 0.00765021, acc 1
2017-09-09T15:51:53.236811: step 5337, loss 0.00515119, acc 1
2017-09-09T15:51:53.563236: step 5338, loss 0.029919, acc 1
2017-09-09T15:51:53.872817: step 5339, loss 0.00348019, acc 1
2017-09-09T15:51:54.264725: step 5340, loss 0.0457223, acc 0.96875
2017-09-09T15:51:54.616674: step 5341, loss 0.0891094, acc 0.953125
2017-09-09T15:51:54.931003: step 5342, loss 0.0390492, acc 0.984375
2017-09-09T15:51:55.291859: step 5343, loss 0.0218447, acc 0.984375
2017-09-09T15:51:55.565291: step 5344, loss 0.0314382, acc 0.984375
2017-09-09T15:51:55.942324: step 5345, loss 0.0762156, acc 0.953125
2017-09-09T15:51:56.251070: step 5346, loss 0.0477301, acc 0.96875
2017-09-09T15:51:56.556212: step 5347, loss 0.0034862, acc 1
2017-09-09T15:51:56.877915: step 5348, loss 0.0619339, acc 0.96875
2017-09-09T15:51:57.170337: step 5349, loss 0.0161686, acc 0.984375
2017-09-09T15:51:57.513688: step 5350, loss 0.0084755, acc 1

Evaluation:
2017-09-09T15:51:57.592795: step 5350, loss 1.78053, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5350

2017-09-09T15:52:00.941273: step 5351, loss 0.050602, acc 0.96875
2017-09-09T15:52:01.271787: step 5352, loss 0.0294194, acc 0.984375
2017-09-09T15:52:01.574878: step 5353, loss 0.0137526, acc 1
2017-09-09T15:52:01.849686: step 5354, loss 0.0101392, acc 1
2017-09-09T15:52:02.172990: step 5355, loss 0.000895201, acc 1
2017-09-09T15:52:02.485605: step 5356, loss 0.0768099, acc 0.96875
2017-09-09T15:52:02.756839: step 5357, loss 0.0269754, acc 0.984375
2017-09-09T15:52:03.074550: step 5358, loss 0.0612998, acc 0.953125
2017-09-09T15:52:03.400610: step 5359, loss 0.0263975, acc 0.984375
2017-09-09T15:52:03.675304: step 5360, loss 0.0190516, acc 1
2017-09-09T15:52:04.030927: step 5361, loss 0.0642955, acc 0.96875
2017-09-09T15:52:04.342542: step 5362, loss 0.0223374, acc 1
2017-09-09T15:52:04.618700: step 5363, loss 0.0414012, acc 0.984375
2017-09-09T15:52:04.972064: step 5364, loss 0.033122, acc 0.984375
2017-09-09T15:52:05.286433: step 5365, loss 0.0365726, acc 0.984375
2017-09-09T15:52:05.626078: step 5366, loss 0.036618, acc 0.984375
2017-09-09T15:52:05.954352: step 5367, loss 0.0429076, acc 0.984375
2017-09-09T15:52:06.272332: step 5368, loss 0.0543754, acc 0.953125
2017-09-09T15:52:06.631658: step 5369, loss 0.0410166, acc 0.984375
2017-09-09T15:52:06.915569: step 5370, loss 0.0158526, acc 0.984375
2017-09-09T15:52:07.233868: step 5371, loss 0.0112569, acc 1
2017-09-09T15:52:07.573316: step 5372, loss 0.0262731, acc 0.984375
2017-09-09T15:52:07.878131: step 5373, loss 0.0560107, acc 0.96875
2017-09-09T15:52:08.240682: step 5374, loss 0.0357968, acc 0.984375
2017-09-09T15:52:08.537088: step 5375, loss 0.0378374, acc 0.984375
2017-09-09T15:52:08.881319: step 5376, loss 0.101418, acc 0.953125
2017-09-09T15:52:09.190125: step 5377, loss 0.023701, acc 0.984375
2017-09-09T15:52:09.482528: step 5378, loss 0.0152126, acc 1
2017-09-09T15:52:09.936509: step 5379, loss 0.00414684, acc 1
2017-09-09T15:52:10.268440: step 5380, loss 0.00869931, acc 1
2017-09-09T15:52:10.585796: step 5381, loss 0.00919772, acc 1
2017-09-09T15:52:10.892421: step 5382, loss 0.0252101, acc 0.984375
2017-09-09T15:52:11.204323: step 5383, loss 0.0237543, acc 0.984375
2017-09-09T15:52:11.502932: step 5384, loss 0.0803888, acc 0.96875
2017-09-09T15:52:11.809838: step 5385, loss 0.0129081, acc 1
2017-09-09T15:52:12.152913: step 5386, loss 0.0247219, acc 0.984375
2017-09-09T15:52:12.436586: step 5387, loss 0.00564693, acc 1
2017-09-09T15:52:12.788050: step 5388, loss 0.0117965, acc 1
2017-09-09T15:52:13.381311: step 5389, loss 0.0737109, acc 0.96875
2017-09-09T15:52:13.656115: step 5390, loss 0.105802, acc 0.921569
2017-09-09T15:52:13.984771: step 5391, loss 0.0672938, acc 0.96875
2017-09-09T15:52:14.313247: step 5392, loss 0.0162144, acc 0.984375
2017-09-09T15:52:14.614760: step 5393, loss 0.0808258, acc 0.953125
2017-09-09T15:52:14.957938: step 5394, loss 0.00328159, acc 1
2017-09-09T15:52:15.232393: step 5395, loss 0.00769497, acc 1
2017-09-09T15:52:15.586056: step 5396, loss 0.0220517, acc 1
2017-09-09T15:52:15.845992: step 5397, loss 0.0174111, acc 1
2017-09-09T15:52:16.299726: step 5398, loss 0.0332307, acc 0.984375
2017-09-09T15:52:16.614676: step 5399, loss 0.0216345, acc 0.984375
2017-09-09T15:52:16.891642: step 5400, loss 0.0518507, acc 0.984375

Evaluation:
2017-09-09T15:52:16.975412: step 5400, loss 2.53849, acc 0.307914

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5400

2017-09-09T15:52:19.090746: step 5401, loss 0.0268966, acc 1
2017-09-09T15:52:19.375074: step 5402, loss 0.0426292, acc 0.984375
2017-09-09T15:52:19.704497: step 5403, loss 0.0326285, acc 1
2017-09-09T15:52:20.008896: step 5404, loss 0.013749, acc 1
2017-09-09T15:52:20.352989: step 5405, loss 0.019278, acc 1
2017-09-09T15:52:20.674766: step 5406, loss 0.00690746, acc 1
2017-09-09T15:52:20.970546: step 5407, loss 0.0438684, acc 0.984375
2017-09-09T15:52:21.335211: step 5408, loss 0.00438178, acc 1
2017-09-09T15:52:21.606946: step 5409, loss 0.0260413, acc 1
2017-09-09T15:52:21.932722: step 5410, loss 0.0165555, acc 0.984375
2017-09-09T15:52:22.254859: step 5411, loss 0.0301402, acc 1
2017-09-09T15:52:22.534687: step 5412, loss 0.0186228, acc 1
2017-09-09T15:52:22.895907: step 5413, loss 0.0736025, acc 0.96875
2017-09-09T15:52:23.208275: step 5414, loss 0.0159141, acc 0.984375
2017-09-09T15:52:23.560592: step 5415, loss 0.012924, acc 1
2017-09-09T15:52:23.898566: step 5416, loss 0.0842412, acc 0.96875
2017-09-09T15:52:24.179114: step 5417, loss 0.0446819, acc 0.984375
2017-09-09T15:52:24.544190: step 5418, loss 0.0157901, acc 1
2017-09-09T15:52:24.840909: step 5419, loss 0.00155983, acc 1
2017-09-09T15:52:25.144937: step 5420, loss 0.0025101, acc 1
2017-09-09T15:52:25.440016: step 5421, loss 0.0218489, acc 1
2017-09-09T15:52:25.730385: step 5422, loss 0.0298908, acc 1
2017-09-09T15:52:26.034221: step 5423, loss 0.0109292, acc 1
2017-09-09T15:52:26.319320: step 5424, loss 0.00704022, acc 1
2017-09-09T15:52:26.631510: step 5425, loss 0.0393308, acc 0.96875
2017-09-09T15:52:26.944337: step 5426, loss 0.0884389, acc 0.9375
2017-09-09T15:52:27.319749: step 5427, loss 0.00210366, acc 1
2017-09-09T15:52:27.630190: step 5428, loss 0.0295894, acc 0.984375
2017-09-09T15:52:27.950246: step 5429, loss 0.0454193, acc 0.984375
2017-09-09T15:52:28.274460: step 5430, loss 0.0217231, acc 0.984375
2017-09-09T15:52:28.584249: step 5431, loss 0.0406127, acc 0.96875
2017-09-09T15:52:28.975162: step 5432, loss 0.0464178, acc 0.96875
2017-09-09T15:52:29.272939: step 5433, loss 0.0339441, acc 0.984375
2017-09-09T15:52:29.624170: step 5434, loss 0.0051495, acc 1
2017-09-09T15:52:30.009437: step 5435, loss 0.00291279, acc 1
2017-09-09T15:52:30.288159: step 5436, loss 0.0120024, acc 1
2017-09-09T15:52:30.742711: step 5437, loss 0.0221942, acc 1
2017-09-09T15:52:31.066498: step 5438, loss 0.0209193, acc 0.984375
2017-09-09T15:52:31.363871: step 5439, loss 0.0499331, acc 0.96875
2017-09-09T15:52:31.648478: step 5440, loss 0.0780845, acc 0.96875
2017-09-09T15:52:31.965103: step 5441, loss 0.0376761, acc 0.984375
2017-09-09T15:52:32.242216: step 5442, loss 0.00631107, acc 1
2017-09-09T15:52:32.544095: step 5443, loss 0.0194324, acc 1
2017-09-09T15:52:32.913414: step 5444, loss 0.0688812, acc 0.96875
2017-09-09T15:52:33.185292: step 5445, loss 0.00403346, acc 1
2017-09-09T15:52:33.523912: step 5446, loss 0.00178702, acc 1
2017-09-09T15:52:33.849615: step 5447, loss 0.00194695, acc 1
2017-09-09T15:52:34.163618: step 5448, loss 0.051427, acc 0.984375
2017-09-09T15:52:34.511338: step 5449, loss 0.0147847, acc 1
2017-09-09T15:52:34.794012: step 5450, loss 0.0148439, acc 1

Evaluation:
2017-09-09T15:52:34.892081: step 5450, loss 1.80274, acc 0.326619

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5450

2017-09-09T15:52:37.649037: step 5451, loss 0.0147254, acc 0.984375
2017-09-09T15:52:37.950597: step 5452, loss 0.0364639, acc 1
2017-09-09T15:52:38.314431: step 5453, loss 0.0270104, acc 1
2017-09-09T15:52:38.698820: step 5454, loss 0.00672072, acc 1
2017-09-09T15:52:39.002020: step 5455, loss 0.0315759, acc 0.984375
2017-09-09T15:52:39.339194: step 5456, loss 0.0480289, acc 0.96875
2017-09-09T15:52:39.647143: step 5457, loss 0.000791472, acc 1
2017-09-09T15:52:39.948225: step 5458, loss 0.0453238, acc 0.984375
2017-09-09T15:52:40.294257: step 5459, loss 0.000419735, acc 1
2017-09-09T15:52:40.567159: step 5460, loss 0.0582505, acc 0.953125
2017-09-09T15:52:40.881768: step 5461, loss 0.0443084, acc 0.984375
2017-09-09T15:52:41.163965: step 5462, loss 0.0130275, acc 1
2017-09-09T15:52:41.480412: step 5463, loss 0.0223645, acc 0.984375
2017-09-09T15:52:41.827083: step 5464, loss 0.0280401, acc 0.984375
2017-09-09T15:52:42.133638: step 5465, loss 0.0374369, acc 0.984375
2017-09-09T15:52:42.467708: step 5466, loss 0.0714207, acc 0.96875
2017-09-09T15:52:42.820700: step 5467, loss 0.0297174, acc 0.984375
2017-09-09T15:52:43.112904: step 5468, loss 0.0203457, acc 1
2017-09-09T15:52:43.447119: step 5469, loss 0.0900982, acc 0.953125
2017-09-09T15:52:43.725766: step 5470, loss 0.0424674, acc 0.984375
2017-09-09T15:52:44.046540: step 5471, loss 0.00759484, acc 1
2017-09-09T15:52:44.363776: step 5472, loss 0.00375998, acc 1
2017-09-09T15:52:44.723398: step 5473, loss 0.002367, acc 1
2017-09-09T15:52:44.977609: step 5474, loss 0.00497368, acc 1
2017-09-09T15:52:45.250997: step 5475, loss 0.0470597, acc 0.984375
2017-09-09T15:52:45.566956: step 5476, loss 0.0254657, acc 0.984375
2017-09-09T15:52:45.901541: step 5477, loss 0.058904, acc 0.96875
2017-09-09T15:52:46.189747: step 5478, loss 0.00440341, acc 1
2017-09-09T15:52:46.472534: step 5479, loss 0.0125329, acc 1
2017-09-09T15:52:46.752958: step 5480, loss 0.0261804, acc 0.984375
2017-09-09T15:52:47.047194: step 5481, loss 0.0496029, acc 0.96875
2017-09-09T15:52:47.334019: step 5482, loss 0.0988231, acc 0.953125
2017-09-09T15:52:47.713257: step 5483, loss 0.0450927, acc 0.96875
2017-09-09T15:52:47.985912: step 5484, loss 0.00253879, acc 1
2017-09-09T15:52:48.308185: step 5485, loss 0.00898111, acc 1
2017-09-09T15:52:48.634254: step 5486, loss 0.0241769, acc 1
2017-09-09T15:52:48.933228: step 5487, loss 0.00747559, acc 1
2017-09-09T15:52:49.272319: step 5488, loss 0.0652506, acc 0.980392
2017-09-09T15:52:49.596283: step 5489, loss 0.00352453, acc 1
2017-09-09T15:52:49.948700: step 5490, loss 0.00367421, acc 1
2017-09-09T15:52:50.273181: step 5491, loss 0.0187572, acc 1
2017-09-09T15:52:50.587518: step 5492, loss 0.00318633, acc 1
2017-09-09T15:52:50.917628: step 5493, loss 0.0137934, acc 1
2017-09-09T15:52:51.202548: step 5494, loss 0.0238498, acc 0.984375
2017-09-09T15:52:51.543348: step 5495, loss 0.0375122, acc 0.984375
2017-09-09T15:52:51.855757: step 5496, loss 0.0042552, acc 1
2017-09-09T15:52:52.191163: step 5497, loss 0.0103363, acc 1
2017-09-09T15:52:52.505859: step 5498, loss 0.0126133, acc 1
2017-09-09T15:52:52.828167: step 5499, loss 0.0478871, acc 0.96875
2017-09-09T15:52:53.186416: step 5500, loss 0.0573389, acc 0.984375

Evaluation:
2017-09-09T15:52:53.271617: step 5500, loss 2.81817, acc 0.309353

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5500

2017-09-09T15:52:56.557104: step 5501, loss 0.0400219, acc 0.984375
2017-09-09T15:52:56.895348: step 5502, loss 0.0182946, acc 1
2017-09-09T15:52:57.258123: step 5503, loss 0.0873683, acc 0.953125
2017-09-09T15:52:57.581403: step 5504, loss 0.0247774, acc 0.984375
2017-09-09T15:52:57.914658: step 5505, loss 0.0919571, acc 0.953125
2017-09-09T15:52:58.258152: step 5506, loss 0.00195038, acc 1
2017-09-09T15:52:58.689001: step 5507, loss 0.00200744, acc 1
2017-09-09T15:52:59.061851: step 5508, loss 0.0285833, acc 0.984375
2017-09-09T15:52:59.327123: step 5509, loss 0.0258847, acc 0.984375
2017-09-09T15:52:59.574106: step 5510, loss 0.0211241, acc 0.984375
2017-09-09T15:52:59.877175: step 5511, loss 0.011108, acc 1
2017-09-09T15:53:00.201551: step 5512, loss 0.0771983, acc 0.953125
2017-09-09T15:53:00.460515: step 5513, loss 0.0561894, acc 0.96875
2017-09-09T15:53:00.711814: step 5514, loss 0.0577542, acc 0.96875
2017-09-09T15:53:01.015963: step 5515, loss 0.0558603, acc 0.96875
2017-09-09T15:53:01.305433: step 5516, loss 0.00911892, acc 1
2017-09-09T15:53:01.551648: step 5517, loss 0.0331107, acc 0.984375
2017-09-09T15:53:01.920502: step 5518, loss 0.0328308, acc 0.984375
2017-09-09T15:53:02.253534: step 5519, loss 0.0167935, acc 1
2017-09-09T15:53:02.550717: step 5520, loss 0.0285815, acc 0.984375
2017-09-09T15:53:02.878936: step 5521, loss 0.00502484, acc 1
2017-09-09T15:53:03.170802: step 5522, loss 0.0200161, acc 1
2017-09-09T15:53:03.542658: step 5523, loss 0.0355316, acc 0.96875
2017-09-09T15:53:03.844109: step 5524, loss 0.00838755, acc 1
2017-09-09T15:53:04.237334: step 5525, loss 0.0178204, acc 0.984375
2017-09-09T15:53:04.573179: step 5526, loss 0.0103479, acc 1
2017-09-09T15:53:04.885129: step 5527, loss 0.0224936, acc 1
2017-09-09T15:53:05.242949: step 5528, loss 0.0601447, acc 0.984375
2017-09-09T15:53:05.603702: step 5529, loss 0.000686612, acc 1
2017-09-09T15:53:05.898919: step 5530, loss 0.0293771, acc 1
2017-09-09T15:53:06.290806: step 5531, loss 0.0261784, acc 1
2017-09-09T15:53:06.595140: step 5532, loss 0.00332552, acc 1
2017-09-09T15:53:06.918952: step 5533, loss 0.0300844, acc 0.984375
2017-09-09T15:53:07.277006: step 5534, loss 0.0160555, acc 1
2017-09-09T15:53:07.560014: step 5535, loss 0.0157574, acc 1
2017-09-09T15:53:07.920914: step 5536, loss 0.0066005, acc 1
2017-09-09T15:53:08.196086: step 5537, loss 0.105873, acc 0.9375
2017-09-09T15:53:08.535705: step 5538, loss 0.0276301, acc 1
2017-09-09T15:53:08.829036: step 5539, loss 0.0205094, acc 0.984375
2017-09-09T15:53:09.149588: step 5540, loss 0.0217671, acc 0.984375
2017-09-09T15:53:09.469417: step 5541, loss 0.0782024, acc 0.953125
2017-09-09T15:53:09.747317: step 5542, loss 0.0160266, acc 0.984375
2017-09-09T15:53:10.055981: step 5543, loss 0.0365264, acc 0.984375
2017-09-09T15:53:10.342870: step 5544, loss 0.0159694, acc 1
2017-09-09T15:53:10.671945: step 5545, loss 0.00194095, acc 1
2017-09-09T15:53:10.978302: step 5546, loss 0.0120349, acc 1
2017-09-09T15:53:11.296048: step 5547, loss 0.00192998, acc 1
2017-09-09T15:53:11.599809: step 5548, loss 0.0419015, acc 0.96875
2017-09-09T15:53:11.914523: step 5549, loss 0.0955903, acc 0.9375
2017-09-09T15:53:12.246837: step 5550, loss 0.0043514, acc 1

Evaluation:
2017-09-09T15:53:12.325884: step 5550, loss 2.03656, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5550

2017-09-09T15:53:14.740456: step 5551, loss 0.0184609, acc 1
2017-09-09T15:53:15.012162: step 5552, loss 0.0596523, acc 0.96875
2017-09-09T15:53:15.282507: step 5553, loss 0.0106813, acc 1
2017-09-09T15:53:15.601317: step 5554, loss 0.0225058, acc 0.984375
2017-09-09T15:53:15.933257: step 5555, loss 0.0422372, acc 0.96875
2017-09-09T15:53:16.241816: step 5556, loss 0.0070278, acc 1
2017-09-09T15:53:16.605898: step 5557, loss 0.0402229, acc 0.984375
2017-09-09T15:53:16.976679: step 5558, loss 0.11964, acc 0.953125
2017-09-09T15:53:17.300337: step 5559, loss 0.0076923, acc 1
2017-09-09T15:53:17.625210: step 5560, loss 0.0564856, acc 0.96875
2017-09-09T15:53:17.934176: step 5561, loss 0.079694, acc 0.953125
2017-09-09T15:53:18.291798: step 5562, loss 0.0123892, acc 1
2017-09-09T15:53:18.611059: step 5563, loss 0.0276585, acc 1
2017-09-09T15:53:18.981436: step 5564, loss 0.155086, acc 0.921875
2017-09-09T15:53:19.292979: step 5565, loss 0.0559224, acc 0.984375
2017-09-09T15:53:19.591320: step 5566, loss 0.0465987, acc 0.984375
2017-09-09T15:53:19.894924: step 5567, loss 0.00642393, acc 1
2017-09-09T15:53:20.210998: step 5568, loss 0.0342924, acc 0.984375
2017-09-09T15:53:20.534376: step 5569, loss 0.0430071, acc 0.96875
2017-09-09T15:53:20.910927: step 5570, loss 0.0417632, acc 0.984375
2017-09-09T15:53:21.213042: step 5571, loss 0.0125176, acc 1
2017-09-09T15:53:21.540221: step 5572, loss 0.0652652, acc 0.953125
2017-09-09T15:53:21.810051: step 5573, loss 0.00634972, acc 1
2017-09-09T15:53:22.146895: step 5574, loss 0.0305689, acc 0.984375
2017-09-09T15:53:22.436878: step 5575, loss 0.0270789, acc 1
2017-09-09T15:53:22.819712: step 5576, loss 0.00761034, acc 1
2017-09-09T15:53:23.127638: step 5577, loss 0.0325717, acc 1
2017-09-09T15:53:23.411174: step 5578, loss 0.0307518, acc 0.984375
2017-09-09T15:53:23.740373: step 5579, loss 0.0205437, acc 1
2017-09-09T15:53:24.065642: step 5580, loss 0.0233681, acc 0.984375
2017-09-09T15:53:24.463110: step 5581, loss 0.00487086, acc 1
2017-09-09T15:53:24.801605: step 5582, loss 0.00996549, acc 1
2017-09-09T15:53:25.113039: step 5583, loss 0.0406586, acc 1
2017-09-09T15:53:25.458172: step 5584, loss 0.00323522, acc 1
2017-09-09T15:53:25.747963: step 5585, loss 0.0202968, acc 1
2017-09-09T15:53:26.090648: step 5586, loss 0.0446914, acc 0.980392
2017-09-09T15:53:26.401495: step 5587, loss 0.0189289, acc 1
2017-09-09T15:53:26.692811: step 5588, loss 0.0472488, acc 0.984375
2017-09-09T15:53:27.038646: step 5589, loss 0.0466404, acc 0.984375
2017-09-09T15:53:27.317520: step 5590, loss 0.024871, acc 1
2017-09-09T15:53:27.673639: step 5591, loss 0.0293979, acc 0.984375
2017-09-09T15:53:27.957515: step 5592, loss 0.0257786, acc 0.984375
2017-09-09T15:53:28.307738: step 5593, loss 0.0305844, acc 0.984375
2017-09-09T15:53:28.612742: step 5594, loss 0.0192929, acc 1
2017-09-09T15:53:28.905310: step 5595, loss 0.0232594, acc 0.984375
2017-09-09T15:53:29.208592: step 5596, loss 0.0620592, acc 0.984375
2017-09-09T15:53:29.519751: step 5597, loss 0.0244958, acc 1
2017-09-09T15:53:29.808209: step 5598, loss 0.053626, acc 0.984375
2017-09-09T15:53:30.111652: step 5599, loss 0.00591803, acc 1
2017-09-09T15:53:30.471169: step 5600, loss 0.00336849, acc 1

Evaluation:
2017-09-09T15:53:30.563142: step 5600, loss 1.7469, acc 0.352518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5600

2017-09-09T15:53:33.689294: step 5601, loss 0.0114898, acc 1
2017-09-09T15:53:34.001578: step 5602, loss 0.013679, acc 1
2017-09-09T15:53:34.285443: step 5603, loss 0.0333498, acc 1
2017-09-09T15:53:34.623745: step 5604, loss 0.0027045, acc 1
2017-09-09T15:53:34.914327: step 5605, loss 0.0503334, acc 0.984375
2017-09-09T15:53:35.275377: step 5606, loss 0.0531172, acc 0.96875
2017-09-09T15:53:35.582702: step 5607, loss 0.0297282, acc 0.984375
2017-09-09T15:53:35.879457: step 5608, loss 0.0270119, acc 0.984375
2017-09-09T15:53:36.173188: step 5609, loss 0.0261372, acc 0.984375
2017-09-09T15:53:36.456150: step 5610, loss 0.013672, acc 1
2017-09-09T15:53:36.776655: step 5611, loss 0.0023577, acc 1
2017-09-09T15:53:37.048316: step 5612, loss 0.0392219, acc 0.96875
2017-09-09T15:53:37.398665: step 5613, loss 0.0133456, acc 1
2017-09-09T15:53:37.682379: step 5614, loss 0.0190127, acc 0.984375
2017-09-09T15:53:38.013395: step 5615, loss 0.00447953, acc 1
2017-09-09T15:53:38.322012: step 5616, loss 0.0298111, acc 0.984375
2017-09-09T15:53:38.602697: step 5617, loss 0.00525123, acc 1
2017-09-09T15:53:38.891903: step 5618, loss 0.00753727, acc 1
2017-09-09T15:53:39.168555: step 5619, loss 0.0600979, acc 0.984375
2017-09-09T15:53:39.563233: step 5620, loss 0.0114756, acc 1
2017-09-09T15:53:39.859469: step 5621, loss 0.0412162, acc 0.984375
2017-09-09T15:53:40.154709: step 5622, loss 0.0155969, acc 0.984375
2017-09-09T15:53:40.433597: step 5623, loss 0.0408748, acc 0.984375
2017-09-09T15:53:40.710638: step 5624, loss 0.0125165, acc 1
2017-09-09T15:53:41.085893: step 5625, loss 0.00320258, acc 1
2017-09-09T15:53:41.393726: step 5626, loss 0.0183615, acc 1
2017-09-09T15:53:41.763068: step 5627, loss 0.0106951, acc 1
2017-09-09T15:53:42.035053: step 5628, loss 0.0382013, acc 0.984375
2017-09-09T15:53:42.480073: step 5629, loss 0.016937, acc 1
2017-09-09T15:53:42.784177: step 5630, loss 0.00401651, acc 1
2017-09-09T15:53:43.072255: step 5631, loss 0.0122441, acc 1
2017-09-09T15:53:43.376291: step 5632, loss 0.00559635, acc 1
2017-09-09T15:53:43.673762: step 5633, loss 0.0140961, acc 1
2017-09-09T15:53:43.953305: step 5634, loss 0.00932461, acc 1
2017-09-09T15:53:44.259121: step 5635, loss 0.0323953, acc 0.984375
2017-09-09T15:53:44.557650: step 5636, loss 0.00289329, acc 1
2017-09-09T15:53:44.839475: step 5637, loss 0.0756897, acc 0.984375
2017-09-09T15:53:45.201819: step 5638, loss 0.0103574, acc 1
2017-09-09T15:53:45.503008: step 5639, loss 0.0312398, acc 1
2017-09-09T15:53:45.810411: step 5640, loss 0.00903063, acc 1
2017-09-09T15:53:46.139224: step 5641, loss 0.0853439, acc 0.953125
2017-09-09T15:53:46.468018: step 5642, loss 0.00912762, acc 1
2017-09-09T15:53:46.812655: step 5643, loss 0.0327714, acc 0.984375
2017-09-09T15:53:47.152140: step 5644, loss 0.0240828, acc 0.984375
2017-09-09T15:53:47.448766: step 5645, loss 0.056272, acc 0.96875
2017-09-09T15:53:47.810515: step 5646, loss 0.00649844, acc 1
2017-09-09T15:53:48.128245: step 5647, loss 0.00933305, acc 1
2017-09-09T15:53:48.440373: step 5648, loss 0.00079917, acc 1
2017-09-09T15:53:48.722517: step 5649, loss 0.022069, acc 0.984375
2017-09-09T15:53:49.065736: step 5650, loss 0.0216699, acc 0.984375

Evaluation:
2017-09-09T15:53:49.146940: step 5650, loss 3.71913, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5650

2017-09-09T15:53:51.551596: step 5651, loss 0.0555183, acc 0.96875
2017-09-09T15:53:51.834418: step 5652, loss 0.0717879, acc 0.984375
2017-09-09T15:53:52.120405: step 5653, loss 0.0483049, acc 0.984375
2017-09-09T15:53:52.454738: step 5654, loss 0.000342063, acc 1
2017-09-09T15:53:52.758007: step 5655, loss 0.00469618, acc 1
2017-09-09T15:53:53.148078: step 5656, loss 0.0318834, acc 0.984375
2017-09-09T15:53:53.440948: step 5657, loss 0.041925, acc 1
2017-09-09T15:53:53.760723: step 5658, loss 0.00810435, acc 1
2017-09-09T15:53:54.102666: step 5659, loss 0.00205133, acc 1
2017-09-09T15:53:54.376272: step 5660, loss 0.0629285, acc 0.96875
2017-09-09T15:53:54.704430: step 5661, loss 0.0321709, acc 0.984375
2017-09-09T15:53:55.024420: step 5662, loss 0.0208159, acc 1
2017-09-09T15:53:55.326128: step 5663, loss 0.00760138, acc 1
2017-09-09T15:53:55.702681: step 5664, loss 0.0249318, acc 0.984375
2017-09-09T15:53:55.984794: step 5665, loss 0.0261608, acc 1
2017-09-09T15:53:56.254625: step 5666, loss 0.00968536, acc 1
2017-09-09T15:53:56.600838: step 5667, loss 0.0307983, acc 0.984375
2017-09-09T15:53:56.950916: step 5668, loss 0.0240936, acc 0.984375
2017-09-09T15:53:57.299136: step 5669, loss 0.00824369, acc 1
2017-09-09T15:53:57.569808: step 5670, loss 0.0521773, acc 0.96875
2017-09-09T15:53:57.838578: step 5671, loss 0.0113198, acc 1
2017-09-09T15:53:58.156397: step 5672, loss 0.041568, acc 0.984375
2017-09-09T15:53:58.494426: step 5673, loss 0.0864746, acc 0.953125
2017-09-09T15:53:58.806099: step 5674, loss 0.0222997, acc 0.984375
2017-09-09T15:53:59.127776: step 5675, loss 0.0809359, acc 0.984375
2017-09-09T15:53:59.467317: step 5676, loss 0.00874434, acc 1
2017-09-09T15:53:59.837970: step 5677, loss 0.0380201, acc 0.96875
2017-09-09T15:54:00.196061: step 5678, loss 0.00237861, acc 1
2017-09-09T15:54:00.492876: step 5679, loss 0.00419107, acc 1
2017-09-09T15:54:00.809126: step 5680, loss 0.00144429, acc 1
2017-09-09T15:54:01.124212: step 5681, loss 0.0151073, acc 0.984375
2017-09-09T15:54:01.402880: step 5682, loss 0.0783452, acc 0.9375
2017-09-09T15:54:01.709203: step 5683, loss 0.0270955, acc 0.984375
2017-09-09T15:54:01.993530: step 5684, loss 0.0160244, acc 1
2017-09-09T15:54:02.340929: step 5685, loss 0.00870811, acc 1
2017-09-09T15:54:02.618570: step 5686, loss 0.0130338, acc 1
2017-09-09T15:54:02.952547: step 5687, loss 0.01817, acc 0.984375
2017-09-09T15:54:03.239457: step 5688, loss 0.0228171, acc 0.984375
2017-09-09T15:54:03.565685: step 5689, loss 0.0369014, acc 0.984375
2017-09-09T15:54:03.958971: step 5690, loss 0.0149922, acc 1
2017-09-09T15:54:04.229922: step 5691, loss 0.0135005, acc 1
2017-09-09T15:54:04.570838: step 5692, loss 0.0469743, acc 0.984375
2017-09-09T15:54:04.881300: step 5693, loss 0.0053506, acc 1
2017-09-09T15:54:05.172112: step 5694, loss 0.0136029, acc 1
2017-09-09T15:54:05.501471: step 5695, loss 0.0374624, acc 0.984375
2017-09-09T15:54:05.793162: step 5696, loss 0.0514536, acc 0.96875
2017-09-09T15:54:06.188258: step 5697, loss 0.0314123, acc 0.984375
2017-09-09T15:54:06.509111: step 5698, loss 0.0064784, acc 1
2017-09-09T15:54:06.807972: step 5699, loss 0.0401722, acc 0.984375
2017-09-09T15:54:07.119205: step 5700, loss 0.0370639, acc 0.984375

Evaluation:
2017-09-09T15:54:07.218364: step 5700, loss 2.30611, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5700

2017-09-09T15:54:10.043964: step 5701, loss 0.0418941, acc 0.984375
2017-09-09T15:54:10.369946: step 5702, loss 0.0517956, acc 0.96875
2017-09-09T15:54:10.670339: step 5703, loss 0.0267105, acc 0.984375
2017-09-09T15:54:11.096501: step 5704, loss 0.0284594, acc 0.984375
2017-09-09T15:54:11.408273: step 5705, loss 0.019242, acc 1
2017-09-09T15:54:11.831508: step 5706, loss 0.000248466, acc 1
2017-09-09T15:54:12.105913: step 5707, loss 0.0185423, acc 1
2017-09-09T15:54:12.404122: step 5708, loss 0.0154818, acc 0.984375
2017-09-09T15:54:12.702435: step 5709, loss 0.00299281, acc 1
2017-09-09T15:54:13.008707: step 5710, loss 0.0423137, acc 0.984375
2017-09-09T15:54:13.284449: step 5711, loss 0.0217881, acc 0.984375
2017-09-09T15:54:13.540123: step 5712, loss 0.0490046, acc 0.984375
2017-09-09T15:54:13.828586: step 5713, loss 0.00439282, acc 1
2017-09-09T15:54:14.143066: step 5714, loss 0.00541053, acc 1
2017-09-09T15:54:14.454998: step 5715, loss 0.0131529, acc 1
2017-09-09T15:54:14.769568: step 5716, loss 0.0340839, acc 0.984375
2017-09-09T15:54:15.082037: step 5717, loss 0.022846, acc 0.984375
2017-09-09T15:54:15.386723: step 5718, loss 0.0250905, acc 1
2017-09-09T15:54:15.742878: step 5719, loss 0.0262681, acc 0.984375
2017-09-09T15:54:16.102794: step 5720, loss 0.0352132, acc 0.984375
2017-09-09T15:54:16.395671: step 5721, loss 0.0157988, acc 0.984375
2017-09-09T15:54:16.752342: step 5722, loss 0.0316168, acc 0.984375
2017-09-09T15:54:17.050343: step 5723, loss 0.0559617, acc 0.96875
2017-09-09T15:54:17.465640: step 5724, loss 0.0289056, acc 0.96875
2017-09-09T15:54:17.769993: step 5725, loss 0.0239581, acc 0.984375
2017-09-09T15:54:18.045178: step 5726, loss 0.00219726, acc 1
2017-09-09T15:54:18.392206: step 5727, loss 0.00292934, acc 1
2017-09-09T15:54:18.692706: step 5728, loss 0.03854, acc 0.984375
2017-09-09T15:54:19.069565: step 5729, loss 0.0177654, acc 0.984375
2017-09-09T15:54:19.356305: step 5730, loss 0.0154609, acc 1
2017-09-09T15:54:19.657264: step 5731, loss 0.055712, acc 0.984375
2017-09-09T15:54:20.002553: step 5732, loss 0.00206958, acc 1
2017-09-09T15:54:20.294332: step 5733, loss 0.0226428, acc 1
2017-09-09T15:54:20.623036: step 5734, loss 0.0256025, acc 0.984375
2017-09-09T15:54:20.943634: step 5735, loss 0.0453555, acc 0.96875
2017-09-09T15:54:21.266199: step 5736, loss 0.0421278, acc 0.984375
2017-09-09T15:54:21.553825: step 5737, loss 0.0298705, acc 0.984375
2017-09-09T15:54:21.857946: step 5738, loss 0.0231024, acc 0.984375
2017-09-09T15:54:22.171092: step 5739, loss 0.00841211, acc 1
2017-09-09T15:54:22.461897: step 5740, loss 0.0586444, acc 0.96875
2017-09-09T15:54:22.822919: step 5741, loss 0.0295939, acc 0.984375
2017-09-09T15:54:23.103978: step 5742, loss 0.0350063, acc 0.984375
2017-09-09T15:54:23.437549: step 5743, loss 0.033347, acc 0.984375
2017-09-09T15:54:23.748187: step 5744, loss 0.0311986, acc 0.984375
2017-09-09T15:54:24.037088: step 5745, loss 0.00993897, acc 1
2017-09-09T15:54:24.350313: step 5746, loss 0.0252075, acc 0.984375
2017-09-09T15:54:24.655956: step 5747, loss 0.0213054, acc 1
2017-09-09T15:54:24.981996: step 5748, loss 0.000529644, acc 1
2017-09-09T15:54:25.283054: step 5749, loss 0.000664242, acc 1
2017-09-09T15:54:25.614050: step 5750, loss 0.0119957, acc 1

Evaluation:
2017-09-09T15:54:25.755777: step 5750, loss 2.44242, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5750

2017-09-09T15:54:29.595893: step 5751, loss 0.00287042, acc 1
2017-09-09T15:54:29.950425: step 5752, loss 0.100147, acc 0.953125
2017-09-09T15:54:30.332751: step 5753, loss 0.0365546, acc 0.984375
2017-09-09T15:54:30.611479: step 5754, loss 0.0306983, acc 0.984375
2017-09-09T15:54:30.923758: step 5755, loss 0.00210699, acc 1
2017-09-09T15:54:31.256315: step 5756, loss 0.0209389, acc 0.984375
2017-09-09T15:54:31.520480: step 5757, loss 0.00933496, acc 1
2017-09-09T15:54:31.872337: step 5758, loss 0.0110135, acc 1
2017-09-09T15:54:32.205450: step 5759, loss 0.028588, acc 1
2017-09-09T15:54:32.496526: step 5760, loss 0.00993849, acc 1
2017-09-09T15:54:32.813861: step 5761, loss 0.0041087, acc 1
2017-09-09T15:54:33.072662: step 5762, loss 0.0418383, acc 0.96875
2017-09-09T15:54:33.406742: step 5763, loss 0.00918801, acc 1
2017-09-09T15:54:33.736599: step 5764, loss 0.00123775, acc 1
2017-09-09T15:54:34.046752: step 5765, loss 0.0355871, acc 0.984375
2017-09-09T15:54:34.425196: step 5766, loss 0.133237, acc 0.9375
2017-09-09T15:54:34.726324: step 5767, loss 0.00600864, acc 1
2017-09-09T15:54:35.074011: step 5768, loss 0.045092, acc 0.96875
2017-09-09T15:54:35.760670: step 5769, loss 0.00650445, acc 1
2017-09-09T15:54:36.118440: step 5770, loss 0.117316, acc 0.953125
2017-09-09T15:54:36.422565: step 5771, loss 0.00576819, acc 1
2017-09-09T15:54:36.722411: step 5772, loss 0.0238022, acc 0.984375
2017-09-09T15:54:36.989397: step 5773, loss 0.0206086, acc 0.984375
2017-09-09T15:54:37.269543: step 5774, loss 0.00267801, acc 1
2017-09-09T15:54:37.555166: step 5775, loss 0.0164152, acc 0.984375
2017-09-09T15:54:37.855903: step 5776, loss 0.0277007, acc 0.984375
2017-09-09T15:54:38.161323: step 5777, loss 0.023826, acc 0.984375
2017-09-09T15:54:38.481061: step 5778, loss 0.0068319, acc 1
2017-09-09T15:54:38.791114: step 5779, loss 0.0160651, acc 0.984375
2017-09-09T15:54:39.168822: step 5780, loss 0.0201292, acc 1
2017-09-09T15:54:39.473454: step 5781, loss 0.0294628, acc 0.984375
2017-09-09T15:54:39.827335: step 5782, loss 0.0248429, acc 1
2017-09-09T15:54:40.165551: step 5783, loss 0.00173332, acc 1
2017-09-09T15:54:40.462763: step 5784, loss 0.0130398, acc 1
2017-09-09T15:54:40.825862: step 5785, loss 0.0184704, acc 1
2017-09-09T15:54:41.115457: step 5786, loss 0.109059, acc 0.953125
2017-09-09T15:54:41.534773: step 5787, loss 0.0236156, acc 1
2017-09-09T15:54:41.803796: step 5788, loss 0.00146933, acc 1
2017-09-09T15:54:42.116399: step 5789, loss 0.0132581, acc 1
2017-09-09T15:54:42.427745: step 5790, loss 0.00295003, acc 1
2017-09-09T15:54:42.736993: step 5791, loss 0.0332075, acc 1
2017-09-09T15:54:42.995738: step 5792, loss 0.0435384, acc 0.96875
2017-09-09T15:54:43.342372: step 5793, loss 0.00454443, acc 1
2017-09-09T15:54:43.687600: step 5794, loss 0.0795127, acc 0.953125
2017-09-09T15:54:43.991540: step 5795, loss 0.000847896, acc 1
2017-09-09T15:54:44.319272: step 5796, loss 0.0386374, acc 0.984375
2017-09-09T15:54:44.598636: step 5797, loss 0.00409616, acc 1
2017-09-09T15:54:44.933879: step 5798, loss 0.00851593, acc 1
2017-09-09T15:54:45.266497: step 5799, loss 0.0108958, acc 1
2017-09-09T15:54:45.593107: step 5800, loss 0.000908908, acc 1

Evaluation:
2017-09-09T15:54:45.698569: step 5800, loss 2.62917, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5800

2017-09-09T15:54:48.261225: step 5801, loss 0.0269901, acc 0.984375
2017-09-09T15:54:48.650617: step 5802, loss 0.0222786, acc 0.984375
2017-09-09T15:54:48.998997: step 5803, loss 0.0457624, acc 0.984375
2017-09-09T15:54:49.326703: step 5804, loss 0.102781, acc 0.96875
2017-09-09T15:54:49.706030: step 5805, loss 0.0655005, acc 0.984375
2017-09-09T15:54:50.017290: step 5806, loss 0.0136536, acc 1
2017-09-09T15:54:50.301825: step 5807, loss 0.0511021, acc 0.984375
2017-09-09T15:54:50.660624: step 5808, loss 0.0040094, acc 1
2017-09-09T15:54:50.943248: step 5809, loss 0.0114246, acc 1
2017-09-09T15:54:51.241095: step 5810, loss 0.0200187, acc 0.984375
2017-09-09T15:54:51.599784: step 5811, loss 0.0290141, acc 0.984375
2017-09-09T15:54:51.894315: step 5812, loss 0.0472124, acc 0.984375
2017-09-09T15:54:52.229706: step 5813, loss 0.00143273, acc 1
2017-09-09T15:54:52.532662: step 5814, loss 0.0109005, acc 1
2017-09-09T15:54:52.836732: step 5815, loss 0.0150593, acc 1
2017-09-09T15:54:53.152132: step 5816, loss 0.0568534, acc 0.984375
2017-09-09T15:54:53.449532: step 5817, loss 0.00871509, acc 1
2017-09-09T15:54:53.789486: step 5818, loss 0.0319495, acc 0.984375
2017-09-09T15:54:54.128167: step 5819, loss 0.00475423, acc 1
2017-09-09T15:54:54.460308: step 5820, loss 0.0394335, acc 0.984375
2017-09-09T15:54:54.854395: step 5821, loss 0.0135657, acc 1
2017-09-09T15:54:55.134486: step 5822, loss 0.0410069, acc 0.96875
2017-09-09T15:54:55.425260: step 5823, loss 0.0245775, acc 0.984375
2017-09-09T15:54:55.726955: step 5824, loss 0.0377138, acc 0.96875
2017-09-09T15:54:56.017054: step 5825, loss 0.010856, acc 1
2017-09-09T15:54:56.301295: step 5826, loss 0.0117034, acc 1
2017-09-09T15:54:56.651757: step 5827, loss 0.0659877, acc 0.96875
2017-09-09T15:54:56.988639: step 5828, loss 0.024316, acc 1
2017-09-09T15:54:57.306213: step 5829, loss 0.0460295, acc 0.96875
2017-09-09T15:54:57.589703: step 5830, loss 0.00946536, acc 1
2017-09-09T15:54:57.953131: step 5831, loss 0.0636855, acc 0.953125
2017-09-09T15:54:58.324888: step 5832, loss 0.0141943, acc 1
2017-09-09T15:54:58.721486: step 5833, loss 0.0176503, acc 1
2017-09-09T15:54:59.122799: step 5834, loss 0.00166088, acc 1
2017-09-09T15:54:59.459936: step 5835, loss 0.0117888, acc 1
2017-09-09T15:54:59.823251: step 5836, loss 0.0819155, acc 0.96875
2017-09-09T15:55:00.165886: step 5837, loss 0.00227268, acc 1
2017-09-09T15:55:00.485361: step 5838, loss 0.0378195, acc 0.984375
2017-09-09T15:55:00.812755: step 5839, loss 0.0585474, acc 0.984375
2017-09-09T15:55:01.192131: step 5840, loss 0.000830416, acc 1
2017-09-09T15:55:01.493639: step 5841, loss 0.0104842, acc 1
2017-09-09T15:55:01.815605: step 5842, loss 0.027258, acc 0.984375
2017-09-09T15:55:02.159592: step 5843, loss 0.0386034, acc 0.96875
2017-09-09T15:55:02.481689: step 5844, loss 0.00257366, acc 1
2017-09-09T15:55:02.871206: step 5845, loss 0.0163519, acc 1
2017-09-09T15:55:03.173918: step 5846, loss 0.00707826, acc 1
2017-09-09T15:55:03.496875: step 5847, loss 0.0313853, acc 1
2017-09-09T15:55:03.845103: step 5848, loss 0.0381523, acc 0.96875
2017-09-09T15:55:04.137059: step 5849, loss 0.0267868, acc 0.984375
2017-09-09T15:55:04.457863: step 5850, loss 0.00101823, acc 1

Evaluation:
2017-09-09T15:55:04.534322: step 5850, loss 2.03082, acc 0.351079

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5850

2017-09-09T15:55:07.902421: step 5851, loss 0.0259291, acc 1
2017-09-09T15:55:08.265716: step 5852, loss 0.0136337, acc 1
2017-09-09T15:55:08.599460: step 5853, loss 0.00700025, acc 1
2017-09-09T15:55:08.877861: step 5854, loss 0.051177, acc 0.96875
2017-09-09T15:55:09.249984: step 5855, loss 0.000549354, acc 1
2017-09-09T15:55:09.506418: step 5856, loss 0.00146268, acc 1
2017-09-09T15:55:09.938895: step 5857, loss 0.0430981, acc 0.984375
2017-09-09T15:55:10.246986: step 5858, loss 0.0567624, acc 0.96875
2017-09-09T15:55:10.546566: step 5859, loss 0.0198939, acc 0.984375
2017-09-09T15:55:10.818441: step 5860, loss 0.0770779, acc 0.96875
2017-09-09T15:55:11.153825: step 5861, loss 0.00457891, acc 1
2017-09-09T15:55:11.421901: step 5862, loss 0.0280084, acc 0.984375
2017-09-09T15:55:11.707137: step 5863, loss 0.00350391, acc 1
2017-09-09T15:55:12.027187: step 5864, loss 0.00317265, acc 1
2017-09-09T15:55:12.333483: step 5865, loss 0.0162925, acc 1
2017-09-09T15:55:12.619106: step 5866, loss 0.00531608, acc 1
2017-09-09T15:55:12.976123: step 5867, loss 0.00822651, acc 1
2017-09-09T15:55:13.344351: step 5868, loss 0.0117048, acc 1
2017-09-09T15:55:13.743897: step 5869, loss 0.00611468, acc 1
2017-09-09T15:55:14.028557: step 5870, loss 0.0352897, acc 0.984375
2017-09-09T15:55:14.352368: step 5871, loss 0.0158675, acc 1
2017-09-09T15:55:14.683619: step 5872, loss 0.000462945, acc 1
2017-09-09T15:55:14.994620: step 5873, loss 0.0226373, acc 1
2017-09-09T15:55:15.334352: step 5874, loss 0.00236943, acc 1
2017-09-09T15:55:15.642076: step 5875, loss 0.0262784, acc 1
2017-09-09T15:55:15.999348: step 5876, loss 0.0347541, acc 0.984375
2017-09-09T15:55:16.293783: step 5877, loss 0.0962846, acc 0.96875
2017-09-09T15:55:16.612551: step 5878, loss 0.0183417, acc 1
2017-09-09T15:55:16.957836: step 5879, loss 0.0266748, acc 0.984375
2017-09-09T15:55:17.297487: step 5880, loss 0.0191734, acc 0.980392
2017-09-09T15:55:17.641732: step 5881, loss 0.0719695, acc 0.96875
2017-09-09T15:55:17.976847: step 5882, loss 0.0311165, acc 0.984375
2017-09-09T15:55:18.279676: step 5883, loss 0.00380901, acc 1
2017-09-09T15:55:18.560573: step 5884, loss 0.0234057, acc 0.984375
2017-09-09T15:55:18.878116: step 5885, loss 0.0480476, acc 0.96875
2017-09-09T15:55:19.252014: step 5886, loss 0.0866434, acc 0.953125
2017-09-09T15:55:19.560469: step 5887, loss 0.0319246, acc 0.984375
2017-09-09T15:55:19.861753: step 5888, loss 0.00730079, acc 1
2017-09-09T15:55:20.198365: step 5889, loss 0.0608198, acc 0.953125
2017-09-09T15:55:20.508499: step 5890, loss 0.0324976, acc 0.984375
2017-09-09T15:55:20.887287: step 5891, loss 0.008606, acc 1
2017-09-09T15:55:21.230638: step 5892, loss 0.0490178, acc 0.984375
2017-09-09T15:55:21.545749: step 5893, loss 0.0411707, acc 0.953125
2017-09-09T15:55:21.936795: step 5894, loss 0.0702699, acc 0.96875
2017-09-09T15:55:22.229857: step 5895, loss 0.0135212, acc 1
2017-09-09T15:55:22.588382: step 5896, loss 0.00595555, acc 1
2017-09-09T15:55:22.866318: step 5897, loss 0.0300945, acc 0.984375
2017-09-09T15:55:23.170590: step 5898, loss 0.000713519, acc 1
2017-09-09T15:55:23.533766: step 5899, loss 0.015259, acc 1
2017-09-09T15:55:23.855580: step 5900, loss 0.065537, acc 0.984375

Evaluation:
2017-09-09T15:55:23.982805: step 5900, loss 2.27428, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5900

2017-09-09T15:55:25.967428: step 5901, loss 0.0264528, acc 1
2017-09-09T15:55:26.243657: step 5902, loss 0.102614, acc 0.9375
2017-09-09T15:55:26.556918: step 5903, loss 0.00370313, acc 1
2017-09-09T15:55:26.836921: step 5904, loss 0.0306726, acc 0.984375
2017-09-09T15:55:27.098292: step 5905, loss 0.05624, acc 0.96875
2017-09-09T15:55:27.446458: step 5906, loss 0.0182066, acc 0.984375
2017-09-09T15:55:27.754216: step 5907, loss 0.000908508, acc 1
2017-09-09T15:55:28.036185: step 5908, loss 0.0215789, acc 0.984375
2017-09-09T15:55:28.348389: step 5909, loss 0.00966049, acc 1
2017-09-09T15:55:28.622454: step 5910, loss 0.0170977, acc 1
2017-09-09T15:55:28.996434: step 5911, loss 0.00402087, acc 1
2017-09-09T15:55:29.306699: step 5912, loss 0.00405378, acc 1
2017-09-09T15:55:29.588772: step 5913, loss 0.0932263, acc 0.953125
2017-09-09T15:55:29.924872: step 5914, loss 0.000515464, acc 1
2017-09-09T15:55:30.225859: step 5915, loss 0.0111989, acc 1
2017-09-09T15:55:30.570198: step 5916, loss 0.00438832, acc 1
2017-09-09T15:55:30.856737: step 5917, loss 0.0982822, acc 0.921875
2017-09-09T15:55:31.197785: step 5918, loss 0.015763, acc 1
2017-09-09T15:55:31.505690: step 5919, loss 0.0569076, acc 0.984375
2017-09-09T15:55:31.833537: step 5920, loss 0.0377366, acc 0.96875
2017-09-09T15:55:32.189374: step 5921, loss 0.0317644, acc 0.984375
2017-09-09T15:55:32.470211: step 5922, loss 0.0198641, acc 0.984375
2017-09-09T15:55:32.831600: step 5923, loss 0.0388137, acc 0.984375
2017-09-09T15:55:33.122595: step 5924, loss 0.0278532, acc 0.984375
2017-09-09T15:55:33.506414: step 5925, loss 0.0393409, acc 0.96875
2017-09-09T15:55:33.841025: step 5926, loss 0.0154519, acc 1
2017-09-09T15:55:34.144890: step 5927, loss 0.0061607, acc 1
2017-09-09T15:55:34.461472: step 5928, loss 0.0407081, acc 0.984375
2017-09-09T15:55:34.731287: step 5929, loss 0.0157577, acc 1
2017-09-09T15:55:35.081703: step 5930, loss 0.00293128, acc 1
2017-09-09T15:55:35.432931: step 5931, loss 0.00570321, acc 1
2017-09-09T15:55:35.737204: step 5932, loss 0.0333831, acc 0.984375
2017-09-09T15:55:36.003928: step 5933, loss 0.00142563, acc 1
2017-09-09T15:55:36.305899: step 5934, loss 0.0667724, acc 0.96875
2017-09-09T15:55:36.610825: step 5935, loss 0.0332143, acc 0.984375
2017-09-09T15:55:36.910382: step 5936, loss 0.00895396, acc 1
2017-09-09T15:55:37.305956: step 5937, loss 0.0374004, acc 1
2017-09-09T15:55:37.583369: step 5938, loss 0.0375487, acc 0.984375
2017-09-09T15:55:37.953271: step 5939, loss 0.0497238, acc 0.96875
2017-09-09T15:55:38.213235: step 5940, loss 0.0522857, acc 0.96875
2017-09-09T15:55:38.532463: step 5941, loss 0.0225109, acc 1
2017-09-09T15:55:38.877888: step 5942, loss 0.0178839, acc 0.984375
2017-09-09T15:55:39.168821: step 5943, loss 0.00990317, acc 1
2017-09-09T15:55:39.534522: step 5944, loss 0.0260706, acc 0.984375
2017-09-09T15:55:39.857909: step 5945, loss 0.0350794, acc 0.984375
2017-09-09T15:55:40.175131: step 5946, loss 0.0181055, acc 1
2017-09-09T15:55:40.432879: step 5947, loss 0.00553889, acc 1
2017-09-09T15:55:40.708385: step 5948, loss 0.021308, acc 0.984375
2017-09-09T15:55:41.041592: step 5949, loss 0.021146, acc 0.984375
2017-09-09T15:55:41.296589: step 5950, loss 0.00400518, acc 1

Evaluation:
2017-09-09T15:55:41.368559: step 5950, loss 2.53686, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-5950

2017-09-09T15:55:44.287116: step 5951, loss 0.0204096, acc 1
2017-09-09T15:55:44.601767: step 5952, loss 0.0333422, acc 0.984375
2017-09-09T15:55:44.905130: step 5953, loss 0.0444311, acc 0.96875
2017-09-09T15:55:45.242547: step 5954, loss 0.00682501, acc 1
2017-09-09T15:55:45.513781: step 5955, loss 0.0313937, acc 0.984375
2017-09-09T15:55:45.828305: step 5956, loss 0.0279252, acc 0.984375
2017-09-09T15:55:46.116579: step 5957, loss 0.00421627, acc 1
2017-09-09T15:55:46.501875: step 5958, loss 0.00682162, acc 1
2017-09-09T15:55:46.806600: step 5959, loss 0.112414, acc 0.953125
2017-09-09T15:55:47.083044: step 5960, loss 0.0274601, acc 0.984375
2017-09-09T15:55:47.380300: step 5961, loss 0.0303257, acc 0.984375
2017-09-09T15:55:47.689976: step 5962, loss 0.0462056, acc 0.984375
2017-09-09T15:55:48.023215: step 5963, loss 0.0413069, acc 0.984375
2017-09-09T15:55:48.349607: step 5964, loss 0.0324787, acc 1
2017-09-09T15:55:48.697720: step 5965, loss 0.059281, acc 0.984375
2017-09-09T15:55:49.058447: step 5966, loss 0.112506, acc 0.953125
2017-09-09T15:55:49.361669: step 5967, loss 0.0464831, acc 0.984375
2017-09-09T15:55:49.692823: step 5968, loss 0.0137279, acc 1
2017-09-09T15:55:50.039456: step 5969, loss 0.0205907, acc 0.984375
2017-09-09T15:55:50.336060: step 5970, loss 0.0367238, acc 0.984375
2017-09-09T15:55:50.698419: step 5971, loss 0.00746795, acc 1
2017-09-09T15:55:51.024052: step 5972, loss 0.0110846, acc 1
2017-09-09T15:55:51.350988: step 5973, loss 0.0102892, acc 1
2017-09-09T15:55:51.713565: step 5974, loss 0.0748665, acc 0.96875
2017-09-09T15:55:52.003187: step 5975, loss 0.0475478, acc 0.96875
2017-09-09T15:55:52.389898: step 5976, loss 0.001904, acc 1
2017-09-09T15:55:52.670589: step 5977, loss 0.0742396, acc 0.984375
2017-09-09T15:55:53.004645: step 5978, loss 0.0611023, acc 0.960784
2017-09-09T15:55:53.302194: step 5979, loss 0.0150207, acc 1
2017-09-09T15:55:53.627068: step 5980, loss 0.0187935, acc 1
2017-09-09T15:55:53.923667: step 5981, loss 0.00503117, acc 1
2017-09-09T15:55:54.269018: step 5982, loss 0.0162706, acc 1
2017-09-09T15:55:54.577234: step 5983, loss 0.0513763, acc 0.984375
2017-09-09T15:55:54.916092: step 5984, loss 0.0201915, acc 0.984375
2017-09-09T15:55:55.186162: step 5985, loss 0.0392422, acc 0.984375
2017-09-09T15:55:55.474645: step 5986, loss 0.034156, acc 0.984375
2017-09-09T15:55:55.769362: step 5987, loss 0.0554485, acc 0.96875
2017-09-09T15:55:56.099597: step 5988, loss 0.00742809, acc 1
2017-09-09T15:55:56.440513: step 5989, loss 0.00329869, acc 1
2017-09-09T15:55:56.803468: step 5990, loss 0.0236639, acc 0.984375
2017-09-09T15:55:57.097106: step 5991, loss 0.0765964, acc 0.984375
2017-09-09T15:55:57.464604: step 5992, loss 0.0385752, acc 0.984375
2017-09-09T15:55:57.771797: step 5993, loss 0.036213, acc 0.96875
2017-09-09T15:55:58.141135: step 5994, loss 0.0442686, acc 0.984375
2017-09-09T15:55:58.426487: step 5995, loss 0.0194569, acc 0.984375
2017-09-09T15:55:58.790153: step 5996, loss 0.0241049, acc 1
2017-09-09T15:55:59.149272: step 5997, loss 0.0336352, acc 0.984375
2017-09-09T15:55:59.472862: step 5998, loss 0.00517371, acc 1
2017-09-09T15:55:59.824841: step 5999, loss 0.0268872, acc 0.984375
2017-09-09T15:56:00.132139: step 6000, loss 0.0514209, acc 0.96875

Evaluation:
2017-09-09T15:56:00.292210: step 6000, loss 3.04298, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6000

2017-09-09T15:56:03.920862: step 6001, loss 0.0186249, acc 1
2017-09-09T15:56:04.268494: step 6002, loss 0.0449333, acc 0.96875
2017-09-09T15:56:04.641197: step 6003, loss 0.0119667, acc 1
2017-09-09T15:56:04.962441: step 6004, loss 0.00250531, acc 1
2017-09-09T15:56:05.242153: step 6005, loss 0.00121647, acc 1
2017-09-09T15:56:05.603360: step 6006, loss 0.075289, acc 0.96875
2017-09-09T15:56:05.902738: step 6007, loss 0.00802542, acc 1
2017-09-09T15:56:06.211292: step 6008, loss 0.0175428, acc 0.984375
2017-09-09T15:56:06.587306: step 6009, loss 0.000795358, acc 1
2017-09-09T15:56:06.883988: step 6010, loss 0.0123506, acc 1
2017-09-09T15:56:07.222749: step 6011, loss 0.00452316, acc 1
2017-09-09T15:56:07.551432: step 6012, loss 0.0209047, acc 0.984375
2017-09-09T15:56:07.847888: step 6013, loss 0.0214204, acc 0.984375
2017-09-09T15:56:08.160150: step 6014, loss 0.0831979, acc 0.9375
2017-09-09T15:56:08.523569: step 6015, loss 0.0689639, acc 0.96875
2017-09-09T15:56:08.838563: step 6016, loss 0.0190971, acc 1
2017-09-09T15:56:09.136767: step 6017, loss 0.00053141, acc 1
2017-09-09T15:56:09.409391: step 6018, loss 0.0721432, acc 0.96875
2017-09-09T15:56:09.768477: step 6019, loss 0.032863, acc 0.984375
2017-09-09T15:56:10.111300: step 6020, loss 0.00896636, acc 1
2017-09-09T15:56:10.393012: step 6021, loss 0.00292432, acc 1
2017-09-09T15:56:10.745236: step 6022, loss 0.0870991, acc 0.984375
2017-09-09T15:56:11.051790: step 6023, loss 0.0399022, acc 0.984375
2017-09-09T15:56:11.372119: step 6024, loss 0.0365465, acc 0.984375
2017-09-09T15:56:11.653373: step 6025, loss 0.00315326, acc 1
2017-09-09T15:56:12.033379: step 6026, loss 0.00485532, acc 1
2017-09-09T15:56:12.337255: step 6027, loss 0.0105631, acc 1
2017-09-09T15:56:12.637816: step 6028, loss 0.024771, acc 0.984375
2017-09-09T15:56:12.940428: step 6029, loss 0.0626298, acc 0.96875
2017-09-09T15:56:13.227145: step 6030, loss 0.0555385, acc 0.96875
2017-09-09T15:56:13.506600: step 6031, loss 0.0035698, acc 1
2017-09-09T15:56:13.819269: step 6032, loss 0.0344796, acc 0.984375
2017-09-09T15:56:14.143308: step 6033, loss 0.00994139, acc 1
2017-09-09T15:56:14.451762: step 6034, loss 0.064871, acc 0.96875
2017-09-09T15:56:14.818589: step 6035, loss 0.0244072, acc 0.984375
2017-09-09T15:56:15.109866: step 6036, loss 0.00113857, acc 1
2017-09-09T15:56:15.407744: step 6037, loss 0.0160352, acc 1
2017-09-09T15:56:15.755389: step 6038, loss 0.00779638, acc 1
2017-09-09T15:56:16.063870: step 6039, loss 0.0102421, acc 1
2017-09-09T15:56:16.375695: step 6040, loss 0.0139657, acc 1
2017-09-09T15:56:16.651960: step 6041, loss 0.0471048, acc 0.96875
2017-09-09T15:56:16.971960: step 6042, loss 0.0148579, acc 1
2017-09-09T15:56:17.271932: step 6043, loss 0.0883371, acc 0.984375
2017-09-09T15:56:17.568038: step 6044, loss 0.00444655, acc 1
2017-09-09T15:56:17.940227: step 6045, loss 0.011047, acc 1
2017-09-09T15:56:18.272305: step 6046, loss 0.00845506, acc 1
2017-09-09T15:56:18.625440: step 6047, loss 0.0109746, acc 1
2017-09-09T15:56:18.943458: step 6048, loss 0.0681529, acc 0.96875
2017-09-09T15:56:19.233213: step 6049, loss 0.0238371, acc 1
2017-09-09T15:56:19.584174: step 6050, loss 0.0275078, acc 0.984375

Evaluation:
2017-09-09T15:56:19.658234: step 6050, loss 1.98084, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6050

2017-09-09T15:56:21.917563: step 6051, loss 0.0753583, acc 0.984375
2017-09-09T15:56:22.358249: step 6052, loss 0.0199294, acc 1
2017-09-09T15:56:22.663401: step 6053, loss 0.0370639, acc 0.984375
2017-09-09T15:56:22.906463: step 6054, loss 0.013148, acc 1
2017-09-09T15:56:23.228627: step 6055, loss 0.0740422, acc 0.953125
2017-09-09T15:56:23.481055: step 6056, loss 0.00145652, acc 1
2017-09-09T15:56:23.743313: step 6057, loss 0.0286782, acc 0.984375
2017-09-09T15:56:24.024264: step 6058, loss 0.0367103, acc 0.984375
2017-09-09T15:56:24.327079: step 6059, loss 0.0621507, acc 0.96875
2017-09-09T15:56:24.587876: step 6060, loss 0.0300691, acc 0.984375
2017-09-09T15:56:24.944566: step 6061, loss 0.0205773, acc 1
2017-09-09T15:56:25.240621: step 6062, loss 0.00835757, acc 1
2017-09-09T15:56:25.583354: step 6063, loss 0.00860856, acc 1
2017-09-09T15:56:25.946236: step 6064, loss 0.00983979, acc 1
2017-09-09T15:56:26.289763: step 6065, loss 0.0404498, acc 0.984375
2017-09-09T15:56:26.621952: step 6066, loss 0.00492983, acc 1
2017-09-09T15:56:27.012476: step 6067, loss 0.0212026, acc 0.984375
2017-09-09T15:56:27.394458: step 6068, loss 0.0240942, acc 1
2017-09-09T15:56:27.738670: step 6069, loss 0.0516318, acc 0.96875
2017-09-09T15:56:28.080406: step 6070, loss 0.0102194, acc 1
2017-09-09T15:56:28.443502: step 6071, loss 0.0624106, acc 0.96875
2017-09-09T15:56:28.769150: step 6072, loss 0.0502029, acc 0.96875
2017-09-09T15:56:29.108901: step 6073, loss 0.0158228, acc 1
2017-09-09T15:56:29.505519: step 6074, loss 0.00582546, acc 1
2017-09-09T15:56:29.840645: step 6075, loss 0.0252177, acc 0.984375
2017-09-09T15:56:30.125107: step 6076, loss 0.032156, acc 0.980392
2017-09-09T15:56:30.468250: step 6077, loss 0.0161934, acc 1
2017-09-09T15:56:30.768602: step 6078, loss 0.0416043, acc 0.984375
2017-09-09T15:56:31.133245: step 6079, loss 0.0274234, acc 0.984375
2017-09-09T15:56:31.419842: step 6080, loss 0.0485652, acc 0.96875
2017-09-09T15:56:31.745935: step 6081, loss 0.0360118, acc 0.984375
2017-09-09T15:56:32.046473: step 6082, loss 0.0348458, acc 0.96875
2017-09-09T15:56:32.371794: step 6083, loss 0.0216206, acc 0.984375
2017-09-09T15:56:32.679525: step 6084, loss 0.00137246, acc 1
2017-09-09T15:56:32.978266: step 6085, loss 0.0351704, acc 0.984375
2017-09-09T15:56:33.362858: step 6086, loss 0.00111028, acc 1
2017-09-09T15:56:33.694004: step 6087, loss 0.0203939, acc 0.984375
2017-09-09T15:56:33.983688: step 6088, loss 0.0318789, acc 0.984375
2017-09-09T15:56:34.315624: step 6089, loss 0.00264914, acc 1
2017-09-09T15:56:34.585914: step 6090, loss 0.0458899, acc 0.984375
2017-09-09T15:56:34.966149: step 6091, loss 0.0636185, acc 0.96875
2017-09-09T15:56:35.244739: step 6092, loss 0.0096306, acc 1
2017-09-09T15:56:35.573028: step 6093, loss 0.026662, acc 0.984375
2017-09-09T15:56:35.906228: step 6094, loss 0.0309152, acc 0.984375
2017-09-09T15:56:36.220807: step 6095, loss 0.027589, acc 0.96875
2017-09-09T15:56:36.562006: step 6096, loss 0.0218962, acc 0.984375
2017-09-09T15:56:36.852657: step 6097, loss 0.00946997, acc 1
2017-09-09T15:56:37.280696: step 6098, loss 0.0208696, acc 1
2017-09-09T15:56:37.597151: step 6099, loss 0.00056872, acc 1
2017-09-09T15:56:37.873241: step 6100, loss 0.0308299, acc 0.984375

Evaluation:
2017-09-09T15:56:37.963029: step 6100, loss 2.1703, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6100

2017-09-09T15:56:41.161747: step 6101, loss 0.0213605, acc 1
2017-09-09T15:56:41.512007: step 6102, loss 0.0211425, acc 0.984375
2017-09-09T15:56:41.829909: step 6103, loss 0.0149, acc 1
2017-09-09T15:56:42.199416: step 6104, loss 0.026168, acc 0.984375
2017-09-09T15:56:42.539259: step 6105, loss 0.0456814, acc 0.984375
2017-09-09T15:56:42.816909: step 6106, loss 0.111769, acc 0.984375
2017-09-09T15:56:43.133894: step 6107, loss 0.0319262, acc 0.984375
2017-09-09T15:56:43.421549: step 6108, loss 0.0493765, acc 0.96875
2017-09-09T15:56:43.710942: step 6109, loss 0.04209, acc 0.984375
2017-09-09T15:56:44.030664: step 6110, loss 0.0424113, acc 1
2017-09-09T15:56:44.301985: step 6111, loss 0.0397423, acc 0.984375
2017-09-09T15:56:44.632782: step 6112, loss 0.00505702, acc 1
2017-09-09T15:56:44.972548: step 6113, loss 0.0458542, acc 0.984375
2017-09-09T15:56:45.294546: step 6114, loss 0.0205094, acc 1
2017-09-09T15:56:45.690111: step 6115, loss 0.016986, acc 0.984375
2017-09-09T15:56:46.022985: step 6116, loss 0.0238122, acc 0.984375
2017-09-09T15:56:46.331788: step 6117, loss 0.045404, acc 0.984375
2017-09-09T15:56:46.681878: step 6118, loss 0.0214592, acc 0.984375
2017-09-09T15:56:46.985880: step 6119, loss 0.0302385, acc 0.984375
2017-09-09T15:56:47.359230: step 6120, loss 0.0268667, acc 0.984375
2017-09-09T15:56:47.672307: step 6121, loss 0.0472862, acc 0.984375
2017-09-09T15:56:47.975238: step 6122, loss 0.0149683, acc 1
2017-09-09T15:56:48.293812: step 6123, loss 0.00170273, acc 1
2017-09-09T15:56:48.597641: step 6124, loss 0.00107646, acc 1
2017-09-09T15:56:48.958022: step 6125, loss 0.00211405, acc 1
2017-09-09T15:56:49.267238: step 6126, loss 0.0361091, acc 0.984375
2017-09-09T15:56:49.587224: step 6127, loss 0.0874787, acc 0.96875
2017-09-09T15:56:49.925514: step 6128, loss 0.0289216, acc 0.984375
2017-09-09T15:56:50.215454: step 6129, loss 0.00454774, acc 1
2017-09-09T15:56:50.536806: step 6130, loss 0.0270697, acc 0.984375
2017-09-09T15:56:50.864657: step 6131, loss 0.0729928, acc 0.96875
2017-09-09T15:56:51.189722: step 6132, loss 0.0364875, acc 0.984375
2017-09-09T15:56:51.512279: step 6133, loss 0.00850862, acc 1
2017-09-09T15:56:51.789990: step 6134, loss 0.0840458, acc 0.984375
2017-09-09T15:56:52.123135: step 6135, loss 0.0414917, acc 0.96875
2017-09-09T15:56:52.440574: step 6136, loss 0.00795564, acc 1
2017-09-09T15:56:52.745814: step 6137, loss 0.0138821, acc 1
2017-09-09T15:56:53.037216: step 6138, loss 0.00262345, acc 1
2017-09-09T15:56:53.380320: step 6139, loss 0.00799013, acc 1
2017-09-09T15:56:53.720267: step 6140, loss 0.0587168, acc 0.96875
2017-09-09T15:56:54.024641: step 6141, loss 0.0264414, acc 0.984375
2017-09-09T15:56:54.359143: step 6142, loss 0.0151312, acc 0.984375
2017-09-09T15:56:54.664994: step 6143, loss 0.0375525, acc 0.984375
2017-09-09T15:56:55.027836: step 6144, loss 0.0543686, acc 0.984375
2017-09-09T15:56:55.343054: step 6145, loss 0.0182407, acc 0.984375
2017-09-09T15:56:55.661294: step 6146, loss 0.0134806, acc 1
2017-09-09T15:56:55.928658: step 6147, loss 0.024857, acc 0.984375
2017-09-09T15:56:56.250660: step 6148, loss 0.0121933, acc 1
2017-09-09T15:56:56.616886: step 6149, loss 0.00456328, acc 1
2017-09-09T15:56:56.921231: step 6150, loss 0.0297464, acc 0.984375

Evaluation:
2017-09-09T15:56:57.053974: step 6150, loss 1.90525, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6150

2017-09-09T15:56:59.259748: step 6151, loss 0.0296835, acc 0.984375
2017-09-09T15:56:59.544549: step 6152, loss 0.00613609, acc 1
2017-09-09T15:56:59.939670: step 6153, loss 0.0173204, acc 0.984375
2017-09-09T15:57:00.233382: step 6154, loss 0.00292015, acc 1
2017-09-09T15:57:00.502766: step 6155, loss 0.0189445, acc 0.984375
2017-09-09T15:57:00.774192: step 6156, loss 0.0417151, acc 0.96875
2017-09-09T15:57:01.084218: step 6157, loss 0.00580291, acc 1
2017-09-09T15:57:01.401025: step 6158, loss 0.00647934, acc 1
2017-09-09T15:57:01.659815: step 6159, loss 0.0241257, acc 1
2017-09-09T15:57:01.994392: step 6160, loss 0.0255882, acc 0.984375
2017-09-09T15:57:02.282135: step 6161, loss 0.00426008, acc 1
2017-09-09T15:57:02.594229: step 6162, loss 0.0062863, acc 1
2017-09-09T15:57:02.873583: step 6163, loss 0.0452815, acc 0.984375
2017-09-09T15:57:03.165406: step 6164, loss 0.00540342, acc 1
2017-09-09T15:57:03.535803: step 6165, loss 0.0820532, acc 0.9375
2017-09-09T15:57:03.816975: step 6166, loss 0.000771907, acc 1
2017-09-09T15:57:04.139557: step 6167, loss 0.0168582, acc 0.984375
2017-09-09T15:57:04.461944: step 6168, loss 0.048263, acc 0.984375
2017-09-09T15:57:04.757213: step 6169, loss 0.0184356, acc 1
2017-09-09T15:57:05.045506: step 6170, loss 0.0347155, acc 0.984375
2017-09-09T15:57:05.347912: step 6171, loss 0.018525, acc 1
2017-09-09T15:57:05.735697: step 6172, loss 0.0443228, acc 1
2017-09-09T15:57:05.996944: step 6173, loss 0.011851, acc 1
2017-09-09T15:57:06.417477: step 6174, loss 0.0113661, acc 1
2017-09-09T15:57:06.674280: step 6175, loss 0.0207552, acc 0.984375
2017-09-09T15:57:06.972035: step 6176, loss 0.014528, acc 1
2017-09-09T15:57:07.273742: step 6177, loss 0.0267679, acc 0.984375
2017-09-09T15:57:07.578955: step 6178, loss 0.0517735, acc 0.984375
2017-09-09T15:57:07.887576: step 6179, loss 0.00290674, acc 1
2017-09-09T15:57:08.206642: step 6180, loss 0.0579765, acc 0.96875
2017-09-09T15:57:08.504327: step 6181, loss 0.00106811, acc 1
2017-09-09T15:57:08.843232: step 6182, loss 0.00160072, acc 1
2017-09-09T15:57:09.155628: step 6183, loss 0.00139914, acc 1
2017-09-09T15:57:09.440977: step 6184, loss 0.00870768, acc 1
2017-09-09T15:57:09.794639: step 6185, loss 0.0154383, acc 0.984375
2017-09-09T15:57:10.117797: step 6186, loss 0.00179682, acc 1
2017-09-09T15:57:10.415129: step 6187, loss 0.0767695, acc 0.953125
2017-09-09T15:57:10.760942: step 6188, loss 0.0391796, acc 0.984375
2017-09-09T15:57:11.125518: step 6189, loss 0.0211774, acc 0.984375
2017-09-09T15:57:11.505685: step 6190, loss 0.0243904, acc 1
2017-09-09T15:57:11.846586: step 6191, loss 0.0207645, acc 1
2017-09-09T15:57:12.152768: step 6192, loss 0.00688567, acc 1
2017-09-09T15:57:12.559572: step 6193, loss 0.0264836, acc 0.984375
2017-09-09T15:57:12.829009: step 6194, loss 0.00463754, acc 1
2017-09-09T15:57:13.198748: step 6195, loss 0.000870557, acc 1
2017-09-09T15:57:13.529665: step 6196, loss 0.0235261, acc 0.984375
2017-09-09T15:57:13.818237: step 6197, loss 0.00268015, acc 1
2017-09-09T15:57:14.161208: step 6198, loss 0.034673, acc 0.984375
2017-09-09T15:57:14.490010: step 6199, loss 0.0158522, acc 0.984375
2017-09-09T15:57:14.847398: step 6200, loss 0.00703635, acc 1

Evaluation:
2017-09-09T15:57:14.914517: step 6200, loss 2.13844, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6200

2017-09-09T15:57:17.994182: step 6201, loss 0.0343525, acc 0.984375
2017-09-09T15:57:18.297327: step 6202, loss 0.00106019, acc 1
2017-09-09T15:57:18.587308: step 6203, loss 0.00307238, acc 1
2017-09-09T15:57:18.878165: step 6204, loss 0.00296895, acc 1
2017-09-09T15:57:19.187640: step 6205, loss 0.00379, acc 1
2017-09-09T15:57:19.508073: step 6206, loss 0.0164678, acc 1
2017-09-09T15:57:19.785417: step 6207, loss 0.020299, acc 1
2017-09-09T15:57:20.215791: step 6208, loss 0.00908727, acc 1
2017-09-09T15:57:20.493091: step 6209, loss 0.00791397, acc 1
2017-09-09T15:57:20.764286: step 6210, loss 0.00983155, acc 1
2017-09-09T15:57:21.116135: step 6211, loss 0.0539359, acc 0.96875
2017-09-09T15:57:21.471711: step 6212, loss 0.00206009, acc 1
2017-09-09T15:57:21.779250: step 6213, loss 0.00442364, acc 1
2017-09-09T15:57:22.056546: step 6214, loss 0.0454421, acc 0.96875
2017-09-09T15:57:22.489510: step 6215, loss 0.000540112, acc 1
2017-09-09T15:57:22.840165: step 6216, loss 0.00193202, acc 1
2017-09-09T15:57:23.152049: step 6217, loss 0.0672467, acc 0.984375
2017-09-09T15:57:23.706889: step 6218, loss 0.00427389, acc 1
2017-09-09T15:57:24.024851: step 6219, loss 0.0031686, acc 1
2017-09-09T15:57:24.371372: step 6220, loss 0.0489701, acc 0.96875
2017-09-09T15:57:24.688397: step 6221, loss 0.00829225, acc 1
2017-09-09T15:57:25.013699: step 6222, loss 0.0010507, acc 1
2017-09-09T15:57:25.345161: step 6223, loss 0.0874804, acc 0.953125
2017-09-09T15:57:25.662840: step 6224, loss 0.064696, acc 0.96875
2017-09-09T15:57:26.062929: step 6225, loss 0.0544037, acc 0.984375
2017-09-09T15:57:26.370530: step 6226, loss 0.0392859, acc 0.984375
2017-09-09T15:57:26.679746: step 6227, loss 0.0352942, acc 0.96875
2017-09-09T15:57:27.009144: step 6228, loss 0.0469436, acc 0.984375
2017-09-09T15:57:27.355899: step 6229, loss 0.024519, acc 1
2017-09-09T15:57:27.651154: step 6230, loss 0.00419233, acc 1
2017-09-09T15:57:27.927063: step 6231, loss 0.0108918, acc 1
2017-09-09T15:57:28.230225: step 6232, loss 0.00142834, acc 1
2017-09-09T15:57:28.581282: step 6233, loss 0.0247139, acc 0.984375
2017-09-09T15:57:28.862612: step 6234, loss 0.0113866, acc 1
2017-09-09T15:57:29.242324: step 6235, loss 0.00373077, acc 1
2017-09-09T15:57:29.579523: step 6236, loss 0.0409953, acc 0.96875
2017-09-09T15:57:29.866640: step 6237, loss 0.00936635, acc 1
2017-09-09T15:57:30.179823: step 6238, loss 0.00334901, acc 1
2017-09-09T15:57:30.502022: step 6239, loss 0.0139644, acc 1
2017-09-09T15:57:30.874942: step 6240, loss 0.0418347, acc 0.96875
2017-09-09T15:57:31.179087: step 6241, loss 0.0921825, acc 0.96875
2017-09-09T15:57:31.551903: step 6242, loss 0.0633755, acc 0.953125
2017-09-09T15:57:31.889933: step 6243, loss 0.00300732, acc 1
2017-09-09T15:57:32.160517: step 6244, loss 0.0418986, acc 0.984375
2017-09-09T15:57:32.508066: step 6245, loss 0.0124158, acc 1
2017-09-09T15:57:32.792424: step 6246, loss 0.00416289, acc 1
2017-09-09T15:57:33.116552: step 6247, loss 0.0113383, acc 1
2017-09-09T15:57:33.384081: step 6248, loss 0.0257293, acc 1
2017-09-09T15:57:33.678382: step 6249, loss 0.0243855, acc 1
2017-09-09T15:57:33.995164: step 6250, loss 0.0100617, acc 1

Evaluation:
2017-09-09T15:57:34.096708: step 6250, loss 1.77574, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6250

2017-09-09T15:57:37.860767: step 6251, loss 0.016894, acc 0.984375
2017-09-09T15:57:38.175466: step 6252, loss 0.0260536, acc 0.984375
2017-09-09T15:57:38.491049: step 6253, loss 0.0470298, acc 0.96875
2017-09-09T15:57:38.786550: step 6254, loss 0.0331593, acc 0.984375
2017-09-09T15:57:39.067537: step 6255, loss 0.00459885, acc 1
2017-09-09T15:57:39.410927: step 6256, loss 0.0168613, acc 1
2017-09-09T15:57:39.692561: step 6257, loss 0.000583055, acc 1
2017-09-09T15:57:40.030763: step 6258, loss 0.0593843, acc 0.96875
2017-09-09T15:57:40.350575: step 6259, loss 0.0527378, acc 0.96875
2017-09-09T15:57:40.655491: step 6260, loss 0.00319674, acc 1
2017-09-09T15:57:40.984820: step 6261, loss 0.0294084, acc 0.984375
2017-09-09T15:57:41.372132: step 6262, loss 0.0283898, acc 1
2017-09-09T15:57:41.750181: step 6263, loss 0.0450981, acc 1
2017-09-09T15:57:42.072961: step 6264, loss 0.000948063, acc 1
2017-09-09T15:57:42.365618: step 6265, loss 0.0190337, acc 1
2017-09-09T15:57:42.763675: step 6266, loss 0.0319781, acc 0.984375
2017-09-09T15:57:43.074377: step 6267, loss 0.0429181, acc 0.984375
2017-09-09T15:57:43.391434: step 6268, loss 0.0233603, acc 0.984375
2017-09-09T15:57:43.693814: step 6269, loss 0.0136727, acc 1
2017-09-09T15:57:44.424822: step 6270, loss 0.0580375, acc 0.984375
2017-09-09T15:57:44.760112: step 6271, loss 0.00500965, acc 1
2017-09-09T15:57:45.091745: step 6272, loss 0.000546364, acc 1
2017-09-09T15:57:45.398917: step 6273, loss 0.00872402, acc 1
2017-09-09T15:57:45.713297: step 6274, loss 0.0436991, acc 0.984375
2017-09-09T15:57:46.109194: step 6275, loss 0.0126064, acc 1
2017-09-09T15:57:46.414804: step 6276, loss 0.00974881, acc 1
2017-09-09T15:57:46.747987: step 6277, loss 0.00580926, acc 1
2017-09-09T15:57:47.056146: step 6278, loss 0.0223319, acc 0.984375
2017-09-09T15:57:47.344879: step 6279, loss 0.00947374, acc 1
2017-09-09T15:57:47.764551: step 6280, loss 0.0019144, acc 1
2017-09-09T15:57:48.058518: step 6281, loss 0.00119818, acc 1
2017-09-09T15:57:48.381538: step 6282, loss 0.0260434, acc 0.984375
2017-09-09T15:57:48.767946: step 6283, loss 0.0428552, acc 0.984375
2017-09-09T15:57:49.844796: step 6284, loss 0.0277649, acc 0.984375
2017-09-09T15:57:50.116297: step 6285, loss 0.00269328, acc 1
2017-09-09T15:57:50.423867: step 6286, loss 0.0251549, acc 0.984375
2017-09-09T15:57:50.695458: step 6287, loss 0.00735425, acc 1
2017-09-09T15:57:50.978585: step 6288, loss 0.00386126, acc 1
2017-09-09T15:57:51.285999: step 6289, loss 0.00381656, acc 1
2017-09-09T15:57:51.558976: step 6290, loss 0.00190451, acc 1
2017-09-09T15:57:51.823765: step 6291, loss 0.00108132, acc 1
2017-09-09T15:57:52.112495: step 6292, loss 0.000904776, acc 1
2017-09-09T15:57:52.388956: step 6293, loss 0.0266178, acc 0.984375
2017-09-09T15:57:52.675024: step 6294, loss 0.0284063, acc 0.984375
2017-09-09T15:57:53.046558: step 6295, loss 0.0180368, acc 0.984375
2017-09-09T15:57:53.348505: step 6296, loss 0.0502177, acc 0.96875
2017-09-09T15:57:53.706261: step 6297, loss 0.0407611, acc 0.984375
2017-09-09T15:57:54.052985: step 6298, loss 0.0336628, acc 0.984375
2017-09-09T15:57:54.322691: step 6299, loss 0.0441818, acc 0.96875
2017-09-09T15:57:54.710249: step 6300, loss 0.00229883, acc 1

Evaluation:
2017-09-09T15:57:54.786558: step 6300, loss 2.60111, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6300

2017-09-09T15:57:57.171000: step 6301, loss 0.00793441, acc 1
2017-09-09T15:57:57.470175: step 6302, loss 0.0696967, acc 0.96875
2017-09-09T15:57:57.775462: step 6303, loss 0.000739861, acc 1
2017-09-09T15:57:58.067082: step 6304, loss 0.0130091, acc 1
2017-09-09T15:57:58.376529: step 6305, loss 0.0509749, acc 0.96875
2017-09-09T15:57:58.713646: step 6306, loss 0.000651842, acc 1
2017-09-09T15:57:58.999690: step 6307, loss 0.0146676, acc 1
2017-09-09T15:57:59.370287: step 6308, loss 0.10612, acc 0.96875
2017-09-09T15:57:59.664745: step 6309, loss 0.044428, acc 0.96875
2017-09-09T15:57:59.945933: step 6310, loss 0.00144838, acc 1
2017-09-09T15:58:00.300584: step 6311, loss 0.0285889, acc 0.984375
2017-09-09T15:58:00.608485: step 6312, loss 0.025405, acc 0.984375
2017-09-09T15:58:00.988755: step 6313, loss 0.0569901, acc 0.953125
2017-09-09T15:58:01.302005: step 6314, loss 0.030794, acc 0.96875
2017-09-09T15:58:01.585460: step 6315, loss 0.00922947, acc 1
2017-09-09T15:58:01.935990: step 6316, loss 0.0156488, acc 1
2017-09-09T15:58:02.228413: step 6317, loss 0.0137153, acc 0.984375
2017-09-09T15:58:02.543902: step 6318, loss 0.0255888, acc 1
2017-09-09T15:58:02.846535: step 6319, loss 0.0201499, acc 0.984375
2017-09-09T15:58:03.145576: step 6320, loss 0.0372303, acc 0.96875
2017-09-09T15:58:03.505138: step 6321, loss 0.026548, acc 0.984375
2017-09-09T15:58:03.822265: step 6322, loss 0.0310973, acc 0.984375
2017-09-09T15:58:04.198698: step 6323, loss 0.0185402, acc 0.984375
2017-09-09T15:58:04.479296: step 6324, loss 0.0480009, acc 0.96875
2017-09-09T15:58:04.845659: step 6325, loss 0.0194176, acc 1
2017-09-09T15:58:05.155636: step 6326, loss 0.000665657, acc 1
2017-09-09T15:58:05.545096: step 6327, loss 0.0146583, acc 0.984375
2017-09-09T15:58:05.845785: step 6328, loss 0.0304905, acc 0.984375
2017-09-09T15:58:06.164595: step 6329, loss 0.0109803, acc 1
2017-09-09T15:58:06.427551: step 6330, loss 0.0150608, acc 1
2017-09-09T15:58:06.719872: step 6331, loss 0.0128354, acc 1
2017-09-09T15:58:07.023400: step 6332, loss 0.0692209, acc 0.96875
2017-09-09T15:58:07.316544: step 6333, loss 0.0298687, acc 0.96875
2017-09-09T15:58:07.665676: step 6334, loss 0.0088988, acc 1
2017-09-09T15:58:07.965884: step 6335, loss 0.043603, acc 0.984375
2017-09-09T15:58:08.279455: step 6336, loss 0.0223196, acc 1
2017-09-09T15:58:08.565435: step 6337, loss 0.0147958, acc 1
2017-09-09T15:58:08.865646: step 6338, loss 0.0231539, acc 0.984375
2017-09-09T15:58:09.302823: step 6339, loss 0.013613, acc 1
2017-09-09T15:58:09.646330: step 6340, loss 0.0317279, acc 0.96875
2017-09-09T15:58:09.948221: step 6341, loss 0.00416284, acc 1
2017-09-09T15:58:10.270280: step 6342, loss 0.0951766, acc 0.953125
2017-09-09T15:58:10.577504: step 6343, loss 0.0719543, acc 0.984375
2017-09-09T15:58:10.987762: step 6344, loss 0.0177931, acc 1
2017-09-09T15:58:11.321956: step 6345, loss 0.0199551, acc 0.984375
2017-09-09T15:58:11.608663: step 6346, loss 0.0539559, acc 0.96875
2017-09-09T15:58:11.948705: step 6347, loss 0.0635678, acc 0.96875
2017-09-09T15:58:12.257599: step 6348, loss 0.00827062, acc 1
2017-09-09T15:58:12.633752: step 6349, loss 0.036288, acc 0.984375
2017-09-09T15:58:12.906290: step 6350, loss 0.00690412, acc 1

Evaluation:
2017-09-09T15:58:13.044626: step 6350, loss 2.53997, acc 0.351079

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6350

2017-09-09T15:58:16.452246: step 6351, loss 0.0166067, acc 0.984375
2017-09-09T15:58:16.821595: step 6352, loss 0.00141007, acc 1
2017-09-09T15:58:17.098800: step 6353, loss 0.0251774, acc 1
2017-09-09T15:58:17.419778: step 6354, loss 0.00318371, acc 1
2017-09-09T15:58:17.738015: step 6355, loss 0.0427232, acc 0.984375
2017-09-09T15:58:18.061276: step 6356, loss 0.100007, acc 0.96875
2017-09-09T15:58:18.368536: step 6357, loss 0.0169546, acc 0.984375
2017-09-09T15:58:18.649831: step 6358, loss 0.0212867, acc 0.984375
2017-09-09T15:58:18.950164: step 6359, loss 0.0397067, acc 0.96875
2017-09-09T15:58:19.228470: step 6360, loss 0.0159185, acc 0.984375
2017-09-09T15:58:19.632214: step 6361, loss 0.038836, acc 0.984375
2017-09-09T15:58:19.983502: step 6362, loss 0.0251863, acc 1
2017-09-09T15:58:20.303560: step 6363, loss 0.00109006, acc 1
2017-09-09T15:58:20.574108: step 6364, loss 0.00439412, acc 1
2017-09-09T15:58:20.893876: step 6365, loss 0.0262018, acc 0.984375
2017-09-09T15:58:21.178715: step 6366, loss 0.00977169, acc 1
2017-09-09T15:58:21.481154: step 6367, loss 0.0283756, acc 1
2017-09-09T15:58:21.852411: step 6368, loss 0.0412872, acc 0.96875
2017-09-09T15:58:22.208653: step 6369, loss 0.0343122, acc 0.984375
2017-09-09T15:58:22.495775: step 6370, loss 0.00291605, acc 1
2017-09-09T15:58:22.830503: step 6371, loss 0.0622474, acc 0.96875
2017-09-09T15:58:23.134625: step 6372, loss 0.045212, acc 0.96875
2017-09-09T15:58:23.475913: step 6373, loss 0.0224868, acc 0.984375
2017-09-09T15:58:23.788214: step 6374, loss 0.0486723, acc 0.96875
2017-09-09T15:58:24.104949: step 6375, loss 0.0119638, acc 1
2017-09-09T15:58:24.473680: step 6376, loss 0.00282147, acc 1
2017-09-09T15:58:24.758205: step 6377, loss 0.0451752, acc 0.96875
2017-09-09T15:58:25.096629: step 6378, loss 0.0194392, acc 1
2017-09-09T15:58:25.447625: step 6379, loss 0.0293053, acc 0.984375
2017-09-09T15:58:25.717027: step 6380, loss 0.0160202, acc 1
2017-09-09T15:58:26.058462: step 6381, loss 0.0185565, acc 1
2017-09-09T15:58:26.358660: step 6382, loss 0.00987715, acc 1
2017-09-09T15:58:26.743625: step 6383, loss 0.0189957, acc 1
2017-09-09T15:58:27.082391: step 6384, loss 0.0640579, acc 0.96875
2017-09-09T15:58:27.357521: step 6385, loss 0.0405441, acc 0.984375
2017-09-09T15:58:27.732589: step 6386, loss 0.00274844, acc 1
2017-09-09T15:58:28.025421: step 6387, loss 0.0345713, acc 1
2017-09-09T15:58:28.395688: step 6388, loss 0.011647, acc 1
2017-09-09T15:58:28.736724: step 6389, loss 0.00240939, acc 1
2017-09-09T15:58:29.048054: step 6390, loss 0.00343555, acc 1
2017-09-09T15:58:29.385013: step 6391, loss 0.0460906, acc 0.984375
2017-09-09T15:58:29.734468: step 6392, loss 0.00440443, acc 1
2017-09-09T15:58:30.041849: step 6393, loss 0.0190717, acc 1
2017-09-09T15:58:30.410728: step 6394, loss 0.0813366, acc 0.96875
2017-09-09T15:58:30.682495: step 6395, loss 0.000518675, acc 1
2017-09-09T15:58:31.041411: step 6396, loss 0.102361, acc 0.9375
2017-09-09T15:58:31.324219: step 6397, loss 0.00628944, acc 1
2017-09-09T15:58:31.673880: step 6398, loss 0.00370313, acc 1
2017-09-09T15:58:32.012526: step 6399, loss 0.0273597, acc 1
2017-09-09T15:58:32.285720: step 6400, loss 0.0029207, acc 1

Evaluation:
2017-09-09T15:58:32.384749: step 6400, loss 2.20307, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6400

2017-09-09T15:58:34.748911: step 6401, loss 0.038636, acc 0.984375
2017-09-09T15:58:35.087249: step 6402, loss 0.0201629, acc 1
2017-09-09T15:58:35.382479: step 6403, loss 0.0181868, acc 0.984375
2017-09-09T15:58:35.644747: step 6404, loss 0.000620897, acc 1
2017-09-09T15:58:35.938237: step 6405, loss 0.0541525, acc 0.96875
2017-09-09T15:58:36.268666: step 6406, loss 0.0277782, acc 0.96875
2017-09-09T15:58:36.562252: step 6407, loss 0.0118505, acc 1
2017-09-09T15:58:36.849691: step 6408, loss 0.00529138, acc 1
2017-09-09T15:58:37.201521: step 6409, loss 0.0398892, acc 0.96875
2017-09-09T15:58:37.484968: step 6410, loss 0.0311013, acc 1
2017-09-09T15:58:37.792596: step 6411, loss 0.0105134, acc 1
2017-09-09T15:58:38.158249: step 6412, loss 0.0162792, acc 1
2017-09-09T15:58:38.476220: step 6413, loss 0.00624603, acc 1
2017-09-09T15:58:38.842729: step 6414, loss 0.0555666, acc 0.96875
2017-09-09T15:58:39.201780: step 6415, loss 0.0052757, acc 1
2017-09-09T15:58:39.475919: step 6416, loss 0.0424268, acc 0.96875
2017-09-09T15:58:39.767362: step 6417, loss 0.00118825, acc 1
2017-09-09T15:58:40.047203: step 6418, loss 0.0330465, acc 1
2017-09-09T15:58:40.383127: step 6419, loss 0.0122378, acc 1
2017-09-09T15:58:40.702297: step 6420, loss 0.0734557, acc 0.96875
2017-09-09T15:58:41.034428: step 6421, loss 0.0091084, acc 1
2017-09-09T15:58:41.358491: step 6422, loss 0.000771666, acc 1
2017-09-09T15:58:41.652503: step 6423, loss 0.017918, acc 0.984375
2017-09-09T15:58:42.009099: step 6424, loss 0.00402301, acc 1
2017-09-09T15:58:42.304902: step 6425, loss 0.00283175, acc 1
2017-09-09T15:58:42.678007: step 6426, loss 0.00762928, acc 1
2017-09-09T15:58:42.984415: step 6427, loss 0.0186833, acc 0.984375
2017-09-09T15:58:43.299665: step 6428, loss 0.0719673, acc 0.953125
2017-09-09T15:58:43.646811: step 6429, loss 0.0426374, acc 0.984375
2017-09-09T15:58:43.995165: step 6430, loss 0.0338665, acc 0.984375
2017-09-09T15:58:44.367383: step 6431, loss 0.0159046, acc 1
2017-09-09T15:58:44.704544: step 6432, loss 0.057326, acc 0.984375
2017-09-09T15:58:44.999351: step 6433, loss 0.00255762, acc 1
2017-09-09T15:58:45.394000: step 6434, loss 0.0169388, acc 0.984375
2017-09-09T15:58:45.662033: step 6435, loss 0.00115117, acc 1
2017-09-09T15:58:45.989463: step 6436, loss 0.0186918, acc 0.984375
2017-09-09T15:58:46.289174: step 6437, loss 0.0328951, acc 0.984375
2017-09-09T15:58:46.635730: step 6438, loss 0.0515803, acc 0.984375
2017-09-09T15:58:47.037714: step 6439, loss 0.0035567, acc 1
2017-09-09T15:58:47.333572: step 6440, loss 0.0280185, acc 0.96875
2017-09-09T15:58:47.699913: step 6441, loss 0.00103713, acc 1
2017-09-09T15:58:47.992851: step 6442, loss 0.0294103, acc 0.984375
2017-09-09T15:58:48.471061: step 6443, loss 0.0486532, acc 0.984375
2017-09-09T15:58:48.777250: step 6444, loss 0.0201693, acc 0.984375
2017-09-09T15:58:49.089927: step 6445, loss 0.00736033, acc 1
2017-09-09T15:58:49.408628: step 6446, loss 0.0552242, acc 0.96875
2017-09-09T15:58:49.679665: step 6447, loss 0.0132817, acc 1
2017-09-09T15:58:50.013563: step 6448, loss 0.0299607, acc 0.984375
2017-09-09T15:58:50.336498: step 6449, loss 0.00946191, acc 1
2017-09-09T15:58:50.621637: step 6450, loss 0.00111901, acc 1

Evaluation:
2017-09-09T15:58:50.714842: step 6450, loss 2.41468, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6450

2017-09-09T15:58:53.415541: step 6451, loss 0.0140129, acc 1
2017-09-09T15:58:53.719855: step 6452, loss 0.0405212, acc 0.96875
2017-09-09T15:58:54.026843: step 6453, loss 0.0084732, acc 1
2017-09-09T15:58:54.339808: step 6454, loss 0.0171411, acc 1
2017-09-09T15:58:54.605233: step 6455, loss 0.0206176, acc 1
2017-09-09T15:58:54.933021: step 6456, loss 0.000494415, acc 1
2017-09-09T15:58:55.189754: step 6457, loss 0.0746197, acc 0.96875
2017-09-09T15:58:55.546176: step 6458, loss 0.0381014, acc 0.984375
2017-09-09T15:58:55.820378: step 6459, loss 0.0202779, acc 1
2017-09-09T15:58:56.150740: step 6460, loss 0.0424745, acc 0.984375
2017-09-09T15:58:56.500825: step 6461, loss 0.0137285, acc 1
2017-09-09T15:58:56.784394: step 6462, loss 0.0127225, acc 1
2017-09-09T15:58:57.123725: step 6463, loss 0.00284109, acc 1
2017-09-09T15:58:57.430857: step 6464, loss 0.0162757, acc 1
2017-09-09T15:58:57.794229: step 6465, loss 0.000660541, acc 1
2017-09-09T15:58:58.120900: step 6466, loss 0.00314083, acc 1
2017-09-09T15:58:58.416538: step 6467, loss 0.0567285, acc 0.953125
2017-09-09T15:58:58.721627: step 6468, loss 0.0771088, acc 0.960784
2017-09-09T15:58:58.999843: step 6469, loss 0.00792542, acc 1
2017-09-09T15:58:59.357399: step 6470, loss 0.000553263, acc 1
2017-09-09T15:58:59.660257: step 6471, loss 0.0175804, acc 1
2017-09-09T15:59:00.025671: step 6472, loss 0.0071742, acc 1
2017-09-09T15:59:00.352820: step 6473, loss 0.0112498, acc 1
2017-09-09T15:59:00.676088: step 6474, loss 0.0406896, acc 0.984375
2017-09-09T15:59:01.061494: step 6475, loss 0.024708, acc 1
2017-09-09T15:59:01.361506: step 6476, loss 0.00539733, acc 1
2017-09-09T15:59:01.660513: step 6477, loss 0.0160142, acc 1
2017-09-09T15:59:01.949822: step 6478, loss 0.0125055, acc 1
2017-09-09T15:59:02.241514: step 6479, loss 0.0390126, acc 0.984375
2017-09-09T15:59:02.566072: step 6480, loss 0.0241424, acc 0.984375
2017-09-09T15:59:02.853603: step 6481, loss 0.0107568, acc 1
2017-09-09T15:59:03.217337: step 6482, loss 0.0449151, acc 0.984375
2017-09-09T15:59:03.500673: step 6483, loss 0.00533779, acc 1
2017-09-09T15:59:03.871314: step 6484, loss 0.0783653, acc 0.96875
2017-09-09T15:59:04.185684: step 6485, loss 0.0133671, acc 1
2017-09-09T15:59:04.474008: step 6486, loss 0.0309552, acc 0.984375
2017-09-09T15:59:04.815815: step 6487, loss 0.00431252, acc 1
2017-09-09T15:59:05.118265: step 6488, loss 0.0329042, acc 0.984375
2017-09-09T15:59:05.425215: step 6489, loss 0.0190573, acc 1
2017-09-09T15:59:05.765309: step 6490, loss 0.072537, acc 0.953125
2017-09-09T15:59:06.107652: step 6491, loss 0.0509333, acc 0.96875
2017-09-09T15:59:06.389158: step 6492, loss 0.00173325, acc 1
2017-09-09T15:59:06.715355: step 6493, loss 0.0432034, acc 0.96875
2017-09-09T15:59:07.079934: step 6494, loss 0.00406517, acc 1
2017-09-09T15:59:07.475374: step 6495, loss 0.0142907, acc 0.984375
2017-09-09T15:59:07.749901: step 6496, loss 0.000457197, acc 1
2017-09-09T15:59:08.125230: step 6497, loss 0.01599, acc 1
2017-09-09T15:59:08.463696: step 6498, loss 0.0330286, acc 0.984375
2017-09-09T15:59:08.769760: step 6499, loss 0.00787028, acc 1
2017-09-09T15:59:09.089411: step 6500, loss 0.00113357, acc 1

Evaluation:
2017-09-09T15:59:09.169540: step 6500, loss 1.84006, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6500

2017-09-09T15:59:12.280660: step 6501, loss 0.00273046, acc 1
2017-09-09T15:59:12.589123: step 6502, loss 0.00183332, acc 1
2017-09-09T15:59:12.899814: step 6503, loss 0.0156851, acc 0.984375
2017-09-09T15:59:13.262788: step 6504, loss 0.0156747, acc 1
2017-09-09T15:59:13.578566: step 6505, loss 0.00651628, acc 1
2017-09-09T15:59:13.882003: step 6506, loss 0.0390496, acc 0.984375
2017-09-09T15:59:14.257688: step 6507, loss 0.072412, acc 0.953125
2017-09-09T15:59:14.556268: step 6508, loss 0.0203135, acc 0.984375
2017-09-09T15:59:14.922748: step 6509, loss 0.0143185, acc 1
2017-09-09T15:59:15.227483: step 6510, loss 0.016631, acc 0.984375
2017-09-09T15:59:15.522798: step 6511, loss 0.00536128, acc 1
2017-09-09T15:59:15.895594: step 6512, loss 0.0116314, acc 1
2017-09-09T15:59:16.235866: step 6513, loss 0.0208984, acc 1
2017-09-09T15:59:16.589069: step 6514, loss 0.0392256, acc 0.984375
2017-09-09T15:59:16.946305: step 6515, loss 0.0519907, acc 0.984375
2017-09-09T15:59:17.263103: step 6516, loss 0.0259465, acc 0.984375
2017-09-09T15:59:17.669735: step 6517, loss 0.00265506, acc 1
2017-09-09T15:59:17.962258: step 6518, loss 0.00951518, acc 1
2017-09-09T15:59:18.246035: step 6519, loss 0.010985, acc 1
2017-09-09T15:59:18.597648: step 6520, loss 0.025586, acc 0.984375
2017-09-09T15:59:18.874774: step 6521, loss 0.0830255, acc 0.96875
2017-09-09T15:59:19.146463: step 6522, loss 0.0446405, acc 0.984375
2017-09-09T15:59:19.444070: step 6523, loss 0.0463576, acc 0.984375
2017-09-09T15:59:19.745864: step 6524, loss 0.0210448, acc 0.984375
2017-09-09T15:59:20.026542: step 6525, loss 0.058314, acc 0.984375
2017-09-09T15:59:20.422510: step 6526, loss 0.0534986, acc 0.96875
2017-09-09T15:59:20.805140: step 6527, loss 0.022145, acc 0.984375
2017-09-09T15:59:21.155067: step 6528, loss 0.0568412, acc 0.953125
2017-09-09T15:59:21.481400: step 6529, loss 0.00499966, acc 1
2017-09-09T15:59:21.835583: step 6530, loss 0.0511458, acc 0.96875
2017-09-09T15:59:22.127331: step 6531, loss 0.00467, acc 1
2017-09-09T15:59:22.489699: step 6532, loss 0.0487133, acc 0.96875
2017-09-09T15:59:22.904292: step 6533, loss 0.0302669, acc 0.984375
2017-09-09T15:59:23.181680: step 6534, loss 0.0200932, acc 0.984375
2017-09-09T15:59:23.533874: step 6535, loss 0.028441, acc 0.984375
2017-09-09T15:59:23.804546: step 6536, loss 0.00520007, acc 1
2017-09-09T15:59:24.117166: step 6537, loss 0.0120358, acc 1
2017-09-09T15:59:24.481755: step 6538, loss 0.020276, acc 0.984375
2017-09-09T15:59:24.756672: step 6539, loss 0.0729287, acc 0.96875
2017-09-09T15:59:25.116901: step 6540, loss 0.00350618, acc 1
2017-09-09T15:59:25.405048: step 6541, loss 0.0174476, acc 1
2017-09-09T15:59:25.747854: step 6542, loss 0.0303926, acc 0.984375
2017-09-09T15:59:26.083087: step 6543, loss 0.0210652, acc 0.984375
2017-09-09T15:59:26.368744: step 6544, loss 0.00263154, acc 1
2017-09-09T15:59:26.733934: step 6545, loss 0.0152693, acc 1
2017-09-09T15:59:27.063579: step 6546, loss 0.0187891, acc 1
2017-09-09T15:59:27.374758: step 6547, loss 0.0316716, acc 0.984375
2017-09-09T15:59:27.713425: step 6548, loss 0.00689145, acc 1
2017-09-09T15:59:28.032065: step 6549, loss 0.0355981, acc 0.984375
2017-09-09T15:59:28.429975: step 6550, loss 0.0376977, acc 0.984375

Evaluation:
2017-09-09T15:59:28.523296: step 6550, loss 2.21502, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6550

2017-09-09T15:59:31.040577: step 6551, loss 0.00324509, acc 1
2017-09-09T15:59:31.330892: step 6552, loss 0.0466353, acc 0.96875
2017-09-09T15:59:31.613838: step 6553, loss 0.0598449, acc 0.984375
2017-09-09T15:59:31.988188: step 6554, loss 0.00851566, acc 1
2017-09-09T15:59:32.286513: step 6555, loss 0.0292333, acc 0.984375
2017-09-09T15:59:32.572890: step 6556, loss 0.0357061, acc 0.984375
2017-09-09T15:59:32.841952: step 6557, loss 0.000885569, acc 1
2017-09-09T15:59:33.220954: step 6558, loss 0.0227433, acc 0.984375
2017-09-09T15:59:33.548492: step 6559, loss 0.0884268, acc 0.984375
2017-09-09T15:59:33.882012: step 6560, loss 0.0339056, acc 1
2017-09-09T15:59:34.160713: step 6561, loss 0.00180949, acc 1
2017-09-09T15:59:34.448698: step 6562, loss 0.00234616, acc 1
2017-09-09T15:59:34.787419: step 6563, loss 0.0063729, acc 1
2017-09-09T15:59:35.107426: step 6564, loss 0.013206, acc 1
2017-09-09T15:59:35.400382: step 6565, loss 0.0511699, acc 0.984375
2017-09-09T15:59:35.782064: step 6566, loss 0.0603712, acc 0.980392
2017-09-09T15:59:36.158385: step 6567, loss 0.034752, acc 1
2017-09-09T15:59:36.464727: step 6568, loss 0.00207543, acc 1
2017-09-09T15:59:36.777602: step 6569, loss 0.0125921, acc 1
2017-09-09T15:59:37.068976: step 6570, loss 0.00943908, acc 1
2017-09-09T15:59:37.401879: step 6571, loss 0.00761424, acc 1
2017-09-09T15:59:37.699881: step 6572, loss 0.0144124, acc 1
2017-09-09T15:59:38.051285: step 6573, loss 0.0212274, acc 1
2017-09-09T15:59:38.353515: step 6574, loss 0.0550125, acc 0.96875
2017-09-09T15:59:38.653658: step 6575, loss 0.00131716, acc 1
2017-09-09T15:59:39.008558: step 6576, loss 0.0514382, acc 0.96875
2017-09-09T15:59:39.327535: step 6577, loss 0.00452542, acc 1
2017-09-09T15:59:39.721176: step 6578, loss 0.0160854, acc 0.984375
2017-09-09T15:59:40.046535: step 6579, loss 0.000289991, acc 1
2017-09-09T15:59:40.432796: step 6580, loss 0.031266, acc 1
2017-09-09T15:59:40.803596: step 6581, loss 0.0209783, acc 1
2017-09-09T15:59:41.105729: step 6582, loss 0.0699393, acc 0.953125
2017-09-09T15:59:41.412817: step 6583, loss 0.0235138, acc 0.984375
2017-09-09T15:59:41.752489: step 6584, loss 0.0580581, acc 0.96875
2017-09-09T15:59:42.053179: step 6585, loss 0.00374449, acc 1
2017-09-09T15:59:42.412096: step 6586, loss 0.0307272, acc 1
2017-09-09T15:59:42.726056: step 6587, loss 0.0443119, acc 0.96875
2017-09-09T15:59:43.129232: step 6588, loss 0.0188788, acc 0.984375
2017-09-09T15:59:43.476387: step 6589, loss 0.0232673, acc 0.984375
2017-09-09T15:59:43.774778: step 6590, loss 0.0386746, acc 0.984375
2017-09-09T15:59:44.138733: step 6591, loss 0.00588576, acc 1
2017-09-09T15:59:44.476958: step 6592, loss 0.00213016, acc 1
2017-09-09T15:59:44.782189: step 6593, loss 0.0366839, acc 0.96875
2017-09-09T15:59:45.134996: step 6594, loss 0.00220904, acc 1
2017-09-09T15:59:45.413958: step 6595, loss 0.0962754, acc 0.9375
2017-09-09T15:59:45.783429: step 6596, loss 0.0152047, acc 1
2017-09-09T15:59:46.067006: step 6597, loss 0.0355408, acc 0.984375
2017-09-09T15:59:46.416896: step 6598, loss 0.0370843, acc 0.984375
2017-09-09T15:59:46.816456: step 6599, loss 0.0154332, acc 0.984375
2017-09-09T15:59:47.144272: step 6600, loss 0.0206351, acc 0.984375

Evaluation:
2017-09-09T15:59:47.249619: step 6600, loss 2.19318, acc 0.352518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6600

2017-09-09T15:59:50.436484: step 6601, loss 0.0182702, acc 0.984375
2017-09-09T15:59:50.741049: step 6602, loss 0.000705891, acc 1
2017-09-09T15:59:51.012175: step 6603, loss 0.029677, acc 0.984375
2017-09-09T15:59:51.315174: step 6604, loss 0.0160262, acc 1
2017-09-09T15:59:51.602481: step 6605, loss 0.0122312, acc 1
2017-09-09T15:59:51.894473: step 6606, loss 0.00850873, acc 1
2017-09-09T15:59:52.192180: step 6607, loss 0.0751106, acc 0.96875
2017-09-09T15:59:52.470961: step 6608, loss 0.0366032, acc 0.984375
2017-09-09T15:59:52.798574: step 6609, loss 0.00029011, acc 1
2017-09-09T15:59:53.085224: step 6610, loss 0.0407835, acc 0.96875
2017-09-09T15:59:53.465174: step 6611, loss 0.0548405, acc 0.984375
2017-09-09T15:59:53.764171: step 6612, loss 0.00257825, acc 1
2017-09-09T15:59:54.078468: step 6613, loss 0.0310367, acc 0.984375
2017-09-09T15:59:54.392788: step 6614, loss 0.0213439, acc 0.984375
2017-09-09T15:59:54.674444: step 6615, loss 0.0462269, acc 0.984375
2017-09-09T15:59:54.988238: step 6616, loss 0.0147301, acc 1
2017-09-09T15:59:55.306714: step 6617, loss 0.0361223, acc 0.96875
2017-09-09T15:59:55.616617: step 6618, loss 0.00587629, acc 1
2017-09-09T15:59:55.915427: step 6619, loss 0.0229338, acc 0.984375
2017-09-09T15:59:56.248250: step 6620, loss 0.0665703, acc 0.953125
2017-09-09T15:59:56.582054: step 6621, loss 0.0161229, acc 0.984375
2017-09-09T15:59:56.923726: step 6622, loss 0.012997, acc 1
2017-09-09T15:59:57.283828: step 6623, loss 0.00208572, acc 1
2017-09-09T15:59:57.564813: step 6624, loss 0.00173877, acc 1
2017-09-09T15:59:57.890954: step 6625, loss 0.020475, acc 1
2017-09-09T15:59:58.226216: step 6626, loss 0.0105171, acc 1
2017-09-09T15:59:58.570802: step 6627, loss 0.0154667, acc 1
2017-09-09T15:59:58.916393: step 6628, loss 0.00956096, acc 1
2017-09-09T15:59:59.188266: step 6629, loss 0.0288982, acc 0.984375
2017-09-09T15:59:59.551432: step 6630, loss 0.000924296, acc 1
2017-09-09T15:59:59.828745: step 6631, loss 0.00178568, acc 1
2017-09-09T16:00:00.159670: step 6632, loss 0.0350572, acc 0.984375
2017-09-09T16:00:00.529674: step 6633, loss 0.003145, acc 1
2017-09-09T16:00:00.800373: step 6634, loss 0.0274864, acc 0.984375
2017-09-09T16:00:01.169940: step 6635, loss 0.0339989, acc 0.984375
2017-09-09T16:00:01.474350: step 6636, loss 0.00215527, acc 1
2017-09-09T16:00:01.765786: step 6637, loss 0.0262487, acc 0.984375
2017-09-09T16:00:02.072576: step 6638, loss 0.0391125, acc 0.96875
2017-09-09T16:00:02.501127: step 6639, loss 0.00786453, acc 1
2017-09-09T16:00:02.801281: step 6640, loss 0.0125137, acc 1
2017-09-09T16:00:03.118540: step 6641, loss 0.0773058, acc 0.953125
2017-09-09T16:00:03.395823: step 6642, loss 0.030438, acc 0.984375
2017-09-09T16:00:03.738615: step 6643, loss 0.00193352, acc 1
2017-09-09T16:00:04.012805: step 6644, loss 0.00615782, acc 1
2017-09-09T16:00:04.332063: step 6645, loss 0.0646085, acc 0.953125
2017-09-09T16:00:04.639467: step 6646, loss 0.0499976, acc 0.96875
2017-09-09T16:00:04.939296: step 6647, loss 0.0355333, acc 1
2017-09-09T16:00:05.262576: step 6648, loss 0.0626211, acc 0.96875
2017-09-09T16:00:05.580986: step 6649, loss 0.0391861, acc 0.96875
2017-09-09T16:00:05.905917: step 6650, loss 0.0168944, acc 0.984375

Evaluation:
2017-09-09T16:00:06.041891: step 6650, loss 1.65144, acc 0.328058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6650

2017-09-09T16:00:08.227939: step 6651, loss 0.0261126, acc 1
2017-09-09T16:00:08.568586: step 6652, loss 0.0225354, acc 1
2017-09-09T16:00:08.891388: step 6653, loss 0.075973, acc 0.96875
2017-09-09T16:00:09.265627: step 6654, loss 0.0212263, acc 0.984375
2017-09-09T16:00:09.556869: step 6655, loss 0.011893, acc 1
2017-09-09T16:00:09.871094: step 6656, loss 0.0211499, acc 1
2017-09-09T16:00:10.273422: step 6657, loss 0.00325067, acc 1
2017-09-09T16:00:10.565418: step 6658, loss 0.0197901, acc 0.984375
2017-09-09T16:00:10.855315: step 6659, loss 0.0286701, acc 0.984375
2017-09-09T16:00:11.159102: step 6660, loss 0.0392129, acc 0.984375
2017-09-09T16:00:11.457767: step 6661, loss 0.0390971, acc 0.984375
2017-09-09T16:00:11.784452: step 6662, loss 0.0495344, acc 0.984375
2017-09-09T16:00:12.086603: step 6663, loss 0.0294844, acc 0.984375
2017-09-09T16:00:12.464008: step 6664, loss 0.00233943, acc 1
2017-09-09T16:00:12.776180: step 6665, loss 0.0409709, acc 0.96875
2017-09-09T16:00:13.051789: step 6666, loss 0.0233085, acc 0.984375
2017-09-09T16:00:13.430135: step 6667, loss 0.0106693, acc 1
2017-09-09T16:00:13.760905: step 6668, loss 0.154625, acc 0.9375
2017-09-09T16:00:14.106605: step 6669, loss 0.0111563, acc 1
2017-09-09T16:00:14.417434: step 6670, loss 0.0204534, acc 0.984375
2017-09-09T16:00:14.702425: step 6671, loss 0.0269406, acc 0.984375
2017-09-09T16:00:15.052356: step 6672, loss 0.00594214, acc 1
2017-09-09T16:00:15.468129: step 6673, loss 0.0342456, acc 0.984375
2017-09-09T16:00:15.748834: step 6674, loss 0.0283006, acc 0.984375
2017-09-09T16:00:16.111155: step 6675, loss 0.00276356, acc 1
2017-09-09T16:00:16.399821: step 6676, loss 0.0336447, acc 0.96875
2017-09-09T16:00:16.675888: step 6677, loss 0.00551064, acc 1
2017-09-09T16:00:17.020748: step 6678, loss 0.0259602, acc 0.984375
2017-09-09T16:00:17.299569: step 6679, loss 0.000974057, acc 1
2017-09-09T16:00:17.585876: step 6680, loss 0.00469758, acc 1
2017-09-09T16:00:17.883641: step 6681, loss 0.000768289, acc 1
2017-09-09T16:00:18.149346: step 6682, loss 0.00248721, acc 1
2017-09-09T16:00:18.475667: step 6683, loss 0.0260953, acc 0.984375
2017-09-09T16:00:18.818979: step 6684, loss 0.0033716, acc 1
2017-09-09T16:00:19.097013: step 6685, loss 0.0299386, acc 0.984375
2017-09-09T16:00:19.445512: step 6686, loss 0.0284376, acc 1
2017-09-09T16:00:19.711459: step 6687, loss 0.0762422, acc 0.96875
2017-09-09T16:00:20.049126: step 6688, loss 0.00323247, acc 1
2017-09-09T16:00:20.336352: step 6689, loss 0.0578336, acc 0.96875
2017-09-09T16:00:20.647744: step 6690, loss 0.00236051, acc 1
2017-09-09T16:00:20.948096: step 6691, loss 0.0410332, acc 0.984375
2017-09-09T16:00:21.274943: step 6692, loss 0.0272287, acc 0.984375
2017-09-09T16:00:21.614967: step 6693, loss 0.0068064, acc 1
2017-09-09T16:00:21.915033: step 6694, loss 0.0387931, acc 0.96875
2017-09-09T16:00:22.318376: step 6695, loss 0.0112107, acc 1
2017-09-09T16:00:22.615215: step 6696, loss 0.0639704, acc 0.96875
2017-09-09T16:00:22.937614: step 6697, loss 0.024424, acc 1
2017-09-09T16:00:23.272867: step 6698, loss 0.0313307, acc 0.984375
2017-09-09T16:00:23.606705: step 6699, loss 0.0261105, acc 1
2017-09-09T16:00:23.947988: step 6700, loss 0.0105732, acc 1

Evaluation:
2017-09-09T16:00:24.010197: step 6700, loss 1.65174, acc 0.316547

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6700

2017-09-09T16:00:27.117944: step 6701, loss 0.00233913, acc 1
2017-09-09T16:00:27.457914: step 6702, loss 0.00950675, acc 1
2017-09-09T16:00:27.741677: step 6703, loss 0.0548628, acc 0.984375
2017-09-09T16:00:28.067234: step 6704, loss 0.0216171, acc 0.984375
2017-09-09T16:00:28.357021: step 6705, loss 0.0206732, acc 1
2017-09-09T16:00:28.629799: step 6706, loss 0.0404011, acc 0.984375
2017-09-09T16:00:28.951639: step 6707, loss 0.058942, acc 0.984375
2017-09-09T16:00:29.253349: step 6708, loss 0.0138993, acc 1
2017-09-09T16:00:29.628364: step 6709, loss 0.0256802, acc 0.984375
2017-09-09T16:00:29.970349: step 6710, loss 0.0050304, acc 1
2017-09-09T16:00:30.294059: step 6711, loss 0.0282989, acc 0.984375
2017-09-09T16:00:30.634384: step 6712, loss 0.0305329, acc 0.984375
2017-09-09T16:00:30.942067: step 6713, loss 0.0295636, acc 0.984375
2017-09-09T16:00:31.328818: step 6714, loss 0.00398614, acc 1
2017-09-09T16:00:31.639434: step 6715, loss 0.0343752, acc 0.96875
2017-09-09T16:00:31.986053: step 6716, loss 0.0308991, acc 0.984375
2017-09-09T16:00:32.407763: step 6717, loss 0.00225348, acc 1
2017-09-09T16:00:32.737126: step 6718, loss 0.0386028, acc 0.984375
2017-09-09T16:00:33.033070: step 6719, loss 0.0175037, acc 0.984375
2017-09-09T16:00:33.343999: step 6720, loss 0.00135579, acc 1
2017-09-09T16:00:33.664363: step 6721, loss 0.0430864, acc 0.984375
2017-09-09T16:00:33.976604: step 6722, loss 0.0270921, acc 0.984375
2017-09-09T16:00:34.314450: step 6723, loss 0.00625849, acc 1
2017-09-09T16:00:34.648114: step 6724, loss 0.0769004, acc 0.953125
2017-09-09T16:00:34.996963: step 6725, loss 0.0388677, acc 0.984375
2017-09-09T16:00:35.408804: step 6726, loss 0.0264145, acc 0.984375
2017-09-09T16:00:35.713473: step 6727, loss 0.0718897, acc 0.96875
2017-09-09T16:00:36.063021: step 6728, loss 0.0267962, acc 0.984375
2017-09-09T16:00:36.455984: step 6729, loss 0.0835869, acc 0.96875
2017-09-09T16:00:36.748671: step 6730, loss 0.018098, acc 0.984375
2017-09-09T16:00:37.060534: step 6731, loss 0.0419239, acc 0.96875
2017-09-09T16:00:37.378004: step 6732, loss 0.0138256, acc 1
2017-09-09T16:00:37.663269: step 6733, loss 0.000624706, acc 1
2017-09-09T16:00:37.945788: step 6734, loss 0.0722717, acc 0.953125
2017-09-09T16:00:38.240171: step 6735, loss 0.0582716, acc 0.96875
2017-09-09T16:00:38.556693: step 6736, loss 0.0293128, acc 0.984375
2017-09-09T16:00:38.833760: step 6737, loss 0.00589159, acc 1
2017-09-09T16:00:39.188764: step 6738, loss 0.0618893, acc 0.96875
2017-09-09T16:00:39.487605: step 6739, loss 0.0209664, acc 0.984375
2017-09-09T16:00:39.856959: step 6740, loss 0.0403513, acc 0.984375
2017-09-09T16:00:40.214475: step 6741, loss 0.0147125, acc 0.984375
2017-09-09T16:00:40.514306: step 6742, loss 0.0332968, acc 1
2017-09-09T16:00:40.854429: step 6743, loss 0.0277312, acc 0.984375
2017-09-09T16:00:41.150067: step 6744, loss 0.0972685, acc 0.953125
2017-09-09T16:00:41.483537: step 6745, loss 0.046197, acc 0.96875
2017-09-09T16:00:41.817128: step 6746, loss 0.0410961, acc 0.96875
2017-09-09T16:00:42.149596: step 6747, loss 0.0142322, acc 1
2017-09-09T16:00:42.933647: step 6748, loss 0.00478701, acc 1
2017-09-09T16:00:43.271094: step 6749, loss 0.00738228, acc 1
2017-09-09T16:00:43.614822: step 6750, loss 0.0731893, acc 0.96875

Evaluation:
2017-09-09T16:00:43.729345: step 6750, loss 2.09665, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6750

2017-09-09T16:00:46.828145: step 6751, loss 0.0354317, acc 1
2017-09-09T16:00:47.152769: step 6752, loss 0.00243528, acc 1
2017-09-09T16:00:47.397145: step 6753, loss 0.0185967, acc 1
2017-09-09T16:00:47.960981: step 6754, loss 0.0658432, acc 0.96875
2017-09-09T16:00:48.244496: step 6755, loss 0.0596068, acc 0.96875
2017-09-09T16:00:48.620091: step 6756, loss 0.0213055, acc 0.984375
2017-09-09T16:00:48.909533: step 6757, loss 0.018728, acc 0.984375
2017-09-09T16:00:49.228149: step 6758, loss 0.0293545, acc 1
2017-09-09T16:00:49.565396: step 6759, loss 0.0279567, acc 0.984375
2017-09-09T16:00:49.872118: step 6760, loss 0.0355921, acc 0.984375
2017-09-09T16:00:50.265431: step 6761, loss 0.0398435, acc 1
2017-09-09T16:00:50.566319: step 6762, loss 0.00364718, acc 1
2017-09-09T16:00:50.854979: step 6763, loss 0.0515186, acc 0.984375
2017-09-09T16:00:51.183942: step 6764, loss 0.0492947, acc 0.96875
2017-09-09T16:00:51.495352: step 6765, loss 0.0406442, acc 0.984375
2017-09-09T16:00:51.800145: step 6766, loss 0.0430973, acc 0.96875
2017-09-09T16:00:52.083944: step 6767, loss 0.00197357, acc 1
2017-09-09T16:00:52.460768: step 6768, loss 0.0368104, acc 1
2017-09-09T16:00:52.744615: step 6769, loss 0.0174847, acc 1
2017-09-09T16:00:53.101603: step 6770, loss 0.0297022, acc 0.984375
2017-09-09T16:00:53.430582: step 6771, loss 0.016168, acc 1
2017-09-09T16:00:53.754846: step 6772, loss 0.012044, acc 1
2017-09-09T16:00:54.128035: step 6773, loss 0.0514975, acc 0.96875
2017-09-09T16:00:54.396970: step 6774, loss 0.00324928, acc 1
2017-09-09T16:00:54.703532: step 6775, loss 0.0564851, acc 0.96875
2017-09-09T16:00:55.004153: step 6776, loss 0.0329063, acc 1
2017-09-09T16:00:55.303335: step 6777, loss 0.00180906, acc 1
2017-09-09T16:00:55.667837: step 6778, loss 0.0382923, acc 0.96875
2017-09-09T16:00:55.948887: step 6779, loss 0.0806838, acc 0.96875
2017-09-09T16:00:56.286969: step 6780, loss 0.000570257, acc 1
2017-09-09T16:00:56.565518: step 6781, loss 0.00863777, acc 1
2017-09-09T16:00:56.947918: step 6782, loss 0.0244725, acc 0.984375
2017-09-09T16:00:57.292506: step 6783, loss 0.0144778, acc 1
2017-09-09T16:00:57.628810: step 6784, loss 0.0155665, acc 0.984375
2017-09-09T16:00:57.982863: step 6785, loss 0.0205837, acc 0.984375
2017-09-09T16:00:58.262523: step 6786, loss 0.0497784, acc 0.984375
2017-09-09T16:00:58.610557: step 6787, loss 0.0383959, acc 0.96875
2017-09-09T16:00:58.909379: step 6788, loss 0.0151915, acc 1
2017-09-09T16:00:59.214647: step 6789, loss 0.00966685, acc 1
2017-09-09T16:00:59.524718: step 6790, loss 0.0154203, acc 0.984375
2017-09-09T16:00:59.805366: step 6791, loss 0.0297696, acc 0.984375
2017-09-09T16:01:00.127750: step 6792, loss 0.0409165, acc 0.96875
2017-09-09T16:01:00.413934: step 6793, loss 0.00441326, acc 1
2017-09-09T16:01:00.788050: step 6794, loss 5.33893e-05, acc 1
2017-09-09T16:01:01.085921: step 6795, loss 0.0174286, acc 0.984375
2017-09-09T16:01:01.417764: step 6796, loss 0.0436177, acc 0.96875
2017-09-09T16:01:01.740306: step 6797, loss 0.00473702, acc 1
2017-09-09T16:01:02.160423: step 6798, loss 0.0196266, acc 0.984375
2017-09-09T16:01:02.493174: step 6799, loss 0.0064076, acc 1
2017-09-09T16:01:02.768479: step 6800, loss 0.0509431, acc 0.96875

Evaluation:
2017-09-09T16:01:02.854301: step 6800, loss 2.22514, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6800

2017-09-09T16:01:05.134571: step 6801, loss 0.00646058, acc 1
2017-09-09T16:01:05.427463: step 6802, loss 0.00320858, acc 1
2017-09-09T16:01:05.742238: step 6803, loss 0.0111331, acc 1
2017-09-09T16:01:06.129189: step 6804, loss 0.0260813, acc 0.984375
2017-09-09T16:01:06.393696: step 6805, loss 0.0215642, acc 0.984375
2017-09-09T16:01:06.711373: step 6806, loss 0.0492832, acc 0.96875
2017-09-09T16:01:07.005409: step 6807, loss 0.0462275, acc 0.96875
2017-09-09T16:01:07.292024: step 6808, loss 0.0163668, acc 0.984375
2017-09-09T16:01:07.659103: step 6809, loss 0.0150753, acc 0.984375
2017-09-09T16:01:07.937606: step 6810, loss 0.0469332, acc 0.984375
2017-09-09T16:01:08.305759: step 6811, loss 0.0286198, acc 0.984375
2017-09-09T16:01:08.606012: step 6812, loss 0.00779584, acc 1
2017-09-09T16:01:08.890180: step 6813, loss 0.0173513, acc 0.984375
2017-09-09T16:01:09.254360: step 6814, loss 0.0016591, acc 1
2017-09-09T16:01:09.544384: step 6815, loss 0.0333123, acc 0.984375
2017-09-09T16:01:09.883059: step 6816, loss 0.000292235, acc 1
2017-09-09T16:01:10.150278: step 6817, loss 0.0152275, acc 0.984375
2017-09-09T16:01:10.478323: step 6818, loss 0.0157392, acc 1
2017-09-09T16:01:10.770125: step 6819, loss 0.0232521, acc 0.984375
2017-09-09T16:01:11.077951: step 6820, loss 0.0252497, acc 0.984375
2017-09-09T16:01:11.426409: step 6821, loss 0.0198489, acc 1
2017-09-09T16:01:11.706926: step 6822, loss 0.0219536, acc 0.984375
2017-09-09T16:01:12.044939: step 6823, loss 0.0482219, acc 0.96875
2017-09-09T16:01:12.379014: step 6824, loss 0.0522898, acc 0.953125
2017-09-09T16:01:12.749032: step 6825, loss 0.0114536, acc 1
2017-09-09T16:01:13.732394: step 6826, loss 0.0146468, acc 1
2017-09-09T16:01:14.034718: step 6827, loss 0.0192846, acc 0.984375
2017-09-09T16:01:14.344767: step 6828, loss 0.0357355, acc 1
2017-09-09T16:01:14.662024: step 6829, loss 0.0306923, acc 0.984375
2017-09-09T16:01:14.970836: step 6830, loss 0.0610241, acc 0.953125
2017-09-09T16:01:15.296594: step 6831, loss 0.00697394, acc 1
2017-09-09T16:01:15.658376: step 6832, loss 0.0194485, acc 0.984375
2017-09-09T16:01:15.988347: step 6833, loss 0.00965868, acc 1
2017-09-09T16:01:16.298170: step 6834, loss 0.0156975, acc 1
2017-09-09T16:01:16.605718: step 6835, loss 0.0038268, acc 1
2017-09-09T16:01:16.956666: step 6836, loss 0.0947235, acc 0.9375
2017-09-09T16:01:17.273407: step 6837, loss 0.0282976, acc 0.984375
2017-09-09T16:01:17.542000: step 6838, loss 0.00295792, acc 1
2017-09-09T16:01:17.851032: step 6839, loss 0.0209118, acc 0.984375
2017-09-09T16:01:18.163706: step 6840, loss 0.00222157, acc 1
2017-09-09T16:01:18.461552: step 6841, loss 0.0136724, acc 1
2017-09-09T16:01:18.820611: step 6842, loss 0.00183093, acc 1
2017-09-09T16:01:19.161263: step 6843, loss 0.0260709, acc 0.984375
2017-09-09T16:01:19.510440: step 6844, loss 0.0469841, acc 0.96875
2017-09-09T16:01:19.841311: step 6845, loss 0.00237846, acc 1
2017-09-09T16:01:20.155367: step 6846, loss 0.0243526, acc 0.984375
2017-09-09T16:01:20.507993: step 6847, loss 0.0476919, acc 0.984375
2017-09-09T16:01:20.918775: step 6848, loss 0.0317159, acc 0.984375
2017-09-09T16:01:21.212308: step 6849, loss 0.0476126, acc 0.984375
2017-09-09T16:01:21.596551: step 6850, loss 0.000833871, acc 1

Evaluation:
2017-09-09T16:01:21.675998: step 6850, loss 2.67519, acc 0.352518

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6850

2017-09-09T16:01:24.543511: step 6851, loss 0.0563207, acc 0.953125
2017-09-09T16:01:24.925638: step 6852, loss 0.0284237, acc 0.984375
2017-09-09T16:01:25.239854: step 6853, loss 0.00419478, acc 1
2017-09-09T16:01:25.604657: step 6854, loss 0.117846, acc 0.921875
2017-09-09T16:01:25.903418: step 6855, loss 0.0209825, acc 0.984375
2017-09-09T16:01:26.278241: step 6856, loss 0.000850852, acc 1
2017-09-09T16:01:26.608499: step 6857, loss 0.00348728, acc 1
2017-09-09T16:01:26.903981: step 6858, loss 0.0415799, acc 0.984375
2017-09-09T16:01:27.281398: step 6859, loss 0.0468053, acc 0.953125
2017-09-09T16:01:27.618106: step 6860, loss 0.0236855, acc 0.980392
2017-09-09T16:01:27.927388: step 6861, loss 0.0426758, acc 0.96875
2017-09-09T16:01:28.280100: step 6862, loss 0.0202425, acc 0.984375
2017-09-09T16:01:28.589262: step 6863, loss 0.0752369, acc 0.984375
2017-09-09T16:01:28.902759: step 6864, loss 0.0225742, acc 0.984375
2017-09-09T16:01:29.223546: step 6865, loss 0.122367, acc 0.984375
2017-09-09T16:01:29.607672: step 6866, loss 0.0316385, acc 0.984375
2017-09-09T16:01:30.015743: step 6867, loss 0.00888842, acc 1
2017-09-09T16:01:30.306347: step 6868, loss 0.0376852, acc 0.984375
2017-09-09T16:01:30.666039: step 6869, loss 0.0438968, acc 0.96875
2017-09-09T16:01:30.999703: step 6870, loss 0.00220262, acc 1
2017-09-09T16:01:31.287180: step 6871, loss 0.00864827, acc 1
2017-09-09T16:01:31.689655: step 6872, loss 0.0276262, acc 0.984375
2017-09-09T16:01:31.967442: step 6873, loss 0.0215028, acc 0.984375
2017-09-09T16:01:32.375592: step 6874, loss 0.011716, acc 0.984375
2017-09-09T16:01:32.697205: step 6875, loss 0.0263267, acc 0.984375
2017-09-09T16:01:32.988862: step 6876, loss 0.0401324, acc 0.96875
2017-09-09T16:01:33.271060: step 6877, loss 0.0244086, acc 0.984375
2017-09-09T16:01:33.615993: step 6878, loss 0.041129, acc 1
2017-09-09T16:01:33.941606: step 6879, loss 0.00379074, acc 1
2017-09-09T16:01:34.262938: step 6880, loss 0.0083693, acc 1
2017-09-09T16:01:34.552463: step 6881, loss 0.020981, acc 0.984375
2017-09-09T16:01:34.845228: step 6882, loss 0.00369487, acc 1
2017-09-09T16:01:35.202550: step 6883, loss 0.0405873, acc 0.96875
2017-09-09T16:01:35.518504: step 6884, loss 0.00241634, acc 1
2017-09-09T16:01:35.892426: step 6885, loss 0.000300435, acc 1
2017-09-09T16:01:36.188484: step 6886, loss 0.0209898, acc 0.984375
2017-09-09T16:01:36.538741: step 6887, loss 0.0517073, acc 1
2017-09-09T16:01:36.862553: step 6888, loss 0.00672223, acc 1
2017-09-09T16:01:37.154994: step 6889, loss 0.017429, acc 1
2017-09-09T16:01:37.479571: step 6890, loss 0.0241497, acc 0.984375
2017-09-09T16:01:37.764075: step 6891, loss 0.0164455, acc 0.984375
2017-09-09T16:01:38.165802: step 6892, loss 0.0112626, acc 0.984375
2017-09-09T16:01:38.480046: step 6893, loss 0.0178936, acc 0.984375
2017-09-09T16:01:38.764199: step 6894, loss 0.00381752, acc 1
2017-09-09T16:01:39.053803: step 6895, loss 0.00110344, acc 1
2017-09-09T16:01:39.347976: step 6896, loss 0.0138943, acc 1
2017-09-09T16:01:39.677819: step 6897, loss 0.0419994, acc 0.96875
2017-09-09T16:01:39.949975: step 6898, loss 0.041893, acc 0.96875
2017-09-09T16:01:40.266718: step 6899, loss 0.0627733, acc 0.96875
2017-09-09T16:01:40.531497: step 6900, loss 0.00257594, acc 1

Evaluation:
2017-09-09T16:01:40.645672: step 6900, loss 1.94758, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6900

2017-09-09T16:01:42.961845: step 6901, loss 0.0533658, acc 0.96875
2017-09-09T16:01:43.246704: step 6902, loss 0.0217639, acc 0.984375
2017-09-09T16:01:43.575576: step 6903, loss 0.086781, acc 0.96875
2017-09-09T16:01:43.869187: step 6904, loss 0.0191194, acc 0.984375
2017-09-09T16:01:44.292618: step 6905, loss 0.00600232, acc 1
2017-09-09T16:01:44.628017: step 6906, loss 0.00108158, acc 1
2017-09-09T16:01:44.894739: step 6907, loss 0.010747, acc 1
2017-09-09T16:01:45.269996: step 6908, loss 0.0234895, acc 0.984375
2017-09-09T16:01:45.554691: step 6909, loss 0.00103387, acc 1
2017-09-09T16:01:45.844962: step 6910, loss 0.0244072, acc 0.984375
2017-09-09T16:01:46.166141: step 6911, loss 0.00197559, acc 1
2017-09-09T16:01:46.472621: step 6912, loss 0.0287425, acc 0.984375
2017-09-09T16:01:46.762537: step 6913, loss 0.000331707, acc 1
2017-09-09T16:01:47.109442: step 6914, loss 0.0122309, acc 1
2017-09-09T16:01:47.447023: step 6915, loss 0.00188778, acc 1
2017-09-09T16:01:47.746445: step 6916, loss 0.0297586, acc 1
2017-09-09T16:01:48.131929: step 6917, loss 0.0171898, acc 1
2017-09-09T16:01:48.396968: step 6918, loss 0.0111279, acc 1
2017-09-09T16:01:48.759412: step 6919, loss 0.0086902, acc 1
2017-09-09T16:01:49.056840: step 6920, loss 0.0368196, acc 0.984375
2017-09-09T16:01:49.352948: step 6921, loss 0.0995491, acc 0.953125
2017-09-09T16:01:49.741536: step 6922, loss 0.04163, acc 0.984375
2017-09-09T16:01:50.005503: step 6923, loss 0.00177569, acc 1
2017-09-09T16:01:50.367294: step 6924, loss 0.0245131, acc 0.984375
2017-09-09T16:01:50.664944: step 6925, loss 0.0321672, acc 0.984375
2017-09-09T16:01:50.965020: step 6926, loss 0.0190508, acc 1
2017-09-09T16:01:51.337050: step 6927, loss 0.00262324, acc 1
2017-09-09T16:01:51.629586: step 6928, loss 0.0050116, acc 1
2017-09-09T16:01:52.030004: step 6929, loss 0.0351615, acc 0.96875
2017-09-09T16:01:52.346909: step 6930, loss 0.0104148, acc 1
2017-09-09T16:01:52.660033: step 6931, loss 0.0103452, acc 1
2017-09-09T16:01:52.984163: step 6932, loss 0.0587008, acc 0.96875
2017-09-09T16:01:53.277145: step 6933, loss 0.0147701, acc 1
2017-09-09T16:01:53.615678: step 6934, loss 0.0246616, acc 0.984375
2017-09-09T16:01:53.923928: step 6935, loss 0.0179145, acc 1
2017-09-09T16:01:54.263969: step 6936, loss 0.00988891, acc 1
2017-09-09T16:01:54.589276: step 6937, loss 0.019998, acc 1
2017-09-09T16:01:54.875572: step 6938, loss 0.00883578, acc 1
2017-09-09T16:01:55.263971: step 6939, loss 0.00941277, acc 1
2017-09-09T16:01:55.539026: step 6940, loss 0.0210327, acc 1
2017-09-09T16:01:55.883756: step 6941, loss 0.0266591, acc 0.984375
2017-09-09T16:01:56.164241: step 6942, loss 0.0380253, acc 0.984375
2017-09-09T16:01:56.440251: step 6943, loss 0.0121198, acc 0.984375
2017-09-09T16:01:56.771116: step 6944, loss 0.0662391, acc 0.96875
2017-09-09T16:01:57.060151: step 6945, loss 0.0288951, acc 1
2017-09-09T16:01:57.480279: step 6946, loss 0.00201577, acc 1
2017-09-09T16:01:57.766768: step 6947, loss 0.0195377, acc 0.984375
2017-09-09T16:01:58.067450: step 6948, loss 0.0494091, acc 0.96875
2017-09-09T16:01:58.380526: step 6949, loss 0.021233, acc 1
2017-09-09T16:01:58.686928: step 6950, loss 0.00208617, acc 1

Evaluation:
2017-09-09T16:01:58.770466: step 6950, loss 3.34561, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-6950

2017-09-09T16:02:01.777334: step 6951, loss 0.0538787, acc 0.96875
2017-09-09T16:02:02.081943: step 6952, loss 0.018361, acc 0.984375
2017-09-09T16:02:02.343927: step 6953, loss 0.0152888, acc 0.984375
2017-09-09T16:02:02.616578: step 6954, loss 0.0346243, acc 0.984375
2017-09-09T16:02:02.920922: step 6955, loss 0.00455631, acc 1
2017-09-09T16:02:03.227496: step 6956, loss 0.0125131, acc 1
2017-09-09T16:02:03.595051: step 6957, loss 0.0236535, acc 1
2017-09-09T16:02:03.876786: step 6958, loss 0.0533962, acc 0.960784
2017-09-09T16:02:04.245369: step 6959, loss 0.054657, acc 0.96875
2017-09-09T16:02:04.564513: step 6960, loss 0.0334451, acc 1
2017-09-09T16:02:04.908754: step 6961, loss 0.00847357, acc 1
2017-09-09T16:02:05.206367: step 6962, loss 0.00287683, acc 1
2017-09-09T16:02:05.524170: step 6963, loss 0.0601019, acc 0.953125
2017-09-09T16:02:05.862842: step 6964, loss 0.0268855, acc 0.984375
2017-09-09T16:02:06.159818: step 6965, loss 0.0102556, acc 1
2017-09-09T16:02:06.521882: step 6966, loss 0.0120682, acc 1
2017-09-09T16:02:06.849849: step 6967, loss 0.0591093, acc 0.984375
2017-09-09T16:02:07.211604: step 6968, loss 0.0237731, acc 1
2017-09-09T16:02:07.524422: step 6969, loss 0.0436012, acc 0.984375
2017-09-09T16:02:07.822125: step 6970, loss 0.00184024, acc 1
2017-09-09T16:02:08.175050: step 6971, loss 0.0249012, acc 1
2017-09-09T16:02:08.490920: step 6972, loss 0.00220458, acc 1
2017-09-09T16:02:08.863414: step 6973, loss 0.00422544, acc 1
2017-09-09T16:02:09.165366: step 6974, loss 0.00163116, acc 1
2017-09-09T16:02:09.456856: step 6975, loss 0.0283992, acc 1
2017-09-09T16:02:09.754647: step 6976, loss 0.0235923, acc 0.984375
2017-09-09T16:02:10.084325: step 6977, loss 0.0085998, acc 1
2017-09-09T16:02:10.410572: step 6978, loss 0.0331311, acc 0.984375
2017-09-09T16:02:10.721614: step 6979, loss 0.00323344, acc 1
2017-09-09T16:02:11.091671: step 6980, loss 0.0089616, acc 1
2017-09-09T16:02:11.432741: step 6981, loss 0.00642758, acc 1
2017-09-09T16:02:11.725441: step 6982, loss 0.0560432, acc 0.96875
2017-09-09T16:02:12.049816: step 6983, loss 0.018354, acc 0.984375
2017-09-09T16:02:12.370529: step 6984, loss 0.014192, acc 1
2017-09-09T16:02:12.703885: step 6985, loss 0.00505363, acc 1
2017-09-09T16:02:13.026882: step 6986, loss 0.0180467, acc 0.984375
2017-09-09T16:02:13.320099: step 6987, loss 0.0319155, acc 0.984375
2017-09-09T16:02:13.626227: step 6988, loss 0.0514195, acc 0.96875
2017-09-09T16:02:13.984699: step 6989, loss 0.0394058, acc 0.96875
2017-09-09T16:02:14.291271: step 6990, loss 0.00501437, acc 1
2017-09-09T16:02:14.602484: step 6991, loss 0.00915216, acc 1
2017-09-09T16:02:14.873651: step 6992, loss 0.00288305, acc 1
2017-09-09T16:02:15.182508: step 6993, loss 0.0223834, acc 1
2017-09-09T16:02:15.503904: step 6994, loss 0.0674226, acc 0.96875
2017-09-09T16:02:15.806348: step 6995, loss 0.0236039, acc 0.984375
2017-09-09T16:02:16.191695: step 6996, loss 0.0475786, acc 0.96875
2017-09-09T16:02:16.554196: step 6997, loss 0.011899, acc 1
2017-09-09T16:02:16.898205: step 6998, loss 0.00386259, acc 1
2017-09-09T16:02:17.230066: step 6999, loss 0.00850738, acc 1
2017-09-09T16:02:17.566337: step 7000, loss 0.0194421, acc 0.984375

Evaluation:
2017-09-09T16:02:17.636831: step 7000, loss 2.69026, acc 0.351079

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7000

2017-09-09T16:02:21.391704: step 7001, loss 0.0176546, acc 0.984375
2017-09-09T16:02:21.761474: step 7002, loss 0.0143459, acc 1
2017-09-09T16:02:22.106281: step 7003, loss 0.0730372, acc 0.96875
2017-09-09T16:02:22.445110: step 7004, loss 0.0671418, acc 0.984375
2017-09-09T16:02:22.852959: step 7005, loss 0.0138649, acc 1
2017-09-09T16:02:23.163160: step 7006, loss 0.0019178, acc 1
2017-09-09T16:02:23.475529: step 7007, loss 0.0165305, acc 1
2017-09-09T16:02:23.844699: step 7008, loss 0.0457086, acc 0.984375
2017-09-09T16:02:24.151570: step 7009, loss 0.0208912, acc 0.984375
2017-09-09T16:02:24.442679: step 7010, loss 0.003558, acc 1
2017-09-09T16:02:24.798059: step 7011, loss 0.00137225, acc 1
2017-09-09T16:02:25.110945: step 7012, loss 0.00758498, acc 1
2017-09-09T16:02:25.432199: step 7013, loss 0.0069858, acc 1
2017-09-09T16:02:25.758023: step 7014, loss 0.019463, acc 1
2017-09-09T16:02:26.058629: step 7015, loss 0.00182548, acc 1
2017-09-09T16:02:26.385109: step 7016, loss 0.0208397, acc 0.984375
2017-09-09T16:02:26.696558: step 7017, loss 0.036201, acc 0.984375
2017-09-09T16:02:27.035772: step 7018, loss 0.00132292, acc 1
2017-09-09T16:02:27.385671: step 7019, loss 0.0315033, acc 0.984375
2017-09-09T16:02:27.666184: step 7020, loss 0.00666592, acc 1
2017-09-09T16:02:28.023319: step 7021, loss 0.00192392, acc 1
2017-09-09T16:02:28.316189: step 7022, loss 0.00800253, acc 1
2017-09-09T16:02:28.608315: step 7023, loss 0.0194688, acc 0.984375
2017-09-09T16:02:28.904921: step 7024, loss 0.0191548, acc 0.984375
2017-09-09T16:02:29.198806: step 7025, loss 0.0398429, acc 0.984375
2017-09-09T16:02:29.585921: step 7026, loss 0.0148039, acc 0.984375
2017-09-09T16:02:29.959671: step 7027, loss 0.0663131, acc 0.96875
2017-09-09T16:02:30.232733: step 7028, loss 0.00393357, acc 1
2017-09-09T16:02:30.551611: step 7029, loss 0.00842946, acc 1
2017-09-09T16:02:30.874007: step 7030, loss 0.010302, acc 1
2017-09-09T16:02:31.164942: step 7031, loss 0.0426814, acc 0.984375
2017-09-09T16:02:31.481548: step 7032, loss 0.0368504, acc 0.96875
2017-09-09T16:02:31.865023: step 7033, loss 0.050486, acc 0.984375
2017-09-09T16:02:32.180272: step 7034, loss 0.0131144, acc 1
2017-09-09T16:02:32.472309: step 7035, loss 0.0364531, acc 0.96875
2017-09-09T16:02:32.803596: step 7036, loss 0.0798151, acc 0.953125
2017-09-09T16:02:33.159888: step 7037, loss 0.00407636, acc 1
2017-09-09T16:02:33.457432: step 7038, loss 0.00328109, acc 1
2017-09-09T16:02:33.804056: step 7039, loss 0.0225045, acc 0.984375
2017-09-09T16:02:34.093072: step 7040, loss 0.00380762, acc 1
2017-09-09T16:02:34.390958: step 7041, loss 0.00489627, acc 1
2017-09-09T16:02:34.689877: step 7042, loss 0.00206103, acc 1
2017-09-09T16:02:35.031668: step 7043, loss 0.034997, acc 0.984375
2017-09-09T16:02:35.394438: step 7044, loss 0.0175296, acc 0.984375
2017-09-09T16:02:35.677214: step 7045, loss 0.0284725, acc 0.984375
2017-09-09T16:02:36.003769: step 7046, loss 0.0382505, acc 0.96875
2017-09-09T16:02:36.366980: step 7047, loss 0.0596233, acc 0.96875
2017-09-09T16:02:36.655993: step 7048, loss 0.0240838, acc 1
2017-09-09T16:02:37.042095: step 7049, loss 0.00635369, acc 1
2017-09-09T16:02:37.385065: step 7050, loss 0.0051856, acc 1

Evaluation:
2017-09-09T16:02:37.480424: step 7050, loss 1.94077, acc 0.335252

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7050

2017-09-09T16:02:39.642531: step 7051, loss 0.0578623, acc 0.96875
2017-09-09T16:02:39.941828: step 7052, loss 0.0144404, acc 1
2017-09-09T16:02:40.260091: step 7053, loss 0.0240248, acc 0.984375
2017-09-09T16:02:40.609229: step 7054, loss 0.0298067, acc 0.984375
2017-09-09T16:02:40.968607: step 7055, loss 0.0462433, acc 0.984375
2017-09-09T16:02:41.248954: step 7056, loss 0.00233792, acc 1
2017-09-09T16:02:41.632721: step 7057, loss 0.0164556, acc 0.984375
2017-09-09T16:02:41.927360: step 7058, loss 0.00729788, acc 1
2017-09-09T16:02:42.256875: step 7059, loss 0.0230559, acc 0.984375
2017-09-09T16:02:42.545525: step 7060, loss 0.0366426, acc 0.984375
2017-09-09T16:02:42.819832: step 7061, loss 0.0204951, acc 1
2017-09-09T16:02:43.149087: step 7062, loss 0.00122928, acc 1
2017-09-09T16:02:43.426353: step 7063, loss 0.0814089, acc 0.96875
2017-09-09T16:02:43.689581: step 7064, loss 0.0320552, acc 0.984375
2017-09-09T16:02:44.017673: step 7065, loss 0.0140397, acc 1
2017-09-09T16:02:44.317675: step 7066, loss 0.0262894, acc 0.984375
2017-09-09T16:02:44.591538: step 7067, loss 0.0179249, acc 1
2017-09-09T16:02:44.916944: step 7068, loss 0.0334283, acc 1
2017-09-09T16:02:45.223956: step 7069, loss 0.0310702, acc 0.984375
2017-09-09T16:02:45.556738: step 7070, loss 0.124867, acc 0.9375
2017-09-09T16:02:45.885536: step 7071, loss 0.0211157, acc 0.984375
2017-09-09T16:02:46.176435: step 7072, loss 0.0179249, acc 0.984375
2017-09-09T16:02:46.559849: step 7073, loss 0.0258229, acc 0.984375
2017-09-09T16:02:46.825578: step 7074, loss 0.0249185, acc 1
2017-09-09T16:02:47.161455: step 7075, loss 0.031934, acc 0.984375
2017-09-09T16:02:47.450639: step 7076, loss 0.000972474, acc 1
2017-09-09T16:02:47.819814: step 7077, loss 0.0271873, acc 0.984375
2017-09-09T16:02:48.171574: step 7078, loss 0.0396452, acc 0.984375
2017-09-09T16:02:48.481917: step 7079, loss 0.0297114, acc 0.984375
2017-09-09T16:02:48.830403: step 7080, loss 0.033268, acc 0.984375
2017-09-09T16:02:49.119647: step 7081, loss 0.00120039, acc 1
2017-09-09T16:02:49.438407: step 7082, loss 0.0680838, acc 0.96875
2017-09-09T16:02:49.782934: step 7083, loss 0.0517852, acc 0.953125
2017-09-09T16:02:50.126225: step 7084, loss 0.0280856, acc 0.984375
2017-09-09T16:02:50.462644: step 7085, loss 0.0268197, acc 1
2017-09-09T16:02:50.768037: step 7086, loss 0.0027158, acc 1
2017-09-09T16:02:51.085729: step 7087, loss 0.0221765, acc 0.984375
2017-09-09T16:02:51.376649: step 7088, loss 0.0230884, acc 1
2017-09-09T16:02:51.687491: step 7089, loss 0.00177988, acc 1
2017-09-09T16:02:52.081398: step 7090, loss 0.006737, acc 1
2017-09-09T16:02:52.379270: step 7091, loss 0.031262, acc 0.984375
2017-09-09T16:02:52.734823: step 7092, loss 0.0336277, acc 0.984375
2017-09-09T16:02:53.070483: step 7093, loss 0.0246523, acc 0.984375
2017-09-09T16:02:53.340492: step 7094, loss 0.00839033, acc 1
2017-09-09T16:02:53.667897: step 7095, loss 0.0347851, acc 0.984375
2017-09-09T16:02:53.952728: step 7096, loss 0.0682959, acc 0.96875
2017-09-09T16:02:54.352563: step 7097, loss 0.00585573, acc 1
2017-09-09T16:02:54.662237: step 7098, loss 0.0108382, acc 1
2017-09-09T16:02:54.985500: step 7099, loss 0.000449224, acc 1
2017-09-09T16:02:55.292452: step 7100, loss 0.0505854, acc 0.96875

Evaluation:
2017-09-09T16:02:55.374405: step 7100, loss 2.06556, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7100

2017-09-09T16:02:58.447923: step 7101, loss 0.0399346, acc 0.984375
2017-09-09T16:02:58.722570: step 7102, loss 0.0048931, acc 1
2017-09-09T16:02:59.077790: step 7103, loss 0.0217566, acc 0.984375
2017-09-09T16:02:59.436929: step 7104, loss 0.0215528, acc 1
2017-09-09T16:02:59.804563: step 7105, loss 0.0335275, acc 0.984375
2017-09-09T16:03:00.093327: step 7106, loss 0.0357463, acc 0.96875
2017-09-09T16:03:00.408390: step 7107, loss 0.0407579, acc 0.984375
2017-09-09T16:03:00.670809: step 7108, loss 0.0380873, acc 0.984375
2017-09-09T16:03:00.998225: step 7109, loss 0.0292501, acc 1
2017-09-09T16:03:01.293053: step 7110, loss 0.00158716, acc 1
2017-09-09T16:03:01.609763: step 7111, loss 0.0486852, acc 0.984375
2017-09-09T16:03:01.923543: step 7112, loss 0.00985882, acc 1
2017-09-09T16:03:02.252341: step 7113, loss 0.00692131, acc 1
2017-09-09T16:03:02.548485: step 7114, loss 0.0332464, acc 0.984375
2017-09-09T16:03:02.877130: step 7115, loss 0.0851343, acc 0.953125
2017-09-09T16:03:03.173189: step 7116, loss 0.0444159, acc 0.984375
2017-09-09T16:03:03.475945: step 7117, loss 0.0174437, acc 1
2017-09-09T16:03:03.788710: step 7118, loss 0.00388104, acc 1
2017-09-09T16:03:04.097093: step 7119, loss 0.0371305, acc 1
2017-09-09T16:03:04.415365: step 7120, loss 0.000438068, acc 1
2017-09-09T16:03:04.707568: step 7121, loss 0.0289851, acc 0.984375
2017-09-09T16:03:05.010277: step 7122, loss 0.00252191, acc 1
2017-09-09T16:03:05.321073: step 7123, loss 0.0255225, acc 0.984375
2017-09-09T16:03:05.622114: step 7124, loss 0.00626028, acc 1
2017-09-09T16:03:05.901941: step 7125, loss 0.0422443, acc 0.96875
2017-09-09T16:03:06.239990: step 7126, loss 0.0245241, acc 0.984375
2017-09-09T16:03:06.558506: step 7127, loss 0.00750104, acc 1
2017-09-09T16:03:06.886681: step 7128, loss 0.0438248, acc 0.96875
2017-09-09T16:03:07.195654: step 7129, loss 0.00235256, acc 1
2017-09-09T16:03:07.491717: step 7130, loss 0.0737285, acc 0.953125
2017-09-09T16:03:07.801496: step 7131, loss 0.0278161, acc 0.984375
2017-09-09T16:03:08.107112: step 7132, loss 0.00215743, acc 1
2017-09-09T16:03:08.419402: step 7133, loss 0.0194385, acc 1
2017-09-09T16:03:08.723792: step 7134, loss 0.0263907, acc 0.984375
2017-09-09T16:03:09.125536: step 7135, loss 0.00030034, acc 1
2017-09-09T16:03:09.465830: step 7136, loss 0.0284751, acc 0.984375
2017-09-09T16:03:09.768407: step 7137, loss 0.0286767, acc 0.984375
2017-09-09T16:03:10.149269: step 7138, loss 0.0237478, acc 0.984375
2017-09-09T16:03:10.487466: step 7139, loss 0.00712561, acc 1
2017-09-09T16:03:10.775764: step 7140, loss 0.0767216, acc 0.953125
2017-09-09T16:03:11.086712: step 7141, loss 0.0296664, acc 1
2017-09-09T16:03:11.395424: step 7142, loss 0.0223803, acc 0.984375
2017-09-09T16:03:11.708980: step 7143, loss 0.0179295, acc 0.984375
2017-09-09T16:03:12.056086: step 7144, loss 0.00606858, acc 1
2017-09-09T16:03:12.366768: step 7145, loss 0.00169349, acc 1
2017-09-09T16:03:12.704071: step 7146, loss 0.0445338, acc 0.984375
2017-09-09T16:03:12.977940: step 7147, loss 0.0168375, acc 1
2017-09-09T16:03:13.318093: step 7148, loss 0.0813146, acc 0.953125
2017-09-09T16:03:13.598129: step 7149, loss 0.00506221, acc 1
2017-09-09T16:03:13.968363: step 7150, loss 0.00230909, acc 1

Evaluation:
2017-09-09T16:03:14.095888: step 7150, loss 1.91107, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7150

2017-09-09T16:03:16.390313: step 7151, loss 0.00548552, acc 1
2017-09-09T16:03:16.707778: step 7152, loss 0.0251436, acc 0.984375
2017-09-09T16:03:17.002018: step 7153, loss 0.0340387, acc 0.984375
2017-09-09T16:03:17.278178: step 7154, loss 0.0171307, acc 1
2017-09-09T16:03:17.663865: step 7155, loss 0.0102924, acc 1
2017-09-09T16:03:17.929434: step 7156, loss 0.0418881, acc 0.984375
2017-09-09T16:03:18.248308: step 7157, loss 0.0524396, acc 0.953125
2017-09-09T16:03:18.654384: step 7158, loss 0.0240995, acc 0.984375
2017-09-09T16:03:18.955653: step 7159, loss 0.05316, acc 0.953125
2017-09-09T16:03:19.323257: step 7160, loss 0.0075714, acc 1
2017-09-09T16:03:19.659412: step 7161, loss 0.0242334, acc 1
2017-09-09T16:03:19.947965: step 7162, loss 0.0466742, acc 0.984375
2017-09-09T16:03:20.340553: step 7163, loss 0.00109014, acc 1
2017-09-09T16:03:20.674825: step 7164, loss 0.0194844, acc 1
2017-09-09T16:03:20.996548: step 7165, loss 0.0270176, acc 0.984375
2017-09-09T16:03:21.291137: step 7166, loss 0.0673144, acc 0.953125
2017-09-09T16:03:21.576654: step 7167, loss 0.0419439, acc 0.984375
2017-09-09T16:03:21.900415: step 7168, loss 0.0101303, acc 1
2017-09-09T16:03:22.189888: step 7169, loss 0.025535, acc 0.984375
2017-09-09T16:03:22.520085: step 7170, loss 0.0236148, acc 0.984375
2017-09-09T16:03:22.804181: step 7171, loss 0.0233711, acc 0.96875
2017-09-09T16:03:23.152796: step 7172, loss 0.0189664, acc 1
2017-09-09T16:03:23.448991: step 7173, loss 0.0491333, acc 0.984375
2017-09-09T16:03:23.738263: step 7174, loss 0.0253853, acc 0.984375
2017-09-09T16:03:24.083016: step 7175, loss 0.0325148, acc 0.984375
2017-09-09T16:03:24.371774: step 7176, loss 0.0509448, acc 0.96875
2017-09-09T16:03:24.715071: step 7177, loss 0.0628461, acc 0.953125
2017-09-09T16:03:25.035408: step 7178, loss 0.0148584, acc 0.984375
2017-09-09T16:03:25.383223: step 7179, loss 0.022823, acc 0.984375
2017-09-09T16:03:25.701472: step 7180, loss 0.0643136, acc 0.953125
2017-09-09T16:03:25.997511: step 7181, loss 0.063387, acc 0.984375
2017-09-09T16:03:26.333320: step 7182, loss 0.0200574, acc 0.984375
2017-09-09T16:03:26.629789: step 7183, loss 0.0443232, acc 0.984375
2017-09-09T16:03:26.976406: step 7184, loss 0.0210064, acc 0.984375
2017-09-09T16:03:27.260649: step 7185, loss 0.0114036, acc 1
2017-09-09T16:03:27.557911: step 7186, loss 0.00867751, acc 1
2017-09-09T16:03:27.907384: step 7187, loss 0.0180422, acc 0.984375
2017-09-09T16:03:28.232219: step 7188, loss 0.0466961, acc 0.984375
2017-09-09T16:03:28.609678: step 7189, loss 0.0812173, acc 0.953125
2017-09-09T16:03:28.890805: step 7190, loss 0.0204412, acc 1
2017-09-09T16:03:29.168394: step 7191, loss 0.0251853, acc 0.984375
2017-09-09T16:03:29.438253: step 7192, loss 0.0261122, acc 0.984375
2017-09-09T16:03:29.722594: step 7193, loss 0.0354096, acc 0.984375
2017-09-09T16:03:30.050417: step 7194, loss 0.024232, acc 0.984375
2017-09-09T16:03:30.392563: step 7195, loss 0.0045399, acc 1
2017-09-09T16:03:30.692088: step 7196, loss 0.0129651, acc 1
2017-09-09T16:03:30.961670: step 7197, loss 0.0760998, acc 0.9375
2017-09-09T16:03:31.373683: step 7198, loss 0.011123, acc 1
2017-09-09T16:03:31.696962: step 7199, loss 0.0573304, acc 0.984375
2017-09-09T16:03:32.040188: step 7200, loss 0.0413576, acc 0.96875

Evaluation:
2017-09-09T16:03:32.118326: step 7200, loss 2.15963, acc 0.164029

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7200

2017-09-09T16:03:34.766708: step 7201, loss 0.00729881, acc 1
2017-09-09T16:03:35.106232: step 7202, loss 0.00779455, acc 1
2017-09-09T16:03:35.432995: step 7203, loss 0.00737625, acc 1
2017-09-09T16:03:35.703483: step 7204, loss 0.0878036, acc 0.953125
2017-09-09T16:03:36.077402: step 7205, loss 0.0115346, acc 1
2017-09-09T16:03:36.406921: step 7206, loss 0.0697995, acc 0.953125
2017-09-09T16:03:36.696958: step 7207, loss 0.0251007, acc 0.984375
2017-09-09T16:03:37.013266: step 7208, loss 0.0558273, acc 0.96875
2017-09-09T16:03:37.341522: step 7209, loss 0.119246, acc 0.984375
2017-09-09T16:03:37.755937: step 7210, loss 0.0143517, acc 1
2017-09-09T16:03:38.052612: step 7211, loss 0.0171149, acc 0.984375
2017-09-09T16:03:38.401263: step 7212, loss 0.00308059, acc 1
2017-09-09T16:03:38.751051: step 7213, loss 0.0380048, acc 0.96875
2017-09-09T16:03:39.051542: step 7214, loss 0.0582753, acc 0.984375
2017-09-09T16:03:39.349282: step 7215, loss 0.0201498, acc 0.984375
2017-09-09T16:03:39.630828: step 7216, loss 0.028746, acc 0.984375
2017-09-09T16:03:39.963578: step 7217, loss 0.00167214, acc 1
2017-09-09T16:03:40.274107: step 7218, loss 0.0354602, acc 0.984375
2017-09-09T16:03:40.674383: step 7219, loss 0.00798175, acc 1
2017-09-09T16:03:41.017880: step 7220, loss 0.0107802, acc 1
2017-09-09T16:03:41.331132: step 7221, loss 0.0429847, acc 0.96875
2017-09-09T16:03:41.598880: step 7222, loss 0.00193556, acc 1
2017-09-09T16:03:41.920057: step 7223, loss 0.00348127, acc 1
2017-09-09T16:03:42.232892: step 7224, loss 0.0186089, acc 1
2017-09-09T16:03:42.500635: step 7225, loss 0.0130905, acc 0.984375
2017-09-09T16:03:42.840243: step 7226, loss 0.0477115, acc 0.96875
2017-09-09T16:03:43.177863: step 7227, loss 0.00151221, acc 1
2017-09-09T16:03:43.517687: step 7228, loss 0.0360396, acc 0.984375
2017-09-09T16:03:43.842427: step 7229, loss 0.0169165, acc 1
2017-09-09T16:03:44.126896: step 7230, loss 0.0213917, acc 0.984375
2017-09-09T16:03:44.453842: step 7231, loss 0.0259167, acc 0.984375
2017-09-09T16:03:44.750029: step 7232, loss 0.04801, acc 0.96875
2017-09-09T16:03:45.098694: step 7233, loss 0.0457111, acc 0.984375
2017-09-09T16:03:45.389535: step 7234, loss 0.0473864, acc 0.984375
2017-09-09T16:03:46.388826: step 7235, loss 0.051262, acc 0.984375
2017-09-09T16:03:46.722198: step 7236, loss 0.0310016, acc 0.984375
2017-09-09T16:03:47.035125: step 7237, loss 0.0179513, acc 0.984375
2017-09-09T16:03:47.377447: step 7238, loss 0.00758199, acc 1
2017-09-09T16:03:47.699565: step 7239, loss 0.00276153, acc 1
2017-09-09T16:03:48.039793: step 7240, loss 0.0221311, acc 0.984375
2017-09-09T16:03:48.314834: step 7241, loss 0.0187167, acc 0.984375
2017-09-09T16:03:48.607016: step 7242, loss 0.00213921, acc 1
2017-09-09T16:03:48.908015: step 7243, loss 0.000945747, acc 1
2017-09-09T16:03:49.230141: step 7244, loss 0.0108757, acc 1
2017-09-09T16:03:49.577976: step 7245, loss 0.0299715, acc 0.984375
2017-09-09T16:03:49.874222: step 7246, loss 0.0143443, acc 1
2017-09-09T16:03:50.262375: step 7247, loss 0.00842769, acc 1
2017-09-09T16:03:50.595525: step 7248, loss 0.0156708, acc 1
2017-09-09T16:03:51.704797: step 7249, loss 0.00398195, acc 1
2017-09-09T16:03:52.020378: step 7250, loss 0.00392552, acc 1

Evaluation:
2017-09-09T16:03:52.141301: step 7250, loss 2.1393, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7250

2017-09-09T16:03:55.228516: step 7251, loss 0.0736385, acc 0.984375
2017-09-09T16:03:55.538801: step 7252, loss 0.00373393, acc 1
2017-09-09T16:03:55.848277: step 7253, loss 0.0753662, acc 0.96875
2017-09-09T16:03:56.159384: step 7254, loss 0.0316411, acc 0.984375
2017-09-09T16:03:56.463802: step 7255, loss 0.00234497, acc 1
2017-09-09T16:03:56.766676: step 7256, loss 0.0036968, acc 1
2017-09-09T16:03:57.082431: step 7257, loss 0.0666676, acc 0.96875
2017-09-09T16:03:57.471783: step 7258, loss 0.0109548, acc 1
2017-09-09T16:03:57.786921: step 7259, loss 0.0256715, acc 0.984375
2017-09-09T16:03:58.128746: step 7260, loss 0.0268355, acc 0.984375
2017-09-09T16:03:58.488039: step 7261, loss 0.00573011, acc 1
2017-09-09T16:03:58.905214: step 7262, loss 0.0125685, acc 0.984375
2017-09-09T16:03:59.183598: step 7263, loss 0.000548656, acc 1
2017-09-09T16:03:59.510195: step 7264, loss 0.0572366, acc 0.984375
2017-09-09T16:03:59.838611: step 7265, loss 0.0370445, acc 0.984375
2017-09-09T16:04:00.148485: step 7266, loss 0.0355428, acc 0.96875
2017-09-09T16:04:00.431165: step 7267, loss 0.0060149, acc 1
2017-09-09T16:04:00.776146: step 7268, loss 0.0126204, acc 1
2017-09-09T16:04:01.064774: step 7269, loss 0.00666446, acc 1
2017-09-09T16:04:01.365712: step 7270, loss 0.0221137, acc 0.984375
2017-09-09T16:04:01.773876: step 7271, loss 0.029558, acc 0.96875
2017-09-09T16:04:02.144563: step 7272, loss 0.00327953, acc 1
2017-09-09T16:04:02.469691: step 7273, loss 0.0319819, acc 0.984375
2017-09-09T16:04:02.827148: step 7274, loss 0.000169507, acc 1
2017-09-09T16:04:03.142043: step 7275, loss 0.00200044, acc 1
2017-09-09T16:04:03.427202: step 7276, loss 0.0298576, acc 0.984375
2017-09-09T16:04:03.758793: step 7277, loss 0.00316242, acc 1
2017-09-09T16:04:04.038490: step 7278, loss 0.0357575, acc 0.96875
2017-09-09T16:04:04.328601: step 7279, loss 0.0337407, acc 0.984375
2017-09-09T16:04:04.631699: step 7280, loss 0.0192612, acc 0.984375
2017-09-09T16:04:04.955383: step 7281, loss 0.0334557, acc 0.984375
2017-09-09T16:04:05.255475: step 7282, loss 0.00558039, acc 1
2017-09-09T16:04:05.657335: step 7283, loss 0.0209869, acc 0.984375
2017-09-09T16:04:05.975812: step 7284, loss 0.0419879, acc 0.96875
2017-09-09T16:04:06.324270: step 7285, loss 0.0215011, acc 1
2017-09-09T16:04:06.663637: step 7286, loss 0.0454951, acc 0.953125
2017-09-09T16:04:07.000501: step 7287, loss 0.0175464, acc 0.984375
2017-09-09T16:04:07.340173: step 7288, loss 0.0537076, acc 0.984375
2017-09-09T16:04:07.712650: step 7289, loss 0.0410563, acc 0.96875
2017-09-09T16:04:08.012878: step 7290, loss 5.32688e-05, acc 1
2017-09-09T16:04:08.368195: step 7291, loss 0.0145167, acc 0.984375
2017-09-09T16:04:08.717222: step 7292, loss 0.0121686, acc 1
2017-09-09T16:04:09.078306: step 7293, loss 0.02625, acc 0.984375
2017-09-09T16:04:09.384906: step 7294, loss 4.67876e-05, acc 1
2017-09-09T16:04:09.783996: step 7295, loss 0.004299, acc 1
2017-09-09T16:04:10.184951: step 7296, loss 0.0320919, acc 0.984375
2017-09-09T16:04:10.448259: step 7297, loss 0.0157049, acc 1
2017-09-09T16:04:10.732847: step 7298, loss 0.0232776, acc 0.984375
2017-09-09T16:04:11.036870: step 7299, loss 0.00799768, acc 1
2017-09-09T16:04:11.333182: step 7300, loss 0.000810223, acc 1

Evaluation:
2017-09-09T16:04:11.433630: step 7300, loss 2.60358, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7300

2017-09-09T16:04:13.373779: step 7301, loss 0.00211901, acc 1
2017-09-09T16:04:13.701419: step 7302, loss 0.0257076, acc 0.984375
2017-09-09T16:04:14.039724: step 7303, loss 0.0206786, acc 0.984375
2017-09-09T16:04:14.367102: step 7304, loss 0.00364878, acc 1
2017-09-09T16:04:14.690605: step 7305, loss 0.0691595, acc 0.953125
2017-09-09T16:04:15.033867: step 7306, loss 0.00340116, acc 1
2017-09-09T16:04:15.360633: step 7307, loss 0.0110667, acc 1
2017-09-09T16:04:15.738761: step 7308, loss 0.0302825, acc 0.984375
2017-09-09T16:04:16.105778: step 7309, loss 0.0220251, acc 0.984375
2017-09-09T16:04:16.523571: step 7310, loss 0.0131712, acc 0.984375
2017-09-09T16:04:16.849348: step 7311, loss 0.0249649, acc 0.984375
2017-09-09T16:04:17.206996: step 7312, loss 0.00334185, acc 1
2017-09-09T16:04:17.538012: step 7313, loss 0.0248857, acc 1
2017-09-09T16:04:17.865137: step 7314, loss 0.0291552, acc 0.984375
2017-09-09T16:04:18.226199: step 7315, loss 0.0139714, acc 1
2017-09-09T16:04:18.543402: step 7316, loss 0.0213651, acc 1
2017-09-09T16:04:18.879123: step 7317, loss 0.000890793, acc 1
2017-09-09T16:04:19.186158: step 7318, loss 0.0158143, acc 1
2017-09-09T16:04:19.535597: step 7319, loss 0.00271551, acc 1
2017-09-09T16:04:19.904765: step 7320, loss 0.0133916, acc 0.984375
2017-09-09T16:04:20.194640: step 7321, loss 0.0040035, acc 1
2017-09-09T16:04:20.511598: step 7322, loss 0.00872818, acc 1
2017-09-09T16:04:20.806837: step 7323, loss 0.0276336, acc 0.984375
2017-09-09T16:04:21.148202: step 7324, loss 0.00276781, acc 1
2017-09-09T16:04:21.490634: step 7325, loss 0.00399992, acc 1
2017-09-09T16:04:21.816660: step 7326, loss 0.0130878, acc 1
2017-09-09T16:04:22.136094: step 7327, loss 0.0205908, acc 1
2017-09-09T16:04:22.457258: step 7328, loss 0.0076129, acc 1
2017-09-09T16:04:22.767947: step 7329, loss 0.0126145, acc 1
2017-09-09T16:04:23.126082: step 7330, loss 0.0289494, acc 0.984375
2017-09-09T16:04:23.465658: step 7331, loss 0.0117626, acc 1
2017-09-09T16:04:23.817580: step 7332, loss 0.0220508, acc 0.984375
2017-09-09T16:04:24.190154: step 7333, loss 0.022876, acc 1
2017-09-09T16:04:24.460562: step 7334, loss 0.00540831, acc 1
2017-09-09T16:04:24.822364: step 7335, loss 0.000588365, acc 1
2017-09-09T16:04:25.165918: step 7336, loss 0.0144787, acc 0.984375
2017-09-09T16:04:25.464051: step 7337, loss 0.0644865, acc 0.96875
2017-09-09T16:04:25.806167: step 7338, loss 0.0265585, acc 0.984375
2017-09-09T16:04:26.097821: step 7339, loss 0.000432436, acc 1
2017-09-09T16:04:26.435067: step 7340, loss 0.0145867, acc 1
2017-09-09T16:04:26.788201: step 7341, loss 0.0165193, acc 0.984375
2017-09-09T16:04:27.087522: step 7342, loss 0.0306025, acc 0.984375
2017-09-09T16:04:27.367081: step 7343, loss 0.0445606, acc 0.953125
2017-09-09T16:04:27.737087: step 7344, loss 0.0413021, acc 0.96875
2017-09-09T16:04:28.096721: step 7345, loss 0.0124933, acc 1
2017-09-09T16:04:28.428738: step 7346, loss 0.0263496, acc 0.984375
2017-09-09T16:04:28.699279: step 7347, loss 0.00355188, acc 1
2017-09-09T16:04:29.028721: step 7348, loss 0.000689397, acc 1
2017-09-09T16:04:29.336542: step 7349, loss 0.0303497, acc 0.984375
2017-09-09T16:04:29.627999: step 7350, loss 0.0186593, acc 0.980392

Evaluation:
2017-09-09T16:04:29.707812: step 7350, loss 2.5521, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7350

2017-09-09T16:04:33.187076: step 7351, loss 0.00567726, acc 1
2017-09-09T16:04:33.503671: step 7352, loss 0.0264061, acc 0.984375
2017-09-09T16:04:33.814039: step 7353, loss 0.00134159, acc 1
2017-09-09T16:04:34.145555: step 7354, loss 0.0107669, acc 1
2017-09-09T16:04:34.435174: step 7355, loss 0.0236194, acc 0.984375
2017-09-09T16:04:34.804247: step 7356, loss 0.0363822, acc 0.984375
2017-09-09T16:04:35.120613: step 7357, loss 0.0320407, acc 0.984375
2017-09-09T16:04:35.418652: step 7358, loss 0.040057, acc 0.984375
2017-09-09T16:04:35.757820: step 7359, loss 0.0149304, acc 1
2017-09-09T16:04:36.042273: step 7360, loss 0.0248938, acc 0.984375
2017-09-09T16:04:36.467195: step 7361, loss 0.000515286, acc 1
2017-09-09T16:04:36.805656: step 7362, loss 0.0321737, acc 0.96875
2017-09-09T16:04:37.114405: step 7363, loss 0.0136541, acc 1
2017-09-09T16:04:37.469602: step 7364, loss 0.00823721, acc 1
2017-09-09T16:04:37.758171: step 7365, loss 0.0382511, acc 0.96875
2017-09-09T16:04:38.060517: step 7366, loss 0.0247817, acc 0.984375
2017-09-09T16:04:38.325060: step 7367, loss 0.0432385, acc 0.984375
2017-09-09T16:04:38.670987: step 7368, loss 0.0205376, acc 0.984375
2017-09-09T16:04:39.012569: step 7369, loss 0.0018068, acc 1
2017-09-09T16:04:39.310649: step 7370, loss 0.0288406, acc 0.96875
2017-09-09T16:04:39.696785: step 7371, loss 0.01128, acc 1
2017-09-09T16:04:40.048765: step 7372, loss 0.0218775, acc 0.984375
2017-09-09T16:04:40.391802: step 7373, loss 0.0308074, acc 0.984375
2017-09-09T16:04:40.705288: step 7374, loss 0.0192864, acc 0.984375
2017-09-09T16:04:40.995777: step 7375, loss 0.0665156, acc 0.96875
2017-09-09T16:04:41.273635: step 7376, loss 0.000829508, acc 1
2017-09-09T16:04:41.598989: step 7377, loss 0.0075326, acc 1
2017-09-09T16:04:41.900330: step 7378, loss 0.000197194, acc 1
2017-09-09T16:04:42.196155: step 7379, loss 0.0174317, acc 0.984375
2017-09-09T16:04:42.522310: step 7380, loss 0.0434988, acc 0.96875
2017-09-09T16:04:42.827114: step 7381, loss 0.00206091, acc 1
2017-09-09T16:04:43.168663: step 7382, loss 0.00511347, acc 1
2017-09-09T16:04:43.441066: step 7383, loss 0.0375251, acc 0.984375
2017-09-09T16:04:43.835684: step 7384, loss 0.0409952, acc 0.96875
2017-09-09T16:04:44.128021: step 7385, loss 0.00307352, acc 1
2017-09-09T16:04:44.436267: step 7386, loss 0.0475388, acc 0.984375
2017-09-09T16:04:44.818488: step 7387, loss 0.0441235, acc 0.984375
2017-09-09T16:04:45.088490: step 7388, loss 0.0195859, acc 1
2017-09-09T16:04:45.452284: step 7389, loss 0.00992366, acc 1
2017-09-09T16:04:45.783470: step 7390, loss 0.0149518, acc 1
2017-09-09T16:04:46.080699: step 7391, loss 0.000623523, acc 1
2017-09-09T16:04:46.454523: step 7392, loss 0.0405845, acc 0.96875
2017-09-09T16:04:46.755260: step 7393, loss 0.00184218, acc 1
2017-09-09T16:04:47.112201: step 7394, loss 0.00207872, acc 1
2017-09-09T16:04:47.430124: step 7395, loss 0.0310162, acc 1
2017-09-09T16:04:47.738861: step 7396, loss 0.0570533, acc 0.984375
2017-09-09T16:04:48.045457: step 7397, loss 0.00240411, acc 1
2017-09-09T16:04:48.339481: step 7398, loss 0.029525, acc 0.984375
2017-09-09T16:04:48.663459: step 7399, loss 0.046069, acc 0.96875
2017-09-09T16:04:48.953994: step 7400, loss 0.00129883, acc 1

Evaluation:
2017-09-09T16:04:49.099612: step 7400, loss 2.64264, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7400

2017-09-09T16:04:51.230066: step 7401, loss 0.0319378, acc 0.984375
2017-09-09T16:04:51.531254: step 7402, loss 0.00879441, acc 1
2017-09-09T16:04:51.840691: step 7403, loss 0.0115249, acc 1
2017-09-09T16:04:52.193003: step 7404, loss 0.0214216, acc 0.984375
2017-09-09T16:04:52.510052: step 7405, loss 0.0439604, acc 0.96875
2017-09-09T16:04:52.806198: step 7406, loss 0.0188322, acc 0.984375
2017-09-09T16:04:53.130630: step 7407, loss 0.000614651, acc 1
2017-09-09T16:04:53.402702: step 7408, loss 0.0475822, acc 0.96875
2017-09-09T16:04:53.779539: step 7409, loss 0.0102635, acc 1
2017-09-09T16:04:54.072413: step 7410, loss 0.00191599, acc 1
2017-09-09T16:04:54.378100: step 7411, loss 0.00228916, acc 1
2017-09-09T16:04:54.679802: step 7412, loss 0.00684695, acc 1
2017-09-09T16:04:54.996589: step 7413, loss 0.0105685, acc 1
2017-09-09T16:04:55.339399: step 7414, loss 0.00517744, acc 1
2017-09-09T16:04:55.644992: step 7415, loss 0.0252213, acc 0.984375
2017-09-09T16:04:55.947221: step 7416, loss 0.00056406, acc 1
2017-09-09T16:04:56.270285: step 7417, loss 0.00688723, acc 1
2017-09-09T16:04:56.550914: step 7418, loss 0.0171383, acc 1
2017-09-09T16:04:56.826161: step 7419, loss 0.0509153, acc 0.96875
2017-09-09T16:04:57.109929: step 7420, loss 0.00732686, acc 1
2017-09-09T16:04:57.433783: step 7421, loss 0.0367869, acc 0.984375
2017-09-09T16:04:57.769678: step 7422, loss 0.029546, acc 1
2017-09-09T16:04:58.031814: step 7423, loss 0.00640829, acc 1
2017-09-09T16:04:58.353687: step 7424, loss 0.0111501, acc 1
2017-09-09T16:04:58.631356: step 7425, loss 0.000129929, acc 1
2017-09-09T16:04:58.916010: step 7426, loss 0.0441159, acc 0.96875
2017-09-09T16:04:59.228883: step 7427, loss 0.0616254, acc 0.96875
2017-09-09T16:04:59.511106: step 7428, loss 0.024672, acc 1
2017-09-09T16:04:59.839538: step 7429, loss 0.000594959, acc 1
2017-09-09T16:05:00.162519: step 7430, loss 0.00405592, acc 1
2017-09-09T16:05:00.489442: step 7431, loss 0.049482, acc 0.96875
2017-09-09T16:05:00.890990: step 7432, loss 0.0134944, acc 1
2017-09-09T16:05:01.195355: step 7433, loss 0.0367263, acc 1
2017-09-09T16:05:01.511031: step 7434, loss 0.0338677, acc 0.96875
2017-09-09T16:05:01.851504: step 7435, loss 0.0448837, acc 0.96875
2017-09-09T16:05:02.160391: step 7436, loss 0.00320597, acc 1
2017-09-09T16:05:02.532767: step 7437, loss 0.0352619, acc 0.984375
2017-09-09T16:05:02.832203: step 7438, loss 0.00541541, acc 1
2017-09-09T16:05:03.151528: step 7439, loss 0.0171796, acc 0.984375
2017-09-09T16:05:03.430777: step 7440, loss 0.0234129, acc 0.984375
2017-09-09T16:05:03.758580: step 7441, loss 0.00154947, acc 1
2017-09-09T16:05:04.087913: step 7442, loss 0.00108001, acc 1
2017-09-09T16:05:04.377994: step 7443, loss 0.0264713, acc 0.984375
2017-09-09T16:05:04.755718: step 7444, loss 0.0315529, acc 0.984375
2017-09-09T16:05:05.037956: step 7445, loss 0.000814673, acc 1
2017-09-09T16:05:05.396297: step 7446, loss 0.0470956, acc 0.984375
2017-09-09T16:05:05.685750: step 7447, loss 0.00679921, acc 1
2017-09-09T16:05:06.041427: step 7448, loss 0.00197924, acc 1
2017-09-09T16:05:06.340887: step 7449, loss 0.0115149, acc 1
2017-09-09T16:05:06.621543: step 7450, loss 0.0432286, acc 0.96875

Evaluation:
2017-09-09T16:05:06.730746: step 7450, loss 2.07733, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7450

2017-09-09T16:05:09.548023: step 7451, loss 0.049655, acc 0.96875
2017-09-09T16:05:09.821502: step 7452, loss 0.0109381, acc 1
2017-09-09T16:05:10.138290: step 7453, loss 0.000976437, acc 1
2017-09-09T16:05:10.418658: step 7454, loss 0.0781162, acc 0.953125
2017-09-09T16:05:10.688145: step 7455, loss 0.0569646, acc 0.953125
2017-09-09T16:05:10.992596: step 7456, loss 0.0264133, acc 1
2017-09-09T16:05:11.289070: step 7457, loss 0.0166007, acc 0.984375
2017-09-09T16:05:11.567137: step 7458, loss 0.00324911, acc 1
2017-09-09T16:05:11.929645: step 7459, loss 0.00537737, acc 1
2017-09-09T16:05:12.193089: step 7460, loss 0.0311973, acc 0.984375
2017-09-09T16:05:12.545651: step 7461, loss 0.0161526, acc 0.984375
2017-09-09T16:05:12.861396: step 7462, loss 0.00286887, acc 1
2017-09-09T16:05:13.181824: step 7463, loss 0.000962816, acc 1
2017-09-09T16:05:13.552275: step 7464, loss 0.0432369, acc 0.984375
2017-09-09T16:05:13.880360: step 7465, loss 0.000538001, acc 1
2017-09-09T16:05:14.195697: step 7466, loss 0.0535795, acc 0.96875
2017-09-09T16:05:14.522141: step 7467, loss 0.0357445, acc 0.984375
2017-09-09T16:05:14.802791: step 7468, loss 0.0270833, acc 0.984375
2017-09-09T16:05:15.127089: step 7469, loss 0.0436161, acc 0.96875
2017-09-09T16:05:15.425560: step 7470, loss 7.26119e-05, acc 1
2017-09-09T16:05:15.793122: step 7471, loss 0.0316386, acc 0.984375
2017-09-09T16:05:16.092289: step 7472, loss 0.0334603, acc 0.984375
2017-09-09T16:05:16.459107: step 7473, loss 0.0345828, acc 0.96875
2017-09-09T16:05:16.773745: step 7474, loss 0.00960337, acc 1
2017-09-09T16:05:17.057625: step 7475, loss 0.00503177, acc 1
2017-09-09T16:05:17.397034: step 7476, loss 0.00190209, acc 1
2017-09-09T16:05:17.698485: step 7477, loss 0.00526241, acc 1
2017-09-09T16:05:18.104532: step 7478, loss 0.00290682, acc 1
2017-09-09T16:05:18.412355: step 7479, loss 0.0497452, acc 0.984375
2017-09-09T16:05:18.750243: step 7480, loss 0.000142359, acc 1
2017-09-09T16:05:19.120162: step 7481, loss 0.0669051, acc 0.953125
2017-09-09T16:05:19.457662: step 7482, loss 0.0161966, acc 0.984375
2017-09-09T16:05:19.763126: step 7483, loss 0.0294667, acc 0.984375
2017-09-09T16:05:20.123974: step 7484, loss 0.0640725, acc 0.953125
2017-09-09T16:05:20.435964: step 7485, loss 0.000740682, acc 1
2017-09-09T16:05:20.798078: step 7486, loss 0.00199344, acc 1
2017-09-09T16:05:21.072099: step 7487, loss 0.0587249, acc 0.96875
2017-09-09T16:05:21.390226: step 7488, loss 0.0200994, acc 1
2017-09-09T16:05:21.670990: step 7489, loss 0.0159991, acc 0.984375
2017-09-09T16:05:21.992025: step 7490, loss 0.0208056, acc 1
2017-09-09T16:05:22.312130: step 7491, loss 0.00224567, acc 1
2017-09-09T16:05:22.574039: step 7492, loss 0.00207838, acc 1
2017-09-09T16:05:22.907044: step 7493, loss 0.00837029, acc 1
2017-09-09T16:05:23.184913: step 7494, loss 0.0200492, acc 0.984375
2017-09-09T16:05:23.554896: step 7495, loss 0.108722, acc 0.9375
2017-09-09T16:05:23.887582: step 7496, loss 0.0257989, acc 0.984375
2017-09-09T16:05:24.174210: step 7497, loss 0.0208427, acc 1
2017-09-09T16:05:24.510448: step 7498, loss 0.000282804, acc 1
2017-09-09T16:05:24.803552: step 7499, loss 0.00187398, acc 1
2017-09-09T16:05:25.217793: step 7500, loss 0.0618662, acc 0.953125

Evaluation:
2017-09-09T16:05:25.283627: step 7500, loss 1.9523, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7500

2017-09-09T16:05:28.488115: step 7501, loss 0.0187213, acc 0.984375
2017-09-09T16:05:28.818706: step 7502, loss 0.0169168, acc 0.984375
2017-09-09T16:05:29.141275: step 7503, loss 0.0112437, acc 1
2017-09-09T16:05:29.465669: step 7504, loss 0.000912768, acc 1
2017-09-09T16:05:29.819835: step 7505, loss 0.0754752, acc 0.96875
2017-09-09T16:05:30.124139: step 7506, loss 0.00155422, acc 1
2017-09-09T16:05:30.481914: step 7507, loss 0.0300544, acc 0.984375
2017-09-09T16:05:30.893241: step 7508, loss 0.0403644, acc 0.96875
2017-09-09T16:05:31.191865: step 7509, loss 0.0253318, acc 1
2017-09-09T16:05:31.530238: step 7510, loss 0.00792375, acc 1
2017-09-09T16:05:31.809082: step 7511, loss 0.0854307, acc 0.96875
2017-09-09T16:05:32.156069: step 7512, loss 0.0284278, acc 1
2017-09-09T16:05:32.464817: step 7513, loss 0.000308151, acc 1
2017-09-09T16:05:32.798864: step 7514, loss 0.0520847, acc 0.96875
2017-09-09T16:05:33.189687: step 7515, loss 0.0295244, acc 0.984375
2017-09-09T16:05:33.489862: step 7516, loss 0.046625, acc 0.96875
2017-09-09T16:05:33.843601: step 7517, loss 0.00715208, acc 1
2017-09-09T16:05:34.167468: step 7518, loss 0.00340324, acc 1
2017-09-09T16:05:34.466974: step 7519, loss 0.00165642, acc 1
2017-09-09T16:05:34.885884: step 7520, loss 0.00533308, acc 1
2017-09-09T16:05:35.164377: step 7521, loss 0.0304436, acc 0.984375
2017-09-09T16:05:35.473889: step 7522, loss 0.0206944, acc 0.984375
2017-09-09T16:05:35.802385: step 7523, loss 0.0695706, acc 0.96875
2017-09-09T16:05:36.106429: step 7524, loss 0.0189663, acc 0.984375
2017-09-09T16:05:36.429179: step 7525, loss 0.000614002, acc 1
2017-09-09T16:05:36.740156: step 7526, loss 0.0201196, acc 0.984375
2017-09-09T16:05:37.113779: step 7527, loss 0.0422216, acc 0.984375
2017-09-09T16:05:37.428362: step 7528, loss 0.0195482, acc 1
2017-09-09T16:05:37.721894: step 7529, loss 0.0207856, acc 1
2017-09-09T16:05:38.058996: step 7530, loss 0.0438961, acc 0.96875
2017-09-09T16:05:38.357206: step 7531, loss 0.066679, acc 0.953125
2017-09-09T16:05:38.778505: step 7532, loss 0.0633248, acc 0.96875
2017-09-09T16:05:39.050177: step 7533, loss 0.083969, acc 0.953125
2017-09-09T16:05:39.369439: step 7534, loss 0.0310727, acc 0.984375
2017-09-09T16:05:39.678421: step 7535, loss 0.0264482, acc 0.984375
2017-09-09T16:05:39.949737: step 7536, loss 0.00424988, acc 1
2017-09-09T16:05:40.253917: step 7537, loss 0.0405622, acc 0.984375
2017-09-09T16:05:40.555201: step 7538, loss 0.00693212, acc 1
2017-09-09T16:05:40.835618: step 7539, loss 0.000245318, acc 1
2017-09-09T16:05:41.193265: step 7540, loss 0.0121139, acc 1
2017-09-09T16:05:41.492544: step 7541, loss 0.0229176, acc 1
2017-09-09T16:05:41.846471: step 7542, loss 0.0166891, acc 0.984375
2017-09-09T16:05:42.129242: step 7543, loss 0.0311784, acc 0.984375
2017-09-09T16:05:42.455240: step 7544, loss 0.0766252, acc 0.9375
2017-09-09T16:05:42.811649: step 7545, loss 0.00153247, acc 1
2017-09-09T16:05:43.075879: step 7546, loss 0.0094235, acc 1
2017-09-09T16:05:43.443689: step 7547, loss 0.0245369, acc 1
2017-09-09T16:05:43.758941: step 7548, loss 0.0395562, acc 0.96875
2017-09-09T16:05:44.055902: step 7549, loss 0.024869, acc 0.984375
2017-09-09T16:05:44.374874: step 7550, loss 0.00373585, acc 1

Evaluation:
2017-09-09T16:05:44.460578: step 7550, loss 1.90757, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7550

2017-09-09T16:05:46.790553: step 7551, loss 0.0346842, acc 0.984375
2017-09-09T16:05:47.081673: step 7552, loss 0.0325275, acc 0.984375
2017-09-09T16:05:47.411036: step 7553, loss 0.0564517, acc 0.984375
2017-09-09T16:05:47.715902: step 7554, loss 0.0204202, acc 1
2017-09-09T16:05:48.054688: step 7555, loss 0.0056456, acc 1
2017-09-09T16:05:48.405197: step 7556, loss 0.0207022, acc 0.984375
2017-09-09T16:05:48.678491: step 7557, loss 0.00325218, acc 1
2017-09-09T16:05:49.032968: step 7558, loss 0.0307408, acc 0.984375
2017-09-09T16:05:49.321284: step 7559, loss 0.00127078, acc 1
2017-09-09T16:05:49.630104: step 7560, loss 0.00727468, acc 1
2017-09-09T16:05:49.951495: step 7561, loss 0.00622766, acc 1
2017-09-09T16:05:50.243990: step 7562, loss 0.0138184, acc 1
2017-09-09T16:05:50.565896: step 7563, loss 0.0235783, acc 0.984375
2017-09-09T16:05:50.875392: step 7564, loss 0.0174482, acc 1
2017-09-09T16:05:51.274300: step 7565, loss 0.00272148, acc 1
2017-09-09T16:05:51.590500: step 7566, loss 0.00912706, acc 1
2017-09-09T16:05:51.898405: step 7567, loss 0.00494188, acc 1
2017-09-09T16:05:52.242216: step 7568, loss 0.0111838, acc 1
2017-09-09T16:05:52.540126: step 7569, loss 0.0320703, acc 0.984375
2017-09-09T16:05:52.875682: step 7570, loss 0.00329415, acc 1
2017-09-09T16:05:53.163667: step 7571, loss 0.00834979, acc 1
2017-09-09T16:05:53.506862: step 7572, loss 0.0021066, acc 1
2017-09-09T16:05:53.802370: step 7573, loss 0.00227196, acc 1
2017-09-09T16:05:54.122606: step 7574, loss 0.0155101, acc 0.984375
2017-09-09T16:05:54.458968: step 7575, loss 0.0118655, acc 0.984375
2017-09-09T16:05:54.816388: step 7576, loss 0.0102896, acc 1
2017-09-09T16:05:55.134189: step 7577, loss 0.00269456, acc 1
2017-09-09T16:05:55.417811: step 7578, loss 0.029894, acc 1
2017-09-09T16:05:55.687073: step 7579, loss 0.0217972, acc 1
2017-09-09T16:05:55.996857: step 7580, loss 0.00155396, acc 1
2017-09-09T16:05:56.315005: step 7581, loss 0.0174906, acc 0.984375
2017-09-09T16:05:56.605453: step 7582, loss 0.00836924, acc 1
2017-09-09T16:05:56.919946: step 7583, loss 0.0513877, acc 0.984375
2017-09-09T16:05:57.241932: step 7584, loss 0.00162463, acc 1
2017-09-09T16:05:57.580584: step 7585, loss 0.0226666, acc 0.984375
2017-09-09T16:05:57.934810: step 7586, loss 0.0149936, acc 1
2017-09-09T16:05:58.246243: step 7587, loss 0.013117, acc 1
2017-09-09T16:05:58.576350: step 7588, loss 0.0162311, acc 0.984375
2017-09-09T16:05:58.914907: step 7589, loss 0.0137307, acc 0.984375
2017-09-09T16:05:59.345409: step 7590, loss 0.00152851, acc 1
2017-09-09T16:05:59.649789: step 7591, loss 0.0142623, acc 1
2017-09-09T16:06:00.015146: step 7592, loss 0.00661827, acc 1
2017-09-09T16:06:00.357948: step 7593, loss 0.0149579, acc 1
2017-09-09T16:06:00.636069: step 7594, loss 0.00675257, acc 1
2017-09-09T16:06:00.996808: step 7595, loss 0.0550934, acc 0.984375
2017-09-09T16:06:01.314181: step 7596, loss 0.00371464, acc 1
2017-09-09T16:06:01.631314: step 7597, loss 0.0177766, acc 0.984375
2017-09-09T16:06:01.927254: step 7598, loss 0.0175961, acc 1
2017-09-09T16:06:02.263007: step 7599, loss 0.0794774, acc 0.9375
2017-09-09T16:06:02.552337: step 7600, loss 0.0267581, acc 0.984375

Evaluation:
2017-09-09T16:06:02.646764: step 7600, loss 3.43193, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7600

2017-09-09T16:06:06.044557: step 7601, loss 0.0344667, acc 0.984375
2017-09-09T16:06:06.382637: step 7602, loss 0.00249889, acc 1
2017-09-09T16:06:06.696002: step 7603, loss 0.0439103, acc 0.984375
2017-09-09T16:06:07.018827: step 7604, loss 0.0033272, acc 1
2017-09-09T16:06:07.366613: step 7605, loss 0.028205, acc 0.984375
2017-09-09T16:06:07.745326: step 7606, loss 0.0178608, acc 0.984375
2017-09-09T16:06:08.062395: step 7607, loss 0.0445709, acc 0.984375
2017-09-09T16:06:08.409212: step 7608, loss 0.0383954, acc 0.984375
2017-09-09T16:06:08.710421: step 7609, loss 0.000119862, acc 1
2017-09-09T16:06:08.999481: step 7610, loss 0.0493978, acc 0.96875
2017-09-09T16:06:09.321336: step 7611, loss 0.00490003, acc 1
2017-09-09T16:06:09.588403: step 7612, loss 0.0215143, acc 0.984375
2017-09-09T16:06:09.867257: step 7613, loss 0.0376692, acc 0.984375
2017-09-09T16:06:10.194336: step 7614, loss 0.0625097, acc 0.96875
2017-09-09T16:06:10.533610: step 7615, loss 0.000523187, acc 1
2017-09-09T16:06:10.821720: step 7616, loss 0.186659, acc 0.96875
2017-09-09T16:06:11.153738: step 7617, loss 0.0270461, acc 0.984375
2017-09-09T16:06:11.524305: step 7618, loss 0.00127565, acc 1
2017-09-09T16:06:11.857540: step 7619, loss 0.0328443, acc 0.984375
2017-09-09T16:06:12.231079: step 7620, loss 0.001629, acc 1
2017-09-09T16:06:12.534828: step 7621, loss 0.0342843, acc 0.984375
2017-09-09T16:06:12.854442: step 7622, loss 0.0266756, acc 0.984375
2017-09-09T16:06:13.202346: step 7623, loss 0.0361415, acc 0.96875
2017-09-09T16:06:13.578734: step 7624, loss 0.000107814, acc 1
2017-09-09T16:06:13.897143: step 7625, loss 0.021716, acc 0.984375
2017-09-09T16:06:14.226216: step 7626, loss 0.0280723, acc 1
2017-09-09T16:06:14.568918: step 7627, loss 0.000893348, acc 1
2017-09-09T16:06:14.849763: step 7628, loss 0.00556345, acc 1
2017-09-09T16:06:15.253087: step 7629, loss 0.0121802, acc 1
2017-09-09T16:06:15.660749: step 7630, loss 0.0214151, acc 0.984375
2017-09-09T16:06:15.940807: step 7631, loss 0.0204072, acc 1
2017-09-09T16:06:16.241775: step 7632, loss 0.0421482, acc 0.984375
2017-09-09T16:06:16.544446: step 7633, loss 0.0219553, acc 0.984375
2017-09-09T16:06:16.858383: step 7634, loss 0.000867793, acc 1
2017-09-09T16:06:17.187059: step 7635, loss 0.00334287, acc 1
2017-09-09T16:06:17.476795: step 7636, loss 0.0200657, acc 0.984375
2017-09-09T16:06:17.849788: step 7637, loss 0.0387404, acc 0.96875
2017-09-09T16:06:18.145154: step 7638, loss 0.0201701, acc 0.984375
2017-09-09T16:06:18.480994: step 7639, loss 0.0319656, acc 0.984375
2017-09-09T16:06:18.759945: step 7640, loss 0.0521816, acc 0.96875
2017-09-09T16:06:19.074681: step 7641, loss 0.0449223, acc 0.984375
2017-09-09T16:06:19.385036: step 7642, loss 0.0337943, acc 0.984375
2017-09-09T16:06:19.667449: step 7643, loss 0.0124829, acc 0.984375
2017-09-09T16:06:20.091813: step 7644, loss 0.00353232, acc 1
2017-09-09T16:06:20.400957: step 7645, loss 0.0288674, acc 0.984375
2017-09-09T16:06:20.702272: step 7646, loss 0.108051, acc 0.96875
2017-09-09T16:06:21.000529: step 7647, loss 0.00452464, acc 1
2017-09-09T16:06:21.299439: step 7648, loss 0.0380227, acc 0.984375
2017-09-09T16:06:21.609909: step 7649, loss 0.0684016, acc 0.953125
2017-09-09T16:06:21.925278: step 7650, loss 0.0019663, acc 1

Evaluation:
2017-09-09T16:06:22.068266: step 7650, loss 2.26964, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7650

2017-09-09T16:06:24.281311: step 7651, loss 0.00418867, acc 1
2017-09-09T16:06:24.993313: step 7652, loss 0.0785136, acc 0.96875
2017-09-09T16:06:25.306165: step 7653, loss 0.0197571, acc 1
2017-09-09T16:06:25.591020: step 7654, loss 0.0584767, acc 0.984375
2017-09-09T16:06:25.884312: step 7655, loss 0.0373069, acc 0.984375
2017-09-09T16:06:26.169412: step 7656, loss 0.0507951, acc 0.953125
2017-09-09T16:06:26.453325: step 7657, loss 0.0540257, acc 0.984375
2017-09-09T16:06:26.784543: step 7658, loss 0.0283005, acc 0.984375
2017-09-09T16:06:27.105770: step 7659, loss 0.0248775, acc 0.984375
2017-09-09T16:06:27.403385: step 7660, loss 0.000963193, acc 1
2017-09-09T16:06:27.785823: step 7661, loss 0.00156288, acc 1
2017-09-09T16:06:28.093015: step 7662, loss 0.000493181, acc 1
2017-09-09T16:06:28.488055: step 7663, loss 0.0221335, acc 0.984375
2017-09-09T16:06:28.809560: step 7664, loss 0.00217017, acc 1
2017-09-09T16:06:29.196458: step 7665, loss 0.00275975, acc 1
2017-09-09T16:06:29.515372: step 7666, loss 0.0509399, acc 0.984375
2017-09-09T16:06:29.864982: step 7667, loss 0.0194498, acc 0.984375
2017-09-09T16:06:30.269904: step 7668, loss 0.067926, acc 0.953125
2017-09-09T16:06:30.619226: step 7669, loss 0.0538859, acc 0.984375
2017-09-09T16:06:30.945487: step 7670, loss 0.00347008, acc 1
2017-09-09T16:06:31.322064: step 7671, loss 0.020744, acc 1
2017-09-09T16:06:31.606347: step 7672, loss 0.0253921, acc 0.984375
2017-09-09T16:06:31.995601: step 7673, loss 0.0500718, acc 0.96875
2017-09-09T16:06:32.361454: step 7674, loss 0.00195397, acc 1
2017-09-09T16:06:32.701037: step 7675, loss 0.0301251, acc 1
2017-09-09T16:06:33.010549: step 7676, loss 0.0173634, acc 0.984375
2017-09-09T16:06:33.326584: step 7677, loss 0.0345033, acc 0.984375
2017-09-09T16:06:33.640419: step 7678, loss 0.00684239, acc 1
2017-09-09T16:06:34.003488: step 7679, loss 0.0708577, acc 0.984375
2017-09-09T16:06:34.404137: step 7680, loss 0.0266259, acc 0.984375
2017-09-09T16:06:34.675633: step 7681, loss 0.0254079, acc 1
2017-09-09T16:06:35.011094: step 7682, loss 0.00164011, acc 1
2017-09-09T16:06:35.371981: step 7683, loss 0.0098053, acc 1
2017-09-09T16:06:35.684107: step 7684, loss 0.0211407, acc 0.984375
2017-09-09T16:06:36.032888: step 7685, loss 0.0486831, acc 0.96875
2017-09-09T16:06:36.388849: step 7686, loss 0.00359818, acc 1
2017-09-09T16:06:36.713950: step 7687, loss 0.0202393, acc 0.984375
2017-09-09T16:06:37.012862: step 7688, loss 0.00617794, acc 1
2017-09-09T16:06:37.310886: step 7689, loss 0.0551583, acc 0.984375
2017-09-09T16:06:37.638772: step 7690, loss 0.0162567, acc 1
2017-09-09T16:06:37.932134: step 7691, loss 0.0108217, acc 1
2017-09-09T16:06:38.229159: step 7692, loss 0.0761516, acc 0.984375
2017-09-09T16:06:38.515549: step 7693, loss 0.0721384, acc 0.96875
2017-09-09T16:06:38.878825: step 7694, loss 0.0175124, acc 0.984375
2017-09-09T16:06:39.187012: step 7695, loss 0.00138613, acc 1
2017-09-09T16:06:39.501317: step 7696, loss 0.0301282, acc 1
2017-09-09T16:06:39.847876: step 7697, loss 0.0187721, acc 1
2017-09-09T16:06:40.215962: step 7698, loss 0.0073627, acc 1
2017-09-09T16:06:40.556380: step 7699, loss 0.0243046, acc 0.984375
2017-09-09T16:06:40.915737: step 7700, loss 0.0248975, acc 0.984375

Evaluation:
2017-09-09T16:06:41.021834: step 7700, loss 1.89584, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7700

2017-09-09T16:06:43.640305: step 7701, loss 0.010313, acc 1
2017-09-09T16:06:43.978607: step 7702, loss 0.0376573, acc 0.984375
2017-09-09T16:06:44.356441: step 7703, loss 0.0263057, acc 0.984375
2017-09-09T16:06:44.658561: step 7704, loss 0.0185777, acc 1
2017-09-09T16:06:45.521620: step 7705, loss 0.00287831, acc 1
2017-09-09T16:06:45.837095: step 7706, loss 0.0308694, acc 1
2017-09-09T16:06:46.160573: step 7707, loss 0.0343638, acc 0.96875
2017-09-09T16:06:46.496018: step 7708, loss 0.0180531, acc 1
2017-09-09T16:06:46.816681: step 7709, loss 0.0263347, acc 0.984375
2017-09-09T16:06:47.151799: step 7710, loss 0.0427609, acc 0.984375
2017-09-09T16:06:47.449614: step 7711, loss 0.00315244, acc 1
2017-09-09T16:06:47.739931: step 7712, loss 0.0162411, acc 0.984375
2017-09-09T16:06:48.090063: step 7713, loss 0.0223548, acc 0.984375
2017-09-09T16:06:48.377147: step 7714, loss 0.00350751, acc 1
2017-09-09T16:06:48.793178: step 7715, loss 0.00190423, acc 1
2017-09-09T16:06:49.113938: step 7716, loss 0.0220297, acc 0.984375
2017-09-09T16:06:49.382853: step 7717, loss 0.0386338, acc 0.984375
2017-09-09T16:06:49.740490: step 7718, loss 0.0161458, acc 1
2017-09-09T16:06:50.042105: step 7719, loss 0.01942, acc 0.984375
2017-09-09T16:06:50.399096: step 7720, loss 0.0346651, acc 0.984375
2017-09-09T16:06:50.719775: step 7721, loss 0.0249501, acc 0.984375
2017-09-09T16:06:51.015572: step 7722, loss 0.000367715, acc 1
2017-09-09T16:06:51.333822: step 7723, loss 0.013806, acc 1
2017-09-09T16:06:51.629419: step 7724, loss 0.0602221, acc 0.96875
2017-09-09T16:06:51.958633: step 7725, loss 0.0094831, acc 1
2017-09-09T16:06:52.259670: step 7726, loss 0.0288744, acc 0.984375
2017-09-09T16:06:52.624491: step 7727, loss 0.004824, acc 1
2017-09-09T16:06:52.922277: step 7728, loss 0.00291445, acc 1
2017-09-09T16:06:53.271961: step 7729, loss 0.0227831, acc 0.984375
2017-09-09T16:06:53.575805: step 7730, loss 0.00930118, acc 1
2017-09-09T16:06:53.905216: step 7731, loss 0.0573598, acc 0.984375
2017-09-09T16:06:54.240196: step 7732, loss 0.0376164, acc 0.96875
2017-09-09T16:06:54.591712: step 7733, loss 0.00971666, acc 1
2017-09-09T16:06:54.915943: step 7734, loss 0.0171755, acc 1
2017-09-09T16:06:55.236027: step 7735, loss 0.0487031, acc 0.984375
2017-09-09T16:06:55.514218: step 7736, loss 0.00632946, acc 1
2017-09-09T16:06:55.842911: step 7737, loss 0.0441342, acc 0.984375
2017-09-09T16:06:56.114766: step 7738, loss 0.0414247, acc 0.984375
2017-09-09T16:06:56.410471: step 7739, loss 0.0209015, acc 1
2017-09-09T16:06:56.719322: step 7740, loss 0.0458666, acc 0.96875
2017-09-09T16:06:57.122377: step 7741, loss 0.0267484, acc 0.984375
2017-09-09T16:06:57.421589: step 7742, loss 0.0205199, acc 0.980392
2017-09-09T16:06:57.762684: step 7743, loss 0.00503927, acc 1
2017-09-09T16:06:58.133269: step 7744, loss 0.0337239, acc 0.984375
2017-09-09T16:06:58.459844: step 7745, loss 0.0355123, acc 0.984375
2017-09-09T16:06:58.795408: step 7746, loss 0.0242727, acc 0.984375
2017-09-09T16:06:59.179256: step 7747, loss 0.000759734, acc 1
2017-09-09T16:06:59.498007: step 7748, loss 0.00840572, acc 1
2017-09-09T16:06:59.827377: step 7749, loss 0.0442757, acc 0.96875
2017-09-09T16:07:00.188115: step 7750, loss 0.000570997, acc 1

Evaluation:
2017-09-09T16:07:00.282887: step 7750, loss 2.4648, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7750

2017-09-09T16:07:03.816513: step 7751, loss 0.0377898, acc 0.984375
2017-09-09T16:07:04.160173: step 7752, loss 0.013713, acc 0.984375
2017-09-09T16:07:04.520913: step 7753, loss 0.020398, acc 0.984375
2017-09-09T16:07:04.845198: step 7754, loss 0.0212429, acc 0.984375
2017-09-09T16:07:05.171116: step 7755, loss 0.00188743, acc 1
2017-09-09T16:07:05.460350: step 7756, loss 0.0174823, acc 0.984375
2017-09-09T16:07:05.759921: step 7757, loss 0.0152356, acc 0.984375
2017-09-09T16:07:06.043783: step 7758, loss 0.0113568, acc 1
2017-09-09T16:07:06.477559: step 7759, loss 0.0292531, acc 0.984375
2017-09-09T16:07:06.790035: step 7760, loss 0.0405093, acc 0.984375
2017-09-09T16:07:07.089614: step 7761, loss 0.0160596, acc 1
2017-09-09T16:07:07.360207: step 7762, loss 0.00020346, acc 1
2017-09-09T16:07:07.672787: step 7763, loss 0.0762972, acc 0.96875
2017-09-09T16:07:07.975059: step 7764, loss 0.0223559, acc 1
2017-09-09T16:07:08.363603: step 7765, loss 0.000257046, acc 1
2017-09-09T16:07:08.664489: step 7766, loss 0.00244462, acc 1
2017-09-09T16:07:09.004434: step 7767, loss 0.0136028, acc 1
2017-09-09T16:07:09.369690: step 7768, loss 0.0184926, acc 1
2017-09-09T16:07:09.647197: step 7769, loss 0.0189436, acc 0.984375
2017-09-09T16:07:10.079964: step 7770, loss 0.0402936, acc 0.984375
2017-09-09T16:07:10.389479: step 7771, loss 0.0529354, acc 0.96875
2017-09-09T16:07:10.706203: step 7772, loss 0.00401047, acc 1
2017-09-09T16:07:11.048001: step 7773, loss 0.0219799, acc 1
2017-09-09T16:07:11.311505: step 7774, loss 0.0199191, acc 0.984375
2017-09-09T16:07:11.641280: step 7775, loss 0.0724905, acc 0.984375
2017-09-09T16:07:11.930113: step 7776, loss 0.00723263, acc 1
2017-09-09T16:07:12.275521: step 7777, loss 0.0237796, acc 1
2017-09-09T16:07:12.592507: step 7778, loss 0.00183579, acc 1
2017-09-09T16:07:12.884514: step 7779, loss 0.0462924, acc 0.984375
2017-09-09T16:07:13.243774: step 7780, loss 0.0353904, acc 0.984375
2017-09-09T16:07:13.560714: step 7781, loss 0.0384585, acc 1
2017-09-09T16:07:13.903451: step 7782, loss 0.0203085, acc 0.984375
2017-09-09T16:07:14.208153: step 7783, loss 0.00352147, acc 1
2017-09-09T16:07:14.523076: step 7784, loss 0.0391288, acc 0.96875
2017-09-09T16:07:14.885050: step 7785, loss 0.00209376, acc 1
2017-09-09T16:07:15.172636: step 7786, loss 0.0279844, acc 0.984375
2017-09-09T16:07:15.538909: step 7787, loss 0.0222033, acc 0.984375
2017-09-09T16:07:15.829197: step 7788, loss 0.0481573, acc 0.984375
2017-09-09T16:07:16.183287: step 7789, loss 0.0118361, acc 1
2017-09-09T16:07:16.495253: step 7790, loss 0.027349, acc 0.984375
2017-09-09T16:07:16.804598: step 7791, loss 0.00725379, acc 1
2017-09-09T16:07:17.179016: step 7792, loss 0.0618626, acc 0.96875
2017-09-09T16:07:17.523564: step 7793, loss 0.00562551, acc 1
2017-09-09T16:07:17.868887: step 7794, loss 0.0228319, acc 1
2017-09-09T16:07:18.183489: step 7795, loss 0.0211861, acc 0.984375
2017-09-09T16:07:18.460671: step 7796, loss 0.0383383, acc 0.984375
2017-09-09T16:07:18.818119: step 7797, loss 0.0474332, acc 0.96875
2017-09-09T16:07:19.148860: step 7798, loss 0.0383057, acc 0.984375
2017-09-09T16:07:19.509375: step 7799, loss 0.0209034, acc 0.984375
2017-09-09T16:07:19.846405: step 7800, loss 0.000618754, acc 1

Evaluation:
2017-09-09T16:07:19.919931: step 7800, loss 2.24439, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7800

2017-09-09T16:07:22.092442: step 7801, loss 0.00407955, acc 1
2017-09-09T16:07:22.470478: step 7802, loss 0.0770572, acc 0.953125
2017-09-09T16:07:22.780829: step 7803, loss 0.0172037, acc 1
2017-09-09T16:07:23.072711: step 7804, loss 0.0465109, acc 1
2017-09-09T16:07:23.416094: step 7805, loss 0.0732716, acc 0.984375
2017-09-09T16:07:23.759429: step 7806, loss 0.00108621, acc 1
2017-09-09T16:07:24.043648: step 7807, loss 0.005387, acc 1
2017-09-09T16:07:24.339289: step 7808, loss 0.00193872, acc 1
2017-09-09T16:07:24.655750: step 7809, loss 0.00251401, acc 1
2017-09-09T16:07:24.914766: step 7810, loss 0.00910503, acc 1
2017-09-09T16:07:25.248447: step 7811, loss 0.0307354, acc 0.984375
2017-09-09T16:07:25.657426: step 7812, loss 0.00141415, acc 1
2017-09-09T16:07:26.021551: step 7813, loss 0.00289412, acc 1
2017-09-09T16:07:26.314471: step 7814, loss 0.0125132, acc 1
2017-09-09T16:07:26.640426: step 7815, loss 0.0337883, acc 0.984375
2017-09-09T16:07:26.912104: step 7816, loss 0.0230866, acc 0.984375
2017-09-09T16:07:27.276749: step 7817, loss 0.0200922, acc 1
2017-09-09T16:07:27.644000: step 7818, loss 0.0345858, acc 1
2017-09-09T16:07:27.925100: step 7819, loss 0.0201033, acc 0.984375
2017-09-09T16:07:28.289226: step 7820, loss 0.0036698, acc 1
2017-09-09T16:07:28.573287: step 7821, loss 0.00086959, acc 1
2017-09-09T16:07:28.919278: step 7822, loss 0.0152571, acc 0.984375
2017-09-09T16:07:29.260902: step 7823, loss 0.0207718, acc 0.984375
2017-09-09T16:07:29.541210: step 7824, loss 0.0144943, acc 1
2017-09-09T16:07:29.893056: step 7825, loss 0.0198888, acc 0.984375
2017-09-09T16:07:30.172362: step 7826, loss 0.000142662, acc 1
2017-09-09T16:07:30.542007: step 7827, loss 0.0120648, acc 1
2017-09-09T16:07:30.850460: step 7828, loss 0.0946737, acc 0.953125
2017-09-09T16:07:31.153382: step 7829, loss 0.000772848, acc 1
2017-09-09T16:07:31.492517: step 7830, loss 0.0252993, acc 0.984375
2017-09-09T16:07:31.788946: step 7831, loss 0.057441, acc 0.984375
2017-09-09T16:07:32.118848: step 7832, loss 0.0488758, acc 0.96875
2017-09-09T16:07:32.449909: step 7833, loss 0.0396639, acc 0.96875
2017-09-09T16:07:32.794087: step 7834, loss 0.000844929, acc 1
2017-09-09T16:07:33.154746: step 7835, loss 0.0466465, acc 0.96875
2017-09-09T16:07:33.462726: step 7836, loss 0.00221152, acc 1
2017-09-09T16:07:33.820329: step 7837, loss 0.0227935, acc 0.984375
2017-09-09T16:07:34.146618: step 7838, loss 0.0242647, acc 0.984375
2017-09-09T16:07:34.433436: step 7839, loss 0.0388631, acc 0.984375
2017-09-09T16:07:34.699515: step 7840, loss 0.000297808, acc 1
2017-09-09T16:07:35.073580: step 7841, loss 0.0010735, acc 1
2017-09-09T16:07:35.425171: step 7842, loss 0.0182023, acc 1
2017-09-09T16:07:35.911124: step 7843, loss 0.00483162, acc 1
2017-09-09T16:07:36.201940: step 7844, loss 0.0114234, acc 0.984375
2017-09-09T16:07:36.523386: step 7845, loss 0.0817042, acc 0.96875
2017-09-09T16:07:36.817727: step 7846, loss 0.0613267, acc 0.953125
2017-09-09T16:07:37.127324: step 7847, loss 0.0316485, acc 0.96875
2017-09-09T16:07:37.423080: step 7848, loss 0.0836747, acc 0.96875
2017-09-09T16:07:37.796439: step 7849, loss 0.0631822, acc 0.96875
2017-09-09T16:07:38.109281: step 7850, loss 0.0112891, acc 1

Evaluation:
2017-09-09T16:07:38.224866: step 7850, loss 1.80499, acc 0.246043

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7850

2017-09-09T16:07:41.340403: step 7851, loss 0.00857315, acc 1
2017-09-09T16:07:41.709673: step 7852, loss 0.00541395, acc 1
2017-09-09T16:07:42.003290: step 7853, loss 0.0122966, acc 1
2017-09-09T16:07:42.353580: step 7854, loss 0.00377936, acc 1
2017-09-09T16:07:42.696631: step 7855, loss 0.0320165, acc 0.984375
2017-09-09T16:07:43.037739: step 7856, loss 0.019026, acc 0.984375
2017-09-09T16:07:43.352777: step 7857, loss 0.00262959, acc 1
2017-09-09T16:07:43.691454: step 7858, loss 0.0388381, acc 0.96875
2017-09-09T16:07:44.069675: step 7859, loss 0.00387695, acc 1
2017-09-09T16:07:44.391818: step 7860, loss 0.00816011, acc 1
2017-09-09T16:07:44.722995: step 7861, loss 0.00338004, acc 1
2017-09-09T16:07:45.107709: step 7862, loss 0.0106363, acc 1
2017-09-09T16:07:45.397020: step 7863, loss 0.0183914, acc 1
2017-09-09T16:07:45.761469: step 7864, loss 0.0147173, acc 1
2017-09-09T16:07:46.064324: step 7865, loss 0.0758805, acc 0.953125
2017-09-09T16:07:46.355603: step 7866, loss 0.0179744, acc 0.984375
2017-09-09T16:07:46.725157: step 7867, loss 0.00891697, acc 1
2017-09-09T16:07:47.032020: step 7868, loss 0.00524378, acc 1
2017-09-09T16:07:47.423731: step 7869, loss 0.0199587, acc 1
2017-09-09T16:07:47.721227: step 7870, loss 0.0268426, acc 0.984375
2017-09-09T16:07:48.017627: step 7871, loss 0.0222857, acc 0.984375
2017-09-09T16:07:48.305408: step 7872, loss 0.0106241, acc 1
2017-09-09T16:07:48.612583: step 7873, loss 0.00348723, acc 1
2017-09-09T16:07:48.948534: step 7874, loss 0.0432262, acc 0.96875
2017-09-09T16:07:49.237639: step 7875, loss 0.0029781, acc 1
2017-09-09T16:07:49.648702: step 7876, loss 0.0401041, acc 0.96875
2017-09-09T16:07:49.922853: step 7877, loss 0.00340164, acc 1
2017-09-09T16:07:50.244958: step 7878, loss 0.00362917, acc 1
2017-09-09T16:07:50.555083: step 7879, loss 0.017177, acc 1
2017-09-09T16:07:50.877268: step 7880, loss 0.0234767, acc 1
2017-09-09T16:07:51.229860: step 7881, loss 0.000785803, acc 1
2017-09-09T16:07:51.521790: step 7882, loss 0.226276, acc 0.96875
2017-09-09T16:07:51.861177: step 7883, loss 0.00859959, acc 1
2017-09-09T16:07:52.279511: step 7884, loss 0.0114218, acc 1
2017-09-09T16:07:52.588128: step 7885, loss 0.0360058, acc 0.984375
2017-09-09T16:07:52.952879: step 7886, loss 0.0359999, acc 0.984375
2017-09-09T16:07:53.252403: step 7887, loss 0.00323175, acc 1
2017-09-09T16:07:53.513380: step 7888, loss 0.0407336, acc 0.96875
2017-09-09T16:07:53.854037: step 7889, loss 0.00978244, acc 1
2017-09-09T16:07:54.182538: step 7890, loss 0.0259572, acc 0.984375
2017-09-09T16:07:54.520995: step 7891, loss 0.00939414, acc 1
2017-09-09T16:07:54.874592: step 7892, loss 0.0282286, acc 0.984375
2017-09-09T16:07:55.157354: step 7893, loss 0.0259625, acc 0.984375
2017-09-09T16:07:55.515393: step 7894, loss 0.0352608, acc 0.984375
2017-09-09T16:07:55.836908: step 7895, loss 0.00398803, acc 1
2017-09-09T16:07:56.131781: step 7896, loss 0.0435333, acc 0.984375
2017-09-09T16:07:56.522965: step 7897, loss 0.0211645, acc 0.984375
2017-09-09T16:07:56.842714: step 7898, loss 0.0182447, acc 1
2017-09-09T16:07:57.194378: step 7899, loss 0.00413605, acc 1
2017-09-09T16:07:57.527034: step 7900, loss 0.0625336, acc 0.953125

Evaluation:
2017-09-09T16:07:57.606113: step 7900, loss 2.10979, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7900

2017-09-09T16:07:59.789215: step 7901, loss 0.00313254, acc 1
2017-09-09T16:08:00.082292: step 7902, loss 0.0650459, acc 0.96875
2017-09-09T16:08:00.448123: step 7903, loss 0.144423, acc 0.9375
2017-09-09T16:08:00.775679: step 7904, loss 0.0599051, acc 0.953125
2017-09-09T16:08:01.078887: step 7905, loss 0.031528, acc 0.96875
2017-09-09T16:08:01.395268: step 7906, loss 0.0480488, acc 0.984375
2017-09-09T16:08:01.683579: step 7907, loss 0.00040152, acc 1
2017-09-09T16:08:02.019674: step 7908, loss 0.00201966, acc 1
2017-09-09T16:08:02.330347: step 7909, loss 0.0213705, acc 0.984375
2017-09-09T16:08:02.634236: step 7910, loss 0.0751081, acc 0.9375
2017-09-09T16:08:02.970878: step 7911, loss 0.0072255, acc 1
2017-09-09T16:08:03.256016: step 7912, loss 0.0904654, acc 0.96875
2017-09-09T16:08:03.591568: step 7913, loss 0.0139665, acc 0.984375
2017-09-09T16:08:03.920192: step 7914, loss 0.0185193, acc 0.984375
2017-09-09T16:08:04.235392: step 7915, loss 0.00107522, acc 1
2017-09-09T16:08:04.513831: step 7916, loss 0.039678, acc 1
2017-09-09T16:08:04.820309: step 7917, loss 0.00600126, acc 1
2017-09-09T16:08:05.150807: step 7918, loss 0.0277885, acc 0.984375
2017-09-09T16:08:05.475362: step 7919, loss 0.0419648, acc 0.984375
2017-09-09T16:08:05.809719: step 7920, loss 0.0587254, acc 0.96875
2017-09-09T16:08:06.089146: step 7921, loss 0.0300186, acc 0.96875
2017-09-09T16:08:06.468570: step 7922, loss 0.00697623, acc 1
2017-09-09T16:08:06.798386: step 7923, loss 0.00658091, acc 1
2017-09-09T16:08:07.166426: step 7924, loss 0.0726298, acc 0.984375
2017-09-09T16:08:07.492808: step 7925, loss 0.0116557, acc 1
2017-09-09T16:08:07.792588: step 7926, loss 0.0030361, acc 1
2017-09-09T16:08:08.135303: step 7927, loss 0.00616367, acc 1
2017-09-09T16:08:08.443703: step 7928, loss 0.0102404, acc 1
2017-09-09T16:08:08.789115: step 7929, loss 0.00941958, acc 1
2017-09-09T16:08:09.133164: step 7930, loss 0.0308507, acc 0.984375
2017-09-09T16:08:09.435067: step 7931, loss 0.0715847, acc 0.96875
2017-09-09T16:08:09.819157: step 7932, loss 0.00302071, acc 1
2017-09-09T16:08:10.106689: step 7933, loss 0.0477341, acc 0.96875
2017-09-09T16:08:10.444766: step 7934, loss 0.0306604, acc 0.984375
2017-09-09T16:08:10.749822: step 7935, loss 0.0175746, acc 1
2017-09-09T16:08:11.060099: step 7936, loss 0.0535634, acc 0.96875
2017-09-09T16:08:11.382234: step 7937, loss 0.0432372, acc 0.96875
2017-09-09T16:08:11.710378: step 7938, loss 0.00368573, acc 1
2017-09-09T16:08:12.072340: step 7939, loss 0.0049268, acc 1
2017-09-09T16:08:12.377199: step 7940, loss 0.00274, acc 1
2017-09-09T16:08:12.702806: step 7941, loss 0.0249513, acc 0.984375
2017-09-09T16:08:13.021760: step 7942, loss 0.0579526, acc 0.96875
2017-09-09T16:08:13.308433: step 7943, loss 0.00139874, acc 1
2017-09-09T16:08:13.672973: step 7944, loss 0.0166579, acc 1
2017-09-09T16:08:13.977945: step 7945, loss 0.061661, acc 0.96875
2017-09-09T16:08:14.337338: step 7946, loss 0.025616, acc 0.984375
2017-09-09T16:08:14.672269: step 7947, loss 0.00273543, acc 1
2017-09-09T16:08:14.963925: step 7948, loss 0.0236924, acc 1
2017-09-09T16:08:15.302980: step 7949, loss 0.0380328, acc 0.96875
2017-09-09T16:08:15.562536: step 7950, loss 0.000676847, acc 1

Evaluation:
2017-09-09T16:08:15.703003: step 7950, loss 1.82222, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-7950

2017-09-09T16:08:18.764729: step 7951, loss 0.108496, acc 0.96875
2017-09-09T16:08:19.053443: step 7952, loss 0.0494982, acc 0.984375
2017-09-09T16:08:19.376257: step 7953, loss 0.0244806, acc 0.984375
2017-09-09T16:08:19.697322: step 7954, loss 0.0204393, acc 0.984375
2017-09-09T16:08:20.082652: step 7955, loss 0.024484, acc 0.984375
2017-09-09T16:08:20.402926: step 7956, loss 0.00279401, acc 1
2017-09-09T16:08:20.692557: step 7957, loss 0.0161373, acc 1
2017-09-09T16:08:21.042320: step 7958, loss 0.0118824, acc 1
2017-09-09T16:08:21.323327: step 7959, loss 7.52278e-05, acc 1
2017-09-09T16:08:21.678889: step 7960, loss 0.0170734, acc 1
2017-09-09T16:08:22.009070: step 7961, loss 0.0199209, acc 0.984375
2017-09-09T16:08:22.313145: step 7962, loss 0.00708638, acc 1
2017-09-09T16:08:22.645287: step 7963, loss 0.00255743, acc 1
2017-09-09T16:08:22.953753: step 7964, loss 0.00925848, acc 1
2017-09-09T16:08:23.240801: step 7965, loss 0.0631862, acc 0.96875
2017-09-09T16:08:23.515173: step 7966, loss 0.0162594, acc 1
2017-09-09T16:08:23.839672: step 7967, loss 0.0206883, acc 0.984375
2017-09-09T16:08:24.242585: step 7968, loss 0.0272462, acc 0.984375
2017-09-09T16:08:24.540842: step 7969, loss 0.0149301, acc 1
2017-09-09T16:08:24.885340: step 7970, loss 0.0177557, acc 0.984375
2017-09-09T16:08:25.166523: step 7971, loss 0.0184098, acc 1
2017-09-09T16:08:25.465135: step 7972, loss 0.0188972, acc 1
2017-09-09T16:08:25.792789: step 7973, loss 0.00714434, acc 1
2017-09-09T16:08:26.089342: step 7974, loss 0.0238981, acc 0.984375
2017-09-09T16:08:26.447111: step 7975, loss 0.00422698, acc 1
2017-09-09T16:08:26.813223: step 7976, loss 0.0243174, acc 0.984375
2017-09-09T16:08:27.132685: step 7977, loss 0.000440973, acc 1
2017-09-09T16:08:27.443342: step 7978, loss 0.00369978, acc 1
2017-09-09T16:08:27.721396: step 7979, loss 0.0154079, acc 1
2017-09-09T16:08:28.095131: step 7980, loss 0.0331566, acc 1
2017-09-09T16:08:28.423062: step 7981, loss 0.000802621, acc 1
2017-09-09T16:08:28.758377: step 7982, loss 0.0604696, acc 0.96875
2017-09-09T16:08:29.127466: step 7983, loss 0.0496242, acc 0.96875
2017-09-09T16:08:29.415288: step 7984, loss 0.0690199, acc 0.96875
2017-09-09T16:08:29.762729: step 7985, loss 0.0448483, acc 0.96875
2017-09-09T16:08:30.102675: step 7986, loss 0.018961, acc 1
2017-09-09T16:08:30.415018: step 7987, loss 0.00177905, acc 1
2017-09-09T16:08:30.714007: step 7988, loss 0.0114788, acc 1
2017-09-09T16:08:31.018748: step 7989, loss 0.0493417, acc 0.96875
2017-09-09T16:08:31.411011: step 7990, loss 0.0685081, acc 0.953125
2017-09-09T16:08:31.703274: step 7991, loss 0.0158741, acc 0.984375
2017-09-09T16:08:32.014736: step 7992, loss 0.0260906, acc 0.984375
2017-09-09T16:08:32.327803: step 7993, loss 0.0294719, acc 0.984375
2017-09-09T16:08:32.635392: step 7994, loss 0.0507405, acc 0.96875
2017-09-09T16:08:32.976307: step 7995, loss 0.0424585, acc 0.984375
2017-09-09T16:08:33.303393: step 7996, loss 0.0410714, acc 0.96875
2017-09-09T16:08:33.597107: step 7997, loss 0.0312745, acc 0.984375
2017-09-09T16:08:33.917670: step 7998, loss 0.0405554, acc 0.984375
2017-09-09T16:08:34.198261: step 7999, loss 0.0121, acc 1
2017-09-09T16:08:34.537921: step 8000, loss 0.0451396, acc 0.984375

Evaluation:
2017-09-09T16:08:34.670992: step 8000, loss 1.98884, acc 0.243165

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8000

2017-09-09T16:08:37.722250: step 8001, loss 0.0535026, acc 0.96875
2017-09-09T16:08:38.014213: step 8002, loss 0.018716, acc 1
2017-09-09T16:08:38.378219: step 8003, loss 0.013947, acc 0.984375
2017-09-09T16:08:38.663195: step 8004, loss 0.00672355, acc 1
2017-09-09T16:08:39.000318: step 8005, loss 0.0721917, acc 0.953125
2017-09-09T16:08:39.359483: step 8006, loss 0.00173635, acc 1
2017-09-09T16:08:39.695571: step 8007, loss 0.0502318, acc 1
2017-09-09T16:08:40.034424: step 8008, loss 0.0108452, acc 1
2017-09-09T16:08:40.397207: step 8009, loss 0.00269344, acc 1
2017-09-09T16:08:40.697375: step 8010, loss 0.0790773, acc 0.953125
2017-09-09T16:08:41.043626: step 8011, loss 0.0330624, acc 0.984375
2017-09-09T16:08:41.344524: step 8012, loss 0.00968939, acc 1
2017-09-09T16:08:41.687582: step 8013, loss 0.0251295, acc 0.984375
2017-09-09T16:08:41.964519: step 8014, loss 0.0204603, acc 0.984375
2017-09-09T16:08:42.254973: step 8015, loss 0.000506791, acc 1
2017-09-09T16:08:42.549523: step 8016, loss 0.0261433, acc 0.984375
2017-09-09T16:08:42.843959: step 8017, loss 0.0111886, acc 1
2017-09-09T16:08:43.173417: step 8018, loss 0.00232993, acc 1
2017-09-09T16:08:43.466100: step 8019, loss 0.0258389, acc 0.984375
2017-09-09T16:08:43.787735: step 8020, loss 0.0281735, acc 0.984375
2017-09-09T16:08:44.090685: step 8021, loss 0.0850739, acc 0.953125
2017-09-09T16:08:44.404417: step 8022, loss 0.0177478, acc 0.984375
2017-09-09T16:08:44.703631: step 8023, loss 0.0014773, acc 1
2017-09-09T16:08:45.024005: step 8024, loss 0.000565814, acc 1
2017-09-09T16:08:45.346016: step 8025, loss 0.0211526, acc 0.984375
2017-09-09T16:08:45.660245: step 8026, loss 0.0919134, acc 0.953125
2017-09-09T16:08:46.024500: step 8027, loss 0.0310269, acc 1
2017-09-09T16:08:46.284107: step 8028, loss 0.0194091, acc 0.984375
2017-09-09T16:08:46.644504: step 8029, loss 0.00122543, acc 1
2017-09-09T16:08:47.024597: step 8030, loss 0.0282144, acc 1
2017-09-09T16:08:47.309902: step 8031, loss 0.024353, acc 0.984375
2017-09-09T16:08:47.682782: step 8032, loss 0.0326195, acc 0.984375
2017-09-09T16:08:47.989951: step 8033, loss 0.0346617, acc 0.96875
2017-09-09T16:08:48.270042: step 8034, loss 0.013995, acc 0.984375
2017-09-09T16:08:48.580402: step 8035, loss 0.00201362, acc 1
2017-09-09T16:08:48.882935: step 8036, loss 0.025837, acc 0.980392
2017-09-09T16:08:49.199862: step 8037, loss 9.14061e-05, acc 1
2017-09-09T16:08:49.540335: step 8038, loss 0.0146966, acc 1
2017-09-09T16:08:49.828239: step 8039, loss 0.0169335, acc 0.984375
2017-09-09T16:08:50.084239: step 8040, loss 0.0349379, acc 0.984375
2017-09-09T16:08:50.381121: step 8041, loss 0.0516, acc 0.96875
2017-09-09T16:08:50.744447: step 8042, loss 0.0409843, acc 0.96875
2017-09-09T16:08:51.079625: step 8043, loss 0.0277683, acc 0.984375
2017-09-09T16:08:51.357069: step 8044, loss 0.000584864, acc 1
2017-09-09T16:08:51.690648: step 8045, loss 0.0490963, acc 0.96875
2017-09-09T16:08:52.047523: step 8046, loss 0.0743802, acc 0.984375
2017-09-09T16:08:52.371550: step 8047, loss 0.0756966, acc 0.953125
2017-09-09T16:08:52.656983: step 8048, loss 0.0532128, acc 0.96875
2017-09-09T16:08:52.993379: step 8049, loss 0.00170423, acc 1
2017-09-09T16:08:53.303484: step 8050, loss 0.0113037, acc 1

Evaluation:
2017-09-09T16:08:53.392728: step 8050, loss 2.1147, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8050

2017-09-09T16:08:55.533987: step 8051, loss 0.00926087, acc 1
2017-09-09T16:08:55.854478: step 8052, loss 0.0145969, acc 0.984375
2017-09-09T16:08:56.150008: step 8053, loss 0.0101375, acc 1
2017-09-09T16:08:56.522820: step 8054, loss 0.00313462, acc 1
2017-09-09T16:08:56.780383: step 8055, loss 0.360259, acc 0.9375
2017-09-09T16:08:57.109081: step 8056, loss 0.0299008, acc 0.984375
2017-09-09T16:08:57.414439: step 8057, loss 0.0237178, acc 0.984375
2017-09-09T16:08:57.783667: step 8058, loss 0.0576624, acc 0.96875
2017-09-09T16:08:58.129106: step 8059, loss 0.0132728, acc 1
2017-09-09T16:08:58.404648: step 8060, loss 0.00567462, acc 1
2017-09-09T16:08:58.796133: step 8061, loss 0.0019963, acc 1
2017-09-09T16:08:59.141708: step 8062, loss 0.0576946, acc 0.96875
2017-09-09T16:08:59.436899: step 8063, loss 0.061864, acc 0.984375
2017-09-09T16:08:59.815877: step 8064, loss 0.0062147, acc 1
2017-09-09T16:09:00.114946: step 8065, loss 0.0372341, acc 0.96875
2017-09-09T16:09:00.436743: step 8066, loss 0.0142665, acc 1
2017-09-09T16:09:00.737604: step 8067, loss 0.0353474, acc 0.984375
2017-09-09T16:09:01.040696: step 8068, loss 0.0405619, acc 0.984375
2017-09-09T16:09:01.332927: step 8069, loss 0.0233704, acc 0.984375
2017-09-09T16:09:01.696574: step 8070, loss 0.00594587, acc 1
2017-09-09T16:09:01.983997: step 8071, loss 0.0118856, acc 1
2017-09-09T16:09:02.274323: step 8072, loss 0.00107017, acc 1
2017-09-09T16:09:02.571064: step 8073, loss 0.0917685, acc 0.96875
2017-09-09T16:09:02.888145: step 8074, loss 0.0199165, acc 1
2017-09-09T16:09:03.164751: step 8075, loss 0.0107522, acc 1
2017-09-09T16:09:03.455454: step 8076, loss 0.0106972, acc 1
2017-09-09T16:09:03.770622: step 8077, loss 0.0989276, acc 0.953125
2017-09-09T16:09:04.076861: step 8078, loss 0.00168861, acc 1
2017-09-09T16:09:04.401443: step 8079, loss 0.00232555, acc 1
2017-09-09T16:09:04.694830: step 8080, loss 0.00693861, acc 1
2017-09-09T16:09:05.027551: step 8081, loss 0.00576768, acc 1
2017-09-09T16:09:05.350001: step 8082, loss 0.00368376, acc 1
2017-09-09T16:09:05.681893: step 8083, loss 0.0148928, acc 1
2017-09-09T16:09:06.032839: step 8084, loss 0.0271561, acc 0.984375
2017-09-09T16:09:06.337445: step 8085, loss 0.0195104, acc 0.984375
2017-09-09T16:09:06.718195: step 8086, loss 0.0403924, acc 0.984375
2017-09-09T16:09:06.998557: step 8087, loss 0.0294881, acc 0.984375
2017-09-09T16:09:07.330490: step 8088, loss 0.0212582, acc 0.984375
2017-09-09T16:09:07.671558: step 8089, loss 0.0197483, acc 0.984375
2017-09-09T16:09:07.950055: step 8090, loss 0.000874192, acc 1
2017-09-09T16:09:08.296202: step 8091, loss 0.00142706, acc 1
2017-09-09T16:09:08.608862: step 8092, loss 0.0240915, acc 0.984375
2017-09-09T16:09:08.941306: step 8093, loss 0.0190048, acc 0.984375
2017-09-09T16:09:09.240472: step 8094, loss 0.00103657, acc 1
2017-09-09T16:09:09.570450: step 8095, loss 0.113339, acc 0.953125
2017-09-09T16:09:09.938289: step 8096, loss 0.00622324, acc 1
2017-09-09T16:09:10.261539: step 8097, loss 0.0260989, acc 0.984375
2017-09-09T16:09:10.618048: step 8098, loss 0.0349767, acc 0.96875
2017-09-09T16:09:10.956671: step 8099, loss 0.0214452, acc 0.984375
2017-09-09T16:09:11.238579: step 8100, loss 0.0171864, acc 1

Evaluation:
2017-09-09T16:09:11.335661: step 8100, loss 1.89615, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8100

2017-09-09T16:09:14.458505: step 8101, loss 0.000683237, acc 1
2017-09-09T16:09:14.774603: step 8102, loss 0.0443029, acc 0.96875
2017-09-09T16:09:15.082184: step 8103, loss 0.02011, acc 0.984375
2017-09-09T16:09:15.413190: step 8104, loss 0.0188984, acc 1
2017-09-09T16:09:15.730456: step 8105, loss 0.00196317, acc 1
2017-09-09T16:09:16.024064: step 8106, loss 0.0743939, acc 0.9375
2017-09-09T16:09:16.413848: step 8107, loss 0.0209384, acc 0.984375
2017-09-09T16:09:16.752831: step 8108, loss 0.0240418, acc 0.984375
2017-09-09T16:09:17.066273: step 8109, loss 0.0149787, acc 0.984375
2017-09-09T16:09:17.418135: step 8110, loss 0.0646588, acc 0.984375
2017-09-09T16:09:17.740905: step 8111, loss 0.0462051, acc 0.96875
2017-09-09T16:09:18.102457: step 8112, loss 0.0034503, acc 1
2017-09-09T16:09:18.394071: step 8113, loss 0.0338015, acc 0.984375
2017-09-09T16:09:18.695638: step 8114, loss 0.0383127, acc 0.96875
2017-09-09T16:09:19.027753: step 8115, loss 0.0191196, acc 0.984375
2017-09-09T16:09:19.479215: step 8116, loss 0.00683104, acc 1
2017-09-09T16:09:19.759162: step 8117, loss 0.0214552, acc 0.984375
2017-09-09T16:09:20.093756: step 8118, loss 0.0591908, acc 0.953125
2017-09-09T16:09:20.361139: step 8119, loss 0.0422328, acc 1
2017-09-09T16:09:20.710719: step 8120, loss 0.0136411, acc 0.984375
2017-09-09T16:09:21.037333: step 8121, loss 0.0188228, acc 0.984375
2017-09-09T16:09:21.411970: step 8122, loss 0.0566944, acc 0.96875
2017-09-09T16:09:21.772531: step 8123, loss 0.0398852, acc 0.984375
2017-09-09T16:09:22.065031: step 8124, loss 0.00662877, acc 1
2017-09-09T16:09:22.397244: step 8125, loss 0.0114868, acc 1
2017-09-09T16:09:22.692272: step 8126, loss 0.0376186, acc 0.984375
2017-09-09T16:09:23.043898: step 8127, loss 0.0059603, acc 1
2017-09-09T16:09:23.347639: step 8128, loss 0.00402702, acc 1
2017-09-09T16:09:23.702991: step 8129, loss 0.0175118, acc 1
2017-09-09T16:09:23.990758: step 8130, loss 0.0433666, acc 0.96875
2017-09-09T16:09:24.331855: step 8131, loss 0.0127558, acc 1
2017-09-09T16:09:24.674420: step 8132, loss 0.0234776, acc 0.984375
2017-09-09T16:09:24.975935: step 8133, loss 0.0315913, acc 0.96875
2017-09-09T16:09:25.337407: step 8134, loss 0.0281057, acc 0.980392
2017-09-09T16:09:25.701919: step 8135, loss 0.00444485, acc 1
2017-09-09T16:09:26.027034: step 8136, loss 0.0166256, acc 1
2017-09-09T16:09:26.351925: step 8137, loss 0.0100528, acc 1
2017-09-09T16:09:26.655662: step 8138, loss 0.00707121, acc 1
2017-09-09T16:09:27.070508: step 8139, loss 0.00372148, acc 1
2017-09-09T16:09:27.395177: step 8140, loss 0.0219478, acc 0.984375
2017-09-09T16:09:27.677403: step 8141, loss 0.00132558, acc 1
2017-09-09T16:09:27.990352: step 8142, loss 0.01732, acc 0.984375
2017-09-09T16:09:28.316807: step 8143, loss 0.000548351, acc 1
2017-09-09T16:09:28.650438: step 8144, loss 0.0207748, acc 0.984375
2017-09-09T16:09:29.001639: step 8145, loss 0.0422071, acc 0.96875
2017-09-09T16:09:29.326817: step 8146, loss 0.0460986, acc 0.984375
2017-09-09T16:09:29.681186: step 8147, loss 0.0334505, acc 0.984375
2017-09-09T16:09:29.988093: step 8148, loss 0.0033126, acc 1
2017-09-09T16:09:30.403554: step 8149, loss 0.0314547, acc 1
2017-09-09T16:09:30.670363: step 8150, loss 0.040543, acc 0.984375

Evaluation:
2017-09-09T16:09:30.814164: step 8150, loss 1.80871, acc 0.297842

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8150

2017-09-09T16:09:32.905403: step 8151, loss 0.00884811, acc 1
2017-09-09T16:09:33.203226: step 8152, loss 0.0375696, acc 0.96875
2017-09-09T16:09:33.492249: step 8153, loss 0.00260666, acc 1
2017-09-09T16:09:33.821098: step 8154, loss 0.0026929, acc 1
2017-09-09T16:09:34.188983: step 8155, loss 0.0260842, acc 1
2017-09-09T16:09:34.473912: step 8156, loss 0.026601, acc 0.984375
2017-09-09T16:09:34.866273: step 8157, loss 0.00172295, acc 1
2017-09-09T16:09:35.197079: step 8158, loss 0.0767418, acc 0.9375
2017-09-09T16:09:35.502492: step 8159, loss 0.0117372, acc 1
2017-09-09T16:09:35.887728: step 8160, loss 0.00434147, acc 1
2017-09-09T16:09:36.210057: step 8161, loss 0.00715229, acc 1
2017-09-09T16:09:36.522990: step 8162, loss 0.047766, acc 0.96875
2017-09-09T16:09:36.827324: step 8163, loss 0.0083579, acc 1
2017-09-09T16:09:37.134996: step 8164, loss 0.00260973, acc 1
2017-09-09T16:09:37.478847: step 8165, loss 0.011561, acc 0.984375
2017-09-09T16:09:37.772444: step 8166, loss 0.000394059, acc 1
2017-09-09T16:09:38.131549: step 8167, loss 0.0155186, acc 1
2017-09-09T16:09:38.459205: step 8168, loss 0.0169619, acc 1
2017-09-09T16:09:38.774436: step 8169, loss 0.0370665, acc 0.984375
2017-09-09T16:09:39.115746: step 8170, loss 0.000102015, acc 1
2017-09-09T16:09:39.411035: step 8171, loss 0.0537216, acc 0.96875
2017-09-09T16:09:39.756173: step 8172, loss 0.107942, acc 0.953125
2017-09-09T16:09:40.048948: step 8173, loss 0.00217008, acc 1
2017-09-09T16:09:40.384542: step 8174, loss 0.00480413, acc 1
2017-09-09T16:09:40.712442: step 8175, loss 0.00136552, acc 1
2017-09-09T16:09:40.992079: step 8176, loss 0.00141809, acc 1
2017-09-09T16:09:41.346431: step 8177, loss 0.0059171, acc 1
2017-09-09T16:09:41.655990: step 8178, loss 0.00643281, acc 1
2017-09-09T16:09:42.001510: step 8179, loss 0.0388736, acc 0.96875
2017-09-09T16:09:42.285694: step 8180, loss 0.0480196, acc 0.96875
2017-09-09T16:09:42.612816: step 8181, loss 0.0637461, acc 0.953125
2017-09-09T16:09:42.934960: step 8182, loss 0.0043961, acc 1
2017-09-09T16:09:43.237982: step 8183, loss 0.0610348, acc 0.984375
2017-09-09T16:09:43.569850: step 8184, loss 0.0368043, acc 0.96875
2017-09-09T16:09:43.877207: step 8185, loss 0.0172031, acc 0.984375
2017-09-09T16:09:44.245726: step 8186, loss 0.0170931, acc 1
2017-09-09T16:09:44.533508: step 8187, loss 0.0208649, acc 1
2017-09-09T16:09:44.910774: step 8188, loss 0.0831742, acc 0.96875
2017-09-09T16:09:45.288303: step 8189, loss 0.0123963, acc 1
2017-09-09T16:09:45.602879: step 8190, loss 0.0202376, acc 0.984375
2017-09-09T16:09:45.926531: step 8191, loss 0.0556081, acc 0.984375
2017-09-09T16:09:46.199425: step 8192, loss 0.0127631, acc 1
2017-09-09T16:09:46.503604: step 8193, loss 0.021268, acc 0.984375
2017-09-09T16:09:46.839477: step 8194, loss 0.0169524, acc 0.984375
2017-09-09T16:09:47.118664: step 8195, loss 0.0189204, acc 0.984375
2017-09-09T16:09:47.521051: step 8196, loss 0.0351221, acc 0.96875
2017-09-09T16:09:47.848736: step 8197, loss 0.0429243, acc 0.984375
2017-09-09T16:09:48.193343: step 8198, loss 0.00699448, acc 1
2017-09-09T16:09:48.471541: step 8199, loss 0.0298132, acc 1
2017-09-09T16:09:48.860818: step 8200, loss 0.000925809, acc 1

Evaluation:
2017-09-09T16:09:48.961706: step 8200, loss 2.13333, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8200

2017-09-09T16:09:51.281258: step 8201, loss 0.0160937, acc 0.984375
2017-09-09T16:09:51.634659: step 8202, loss 0.0158861, acc 1
2017-09-09T16:09:51.930218: step 8203, loss 0.032103, acc 0.96875
2017-09-09T16:09:52.322184: step 8204, loss 0.0220113, acc 1
2017-09-09T16:09:52.663434: step 8205, loss 0.000909816, acc 1
2017-09-09T16:09:52.992297: step 8206, loss 0.0765341, acc 0.96875
2017-09-09T16:09:53.336944: step 8207, loss 0.0223654, acc 0.984375
2017-09-09T16:09:53.662990: step 8208, loss 0.00377664, acc 1
2017-09-09T16:09:53.955532: step 8209, loss 0.0126998, acc 1
2017-09-09T16:09:54.317980: step 8210, loss 0.0419844, acc 0.984375
2017-09-09T16:09:54.682766: step 8211, loss 0.00175218, acc 1
2017-09-09T16:09:55.009996: step 8212, loss 0.0093733, acc 1
2017-09-09T16:09:55.379690: step 8213, loss 0.0194549, acc 0.984375
2017-09-09T16:09:55.689737: step 8214, loss 0.0208704, acc 1
2017-09-09T16:09:56.055553: step 8215, loss 0.0580825, acc 0.96875
2017-09-09T16:09:56.355106: step 8216, loss 0.00841703, acc 1
2017-09-09T16:09:56.684065: step 8217, loss 0.0275569, acc 1
2017-09-09T16:09:56.982149: step 8218, loss 0.0570728, acc 0.984375
2017-09-09T16:09:57.277912: step 8219, loss 0.00130119, acc 1
2017-09-09T16:09:57.602495: step 8220, loss 0.0727784, acc 0.984375
2017-09-09T16:09:57.883184: step 8221, loss 0.00277718, acc 1
2017-09-09T16:09:58.228429: step 8222, loss 0.03715, acc 0.984375
2017-09-09T16:09:58.518707: step 8223, loss 0.00210092, acc 1
2017-09-09T16:09:58.872537: step 8224, loss 0.0632264, acc 0.96875
2017-09-09T16:09:59.151619: step 8225, loss 0.0287864, acc 1
2017-09-09T16:09:59.486974: step 8226, loss 0.000919581, acc 1
2017-09-09T16:09:59.823789: step 8227, loss 0.000203694, acc 1
2017-09-09T16:10:00.129323: step 8228, loss 0.0168373, acc 1
2017-09-09T16:10:00.426946: step 8229, loss 0.0263131, acc 0.96875
2017-09-09T16:10:00.801310: step 8230, loss 0.00034146, acc 1
2017-09-09T16:10:01.100372: step 8231, loss 0.0204419, acc 1
2017-09-09T16:10:01.406562: step 8232, loss 0.0018554, acc 1
2017-09-09T16:10:01.695271: step 8233, loss 0.046173, acc 0.984375
2017-09-09T16:10:02.053056: step 8234, loss 0.0115399, acc 1
2017-09-09T16:10:02.351274: step 8235, loss 0.021754, acc 0.984375
2017-09-09T16:10:02.628179: step 8236, loss 0.0565315, acc 0.96875
2017-09-09T16:10:02.999442: step 8237, loss 0.00389365, acc 1
2017-09-09T16:10:03.318395: step 8238, loss 0.0147125, acc 1
2017-09-09T16:10:03.614011: step 8239, loss 0.063107, acc 0.96875
2017-09-09T16:10:03.957001: step 8240, loss 0.000397733, acc 1
2017-09-09T16:10:04.288381: step 8241, loss 0.0542915, acc 0.984375
2017-09-09T16:10:04.592934: step 8242, loss 0.0274586, acc 0.984375
2017-09-09T16:10:04.969653: step 8243, loss 0.0289556, acc 1
2017-09-09T16:10:05.321495: step 8244, loss 0.0285037, acc 0.984375
2017-09-09T16:10:05.667754: step 8245, loss 0.0147331, acc 1
2017-09-09T16:10:05.995511: step 8246, loss 0.0258918, acc 0.984375
2017-09-09T16:10:06.291861: step 8247, loss 0.0683791, acc 0.96875
2017-09-09T16:10:06.663031: step 8248, loss 0.0280784, acc 0.984375
2017-09-09T16:10:06.951972: step 8249, loss 0.00337195, acc 1
2017-09-09T16:10:07.307014: step 8250, loss 0.00662613, acc 1

Evaluation:
2017-09-09T16:10:07.367711: step 8250, loss 2.38569, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8250

2017-09-09T16:10:10.688968: step 8251, loss 0.0114519, acc 1
2017-09-09T16:10:10.976744: step 8252, loss 0.0100575, acc 1
2017-09-09T16:10:11.308613: step 8253, loss 0.0352854, acc 0.96875
2017-09-09T16:10:11.637660: step 8254, loss 0.00190764, acc 1
2017-09-09T16:10:11.960593: step 8255, loss 0.0262655, acc 0.984375
2017-09-09T16:10:12.253684: step 8256, loss 0.0314833, acc 0.96875
2017-09-09T16:10:12.583258: step 8257, loss 0.0111223, acc 1
2017-09-09T16:10:12.938112: step 8258, loss 0.000129097, acc 1
2017-09-09T16:10:13.249300: step 8259, loss 0.000669694, acc 1
2017-09-09T16:10:13.539580: step 8260, loss 0.0410567, acc 0.984375
2017-09-09T16:10:13.895212: step 8261, loss 0.0158366, acc 1
2017-09-09T16:10:14.203416: step 8262, loss 0.0514737, acc 0.96875
2017-09-09T16:10:14.553156: step 8263, loss 0.0301652, acc 0.984375
2017-09-09T16:10:14.863478: step 8264, loss 0.00585431, acc 1
2017-09-09T16:10:15.184369: step 8265, loss 0.0414614, acc 0.984375
2017-09-09T16:10:15.579105: step 8266, loss 0.0359443, acc 0.984375
2017-09-09T16:10:15.872684: step 8267, loss 0.00280462, acc 1
2017-09-09T16:10:16.199874: step 8268, loss 0.00213386, acc 1
2017-09-09T16:10:16.470965: step 8269, loss 0.0449142, acc 0.984375
2017-09-09T16:10:16.821004: step 8270, loss 0.0194996, acc 0.984375
2017-09-09T16:10:17.129871: step 8271, loss 0.0103088, acc 1
2017-09-09T16:10:17.456429: step 8272, loss 0.0239125, acc 0.984375
2017-09-09T16:10:17.718255: step 8273, loss 0.00613719, acc 1
2017-09-09T16:10:18.090421: step 8274, loss 0.00587426, acc 1
2017-09-09T16:10:18.429729: step 8275, loss 0.0016028, acc 1
2017-09-09T16:10:18.757926: step 8276, loss 0.0334996, acc 0.984375
2017-09-09T16:10:19.073387: step 8277, loss 0.0211047, acc 0.984375
2017-09-09T16:10:19.377166: step 8278, loss 0.0125808, acc 0.984375
2017-09-09T16:10:19.713154: step 8279, loss 0.00676372, acc 1
2017-09-09T16:10:20.059387: step 8280, loss 0.0226879, acc 1
2017-09-09T16:10:20.379576: step 8281, loss 0.00521637, acc 1
2017-09-09T16:10:20.683436: step 8282, loss 0.0162973, acc 0.984375
2017-09-09T16:10:20.959430: step 8283, loss 0.00129951, acc 1
2017-09-09T16:10:21.285137: step 8284, loss 0.0360503, acc 0.984375
2017-09-09T16:10:21.574866: step 8285, loss 0.0430824, acc 0.96875
2017-09-09T16:10:21.905834: step 8286, loss 0.0299389, acc 0.984375
2017-09-09T16:10:22.233343: step 8287, loss 0.00226898, acc 1
2017-09-09T16:10:22.506445: step 8288, loss 0.0264186, acc 0.984375
2017-09-09T16:10:22.892348: step 8289, loss 0.00107485, acc 1
2017-09-09T16:10:23.195721: step 8290, loss 0.0204776, acc 0.984375
2017-09-09T16:10:23.516389: step 8291, loss 0.0385766, acc 0.984375
2017-09-09T16:10:23.828447: step 8292, loss 0.00127375, acc 1
2017-09-09T16:10:24.154342: step 8293, loss 0.00329769, acc 1
2017-09-09T16:10:24.507371: step 8294, loss 0.00476791, acc 1
2017-09-09T16:10:24.787296: step 8295, loss 0.000864567, acc 1
2017-09-09T16:10:25.118112: step 8296, loss 0.0174221, acc 0.984375
2017-09-09T16:10:25.459272: step 8297, loss 0.0376405, acc 0.984375
2017-09-09T16:10:25.766106: step 8298, loss 0.0850702, acc 0.953125
2017-09-09T16:10:26.173305: step 8299, loss 0.0375963, acc 0.96875
2017-09-09T16:10:26.476446: step 8300, loss 0.0636887, acc 0.96875

Evaluation:
2017-09-09T16:10:26.597428: step 8300, loss 2.11177, acc 0.353957

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8300

2017-09-09T16:10:28.663382: step 8301, loss 0.000665526, acc 1
2017-09-09T16:10:28.938474: step 8302, loss 0.0202448, acc 0.984375
2017-09-09T16:10:29.263190: step 8303, loss 0.0135438, acc 1
2017-09-09T16:10:29.548949: step 8304, loss 0.00828581, acc 1
2017-09-09T16:10:29.844967: step 8305, loss 0.00253, acc 1
2017-09-09T16:10:30.154242: step 8306, loss 0.00269153, acc 1
2017-09-09T16:10:30.521588: step 8307, loss 0.013744, acc 1
2017-09-09T16:10:30.830239: step 8308, loss 0.0344761, acc 0.984375
2017-09-09T16:10:31.147825: step 8309, loss 0.0458299, acc 0.984375
2017-09-09T16:10:31.425957: step 8310, loss 0.0189862, acc 0.984375
2017-09-09T16:10:31.732982: step 8311, loss 0.0358607, acc 0.984375
2017-09-09T16:10:32.046787: step 8312, loss 0.0347049, acc 0.984375
2017-09-09T16:10:32.332401: step 8313, loss 0.00306536, acc 1
2017-09-09T16:10:32.697746: step 8314, loss 0.000661078, acc 1
2017-09-09T16:10:32.978970: step 8315, loss 0.0793609, acc 0.953125
2017-09-09T16:10:33.266695: step 8316, loss 0.0665936, acc 0.96875
2017-09-09T16:10:33.599534: step 8317, loss 0.0131441, acc 1
2017-09-09T16:10:33.888974: step 8318, loss 0.0004789, acc 1
2017-09-09T16:10:34.268191: step 8319, loss 0.00626742, acc 1
2017-09-09T16:10:34.620932: step 8320, loss 0.0457977, acc 0.984375
2017-09-09T16:10:34.917714: step 8321, loss 0.0419391, acc 0.984375
2017-09-09T16:10:35.224996: step 8322, loss 0.0157627, acc 0.984375
2017-09-09T16:10:35.510203: step 8323, loss 0.0572918, acc 0.96875
2017-09-09T16:10:35.835993: step 8324, loss 0.0741696, acc 0.96875
2017-09-09T16:10:36.138192: step 8325, loss 0.000520864, acc 1
2017-09-09T16:10:36.464987: step 8326, loss 0.0292044, acc 0.984375
2017-09-09T16:10:36.741315: step 8327, loss 0.0359595, acc 0.984375
2017-09-09T16:10:37.151479: step 8328, loss 0.0613079, acc 0.953125
2017-09-09T16:10:37.528929: step 8329, loss 0.0322497, acc 0.984375
2017-09-09T16:10:37.807864: step 8330, loss 0.000296776, acc 1
2017-09-09T16:10:38.182685: step 8331, loss 0.0161512, acc 0.984375
2017-09-09T16:10:38.484731: step 8332, loss 0.0144264, acc 1
2017-09-09T16:10:38.790236: step 8333, loss 0.000164588, acc 1
2017-09-09T16:10:39.026811: step 8334, loss 0.00174563, acc 1
2017-09-09T16:10:39.730828: step 8335, loss 0.0152528, acc 0.984375
2017-09-09T16:10:40.066229: step 8336, loss 0.0205151, acc 0.984375
2017-09-09T16:10:40.435670: step 8337, loss 0.080317, acc 0.953125
2017-09-09T16:10:40.740466: step 8338, loss 0.0369066, acc 0.96875
2017-09-09T16:10:41.070955: step 8339, loss 0.0264496, acc 0.984375
2017-09-09T16:10:41.483411: step 8340, loss 0.0240149, acc 0.984375
2017-09-09T16:10:41.812138: step 8341, loss 0.0266259, acc 0.984375
2017-09-09T16:10:42.149458: step 8342, loss 0.0449651, acc 0.96875
2017-09-09T16:10:42.490469: step 8343, loss 0.0508027, acc 0.96875
2017-09-09T16:10:42.821601: step 8344, loss 0.00068036, acc 1
2017-09-09T16:10:43.170722: step 8345, loss 0.0459813, acc 0.96875
2017-09-09T16:10:43.486275: step 8346, loss 0.0324162, acc 0.984375
2017-09-09T16:10:43.798665: step 8347, loss 0.020384, acc 0.984375
2017-09-09T16:10:44.124379: step 8348, loss 0.0361156, acc 0.984375
2017-09-09T16:10:44.408202: step 8349, loss 0.0240115, acc 0.984375
2017-09-09T16:10:44.772844: step 8350, loss 0.0104598, acc 1

Evaluation:
2017-09-09T16:10:44.854786: step 8350, loss 2.0732, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8350

2017-09-09T16:10:47.757201: step 8351, loss 0.0290465, acc 0.984375
2017-09-09T16:10:48.082252: step 8352, loss 0.0188274, acc 1
2017-09-09T16:10:48.346870: step 8353, loss 0.0537011, acc 0.953125
2017-09-09T16:10:48.643317: step 8354, loss 0.073397, acc 0.953125
2017-09-09T16:10:48.950531: step 8355, loss 0.00418972, acc 1
2017-09-09T16:10:49.221476: step 8356, loss 0.020537, acc 1
2017-09-09T16:10:49.529404: step 8357, loss 0.0126154, acc 0.984375
2017-09-09T16:10:49.891193: step 8358, loss 0.00449575, acc 1
2017-09-09T16:10:50.192511: step 8359, loss 0.0200563, acc 1
2017-09-09T16:10:50.541177: step 8360, loss 0.00124444, acc 1
2017-09-09T16:10:50.902846: step 8361, loss 0.0186588, acc 1
2017-09-09T16:10:51.200949: step 8362, loss 0.0240482, acc 0.984375
2017-09-09T16:10:51.555730: step 8363, loss 0.000154805, acc 1
2017-09-09T16:10:51.837277: step 8364, loss 0.0749636, acc 0.984375
2017-09-09T16:10:52.188140: step 8365, loss 0.0379038, acc 0.96875
2017-09-09T16:10:52.501088: step 8366, loss 0.000739896, acc 1
2017-09-09T16:10:52.826415: step 8367, loss 0.000387867, acc 1
2017-09-09T16:10:53.189090: step 8368, loss 0.000508778, acc 1
2017-09-09T16:10:53.488653: step 8369, loss 0.00569752, acc 1
2017-09-09T16:10:53.844622: step 8370, loss 0.00366218, acc 1
2017-09-09T16:10:54.173179: step 8371, loss 0.0132236, acc 1
2017-09-09T16:10:54.479958: step 8372, loss 0.0142906, acc 1
2017-09-09T16:10:54.789484: step 8373, loss 0.0535571, acc 0.984375
2017-09-09T16:10:55.099444: step 8374, loss 0.00126514, acc 1
2017-09-09T16:10:55.513706: step 8375, loss 0.070524, acc 0.953125
2017-09-09T16:10:55.882072: step 8376, loss 0.000523151, acc 1
2017-09-09T16:10:56.158329: step 8377, loss 0.020449, acc 0.984375
2017-09-09T16:10:56.534922: step 8378, loss 0.0362201, acc 0.984375
2017-09-09T16:10:56.883054: step 8379, loss 0.026117, acc 1
2017-09-09T16:10:57.162556: step 8380, loss 0.0452637, acc 0.96875
2017-09-09T16:10:57.504049: step 8381, loss 0.0149398, acc 0.984375
2017-09-09T16:10:57.783560: step 8382, loss 0.0720962, acc 0.984375
2017-09-09T16:10:58.104938: step 8383, loss 0.000872994, acc 1
2017-09-09T16:10:58.409055: step 8384, loss 0.0087966, acc 1
2017-09-09T16:10:58.748046: step 8385, loss 0.0422964, acc 0.953125
2017-09-09T16:10:59.081694: step 8386, loss 0.000613732, acc 1
2017-09-09T16:10:59.374456: step 8387, loss 0.0224521, acc 0.984375
2017-09-09T16:10:59.757178: step 8388, loss 0.0527768, acc 0.96875
2017-09-09T16:11:00.088952: step 8389, loss 0.0270435, acc 0.984375
2017-09-09T16:11:00.357168: step 8390, loss 0.0176797, acc 1
2017-09-09T16:11:00.665002: step 8391, loss 0.0114068, acc 1
2017-09-09T16:11:01.000815: step 8392, loss 0.0169044, acc 1
2017-09-09T16:11:01.263244: step 8393, loss 0.00187238, acc 1
2017-09-09T16:11:01.587844: step 8394, loss 0.0165236, acc 0.984375
2017-09-09T16:11:01.906795: step 8395, loss 0.0179133, acc 0.984375
2017-09-09T16:11:02.251820: step 8396, loss 0.00384205, acc 1
2017-09-09T16:11:02.578261: step 8397, loss 0.0206228, acc 1
2017-09-09T16:11:02.954982: step 8398, loss 0.00106831, acc 1
2017-09-09T16:11:03.271606: step 8399, loss 0.0265073, acc 0.984375
2017-09-09T16:11:03.615034: step 8400, loss 0.0281877, acc 0.984375

Evaluation:
2017-09-09T16:11:03.719541: step 8400, loss 1.85261, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8400

2017-09-09T16:11:06.136664: step 8401, loss 0.0215785, acc 0.984375
2017-09-09T16:11:06.469749: step 8402, loss 0.00234808, acc 1
2017-09-09T16:11:06.877978: step 8403, loss 0.00927477, acc 1
2017-09-09T16:11:07.193477: step 8404, loss 0.0241313, acc 0.984375
2017-09-09T16:11:07.525046: step 8405, loss 0.043289, acc 0.96875
2017-09-09T16:11:07.935114: step 8406, loss 0.000498223, acc 1
2017-09-09T16:11:08.247175: step 8407, loss 0.00351436, acc 1
2017-09-09T16:11:08.512542: step 8408, loss 0.0539741, acc 0.953125
2017-09-09T16:11:08.912952: step 8409, loss 0.0432185, acc 0.96875
2017-09-09T16:11:09.213001: step 8410, loss 0.00192921, acc 1
2017-09-09T16:11:09.536408: step 8411, loss 0.0409242, acc 0.984375
2017-09-09T16:11:09.827159: step 8412, loss 0.0664496, acc 0.96875
2017-09-09T16:11:10.122675: step 8413, loss 0.00299234, acc 1
2017-09-09T16:11:10.493083: step 8414, loss 0.0275709, acc 0.96875
2017-09-09T16:11:10.791923: step 8415, loss 0.0423935, acc 0.984375
2017-09-09T16:11:11.156557: step 8416, loss 0.0201365, acc 0.984375
2017-09-09T16:11:11.476493: step 8417, loss 0.00608874, acc 1
2017-09-09T16:11:11.796673: step 8418, loss 0.0204876, acc 0.984375
2017-09-09T16:11:12.177098: step 8419, loss 0.0522149, acc 0.984375
2017-09-09T16:11:12.469726: step 8420, loss 0.0335276, acc 0.984375
2017-09-09T16:11:12.808232: step 8421, loss 0.00283676, acc 1
2017-09-09T16:11:13.149147: step 8422, loss 0.0058536, acc 1
2017-09-09T16:11:13.468563: step 8423, loss 0.00133025, acc 1
2017-09-09T16:11:13.806059: step 8424, loss 0.00104559, acc 1
2017-09-09T16:11:14.166380: step 8425, loss 0.01277, acc 1
2017-09-09T16:11:14.493178: step 8426, loss 0.0169695, acc 0.984375
2017-09-09T16:11:14.883347: step 8427, loss 0.02629, acc 0.96875
2017-09-09T16:11:15.161258: step 8428, loss 0.0261119, acc 0.980392
2017-09-09T16:11:15.477338: step 8429, loss 0.00458175, acc 1
2017-09-09T16:11:15.774218: step 8430, loss 0.0470551, acc 0.96875
2017-09-09T16:11:16.131897: step 8431, loss 0.0079892, acc 1
2017-09-09T16:11:16.386821: step 8432, loss 0.0412716, acc 0.984375
2017-09-09T16:11:16.661492: step 8433, loss 0.0774394, acc 0.953125
2017-09-09T16:11:17.012903: step 8434, loss 0.000312394, acc 1
2017-09-09T16:11:17.296752: step 8435, loss 9.03106e-05, acc 1
2017-09-09T16:11:17.568155: step 8436, loss 0.0376319, acc 0.96875
2017-09-09T16:11:17.907940: step 8437, loss 0.0109937, acc 1
2017-09-09T16:11:18.210489: step 8438, loss 0.0289448, acc 0.984375
2017-09-09T16:11:18.546492: step 8439, loss 0.00963299, acc 1
2017-09-09T16:11:18.866153: step 8440, loss 0.0140159, acc 0.984375
2017-09-09T16:11:19.247321: step 8441, loss 0.0395096, acc 0.984375
2017-09-09T16:11:19.558345: step 8442, loss 0.021873, acc 0.984375
2017-09-09T16:11:19.862209: step 8443, loss 0.00304361, acc 1
2017-09-09T16:11:20.189493: step 8444, loss 0.0180531, acc 0.984375
2017-09-09T16:11:20.565177: step 8445, loss 0.00189145, acc 1
2017-09-09T16:11:20.911422: step 8446, loss 0.00042399, acc 1
2017-09-09T16:11:21.323660: step 8447, loss 0.0667676, acc 0.96875
2017-09-09T16:11:21.671466: step 8448, loss 0.0630292, acc 0.953125
2017-09-09T16:11:22.014515: step 8449, loss 0.0411585, acc 0.984375
2017-09-09T16:11:22.337100: step 8450, loss 0.00426052, acc 1

Evaluation:
2017-09-09T16:11:22.424936: step 8450, loss 1.93249, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8450

2017-09-09T16:11:26.008185: step 8451, loss 0.0118583, acc 1
2017-09-09T16:11:26.296797: step 8452, loss 0.00410279, acc 1
2017-09-09T16:11:26.597549: step 8453, loss 0.0490302, acc 1
2017-09-09T16:11:26.945025: step 8454, loss 0.00220233, acc 1
2017-09-09T16:11:27.225749: step 8455, loss 0.00262283, acc 1
2017-09-09T16:11:27.517547: step 8456, loss 0.0634313, acc 0.953125
2017-09-09T16:11:27.844906: step 8457, loss 0.062031, acc 0.953125
2017-09-09T16:11:28.193156: step 8458, loss 0.0264638, acc 0.984375
2017-09-09T16:11:28.496002: step 8459, loss 0.0148789, acc 0.984375
2017-09-09T16:11:28.749255: step 8460, loss 0.0185753, acc 0.984375
2017-09-09T16:11:29.037875: step 8461, loss 0.00871868, acc 1
2017-09-09T16:11:29.325487: step 8462, loss 0.0485551, acc 0.953125
2017-09-09T16:11:29.588413: step 8463, loss 0.000514795, acc 1
2017-09-09T16:11:29.921651: step 8464, loss 0.0472947, acc 0.984375
2017-09-09T16:11:30.223143: step 8465, loss 0.00257396, acc 1
2017-09-09T16:11:30.521907: step 8466, loss 0.0116072, acc 1
2017-09-09T16:11:30.836886: step 8467, loss 0.037373, acc 0.984375
2017-09-09T16:11:31.155298: step 8468, loss 0.021118, acc 1
2017-09-09T16:11:31.489903: step 8469, loss 0.0213217, acc 0.984375
2017-09-09T16:11:31.830623: step 8470, loss 0.0497652, acc 0.984375
2017-09-09T16:11:32.194634: step 8471, loss 0.00191031, acc 1
2017-09-09T16:11:32.519483: step 8472, loss 0.019283, acc 0.984375
2017-09-09T16:11:32.818689: step 8473, loss 0.0185407, acc 0.984375
2017-09-09T16:11:33.135616: step 8474, loss 0.0160781, acc 0.984375
2017-09-09T16:11:33.430128: step 8475, loss 0.0118514, acc 1
2017-09-09T16:11:33.852349: step 8476, loss 0.000317064, acc 1
2017-09-09T16:11:34.151658: step 8477, loss 0.0142293, acc 1
2017-09-09T16:11:34.465707: step 8478, loss 0.0211118, acc 0.984375
2017-09-09T16:11:34.805897: step 8479, loss 0.00158519, acc 1
2017-09-09T16:11:35.095767: step 8480, loss 0.0266685, acc 0.984375
2017-09-09T16:11:35.422154: step 8481, loss 0.0540771, acc 0.984375
2017-09-09T16:11:35.696446: step 8482, loss 0.0138765, acc 1
2017-09-09T16:11:36.109208: step 8483, loss 0.0374233, acc 0.984375
2017-09-09T16:11:36.460598: step 8484, loss 0.0370675, acc 0.984375
2017-09-09T16:11:36.739542: step 8485, loss 0.0103694, acc 1
2017-09-09T16:11:37.048301: step 8486, loss 0.00190173, acc 1
2017-09-09T16:11:37.374343: step 8487, loss 0.0527576, acc 0.96875
2017-09-09T16:11:37.681741: step 8488, loss 0.0252851, acc 0.984375
2017-09-09T16:11:38.007617: step 8489, loss 0.0174852, acc 1
2017-09-09T16:11:38.293766: step 8490, loss 0.0165351, acc 1
2017-09-09T16:11:38.596945: step 8491, loss 0.0138445, acc 1
2017-09-09T16:11:38.930620: step 8492, loss 0.00372578, acc 1
2017-09-09T16:11:39.327850: step 8493, loss 9.58179e-05, acc 1
2017-09-09T16:11:39.628370: step 8494, loss 0.00243262, acc 1
2017-09-09T16:11:39.961862: step 8495, loss 0.0555805, acc 0.984375
2017-09-09T16:11:40.255645: step 8496, loss 0.0068917, acc 1
2017-09-09T16:11:40.891081: step 8497, loss 0.0607004, acc 0.96875
2017-09-09T16:11:41.182040: step 8498, loss 0.052643, acc 0.984375
2017-09-09T16:11:41.528285: step 8499, loss 0.0231387, acc 1
2017-09-09T16:11:41.809418: step 8500, loss 0.0317436, acc 0.984375

Evaluation:
2017-09-09T16:11:41.936714: step 8500, loss 2.55673, acc 0.351079

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8500

2017-09-09T16:11:45.123397: step 8501, loss 0.00179899, acc 1
2017-09-09T16:11:45.446438: step 8502, loss 0.0157267, acc 0.984375
2017-09-09T16:11:45.755775: step 8503, loss 0.0116826, acc 1
2017-09-09T16:11:46.029041: step 8504, loss 0.00112348, acc 1
2017-09-09T16:11:46.328626: step 8505, loss 0.00936678, acc 1
2017-09-09T16:11:46.627846: step 8506, loss 0.000144509, acc 1
2017-09-09T16:11:46.906340: step 8507, loss 0.0501989, acc 0.953125
2017-09-09T16:11:47.249056: step 8508, loss 0.0318919, acc 1
2017-09-09T16:11:47.556170: step 8509, loss 0.0441964, acc 0.984375
2017-09-09T16:11:47.864706: step 8510, loss 0.139553, acc 0.9375
2017-09-09T16:11:48.189693: step 8511, loss 0.0297738, acc 0.984375
2017-09-09T16:11:48.488414: step 8512, loss 0.0273355, acc 0.96875
2017-09-09T16:11:48.826427: step 8513, loss 0.0281257, acc 0.984375
2017-09-09T16:11:49.115289: step 8514, loss 0.0253282, acc 0.984375
2017-09-09T16:11:49.439912: step 8515, loss 0.0255244, acc 1
2017-09-09T16:11:49.744372: step 8516, loss 0.00541788, acc 1
2017-09-09T16:11:50.083756: step 8517, loss 0.0149212, acc 0.984375
2017-09-09T16:11:50.385784: step 8518, loss 0.0275231, acc 1
2017-09-09T16:11:50.718570: step 8519, loss 0.0390242, acc 0.984375
2017-09-09T16:11:51.140395: step 8520, loss 0.0387434, acc 0.984375
2017-09-09T16:11:51.442063: step 8521, loss 0.0313589, acc 0.984375
2017-09-09T16:11:51.801725: step 8522, loss 0.0500603, acc 0.96875
2017-09-09T16:11:52.137154: step 8523, loss 0.0216507, acc 0.984375
2017-09-09T16:11:52.430145: step 8524, loss 0.023236, acc 0.984375
2017-09-09T16:11:52.815698: step 8525, loss 0.0278444, acc 1
2017-09-09T16:11:53.134552: step 8526, loss 0.00475234, acc 1
2017-09-09T16:11:53.429865: step 8527, loss 0.0429466, acc 0.984375
2017-09-09T16:11:53.727079: step 8528, loss 0.000323863, acc 1
2017-09-09T16:11:54.039990: step 8529, loss 0.0246675, acc 0.96875
2017-09-09T16:11:54.417627: step 8530, loss 0.0111221, acc 1
2017-09-09T16:11:54.712378: step 8531, loss 0.00509426, acc 1
2017-09-09T16:11:55.103346: step 8532, loss 0.0976347, acc 0.96875
2017-09-09T16:11:55.398206: step 8533, loss 0.0466448, acc 0.984375
2017-09-09T16:11:55.700503: step 8534, loss 0.00204344, acc 1
2017-09-09T16:11:56.002902: step 8535, loss 0.00115621, acc 1
2017-09-09T16:11:56.281376: step 8536, loss 0.0348791, acc 0.984375
2017-09-09T16:11:56.609166: step 8537, loss 0.0402409, acc 0.984375
2017-09-09T16:11:56.929549: step 8538, loss 0.0218916, acc 1
2017-09-09T16:11:57.305900: step 8539, loss 0.0724925, acc 0.984375
2017-09-09T16:11:57.611152: step 8540, loss 0.000428479, acc 1
2017-09-09T16:11:57.932013: step 8541, loss 0.00397256, acc 1
2017-09-09T16:11:58.235472: step 8542, loss 0.0324594, acc 0.96875
2017-09-09T16:11:58.610379: step 8543, loss 0.0313958, acc 0.96875
2017-09-09T16:11:58.949365: step 8544, loss 0.0113519, acc 1
2017-09-09T16:11:59.259516: step 8545, loss 0.115162, acc 0.984375
2017-09-09T16:11:59.547739: step 8546, loss 0.0158002, acc 1
2017-09-09T16:11:59.861529: step 8547, loss 0.00669064, acc 1
2017-09-09T16:12:00.160779: step 8548, loss 0.0015757, acc 1
2017-09-09T16:12:00.454870: step 8549, loss 0.0282128, acc 0.984375
2017-09-09T16:12:00.852926: step 8550, loss 0.050657, acc 0.984375

Evaluation:
2017-09-09T16:12:00.938165: step 8550, loss 2.16643, acc 0.342446

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8550

2017-09-09T16:12:03.088399: step 8551, loss 0.000893735, acc 1
2017-09-09T16:12:03.384382: step 8552, loss 0.00205835, acc 1
2017-09-09T16:12:03.712200: step 8553, loss 0.032984, acc 0.984375
2017-09-09T16:12:03.994898: step 8554, loss 0.0258149, acc 0.984375
2017-09-09T16:12:04.291830: step 8555, loss 0.0159362, acc 0.984375
2017-09-09T16:12:04.564569: step 8556, loss 0.0104441, acc 1
2017-09-09T16:12:04.913955: step 8557, loss 0.0106202, acc 1
2017-09-09T16:12:05.247628: step 8558, loss 0.00441306, acc 1
2017-09-09T16:12:05.557391: step 8559, loss 0.0155228, acc 1
2017-09-09T16:12:05.940299: step 8560, loss 0.038208, acc 0.984375
2017-09-09T16:12:06.303924: step 8561, loss 0.00368958, acc 1
2017-09-09T16:12:06.593120: step 8562, loss 0.0166812, acc 1
2017-09-09T16:12:06.974563: step 8563, loss 0.043945, acc 0.984375
2017-09-09T16:12:07.268698: step 8564, loss 0.00291812, acc 1
2017-09-09T16:12:07.623606: step 8565, loss 0.0568381, acc 0.984375
2017-09-09T16:12:07.976688: step 8566, loss 0.042588, acc 0.984375
2017-09-09T16:12:08.273883: step 8567, loss 0.0647586, acc 0.96875
2017-09-09T16:12:08.651433: step 8568, loss 0.0240287, acc 1
2017-09-09T16:12:08.952372: step 8569, loss 0.00517769, acc 1
2017-09-09T16:12:09.261917: step 8570, loss 0.0224023, acc 1
2017-09-09T16:12:09.553425: step 8571, loss 0.0196925, acc 1
2017-09-09T16:12:09.879114: step 8572, loss 0.0333545, acc 0.984375
2017-09-09T16:12:10.237437: step 8573, loss 0.0351464, acc 0.96875
2017-09-09T16:12:10.544779: step 8574, loss 0.0239208, acc 0.984375
2017-09-09T16:12:10.907588: step 8575, loss 0.0126466, acc 1
2017-09-09T16:12:11.328999: step 8576, loss 0.0212244, acc 0.984375
2017-09-09T16:12:11.616718: step 8577, loss 0.015467, acc 1
2017-09-09T16:12:11.948080: step 8578, loss 0.013292, acc 1
2017-09-09T16:12:12.258010: step 8579, loss 0.000271563, acc 1
2017-09-09T16:12:12.560536: step 8580, loss 0.0171648, acc 0.984375
2017-09-09T16:12:12.881695: step 8581, loss 0.0554383, acc 0.96875
2017-09-09T16:12:13.210605: step 8582, loss 0.0313154, acc 0.984375
2017-09-09T16:12:13.507751: step 8583, loss 0.00120082, acc 1
2017-09-09T16:12:13.842881: step 8584, loss 0.0547211, acc 0.96875
2017-09-09T16:12:14.176885: step 8585, loss 0.0627596, acc 0.984375
2017-09-09T16:12:14.479982: step 8586, loss 0.023233, acc 0.984375
2017-09-09T16:12:14.750923: step 8587, loss 0.0244239, acc 0.984375
2017-09-09T16:12:15.119845: step 8588, loss 0.0453171, acc 0.984375
2017-09-09T16:12:15.399964: step 8589, loss 0.0642241, acc 0.96875
2017-09-09T16:12:15.679950: step 8590, loss 0.00276873, acc 1
2017-09-09T16:12:15.983215: step 8591, loss 0.0115093, acc 1
2017-09-09T16:12:16.260231: step 8592, loss 0.0361136, acc 0.984375
2017-09-09T16:12:16.607655: step 8593, loss 0.0132982, acc 0.984375
2017-09-09T16:12:16.993147: step 8594, loss 0.00688426, acc 1
2017-09-09T16:12:17.336052: step 8595, loss 0.0107417, acc 1
2017-09-09T16:12:17.714393: step 8596, loss 0.0449362, acc 0.96875
2017-09-09T16:12:18.020860: step 8597, loss 0.0651444, acc 0.953125
2017-09-09T16:12:18.388098: step 8598, loss 0.0360533, acc 0.984375
2017-09-09T16:12:18.743472: step 8599, loss 0.0247683, acc 0.984375
2017-09-09T16:12:19.083121: step 8600, loss 0.0329569, acc 0.96875

Evaluation:
2017-09-09T16:12:19.192594: step 8600, loss 1.81798, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8600

2017-09-09T16:12:22.703719: step 8601, loss 0.00237156, acc 1
2017-09-09T16:12:22.974333: step 8602, loss 0.00426949, acc 1
2017-09-09T16:12:23.287175: step 8603, loss 0.0191274, acc 0.984375
2017-09-09T16:12:23.636109: step 8604, loss 0.0167981, acc 1
2017-09-09T16:12:23.922694: step 8605, loss 0.0299184, acc 0.984375
2017-09-09T16:12:24.243167: step 8606, loss 0.000889678, acc 1
2017-09-09T16:12:24.551926: step 8607, loss 0.0170124, acc 1
2017-09-09T16:12:24.913326: step 8608, loss 0.000912465, acc 1
2017-09-09T16:12:25.217664: step 8609, loss 0.0193664, acc 0.984375
2017-09-09T16:12:25.573148: step 8610, loss 0.0338851, acc 0.984375
2017-09-09T16:12:25.845380: step 8611, loss 1.71461e-05, acc 1
2017-09-09T16:12:26.168685: step 8612, loss 0.0412243, acc 0.96875
2017-09-09T16:12:26.422864: step 8613, loss 0.0106522, acc 1
2017-09-09T16:12:26.775340: step 8614, loss 0.0183089, acc 0.984375
2017-09-09T16:12:27.124867: step 8615, loss 0.00211314, acc 1
2017-09-09T16:12:27.428686: step 8616, loss 0.0719171, acc 0.96875
2017-09-09T16:12:27.747959: step 8617, loss 0.0664484, acc 0.96875
2017-09-09T16:12:28.103548: step 8618, loss 0.0108948, acc 1
2017-09-09T16:12:28.407764: step 8619, loss 0.0344863, acc 1
2017-09-09T16:12:28.731164: step 8620, loss 0.0178592, acc 0.984375
2017-09-09T16:12:29.005810: step 8621, loss 0.0125601, acc 0.984375
2017-09-09T16:12:29.348177: step 8622, loss 0.0122508, acc 1
2017-09-09T16:12:29.660640: step 8623, loss 0.0310775, acc 0.984375
2017-09-09T16:12:29.973396: step 8624, loss 0.0217863, acc 0.980392
2017-09-09T16:12:30.311364: step 8625, loss 0.0485025, acc 0.984375
2017-09-09T16:12:30.638023: step 8626, loss 0.0150576, acc 1
2017-09-09T16:12:31.013977: step 8627, loss 0.0106838, acc 1
2017-09-09T16:12:31.345383: step 8628, loss 0.0237532, acc 0.984375
2017-09-09T16:12:31.656574: step 8629, loss 0.00556208, acc 1
2017-09-09T16:12:32.039871: step 8630, loss 0.00218488, acc 1
2017-09-09T16:12:32.332292: step 8631, loss 0.0864864, acc 0.96875
2017-09-09T16:12:32.620431: step 8632, loss 0.104392, acc 0.953125
2017-09-09T16:12:32.955285: step 8633, loss 0.0137654, acc 0.984375
2017-09-09T16:12:33.251942: step 8634, loss 2.52618e-05, acc 1
2017-09-09T16:12:33.612784: step 8635, loss 0.0208837, acc 0.984375
2017-09-09T16:12:33.903851: step 8636, loss 0.0207052, acc 1
2017-09-09T16:12:34.215612: step 8637, loss 0.00693544, acc 1
2017-09-09T16:12:34.515261: step 8638, loss 0.00178134, acc 1
2017-09-09T16:12:34.804244: step 8639, loss 0.0172516, acc 0.984375
2017-09-09T16:12:35.119021: step 8640, loss 0.0159997, acc 0.984375
2017-09-09T16:12:35.434001: step 8641, loss 0.0456765, acc 0.984375
2017-09-09T16:12:35.782126: step 8642, loss 0.0139786, acc 1
2017-09-09T16:12:36.097840: step 8643, loss 0.0158701, acc 0.984375
2017-09-09T16:12:36.476140: step 8644, loss 0.0475671, acc 0.984375
2017-09-09T16:12:36.781999: step 8645, loss 0.00681359, acc 1
2017-09-09T16:12:37.101655: step 8646, loss 0.0181518, acc 0.984375
2017-09-09T16:12:37.450421: step 8647, loss 0.0458717, acc 0.984375
2017-09-09T16:12:37.766544: step 8648, loss 0.00132204, acc 1
2017-09-09T16:12:38.092209: step 8649, loss 0.000369149, acc 1
2017-09-09T16:12:38.357152: step 8650, loss 0.000708123, acc 1

Evaluation:
2017-09-09T16:12:38.468830: step 8650, loss 2.44283, acc 0.348201

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8650

2017-09-09T16:12:40.783486: step 8651, loss 0.000708086, acc 1
2017-09-09T16:12:41.176100: step 8652, loss 0.0233936, acc 0.984375
2017-09-09T16:12:41.475781: step 8653, loss 0.067348, acc 0.96875
2017-09-09T16:12:41.780934: step 8654, loss 0.00587696, acc 1
2017-09-09T16:12:42.148891: step 8655, loss 0.00141784, acc 1
2017-09-09T16:12:42.455080: step 8656, loss 0.0229957, acc 0.984375
2017-09-09T16:12:42.831572: step 8657, loss 0.0238949, acc 1
2017-09-09T16:12:43.138274: step 8658, loss 0.0364039, acc 0.984375
2017-09-09T16:12:43.511290: step 8659, loss 0.0533826, acc 0.96875
2017-09-09T16:12:43.836416: step 8660, loss 0.0671992, acc 0.96875
2017-09-09T16:12:44.145549: step 8661, loss 0.0136622, acc 1
2017-09-09T16:12:44.487563: step 8662, loss 0.031271, acc 0.984375
2017-09-09T16:12:44.889243: step 8663, loss 0.0498751, acc 0.96875
2017-09-09T16:12:45.199604: step 8664, loss 0.0311527, acc 1
2017-09-09T16:12:45.542738: step 8665, loss 0.00909064, acc 1
2017-09-09T16:12:45.823486: step 8666, loss 0.0303309, acc 0.984375
2017-09-09T16:12:46.195434: step 8667, loss 0.0205284, acc 1
2017-09-09T16:12:46.513872: step 8668, loss 0.0461169, acc 0.984375
2017-09-09T16:12:46.836336: step 8669, loss 0.00166831, acc 1
2017-09-09T16:12:47.198240: step 8670, loss 0.0511179, acc 0.984375
2017-09-09T16:12:47.465803: step 8671, loss 0.0441004, acc 0.96875
2017-09-09T16:12:47.806877: step 8672, loss 0.0198998, acc 0.984375
2017-09-09T16:12:48.189502: step 8673, loss 0.0309546, acc 0.984375
2017-09-09T16:12:48.495250: step 8674, loss 0.00639192, acc 1
2017-09-09T16:12:48.871342: step 8675, loss 0.000172807, acc 1
2017-09-09T16:12:49.146756: step 8676, loss 0.0110707, acc 1
2017-09-09T16:12:49.466951: step 8677, loss 0.00540971, acc 1
2017-09-09T16:12:49.772227: step 8678, loss 0.00230749, acc 1
2017-09-09T16:12:50.106630: step 8679, loss 0.0257807, acc 0.984375
2017-09-09T16:12:50.497713: step 8680, loss 0.0403145, acc 0.96875
2017-09-09T16:12:50.844883: step 8681, loss 0.00908459, acc 1
2017-09-09T16:12:51.164080: step 8682, loss 0.000894126, acc 1
2017-09-09T16:12:51.491980: step 8683, loss 0.0231259, acc 0.984375
2017-09-09T16:12:51.800489: step 8684, loss 0.000503883, acc 1
2017-09-09T16:12:52.170418: step 8685, loss 0.0134008, acc 0.984375
2017-09-09T16:12:52.506739: step 8686, loss 0.0330043, acc 0.984375
2017-09-09T16:12:52.799824: step 8687, loss 0.0079882, acc 1
2017-09-09T16:12:53.120058: step 8688, loss 0.0405778, acc 0.96875
2017-09-09T16:12:53.436609: step 8689, loss 0.0346405, acc 0.984375
2017-09-09T16:12:53.828249: step 8690, loss 0.000969024, acc 1
2017-09-09T16:12:54.127892: step 8691, loss 0.00492744, acc 1
2017-09-09T16:12:54.413659: step 8692, loss 0.0150011, acc 1
2017-09-09T16:12:54.707460: step 8693, loss 0.000868696, acc 1
2017-09-09T16:12:55.043321: step 8694, loss 0.00435437, acc 1
2017-09-09T16:12:55.397027: step 8695, loss 0.00287003, acc 1
2017-09-09T16:12:55.690178: step 8696, loss 0.0360878, acc 0.984375
2017-09-09T16:12:56.091821: step 8697, loss 0.00151203, acc 1
2017-09-09T16:12:56.380002: step 8698, loss 0.000270532, acc 1
2017-09-09T16:12:56.794306: step 8699, loss 0.0376603, acc 0.984375
2017-09-09T16:12:57.098736: step 8700, loss 0.0400247, acc 0.96875

Evaluation:
2017-09-09T16:12:57.175845: step 8700, loss 2.06835, acc 0.297842

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8700

2017-09-09T16:12:59.581430: step 8701, loss 0.0439187, acc 0.96875
2017-09-09T16:12:59.912019: step 8702, loss 0.0549528, acc 0.984375
2017-09-09T16:13:00.211336: step 8703, loss 0.0244869, acc 0.984375
2017-09-09T16:13:00.508469: step 8704, loss 0.061157, acc 0.96875
2017-09-09T16:13:00.846063: step 8705, loss 0.00606789, acc 1
2017-09-09T16:13:01.123203: step 8706, loss 0.0260718, acc 0.984375
2017-09-09T16:13:01.510722: step 8707, loss 0.0063781, acc 1
2017-09-09T16:13:01.803898: step 8708, loss 0.0032248, acc 1
2017-09-09T16:13:02.106993: step 8709, loss 0.031153, acc 0.984375
2017-09-09T16:13:02.468191: step 8710, loss 0.0270219, acc 0.984375
2017-09-09T16:13:02.809860: step 8711, loss 0.0437178, acc 0.96875
2017-09-09T16:13:03.178166: step 8712, loss 0.00988767, acc 1
2017-09-09T16:13:03.570603: step 8713, loss 0.0099929, acc 1
2017-09-09T16:13:03.908736: step 8714, loss 0.019582, acc 1
2017-09-09T16:13:04.232686: step 8715, loss 0.00877454, acc 1
2017-09-09T16:13:04.563036: step 8716, loss 0.00445518, acc 1
2017-09-09T16:13:04.879235: step 8717, loss 0.0248776, acc 1
2017-09-09T16:13:05.203783: step 8718, loss 0.00168187, acc 1
2017-09-09T16:13:05.501579: step 8719, loss 0.00433932, acc 1
2017-09-09T16:13:05.840092: step 8720, loss 0.0311837, acc 0.984375
2017-09-09T16:13:06.172276: step 8721, loss 0.0514216, acc 0.984375
2017-09-09T16:13:06.495088: step 8722, loss 0.0303252, acc 0.980392
2017-09-09T16:13:06.820608: step 8723, loss 0.00838316, acc 1
2017-09-09T16:13:07.122649: step 8724, loss 0.00812886, acc 1
2017-09-09T16:13:07.523366: step 8725, loss 0.0137348, acc 1
2017-09-09T16:13:07.826170: step 8726, loss 8.78115e-05, acc 1
2017-09-09T16:13:08.145645: step 8727, loss 0.000134842, acc 1
2017-09-09T16:13:08.513425: step 8728, loss 0.0331224, acc 0.984375
2017-09-09T16:13:08.803402: step 8729, loss 0.00993147, acc 1
2017-09-09T16:13:09.132601: step 8730, loss 0.0922348, acc 0.96875
2017-09-09T16:13:09.469297: step 8731, loss 0.0550173, acc 0.96875
2017-09-09T16:13:09.774700: step 8732, loss 0.000582421, acc 1
2017-09-09T16:13:10.055726: step 8733, loss 0.0352518, acc 0.96875
2017-09-09T16:13:10.383852: step 8734, loss 0.0557751, acc 0.96875
2017-09-09T16:13:10.747463: step 8735, loss 0.00938074, acc 1
2017-09-09T16:13:11.063227: step 8736, loss 0.0233285, acc 0.984375
2017-09-09T16:13:11.383863: step 8737, loss 0.0228302, acc 0.984375
2017-09-09T16:13:11.672924: step 8738, loss 0.00547783, acc 1
2017-09-09T16:13:12.124930: step 8739, loss 0.00755779, acc 1
2017-09-09T16:13:12.398072: step 8740, loss 0.0452211, acc 0.96875
2017-09-09T16:13:12.702597: step 8741, loss 0.0306642, acc 0.984375
2017-09-09T16:13:13.034525: step 8742, loss 0.0212592, acc 0.984375
2017-09-09T16:13:13.308330: step 8743, loss 0.019323, acc 0.984375
2017-09-09T16:13:13.592616: step 8744, loss 0.0120238, acc 1
2017-09-09T16:13:13.886839: step 8745, loss 0.0186917, acc 0.984375
2017-09-09T16:13:14.183825: step 8746, loss 0.0307015, acc 0.984375
2017-09-09T16:13:14.512248: step 8747, loss 0.00817365, acc 1
2017-09-09T16:13:14.816894: step 8748, loss 0.0106692, acc 1
2017-09-09T16:13:15.161905: step 8749, loss 0.04553, acc 0.96875
2017-09-09T16:13:15.496291: step 8750, loss 0.0197427, acc 1

Evaluation:
2017-09-09T16:13:15.601209: step 8750, loss 2.90389, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8750

2017-09-09T16:13:18.957227: step 8751, loss 0.00179171, acc 1
2017-09-09T16:13:19.229158: step 8752, loss 0.0117391, acc 1
2017-09-09T16:13:19.559108: step 8753, loss 0.0521121, acc 0.984375
2017-09-09T16:13:19.900742: step 8754, loss 0.0226559, acc 0.984375
2017-09-09T16:13:20.243434: step 8755, loss 0.173648, acc 0.96875
2017-09-09T16:13:20.583122: step 8756, loss 0.0226632, acc 0.984375
2017-09-09T16:13:20.903386: step 8757, loss 0.00479143, acc 1
2017-09-09T16:13:21.228317: step 8758, loss 0.00559555, acc 1
2017-09-09T16:13:21.549259: step 8759, loss 0.000109403, acc 1
2017-09-09T16:13:21.886309: step 8760, loss 0.00189206, acc 1
2017-09-09T16:13:22.205643: step 8761, loss 0.0899682, acc 0.9375
2017-09-09T16:13:22.537920: step 8762, loss 0.0455314, acc 0.984375
2017-09-09T16:13:22.882018: step 8763, loss 0.0161364, acc 0.984375
2017-09-09T16:13:23.157436: step 8764, loss 0.00565166, acc 1
2017-09-09T16:13:23.503389: step 8765, loss 0.00877052, acc 1
2017-09-09T16:13:23.818127: step 8766, loss 0.00886509, acc 1
2017-09-09T16:13:24.116635: step 8767, loss 0.0185706, acc 0.984375
2017-09-09T16:13:24.454295: step 8768, loss 0.0126261, acc 1
2017-09-09T16:13:24.791849: step 8769, loss 0.0188834, acc 1
2017-09-09T16:13:25.122365: step 8770, loss 0.0362042, acc 0.984375
2017-09-09T16:13:25.471217: step 8771, loss 0.0621586, acc 0.984375
2017-09-09T16:13:25.861024: step 8772, loss 0.0578361, acc 0.984375
2017-09-09T16:13:26.174843: step 8773, loss 0.0174525, acc 0.984375
2017-09-09T16:13:26.527232: step 8774, loss 0.0040329, acc 1
2017-09-09T16:13:26.819593: step 8775, loss 0.0182161, acc 0.984375
2017-09-09T16:13:27.098510: step 8776, loss 0.0180087, acc 1
2017-09-09T16:13:27.429359: step 8777, loss 0.0466054, acc 0.96875
2017-09-09T16:13:27.771000: step 8778, loss 0.00214962, acc 1
2017-09-09T16:13:28.123495: step 8779, loss 0.0417124, acc 0.96875
2017-09-09T16:13:28.463644: step 8780, loss 0.0511595, acc 0.96875
2017-09-09T16:13:28.829248: step 8781, loss 0.0132382, acc 1
2017-09-09T16:13:29.165083: step 8782, loss 0.0429418, acc 0.96875
2017-09-09T16:13:29.503367: step 8783, loss 0.019123, acc 0.984375
2017-09-09T16:13:29.796221: step 8784, loss 0.00822687, acc 1
2017-09-09T16:13:30.103843: step 8785, loss 0.0148422, acc 1
2017-09-09T16:13:30.445519: step 8786, loss 0.0199018, acc 0.984375
2017-09-09T16:13:30.740797: step 8787, loss 0.0212163, acc 1
2017-09-09T16:13:31.073054: step 8788, loss 0.0299824, acc 0.984375
2017-09-09T16:13:31.361646: step 8789, loss 0.0162722, acc 1
2017-09-09T16:13:31.746395: step 8790, loss 0.0169675, acc 0.984375
2017-09-09T16:13:32.054750: step 8791, loss 0.0117041, acc 0.984375
2017-09-09T16:13:32.371396: step 8792, loss 0.0210985, acc 1
2017-09-09T16:13:32.682004: step 8793, loss 0.00488619, acc 1
2017-09-09T16:13:32.966190: step 8794, loss 0.0298314, acc 0.96875
2017-09-09T16:13:33.316696: step 8795, loss 0.0325559, acc 0.984375
2017-09-09T16:13:33.599316: step 8796, loss 0.0377805, acc 0.984375
2017-09-09T16:13:33.968772: step 8797, loss 0.00324063, acc 1
2017-09-09T16:13:34.337990: step 8798, loss 0.0220207, acc 1
2017-09-09T16:13:34.691647: step 8799, loss 0.0384373, acc 0.96875
2017-09-09T16:13:35.065741: step 8800, loss 0.000679627, acc 1

Evaluation:
2017-09-09T16:13:35.130981: step 8800, loss 1.73403, acc 0.329496

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8800

2017-09-09T16:13:37.340913: step 8801, loss 0.0223598, acc 0.984375
2017-09-09T16:13:37.653345: step 8802, loss 0.0179112, acc 1
2017-09-09T16:13:37.999116: step 8803, loss 0.0236923, acc 0.984375
2017-09-09T16:13:38.289769: step 8804, loss 0.0295633, acc 0.96875
2017-09-09T16:13:38.597295: step 8805, loss 0.000131595, acc 1
2017-09-09T16:13:38.916501: step 8806, loss 0.0055419, acc 1
2017-09-09T16:13:39.231191: step 8807, loss 0.0313084, acc 0.984375
2017-09-09T16:13:39.555366: step 8808, loss 0.0776466, acc 0.96875
2017-09-09T16:13:39.851518: step 8809, loss 0.0446214, acc 0.984375
2017-09-09T16:13:40.174831: step 8810, loss 0.00299368, acc 1
2017-09-09T16:13:40.465700: step 8811, loss 0.0157681, acc 1
2017-09-09T16:13:40.955995: step 8812, loss 0.0238905, acc 0.96875
2017-09-09T16:13:41.286415: step 8813, loss 6.7241e-05, acc 1
2017-09-09T16:13:41.606074: step 8814, loss 0.0315193, acc 0.984375
2017-09-09T16:13:41.886583: step 8815, loss 0.026277, acc 1
2017-09-09T16:13:42.202057: step 8816, loss 0.039662, acc 0.96875
2017-09-09T16:13:42.496724: step 8817, loss 0.0541444, acc 0.96875
2017-09-09T16:13:42.788863: step 8818, loss 0.0134729, acc 1
2017-09-09T16:13:43.115334: step 8819, loss 0.0272656, acc 0.984375
2017-09-09T16:13:43.410613: step 8820, loss 0.00449436, acc 1
2017-09-09T16:13:43.703417: step 8821, loss 0.042809, acc 0.984375
2017-09-09T16:13:44.032618: step 8822, loss 0.00032918, acc 1
2017-09-09T16:13:44.347878: step 8823, loss 0.0337454, acc 0.984375
2017-09-09T16:13:44.679431: step 8824, loss 0.000669275, acc 1
2017-09-09T16:13:45.117317: step 8825, loss 0.0277075, acc 1
2017-09-09T16:13:45.392550: step 8826, loss 0.0192618, acc 1
2017-09-09T16:13:45.726119: step 8827, loss 0.0352382, acc 0.984375
2017-09-09T16:13:46.016361: step 8828, loss 0.0309975, acc 0.984375
2017-09-09T16:13:46.322544: step 8829, loss 0.0128675, acc 1
2017-09-09T16:13:46.704335: step 8830, loss 0.000129637, acc 1
2017-09-09T16:13:46.984885: step 8831, loss 0.0499481, acc 0.984375
2017-09-09T16:13:47.354633: step 8832, loss 0.0478082, acc 0.984375
2017-09-09T16:13:47.668471: step 8833, loss 0.0792256, acc 0.953125
2017-09-09T16:13:47.972527: step 8834, loss 0.0122509, acc 1
2017-09-09T16:13:48.355072: step 8835, loss 0.00915125, acc 1
2017-09-09T16:13:48.627101: step 8836, loss 0.0103863, acc 1
2017-09-09T16:13:48.960934: step 8837, loss 0.0222147, acc 0.984375
2017-09-09T16:13:49.268751: step 8838, loss 0.0216547, acc 0.984375
2017-09-09T16:13:49.557331: step 8839, loss 0.000451709, acc 1
2017-09-09T16:13:49.884532: step 8840, loss 0.0417774, acc 0.984375
2017-09-09T16:13:50.193208: step 8841, loss 0.00353518, acc 1
2017-09-09T16:13:50.538839: step 8842, loss 0.011557, acc 1
2017-09-09T16:13:50.832057: step 8843, loss 0.000255983, acc 1
2017-09-09T16:13:51.151098: step 8844, loss 0.03098, acc 0.984375
2017-09-09T16:13:51.450630: step 8845, loss 0.0466145, acc 0.96875
2017-09-09T16:13:51.771152: step 8846, loss 0.00133778, acc 1
2017-09-09T16:13:52.126771: step 8847, loss 0.000354309, acc 1
2017-09-09T16:13:52.457324: step 8848, loss 0.0171955, acc 1
2017-09-09T16:13:52.822377: step 8849, loss 0.015762, acc 0.984375
2017-09-09T16:13:53.177759: step 8850, loss 0.0363365, acc 0.984375

Evaluation:
2017-09-09T16:13:53.285052: step 8850, loss 1.98011, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8850

2017-09-09T16:13:56.543652: step 8851, loss 0.0156819, acc 1
2017-09-09T16:13:56.877257: step 8852, loss 0.0181898, acc 0.984375
2017-09-09T16:13:57.191086: step 8853, loss 0.0239789, acc 0.984375
2017-09-09T16:13:57.539556: step 8854, loss 0.00505284, acc 1
2017-09-09T16:13:57.871571: step 8855, loss 0.00174643, acc 1
2017-09-09T16:13:58.174989: step 8856, loss 0.0342002, acc 0.984375
2017-09-09T16:13:58.520657: step 8857, loss 0.027713, acc 0.984375
2017-09-09T16:13:58.800000: step 8858, loss 0.00566599, acc 1
2017-09-09T16:13:59.102391: step 8859, loss 0.0479584, acc 0.96875
2017-09-09T16:13:59.393345: step 8860, loss 0.000207356, acc 1
2017-09-09T16:13:59.707608: step 8861, loss 0.0290676, acc 0.96875
2017-09-09T16:14:00.050420: step 8862, loss 0.026972, acc 1
2017-09-09T16:14:00.380117: step 8863, loss 0.0154818, acc 0.984375
2017-09-09T16:14:00.705043: step 8864, loss 0.0225218, acc 0.984375
2017-09-09T16:14:00.972895: step 8865, loss 0.037449, acc 0.96875
2017-09-09T16:14:01.318573: step 8866, loss 0.025487, acc 0.984375
2017-09-09T16:14:01.665336: step 8867, loss 0.0262107, acc 1
2017-09-09T16:14:01.996416: step 8868, loss 0.00927795, acc 1
2017-09-09T16:14:02.340947: step 8869, loss 0.0127955, acc 1
2017-09-09T16:14:02.647996: step 8870, loss 0.00314134, acc 1
2017-09-09T16:14:02.980639: step 8871, loss 0.016153, acc 0.984375
2017-09-09T16:14:03.252212: step 8872, loss 0.0416397, acc 0.984375
2017-09-09T16:14:03.608808: step 8873, loss 0.0204103, acc 1
2017-09-09T16:14:03.951603: step 8874, loss 0.0204055, acc 0.984375
2017-09-09T16:14:04.286578: step 8875, loss 0.0367278, acc 0.96875
2017-09-09T16:14:04.599819: step 8876, loss 0.0485924, acc 0.953125
2017-09-09T16:14:04.889505: step 8877, loss 0.0214896, acc 0.984375
2017-09-09T16:14:05.260403: step 8878, loss 0.00879854, acc 1
2017-09-09T16:14:05.577073: step 8879, loss 0.0388859, acc 0.96875
2017-09-09T16:14:05.870454: step 8880, loss 0.0267909, acc 0.984375
2017-09-09T16:14:06.197979: step 8881, loss 0.0118857, acc 1
2017-09-09T16:14:06.509285: step 8882, loss 0.0030041, acc 1
2017-09-09T16:14:06.858116: step 8883, loss 0.0199603, acc 1
2017-09-09T16:14:07.189624: step 8884, loss 0.00810491, acc 1
2017-09-09T16:14:07.511896: step 8885, loss 0.017699, acc 0.984375
2017-09-09T16:14:07.899103: step 8886, loss 0.0211531, acc 0.984375
2017-09-09T16:14:08.163487: step 8887, loss 0.0141329, acc 1
2017-09-09T16:14:08.502035: step 8888, loss 0.000103979, acc 1
2017-09-09T16:14:08.872901: step 8889, loss 0.0171892, acc 1
2017-09-09T16:14:09.179731: step 8890, loss 0.0266544, acc 0.984375
2017-09-09T16:14:09.543919: step 8891, loss 0.0606137, acc 0.96875
2017-09-09T16:14:09.825455: step 8892, loss 0.000645374, acc 1
2017-09-09T16:14:10.186262: step 8893, loss 0.110812, acc 0.96875
2017-09-09T16:14:10.492731: step 8894, loss 0.155524, acc 0.921875
2017-09-09T16:14:10.874806: step 8895, loss 0.00299316, acc 1
2017-09-09T16:14:11.229422: step 8896, loss 0.0284019, acc 1
2017-09-09T16:14:11.536790: step 8897, loss 0.0315447, acc 0.96875
2017-09-09T16:14:11.819251: step 8898, loss 0.0425283, acc 0.984375
2017-09-09T16:14:12.145894: step 8899, loss 0.0356323, acc 0.96875
2017-09-09T16:14:12.456250: step 8900, loss 0.0140614, acc 1

Evaluation:
2017-09-09T16:14:12.524579: step 8900, loss 3.15158, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8900

2017-09-09T16:14:14.913757: step 8901, loss 0.0365828, acc 0.96875
2017-09-09T16:14:15.233369: step 8902, loss 0.0948059, acc 0.96875
2017-09-09T16:14:15.584842: step 8903, loss 0.0480381, acc 0.96875
2017-09-09T16:14:15.906132: step 8904, loss 0.0346303, acc 0.984375
2017-09-09T16:14:16.212386: step 8905, loss 0.041475, acc 0.96875
2017-09-09T16:14:16.636067: step 8906, loss 0.0277416, acc 1
2017-09-09T16:14:16.945418: step 8907, loss 0.000219528, acc 1
2017-09-09T16:14:17.248229: step 8908, loss 0.0311707, acc 0.984375
2017-09-09T16:14:17.588941: step 8909, loss 0.0638679, acc 0.953125
2017-09-09T16:14:17.867266: step 8910, loss 0.000181336, acc 1
2017-09-09T16:14:18.219959: step 8911, loss 0.0200033, acc 0.984375
2017-09-09T16:14:18.536877: step 8912, loss 0.0365901, acc 1
2017-09-09T16:14:18.877399: step 8913, loss 0.0245969, acc 1
2017-09-09T16:14:19.207093: step 8914, loss 0.0177924, acc 0.984375
2017-09-09T16:14:19.522185: step 8915, loss 0.00357945, acc 1
2017-09-09T16:14:19.905394: step 8916, loss 0.0158736, acc 1
2017-09-09T16:14:20.201429: step 8917, loss 0.0210452, acc 1
2017-09-09T16:14:20.538660: step 8918, loss 0.000624724, acc 1
2017-09-09T16:14:20.895178: step 8919, loss 0.00269006, acc 1
2017-09-09T16:14:21.190929: step 8920, loss 0.0292405, acc 0.984375
2017-09-09T16:14:21.551183: step 8921, loss 0.000589748, acc 1
2017-09-09T16:14:21.849612: step 8922, loss 0.0397152, acc 0.96875
2017-09-09T16:14:22.196163: step 8923, loss 0.0154732, acc 0.984375
2017-09-09T16:14:22.494665: step 8924, loss 0.00452153, acc 1
2017-09-09T16:14:22.790873: step 8925, loss 0.00302937, acc 1
2017-09-09T16:14:23.192619: step 8926, loss 0.000450817, acc 1
2017-09-09T16:14:23.513573: step 8927, loss 0.0401939, acc 0.984375
2017-09-09T16:14:23.819040: step 8928, loss 0.0360832, acc 0.984375
2017-09-09T16:14:24.129757: step 8929, loss 0.0267187, acc 1
2017-09-09T16:14:24.424293: step 8930, loss 0.0424617, acc 0.96875
2017-09-09T16:14:24.717645: step 8931, loss 0.00300332, acc 1
2017-09-09T16:14:25.016318: step 8932, loss 0.0275979, acc 0.984375
2017-09-09T16:14:25.305282: step 8933, loss 0.0151545, acc 0.984375
2017-09-09T16:14:25.653082: step 8934, loss 0.0731362, acc 0.96875
2017-09-09T16:14:25.971521: step 8935, loss 0.024585, acc 0.984375
2017-09-09T16:14:26.256705: step 8936, loss 0.00131929, acc 1
2017-09-09T16:14:26.585195: step 8937, loss 0.0343619, acc 0.984375
2017-09-09T16:14:26.875082: step 8938, loss 0.0313515, acc 0.96875
2017-09-09T16:14:27.206331: step 8939, loss 0.00160253, acc 1
2017-09-09T16:14:27.575062: step 8940, loss 0.0720237, acc 0.96875
2017-09-09T16:14:27.857885: step 8941, loss 0.000725756, acc 1
2017-09-09T16:14:28.144160: step 8942, loss 0.026636, acc 0.984375
2017-09-09T16:14:28.455281: step 8943, loss 0.0172758, acc 0.984375
2017-09-09T16:14:28.908083: step 8944, loss 0.0153024, acc 0.984375
2017-09-09T16:14:29.206783: step 8945, loss 0.00989338, acc 1
2017-09-09T16:14:29.570928: step 8946, loss 0.00898482, acc 1
2017-09-09T16:14:29.859399: step 8947, loss 0.00897415, acc 1
2017-09-09T16:14:30.173440: step 8948, loss 0.0453715, acc 0.984375
2017-09-09T16:14:30.499447: step 8949, loss 0.0146372, acc 0.984375
2017-09-09T16:14:30.828141: step 8950, loss 0.020761, acc 0.984375

Evaluation:
2017-09-09T16:14:30.979232: step 8950, loss 1.89263, acc 0.345324

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-8950

2017-09-09T16:14:33.903890: step 8951, loss 0.0616459, acc 0.96875
2017-09-09T16:14:34.262604: step 8952, loss 0.0176855, acc 1
2017-09-09T16:14:34.546647: step 8953, loss 0.00506752, acc 1
2017-09-09T16:14:34.865363: step 8954, loss 0.0227444, acc 0.984375
2017-09-09T16:14:35.257024: step 8955, loss 0.0399056, acc 0.984375
2017-09-09T16:14:35.601298: step 8956, loss 0.0158534, acc 0.984375
2017-09-09T16:14:35.926874: step 8957, loss 0.0509322, acc 0.96875
2017-09-09T16:14:36.220583: step 8958, loss 0.0165575, acc 0.984375
2017-09-09T16:14:36.509686: step 8959, loss 0.000224003, acc 1
2017-09-09T16:14:36.855969: step 8960, loss 0.000332346, acc 1
2017-09-09T16:14:37.167021: step 8961, loss 0.0409586, acc 0.96875
2017-09-09T16:14:37.539373: step 8962, loss 0.0125095, acc 1
2017-09-09T16:14:37.841818: step 8963, loss 0.0203029, acc 0.984375
2017-09-09T16:14:38.157101: step 8964, loss 0.0151039, acc 1
2017-09-09T16:14:38.469361: step 8965, loss 0.00418321, acc 1
2017-09-09T16:14:38.803718: step 8966, loss 0.000518309, acc 1
2017-09-09T16:14:39.197797: step 8967, loss 0.0284147, acc 1
2017-09-09T16:14:39.504454: step 8968, loss 0.0149529, acc 0.984375
2017-09-09T16:14:39.949771: step 8969, loss 0.00721736, acc 1
2017-09-09T16:14:40.259715: step 8970, loss 0.0293474, acc 0.984375
2017-09-09T16:14:40.578262: step 8971, loss 0.0529706, acc 0.984375
2017-09-09T16:14:40.852088: step 8972, loss 0.0153679, acc 0.984375
2017-09-09T16:14:41.136277: step 8973, loss 0.0332489, acc 0.984375
2017-09-09T16:14:41.413939: step 8974, loss 0.0567757, acc 0.96875
2017-09-09T16:14:41.723292: step 8975, loss 0.023618, acc 0.984375
2017-09-09T16:14:42.062241: step 8976, loss 0.0224085, acc 1
2017-09-09T16:14:42.355443: step 8977, loss 0.0391393, acc 0.96875
2017-09-09T16:14:42.717464: step 8978, loss 0.0383899, acc 1
2017-09-09T16:14:43.107073: step 8979, loss 0.0185143, acc 0.984375
2017-09-09T16:14:43.473459: step 8980, loss 0.0628677, acc 0.984375
2017-09-09T16:14:43.834008: step 8981, loss 0.0177212, acc 1
2017-09-09T16:14:44.153901: step 8982, loss 0.0441281, acc 0.96875
2017-09-09T16:14:44.453816: step 8983, loss 0.000588626, acc 1
2017-09-09T16:14:44.822158: step 8984, loss 0.00273421, acc 1
2017-09-09T16:14:45.094664: step 8985, loss 0.0165239, acc 0.984375
2017-09-09T16:14:45.428498: step 8986, loss 0.0204776, acc 0.984375
2017-09-09T16:14:45.775514: step 8987, loss 0.0212263, acc 0.984375
2017-09-09T16:14:46.065185: step 8988, loss 0.0175536, acc 0.984375
2017-09-09T16:14:46.409227: step 8989, loss 0.017539, acc 1
2017-09-09T16:14:46.684000: step 8990, loss 0.0222433, acc 0.984375
2017-09-09T16:14:47.052226: step 8991, loss 0.0347176, acc 0.984375
2017-09-09T16:14:47.342865: step 8992, loss 0.0405281, acc 0.96875
2017-09-09T16:14:47.668145: step 8993, loss 0.0204635, acc 0.984375
2017-09-09T16:14:48.000592: step 8994, loss 0.0165774, acc 0.984375
2017-09-09T16:14:48.286981: step 8995, loss 0.00464811, acc 1
2017-09-09T16:14:48.642583: step 8996, loss 0.0443439, acc 0.96875
2017-09-09T16:14:48.915674: step 8997, loss 0.00991228, acc 1
2017-09-09T16:14:49.234728: step 8998, loss 0.000678847, acc 1
2017-09-09T16:14:49.549704: step 8999, loss 0.00353839, acc 1
2017-09-09T16:14:49.820131: step 9000, loss 0.00316445, acc 1

Evaluation:
2017-09-09T16:14:49.895531: step 9000, loss 1.99285, acc 0.316547

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9000

2017-09-09T16:14:53.098768: step 9001, loss 0.0308733, acc 0.984375
2017-09-09T16:14:53.350937: step 9002, loss 0.0280537, acc 0.984375
2017-09-09T16:14:53.682255: step 9003, loss 0.0431894, acc 0.984375
2017-09-09T16:14:54.023147: step 9004, loss 0.030725, acc 0.984375
2017-09-09T16:14:54.312090: step 9005, loss 0.0442456, acc 0.984375
2017-09-09T16:14:54.574133: step 9006, loss 0.0378246, acc 0.984375
2017-09-09T16:14:54.944683: step 9007, loss 0.0256928, acc 0.984375
2017-09-09T16:14:55.259625: step 9008, loss 0.0197988, acc 0.984375
2017-09-09T16:14:55.550352: step 9009, loss 0.00113789, acc 1
2017-09-09T16:14:55.847328: step 9010, loss 0.0390483, acc 0.984375
2017-09-09T16:14:56.147224: step 9011, loss 0.0134906, acc 1
2017-09-09T16:14:56.480274: step 9012, loss 0.0610095, acc 0.953125
2017-09-09T16:14:56.760021: step 9013, loss 0.0559374, acc 0.984375
2017-09-09T16:14:57.164890: step 9014, loss 0.0224004, acc 1
2017-09-09T16:14:57.447729: step 9015, loss 0.0842799, acc 0.984375
2017-09-09T16:14:57.784868: step 9016, loss 0.00907532, acc 1
2017-09-09T16:14:58.075284: step 9017, loss 0.0275772, acc 0.984375
2017-09-09T16:14:58.402785: step 9018, loss 0.034536, acc 0.984375
2017-09-09T16:14:58.736482: step 9019, loss 0.0296105, acc 0.984375
2017-09-09T16:14:59.022739: step 9020, loss 0.0394645, acc 0.96875
2017-09-09T16:14:59.363930: step 9021, loss 0.0374586, acc 0.984375
2017-09-09T16:14:59.699226: step 9022, loss 0.0245718, acc 1
2017-09-09T16:14:59.995153: step 9023, loss 0.0024652, acc 1
2017-09-09T16:15:00.352366: step 9024, loss 0.0609691, acc 0.984375
2017-09-09T16:15:00.657583: step 9025, loss 0.0232178, acc 1
2017-09-09T16:15:00.947673: step 9026, loss 0.000229459, acc 1
2017-09-09T16:15:01.247093: step 9027, loss 0.0533407, acc 0.984375
2017-09-09T16:15:01.583671: step 9028, loss 0.021999, acc 0.984375
2017-09-09T16:15:01.964838: step 9029, loss 0.00622795, acc 1
2017-09-09T16:15:02.284947: step 9030, loss 0.0110713, acc 1
2017-09-09T16:15:02.600013: step 9031, loss 0.00572749, acc 1
2017-09-09T16:15:02.914665: step 9032, loss 0.00202975, acc 1
2017-09-09T16:15:03.241988: step 9033, loss 0.00636161, acc 1
2017-09-09T16:15:03.637713: step 9034, loss 0.00540809, acc 1
2017-09-09T16:15:03.990933: step 9035, loss 0.0120813, acc 1
2017-09-09T16:15:04.305909: step 9036, loss 0.00520204, acc 1
2017-09-09T16:15:04.664216: step 9037, loss 0.0191429, acc 0.984375
2017-09-09T16:15:05.008282: step 9038, loss 0.0421984, acc 0.984375
2017-09-09T16:15:05.296482: step 9039, loss 0.0141528, acc 1
2017-09-09T16:15:05.666965: step 9040, loss 0.00951175, acc 1
2017-09-09T16:15:05.948629: step 9041, loss 0.0428868, acc 0.96875
2017-09-09T16:15:06.341371: step 9042, loss 0.00499925, acc 1
2017-09-09T16:15:06.611977: step 9043, loss 0.0217288, acc 1
2017-09-09T16:15:06.956257: step 9044, loss 0.00146351, acc 1
2017-09-09T16:15:07.278830: step 9045, loss 0.0335882, acc 1
2017-09-09T16:15:07.583719: step 9046, loss 0.00034833, acc 1
2017-09-09T16:15:08.000503: step 9047, loss 0.04525, acc 0.96875
2017-09-09T16:15:08.297810: step 9048, loss 0.02258, acc 1
2017-09-09T16:15:08.604584: step 9049, loss 0.0611097, acc 0.96875
2017-09-09T16:15:09.035137: step 9050, loss 0.00294767, acc 1

Evaluation:
2017-09-09T16:15:09.180636: step 9050, loss 1.91124, acc 0.336691

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9050

2017-09-09T16:15:11.579828: step 9051, loss 0.0446015, acc 0.96875
2017-09-09T16:15:11.874077: step 9052, loss 0.0128576, acc 0.984375
2017-09-09T16:15:12.170206: step 9053, loss 0.00566322, acc 1
2017-09-09T16:15:12.505874: step 9054, loss 0.030923, acc 0.984375
2017-09-09T16:15:12.815467: step 9055, loss 0.0950978, acc 0.953125
2017-09-09T16:15:13.188941: step 9056, loss 0.00422342, acc 1
2017-09-09T16:15:13.479830: step 9057, loss 0.0247547, acc 1
2017-09-09T16:15:13.820602: step 9058, loss 0.0102416, acc 1
2017-09-09T16:15:14.145595: step 9059, loss 0.0739747, acc 0.96875
2017-09-09T16:15:14.463341: step 9060, loss 0.0686395, acc 0.953125
2017-09-09T16:15:14.855680: step 9061, loss 0.0328249, acc 0.984375
2017-09-09T16:15:15.191537: step 9062, loss 0.0214064, acc 1
2017-09-09T16:15:15.525011: step 9063, loss 0.0214113, acc 1
2017-09-09T16:15:15.896031: step 9064, loss 0.00474941, acc 1
2017-09-09T16:15:16.218407: step 9065, loss 0.107462, acc 0.96875
2017-09-09T16:15:16.581212: step 9066, loss 0.0964547, acc 0.984375
2017-09-09T16:15:16.883340: step 9067, loss 0.00260661, acc 1
2017-09-09T16:15:17.161592: step 9068, loss 0.0213602, acc 1
2017-09-09T16:15:17.469070: step 9069, loss 0.0033421, acc 1
2017-09-09T16:15:17.768826: step 9070, loss 0.0286798, acc 0.984375
2017-09-09T16:15:18.123707: step 9071, loss 0.0264444, acc 0.984375
2017-09-09T16:15:18.454006: step 9072, loss 0.000918885, acc 1
2017-09-09T16:15:18.780323: step 9073, loss 0.0449665, acc 0.984375
2017-09-09T16:15:19.071292: step 9074, loss 0.0172068, acc 1
2017-09-09T16:15:19.401742: step 9075, loss 0.00650304, acc 1
2017-09-09T16:15:19.745569: step 9076, loss 9.39741e-05, acc 1
2017-09-09T16:15:20.069808: step 9077, loss 0.0157554, acc 0.984375
2017-09-09T16:15:20.405159: step 9078, loss 0.00411416, acc 1
2017-09-09T16:15:20.737685: step 9079, loss 0.00958724, acc 1
2017-09-09T16:15:21.070356: step 9080, loss 0.0209157, acc 0.984375
2017-09-09T16:15:21.369649: step 9081, loss 0.0356665, acc 0.984375
2017-09-09T16:15:21.761193: step 9082, loss 0.0103202, acc 1
2017-09-09T16:15:22.098421: step 9083, loss 0.0154616, acc 1
2017-09-09T16:15:22.405874: step 9084, loss 0.0242747, acc 0.984375
2017-09-09T16:15:22.705310: step 9085, loss 0.0214218, acc 0.984375
2017-09-09T16:15:23.032564: step 9086, loss 0.0583351, acc 0.96875
2017-09-09T16:15:23.397730: step 9087, loss 0.0317051, acc 0.96875
2017-09-09T16:15:23.716269: step 9088, loss 0.0241159, acc 1
2017-09-09T16:15:24.082438: step 9089, loss 1.30181e-05, acc 1
2017-09-09T16:15:24.430520: step 9090, loss 0.0261924, acc 0.984375
2017-09-09T16:15:24.792682: step 9091, loss 0.108982, acc 0.96875
2017-09-09T16:15:25.118454: step 9092, loss 0.00725044, acc 1
2017-09-09T16:15:25.543588: step 9093, loss 0.0131831, acc 1
2017-09-09T16:15:25.885640: step 9094, loss 0.0652896, acc 0.96875
2017-09-09T16:15:26.193751: step 9095, loss 0.0424591, acc 0.984375
2017-09-09T16:15:26.616625: step 9096, loss 0.0116339, acc 1
2017-09-09T16:15:26.952340: step 9097, loss 0.0170197, acc 0.984375
2017-09-09T16:15:27.285311: step 9098, loss 0.0270357, acc 0.984375
2017-09-09T16:15:27.655068: step 9099, loss 0.0293358, acc 0.96875
2017-09-09T16:15:27.976992: step 9100, loss 0.00149577, acc 1

Evaluation:
2017-09-09T16:15:28.126524: step 9100, loss 2.1367, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9100

2017-09-09T16:15:30.828817: step 9101, loss 0.00324713, acc 1
2017-09-09T16:15:31.154603: step 9102, loss 0.0025555, acc 1
2017-09-09T16:15:31.432604: step 9103, loss 0.0354307, acc 0.984375
2017-09-09T16:15:31.793526: step 9104, loss 0.0343055, acc 0.984375
2017-09-09T16:15:32.079062: step 9105, loss 0.000225221, acc 1
2017-09-09T16:15:32.411521: step 9106, loss 0.0244974, acc 0.984375
2017-09-09T16:15:32.796315: step 9107, loss 0.0162753, acc 1
2017-09-09T16:15:33.130243: step 9108, loss 0.039153, acc 0.96875
2017-09-09T16:15:33.467420: step 9109, loss 0.0126048, acc 1
2017-09-09T16:15:33.744994: step 9110, loss 0.022129, acc 0.984375
2017-09-09T16:15:34.088456: step 9111, loss 0.0484669, acc 0.984375
2017-09-09T16:15:34.380863: step 9112, loss 0.0136724, acc 0.984375
2017-09-09T16:15:34.674957: step 9113, loss 0.042544, acc 0.984375
2017-09-09T16:15:34.998061: step 9114, loss 0.0298006, acc 0.980392
2017-09-09T16:15:35.319957: step 9115, loss 0.0194164, acc 0.984375
2017-09-09T16:15:35.678087: step 9116, loss 0.0145532, acc 0.984375
2017-09-09T16:15:36.030900: step 9117, loss 0.00353094, acc 1
2017-09-09T16:15:36.383197: step 9118, loss 0.0398895, acc 0.96875
2017-09-09T16:15:36.692127: step 9119, loss 0.000915201, acc 1
2017-09-09T16:15:36.984160: step 9120, loss 7.49939e-05, acc 1
2017-09-09T16:15:37.335779: step 9121, loss 0.0371501, acc 0.96875
2017-09-09T16:15:37.620442: step 9122, loss 0.0866944, acc 0.9375
2017-09-09T16:15:38.132374: step 9123, loss 0.00743465, acc 1
2017-09-09T16:15:38.437325: step 9124, loss 0.00693183, acc 1
2017-09-09T16:15:38.736579: step 9125, loss 0.0262917, acc 0.984375
2017-09-09T16:15:39.058076: step 9126, loss 0.0250894, acc 0.984375
2017-09-09T16:15:39.402689: step 9127, loss 0.0452488, acc 0.96875
2017-09-09T16:15:39.744077: step 9128, loss 0.0272941, acc 0.984375
2017-09-09T16:15:40.021855: step 9129, loss 0.026203, acc 0.984375
2017-09-09T16:15:40.378120: step 9130, loss 0.0195592, acc 1
2017-09-09T16:15:40.742896: step 9131, loss 0.0438928, acc 0.984375
2017-09-09T16:15:41.098363: step 9132, loss 0.00195039, acc 1
2017-09-09T16:15:41.382349: step 9133, loss 0.019302, acc 0.984375
2017-09-09T16:15:41.730527: step 9134, loss 0.0272098, acc 0.984375
2017-09-09T16:15:42.047982: step 9135, loss 0.00202093, acc 1
2017-09-09T16:15:42.370584: step 9136, loss 3.31064e-05, acc 1
2017-09-09T16:15:42.671614: step 9137, loss 0.00623247, acc 1
2017-09-09T16:15:42.995887: step 9138, loss 0.000133996, acc 1
2017-09-09T16:15:43.365868: step 9139, loss 0.000287979, acc 1
2017-09-09T16:15:43.696335: step 9140, loss 0.0106349, acc 1
2017-09-09T16:15:44.015659: step 9141, loss 0.0415642, acc 0.96875
2017-09-09T16:15:44.341468: step 9142, loss 0.00390197, acc 1
2017-09-09T16:15:44.650612: step 9143, loss 0.0279242, acc 0.984375
2017-09-09T16:15:44.990425: step 9144, loss 0.00391118, acc 1
2017-09-09T16:15:45.278044: step 9145, loss 0.0440441, acc 0.984375
2017-09-09T16:15:45.662217: step 9146, loss 0.00497053, acc 1
2017-09-09T16:15:45.961020: step 9147, loss 0.000522828, acc 1
2017-09-09T16:15:46.269138: step 9148, loss 0.0457207, acc 0.984375
2017-09-09T16:15:46.624935: step 9149, loss 0.0267718, acc 0.984375
2017-09-09T16:15:46.913312: step 9150, loss 0.0307331, acc 0.96875

Evaluation:
2017-09-09T16:15:47.023818: step 9150, loss 2.17651, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9150

2017-09-09T16:15:49.261006: step 9151, loss 0.0266532, acc 0.984375
2017-09-09T16:15:49.619238: step 9152, loss 0.0124162, acc 1
2017-09-09T16:15:49.911459: step 9153, loss 0.00102707, acc 1
2017-09-09T16:15:50.317277: step 9154, loss 0.0087635, acc 1
2017-09-09T16:15:50.633781: step 9155, loss 0.00596023, acc 1
2017-09-09T16:15:50.935667: step 9156, loss 0.084934, acc 0.953125
2017-09-09T16:15:51.280776: step 9157, loss 0.012707, acc 1
2017-09-09T16:15:51.620783: step 9158, loss 0.0331862, acc 0.984375
2017-09-09T16:15:51.927567: step 9159, loss 0.0368534, acc 0.96875
2017-09-09T16:15:52.267775: step 9160, loss 0.0374329, acc 0.96875
2017-09-09T16:15:52.649952: step 9161, loss 0.00469546, acc 1
2017-09-09T16:15:52.980929: step 9162, loss 0.0129714, acc 1
2017-09-09T16:15:53.266484: step 9163, loss 0.0168386, acc 1
2017-09-09T16:15:53.631769: step 9164, loss 0.00350238, acc 1
2017-09-09T16:15:53.933859: step 9165, loss 0.012854, acc 1
2017-09-09T16:15:54.243856: step 9166, loss 0.0254368, acc 0.984375
2017-09-09T16:15:54.537085: step 9167, loss 0.0035389, acc 1
2017-09-09T16:15:54.872091: step 9168, loss 0.0110196, acc 1
2017-09-09T16:15:55.161185: step 9169, loss 0.0682919, acc 0.96875
2017-09-09T16:15:55.462874: step 9170, loss 0.0148068, acc 1
2017-09-09T16:15:55.839079: step 9171, loss 0.0687034, acc 0.984375
2017-09-09T16:15:56.135176: step 9172, loss 0.00517838, acc 1
2017-09-09T16:15:56.514723: step 9173, loss 0.051751, acc 0.953125
2017-09-09T16:15:56.906954: step 9174, loss 0.00275097, acc 1
2017-09-09T16:15:57.224602: step 9175, loss 0.0143849, acc 1
2017-09-09T16:15:57.551717: step 9176, loss 0.0676827, acc 0.984375
2017-09-09T16:15:57.848003: step 9177, loss 0.0313133, acc 0.984375
2017-09-09T16:15:58.136353: step 9178, loss 0.0312353, acc 0.984375
2017-09-09T16:15:58.468958: step 9179, loss 0.00121359, acc 1
2017-09-09T16:15:58.769181: step 9180, loss 0.0246944, acc 1
2017-09-09T16:15:59.114317: step 9181, loss 0.0505119, acc 0.96875
2017-09-09T16:15:59.405208: step 9182, loss 0.00304611, acc 1
2017-09-09T16:15:59.762843: step 9183, loss 0.052243, acc 0.96875
2017-09-09T16:16:00.100369: step 9184, loss 0.0216441, acc 0.984375
2017-09-09T16:16:00.398407: step 9185, loss 0.0531251, acc 0.96875
2017-09-09T16:16:00.758427: step 9186, loss 0.039733, acc 0.96875
2017-09-09T16:16:01.122637: step 9187, loss 0.0745701, acc 0.96875
2017-09-09T16:16:01.420675: step 9188, loss 0.000184367, acc 1
2017-09-09T16:16:01.712894: step 9189, loss 0.00704073, acc 1
2017-09-09T16:16:02.015600: step 9190, loss 0.0246643, acc 0.984375
2017-09-09T16:16:02.335695: step 9191, loss 0.00317106, acc 1
2017-09-09T16:16:02.636002: step 9192, loss 0.0431109, acc 0.96875
2017-09-09T16:16:03.031162: step 9193, loss 0.00495013, acc 1
2017-09-09T16:16:03.307474: step 9194, loss 0.0254546, acc 0.984375
2017-09-09T16:16:03.647791: step 9195, loss 0.010391, acc 1
2017-09-09T16:16:03.983889: step 9196, loss 0.00606943, acc 1
2017-09-09T16:16:04.276886: step 9197, loss 0.030745, acc 0.984375
2017-09-09T16:16:04.665479: step 9198, loss 0.0117925, acc 1
2017-09-09T16:16:04.957718: step 9199, loss 0.0276848, acc 0.984375
2017-09-09T16:16:05.289767: step 9200, loss 0.00570181, acc 1

Evaluation:
2017-09-09T16:16:05.346155: step 9200, loss 1.73525, acc 0.328058

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9200

2017-09-09T16:16:08.139937: step 9201, loss 0.0216595, acc 0.984375
2017-09-09T16:16:08.445673: step 9202, loss 0.0867299, acc 0.96875
2017-09-09T16:16:08.715475: step 9203, loss 0.0108854, acc 1
2017-09-09T16:16:09.051830: step 9204, loss 0.018493, acc 0.984375
2017-09-09T16:16:09.394654: step 9205, loss 0.0148003, acc 1
2017-09-09T16:16:09.668991: step 9206, loss 0.0430996, acc 0.96875
2017-09-09T16:16:09.985721: step 9207, loss 0.06461, acc 0.96875
2017-09-09T16:16:10.386024: step 9208, loss 0.0175209, acc 0.984375
2017-09-09T16:16:10.796494: step 9209, loss 0.00018086, acc 1
2017-09-09T16:16:11.110770: step 9210, loss 0.0145565, acc 1
2017-09-09T16:16:11.445195: step 9211, loss 0.000752369, acc 1
2017-09-09T16:16:11.742299: step 9212, loss 0.0638232, acc 0.980392
2017-09-09T16:16:12.056779: step 9213, loss 0.0129197, acc 1
2017-09-09T16:16:12.392834: step 9214, loss 0.00926564, acc 1
2017-09-09T16:16:12.717132: step 9215, loss 0.00873511, acc 1
2017-09-09T16:16:13.078694: step 9216, loss 0.00250076, acc 1
2017-09-09T16:16:13.371867: step 9217, loss 0.0381156, acc 0.984375
2017-09-09T16:16:13.748179: step 9218, loss 0.0199319, acc 0.984375
2017-09-09T16:16:14.105426: step 9219, loss 0.0316359, acc 0.984375
2017-09-09T16:16:14.381624: step 9220, loss 0.003861, acc 1
2017-09-09T16:16:14.755882: step 9221, loss 0.0316776, acc 0.984375
2017-09-09T16:16:15.035560: step 9222, loss 0.0560898, acc 0.96875
2017-09-09T16:16:15.382337: step 9223, loss 0.0409779, acc 0.96875
2017-09-09T16:16:15.782782: step 9224, loss 0.00404077, acc 1
2017-09-09T16:16:16.112048: step 9225, loss 0.0721933, acc 0.953125
2017-09-09T16:16:16.415690: step 9226, loss 0.0298325, acc 0.984375
2017-09-09T16:16:16.740513: step 9227, loss 0.00785567, acc 1
2017-09-09T16:16:17.021218: step 9228, loss 0.00971182, acc 1
2017-09-09T16:16:17.435069: step 9229, loss 0.0533576, acc 0.984375
2017-09-09T16:16:17.751615: step 9230, loss 0.00395459, acc 1
2017-09-09T16:16:18.082180: step 9231, loss 0.00189971, acc 1
2017-09-09T16:16:18.412589: step 9232, loss 0.0519214, acc 0.96875
2017-09-09T16:16:18.689919: step 9233, loss 0.00199643, acc 1
2017-09-09T16:16:19.072789: step 9234, loss 0.0206552, acc 0.984375
2017-09-09T16:16:19.382284: step 9235, loss 0.0717258, acc 0.984375
2017-09-09T16:16:19.668503: step 9236, loss 0.0292129, acc 0.984375
2017-09-09T16:16:20.045798: step 9237, loss 0.0230437, acc 0.984375
2017-09-09T16:16:20.326366: step 9238, loss 0.11129, acc 0.96875
2017-09-09T16:16:20.606041: step 9239, loss 0.00681986, acc 1
2017-09-09T16:16:20.899790: step 9240, loss 0.0708203, acc 0.984375
2017-09-09T16:16:21.206049: step 9241, loss 0.006393, acc 1
2017-09-09T16:16:21.491373: step 9242, loss 0.000566752, acc 1
2017-09-09T16:16:21.767667: step 9243, loss 0.0390607, acc 0.984375
2017-09-09T16:16:22.116322: step 9244, loss 0.0580349, acc 0.984375
2017-09-09T16:16:22.415481: step 9245, loss 0.00996922, acc 1
2017-09-09T16:16:22.778655: step 9246, loss 0.00234259, acc 1
2017-09-09T16:16:23.088242: step 9247, loss 0.0185145, acc 1
2017-09-09T16:16:23.438035: step 9248, loss 0.0602213, acc 0.96875
2017-09-09T16:16:23.747985: step 9249, loss 0.00906151, acc 1
2017-09-09T16:16:24.100879: step 9250, loss 0.00611995, acc 1

Evaluation:
2017-09-09T16:16:24.162862: step 9250, loss 1.84162, acc 0.315108

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9250

2017-09-09T16:16:27.195783: step 9251, loss 0.0535136, acc 0.984375
2017-09-09T16:16:27.550152: step 9252, loss 0.0284463, acc 0.984375
2017-09-09T16:16:27.915128: step 9253, loss 0.033372, acc 0.984375
2017-09-09T16:16:28.207910: step 9254, loss 0.00740276, acc 1
2017-09-09T16:16:28.576714: step 9255, loss 0.0041092, acc 1
2017-09-09T16:16:28.859633: step 9256, loss 0.00713675, acc 1
2017-09-09T16:16:29.163779: step 9257, loss 0.00525985, acc 1
2017-09-09T16:16:29.500625: step 9258, loss 0.0347572, acc 0.984375
2017-09-09T16:16:29.790504: step 9259, loss 0.0188789, acc 1
2017-09-09T16:16:30.133552: step 9260, loss 0.000191162, acc 1
2017-09-09T16:16:30.438163: step 9261, loss 0.0107423, acc 1
2017-09-09T16:16:30.764529: step 9262, loss 0.00739816, acc 1
2017-09-09T16:16:31.045513: step 9263, loss 0.0469296, acc 0.984375
2017-09-09T16:16:31.439339: step 9264, loss 0.023235, acc 0.984375
2017-09-09T16:16:31.725472: step 9265, loss 0.023219, acc 0.984375
2017-09-09T16:16:32.019894: step 9266, loss 0.00139725, acc 1
2017-09-09T16:16:32.350541: step 9267, loss 0.0219757, acc 0.984375
2017-09-09T16:16:32.638579: step 9268, loss 0.00267392, acc 1
2017-09-09T16:16:33.021869: step 9269, loss 0.00491526, acc 1
2017-09-09T16:16:33.304059: step 9270, loss 0.0386219, acc 0.96875
2017-09-09T16:16:33.649861: step 9271, loss 0.0192686, acc 0.984375
2017-09-09T16:16:33.961574: step 9272, loss 0.0604728, acc 0.96875
2017-09-09T16:16:34.249972: step 9273, loss 0.0132341, acc 1
2017-09-09T16:16:34.603473: step 9274, loss 0.0019009, acc 1
2017-09-09T16:16:34.905574: step 9275, loss 0.0317322, acc 0.984375
2017-09-09T16:16:35.309054: step 9276, loss 0.00642946, acc 1
2017-09-09T16:16:35.684602: step 9277, loss 0.000164264, acc 1
2017-09-09T16:16:35.988164: step 9278, loss 0.0185154, acc 1
2017-09-09T16:16:36.356464: step 9279, loss 0.0177383, acc 0.984375
2017-09-09T16:16:36.665635: step 9280, loss 0.0263472, acc 0.984375
2017-09-09T16:16:37.154077: step 9281, loss 0.0628897, acc 0.96875
2017-09-09T16:16:37.491904: step 9282, loss 0.0333526, acc 0.984375
2017-09-09T16:16:37.838326: step 9283, loss 0.0218766, acc 0.984375
2017-09-09T16:16:38.183412: step 9284, loss 0.00425532, acc 1
2017-09-09T16:16:38.478079: step 9285, loss 0.00406021, acc 1
2017-09-09T16:16:38.758090: step 9286, loss 0.0645004, acc 0.96875
2017-09-09T16:16:39.117198: step 9287, loss 0.0346598, acc 0.984375
2017-09-09T16:16:39.443014: step 9288, loss 5.64627e-05, acc 1
2017-09-09T16:16:39.756361: step 9289, loss 0.0407346, acc 0.96875
2017-09-09T16:16:40.093968: step 9290, loss 0.0202254, acc 1
2017-09-09T16:16:40.413334: step 9291, loss 0.0390808, acc 0.984375
2017-09-09T16:16:40.757221: step 9292, loss 0.0651794, acc 0.96875
2017-09-09T16:16:41.071528: step 9293, loss 0.0312085, acc 0.984375
2017-09-09T16:16:41.344673: step 9294, loss 0.032736, acc 0.984375
2017-09-09T16:16:41.736571: step 9295, loss 0.0479843, acc 0.96875
2017-09-09T16:16:42.036971: step 9296, loss 0.0410972, acc 0.96875
2017-09-09T16:16:42.366752: step 9297, loss 0.0344267, acc 0.984375
2017-09-09T16:16:42.648253: step 9298, loss 0.0585893, acc 0.96875
2017-09-09T16:16:42.967060: step 9299, loss 0.0231613, acc 0.984375
2017-09-09T16:16:43.264688: step 9300, loss 0.0183028, acc 0.984375

Evaluation:
2017-09-09T16:16:43.358482: step 9300, loss 2.32557, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9300

2017-09-09T16:16:45.530236: step 9301, loss 0.0327873, acc 1
2017-09-09T16:16:45.918215: step 9302, loss 0.00882246, acc 1
2017-09-09T16:16:46.231868: step 9303, loss 0.0300212, acc 0.984375
2017-09-09T16:16:46.514453: step 9304, loss 0.0409646, acc 0.96875
2017-09-09T16:16:46.893786: step 9305, loss 0.0111003, acc 1
2017-09-09T16:16:47.199915: step 9306, loss 0.0328328, acc 1
2017-09-09T16:16:47.567437: step 9307, loss 0.0582044, acc 0.984375
2017-09-09T16:16:47.884067: step 9308, loss 0.0130082, acc 1
2017-09-09T16:16:48.161118: step 9309, loss 0.045824, acc 0.984375
2017-09-09T16:16:48.495814: step 9310, loss 0.0633152, acc 0.960784
2017-09-09T16:16:48.897562: step 9311, loss 0.028284, acc 1
2017-09-09T16:16:49.230481: step 9312, loss 0.00192008, acc 1
2017-09-09T16:16:49.533921: step 9313, loss 0.0516016, acc 0.96875
2017-09-09T16:16:49.821074: step 9314, loss 0.0110962, acc 1
2017-09-09T16:16:50.147021: step 9315, loss 0.00373232, acc 1
2017-09-09T16:16:50.457402: step 9316, loss 0.0205124, acc 0.984375
2017-09-09T16:16:50.728358: step 9317, loss 0.0109004, acc 1
2017-09-09T16:16:51.116863: step 9318, loss 0.0187822, acc 0.984375
2017-09-09T16:16:51.394880: step 9319, loss 0.0235547, acc 0.984375
2017-09-09T16:16:51.705482: step 9320, loss 0.0268253, acc 0.984375
2017-09-09T16:16:52.075870: step 9321, loss 0.00356675, acc 1
2017-09-09T16:16:52.361812: step 9322, loss 0.0273607, acc 0.984375
2017-09-09T16:16:52.679674: step 9323, loss 1.99028e-05, acc 1
2017-09-09T16:16:53.004301: step 9324, loss 0.0360202, acc 0.984375
2017-09-09T16:16:53.333949: step 9325, loss 0.0209056, acc 0.984375
2017-09-09T16:16:53.628769: step 9326, loss 0.050247, acc 0.96875
2017-09-09T16:16:53.961139: step 9327, loss 0.022664, acc 0.984375
2017-09-09T16:16:54.312778: step 9328, loss 0.0414287, acc 0.984375
2017-09-09T16:16:54.577132: step 9329, loss 0.062262, acc 0.953125
2017-09-09T16:16:54.929253: step 9330, loss 0.00943381, acc 1
2017-09-09T16:16:55.228630: step 9331, loss 0.0174142, acc 0.984375
2017-09-09T16:16:55.550675: step 9332, loss 0.0567157, acc 0.96875
2017-09-09T16:16:55.895882: step 9333, loss 0.00440435, acc 1
2017-09-09T16:16:56.195520: step 9334, loss 7.39814e-05, acc 1
2017-09-09T16:16:56.586064: step 9335, loss 0.0348107, acc 0.984375
2017-09-09T16:16:56.944287: step 9336, loss 0.0063228, acc 1
2017-09-09T16:16:57.301396: step 9337, loss 0.0934994, acc 0.953125
2017-09-09T16:16:57.656722: step 9338, loss 0.0304608, acc 0.984375
2017-09-09T16:16:57.958597: step 9339, loss 0.00603629, acc 1
2017-09-09T16:16:58.265825: step 9340, loss 0.0339921, acc 0.984375
2017-09-09T16:16:58.587926: step 9341, loss 0.0160892, acc 1
2017-09-09T16:16:58.867123: step 9342, loss 0.00886168, acc 1
2017-09-09T16:16:59.205786: step 9343, loss 0.0383246, acc 0.96875
2017-09-09T16:16:59.489771: step 9344, loss 0.0143893, acc 0.984375
2017-09-09T16:16:59.882266: step 9345, loss 0.0130714, acc 1
2017-09-09T16:17:00.180704: step 9346, loss 0.01358, acc 0.984375
2017-09-09T16:17:00.495902: step 9347, loss 0.0681116, acc 0.984375
2017-09-09T16:17:00.831932: step 9348, loss 0.035154, acc 0.984375
2017-09-09T16:17:01.141498: step 9349, loss 0.0410856, acc 0.984375
2017-09-09T16:17:01.503816: step 9350, loss 0.0489683, acc 0.96875

Evaluation:
2017-09-09T16:17:01.584794: step 9350, loss 2.37692, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9350

2017-09-09T16:17:04.639517: step 9351, loss 0.000458291, acc 1
2017-09-09T16:17:04.902335: step 9352, loss 0.0197453, acc 1
2017-09-09T16:17:05.255764: step 9353, loss 0.0213871, acc 0.984375
2017-09-09T16:17:05.516399: step 9354, loss 0.00145727, acc 1
2017-09-09T16:17:05.849579: step 9355, loss 0.0126227, acc 1
2017-09-09T16:17:06.165330: step 9356, loss 0.0171119, acc 0.984375
2017-09-09T16:17:06.439510: step 9357, loss 0.00282597, acc 1
2017-09-09T16:17:06.754620: step 9358, loss 0.0472465, acc 0.96875
2017-09-09T16:17:07.093983: step 9359, loss 0.012145, acc 1
2017-09-09T16:17:07.414349: step 9360, loss 0.0281232, acc 0.984375
2017-09-09T16:17:07.705800: step 9361, loss 0.000429491, acc 1
2017-09-09T16:17:08.069089: step 9362, loss 0.0250539, acc 0.984375
2017-09-09T16:17:08.340519: step 9363, loss 0.00207363, acc 1
2017-09-09T16:17:08.607202: step 9364, loss 0.00337727, acc 1
2017-09-09T16:17:08.968635: step 9365, loss 0.0391487, acc 1
2017-09-09T16:17:09.256380: step 9366, loss 0.000721022, acc 1
2017-09-09T16:17:09.580000: step 9367, loss 0.0471538, acc 0.96875
2017-09-09T16:17:09.889252: step 9368, loss 0.0225826, acc 0.984375
2017-09-09T16:17:10.179368: step 9369, loss 0.0181564, acc 0.984375
2017-09-09T16:17:10.518218: step 9370, loss 0.0382229, acc 0.96875
2017-09-09T16:17:10.809141: step 9371, loss 0.0484321, acc 0.96875
2017-09-09T16:17:11.127765: step 9372, loss 0.00722436, acc 1
2017-09-09T16:17:11.414125: step 9373, loss 0.00122664, acc 1
2017-09-09T16:17:11.710537: step 9374, loss 0.0161396, acc 1
2017-09-09T16:17:12.278056: step 9375, loss 0.0150576, acc 0.984375
2017-09-09T16:17:12.572204: step 9376, loss 0.0299977, acc 0.984375
2017-09-09T16:17:12.890130: step 9377, loss 0.000753438, acc 1
2017-09-09T16:17:13.202232: step 9378, loss 0.019984, acc 0.984375
2017-09-09T16:17:13.500363: step 9379, loss 0.0442489, acc 0.96875
2017-09-09T16:17:13.843280: step 9380, loss 0.00017116, acc 1
2017-09-09T16:17:14.136389: step 9381, loss 0.0143769, acc 1
2017-09-09T16:17:14.502691: step 9382, loss 0.00605359, acc 1
2017-09-09T16:17:14.787049: step 9383, loss 0.0127497, acc 0.984375
2017-09-09T16:17:15.122324: step 9384, loss 0.0264717, acc 0.984375
2017-09-09T16:17:15.496722: step 9385, loss 0.0184083, acc 1
2017-09-09T16:17:15.761492: step 9386, loss 0.00657501, acc 1
2017-09-09T16:17:16.144912: step 9387, loss 0.00407719, acc 1
2017-09-09T16:17:16.476528: step 9388, loss 0.0376945, acc 0.984375
2017-09-09T16:17:16.797075: step 9389, loss 0.000857003, acc 1
2017-09-09T16:17:17.106206: step 9390, loss 0.0201551, acc 0.984375
2017-09-09T16:17:17.427986: step 9391, loss 0.0272531, acc 0.96875
2017-09-09T16:17:17.772350: step 9392, loss 0.019756, acc 0.984375
2017-09-09T16:17:18.113541: step 9393, loss 0.00511112, acc 1
2017-09-09T16:17:18.450490: step 9394, loss 0.0218328, acc 0.984375
2017-09-09T16:17:18.753117: step 9395, loss 0.0220078, acc 1
2017-09-09T16:17:19.033677: step 9396, loss 0.0184041, acc 1
2017-09-09T16:17:19.426145: step 9397, loss 0.0275268, acc 1
2017-09-09T16:17:19.699664: step 9398, loss 0.0151738, acc 1
2017-09-09T16:17:19.971551: step 9399, loss 0.0101498, acc 1
2017-09-09T16:17:20.285245: step 9400, loss 0.0203567, acc 0.984375

Evaluation:
2017-09-09T16:17:20.350070: step 9400, loss 2.35184, acc 0.343885

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9400

2017-09-09T16:17:22.679151: step 9401, loss 0.0649826, acc 0.96875
2017-09-09T16:17:23.002199: step 9402, loss 0.0280784, acc 0.96875
2017-09-09T16:17:23.306458: step 9403, loss 0.0482713, acc 0.96875
2017-09-09T16:17:23.626422: step 9404, loss 0.0307474, acc 0.96875
2017-09-09T16:17:23.921804: step 9405, loss 0.00168335, acc 1
2017-09-09T16:17:24.243322: step 9406, loss 0.00482221, acc 1
2017-09-09T16:17:24.553246: step 9407, loss 0.0472461, acc 0.984375
2017-09-09T16:17:24.891538: step 9408, loss 0.000275102, acc 1
2017-09-09T16:17:25.220931: step 9409, loss 0.0373226, acc 0.984375
2017-09-09T16:17:25.526862: step 9410, loss 0.00883538, acc 1
2017-09-09T16:17:25.874752: step 9411, loss 0.00793711, acc 1
2017-09-09T16:17:26.175465: step 9412, loss 0.014589, acc 1
2017-09-09T16:17:26.544185: step 9413, loss 0.082218, acc 0.953125
2017-09-09T16:17:26.834700: step 9414, loss 0.00568791, acc 1
2017-09-09T16:17:27.177969: step 9415, loss 0.047072, acc 0.984375
2017-09-09T16:17:27.445204: step 9416, loss 0.00706211, acc 1
2017-09-09T16:17:27.734723: step 9417, loss 0.00914345, acc 1
2017-09-09T16:17:28.107045: step 9418, loss 0.0158355, acc 0.984375
2017-09-09T16:17:28.422891: step 9419, loss 0.0118118, acc 1
2017-09-09T16:17:28.751342: step 9420, loss 0.018063, acc 0.984375
2017-09-09T16:17:29.085067: step 9421, loss 0.0108234, acc 1
2017-09-09T16:17:29.424358: step 9422, loss 0.0231762, acc 0.984375
2017-09-09T16:17:29.758160: step 9423, loss 0.0206822, acc 0.984375
2017-09-09T16:17:30.074694: step 9424, loss 0.0196147, acc 0.984375
2017-09-09T16:17:30.436495: step 9425, loss 1.52422e-05, acc 1
2017-09-09T16:17:30.729813: step 9426, loss 0.00113047, acc 1
2017-09-09T16:17:31.037305: step 9427, loss 0.0207815, acc 1
2017-09-09T16:17:31.351082: step 9428, loss 0.0201657, acc 1
2017-09-09T16:17:31.653170: step 9429, loss 0.00045235, acc 1
2017-09-09T16:17:31.942804: step 9430, loss 0.00165133, acc 1
2017-09-09T16:17:32.254078: step 9431, loss 3.56681e-05, acc 1
2017-09-09T16:17:32.583088: step 9432, loss 0.00615409, acc 1
2017-09-09T16:17:32.863329: step 9433, loss 0.00391999, acc 1
2017-09-09T16:17:33.216782: step 9434, loss 0.00297349, acc 1
2017-09-09T16:17:33.520970: step 9435, loss 0.0419087, acc 0.96875
2017-09-09T16:17:33.891704: step 9436, loss 0.024639, acc 0.984375
2017-09-09T16:17:34.209129: step 9437, loss 0.147339, acc 0.953125
2017-09-09T16:17:34.503123: step 9438, loss 0.00354191, acc 1
2017-09-09T16:17:34.812863: step 9439, loss 0.0306207, acc 0.96875
2017-09-09T16:17:35.120112: step 9440, loss 0.000799423, acc 1
2017-09-09T16:17:35.509848: step 9441, loss 0.00131009, acc 1
2017-09-09T16:17:35.806726: step 9442, loss 0.000453513, acc 1
2017-09-09T16:17:36.249281: step 9443, loss 0.0638578, acc 0.96875
2017-09-09T16:17:36.560699: step 9444, loss 0.0982491, acc 0.96875
2017-09-09T16:17:36.878470: step 9445, loss 0.00168239, acc 1
2017-09-09T16:17:37.162907: step 9446, loss 0.0174608, acc 1
2017-09-09T16:17:37.470164: step 9447, loss 0.0225175, acc 0.984375
2017-09-09T16:17:37.793898: step 9448, loss 0.00124494, acc 1
2017-09-09T16:17:38.072050: step 9449, loss 0.0557662, acc 0.953125
2017-09-09T16:17:38.423297: step 9450, loss 0.00119138, acc 1

Evaluation:
2017-09-09T16:17:38.498306: step 9450, loss 2.41602, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9450

2017-09-09T16:17:41.363346: step 9451, loss 0.0565312, acc 0.984375
2017-09-09T16:17:41.662999: step 9452, loss 0.0275946, acc 0.984375
2017-09-09T16:17:42.004549: step 9453, loss 0.0482733, acc 0.984375
2017-09-09T16:17:42.318406: step 9454, loss 0.0186255, acc 1
2017-09-09T16:17:42.636757: step 9455, loss 0.00588516, acc 1
2017-09-09T16:17:42.916067: step 9456, loss 0.0253204, acc 0.984375
2017-09-09T16:17:43.223175: step 9457, loss 0.00104898, acc 1
2017-09-09T16:17:43.567385: step 9458, loss 0.00259945, acc 1
2017-09-09T16:17:43.864463: step 9459, loss 0.0100406, acc 1
2017-09-09T16:17:44.176827: step 9460, loss 0.0220137, acc 0.984375
2017-09-09T16:17:44.472581: step 9461, loss 0.0144788, acc 1
2017-09-09T16:17:44.830659: step 9462, loss 0.0101963, acc 1
2017-09-09T16:17:45.174821: step 9463, loss 0.00150495, acc 1
2017-09-09T16:17:45.472281: step 9464, loss 0.0141024, acc 0.984375
2017-09-09T16:17:45.817940: step 9465, loss 0.033083, acc 0.984375
2017-09-09T16:17:46.142681: step 9466, loss 0.00672245, acc 1
2017-09-09T16:17:46.511770: step 9467, loss 0.016144, acc 0.984375
2017-09-09T16:17:46.843564: step 9468, loss 0.0324219, acc 0.984375
2017-09-09T16:17:47.188294: step 9469, loss 0.00677568, acc 1
2017-09-09T16:17:47.582972: step 9470, loss 0.000447759, acc 1
2017-09-09T16:17:47.852703: step 9471, loss 0.0268935, acc 0.984375
2017-09-09T16:17:48.300222: step 9472, loss 0.0414381, acc 0.984375
2017-09-09T16:17:48.630327: step 9473, loss 0.0507118, acc 0.96875
2017-09-09T16:17:48.940179: step 9474, loss 0.017472, acc 1
2017-09-09T16:17:49.232118: step 9475, loss 0.0123537, acc 0.984375
2017-09-09T16:17:49.607629: step 9476, loss 0.0274701, acc 0.984375
2017-09-09T16:17:49.958968: step 9477, loss 0.0139981, acc 1
2017-09-09T16:17:50.330329: step 9478, loss 0.0306357, acc 0.984375
2017-09-09T16:17:50.644448: step 9479, loss 0.0421652, acc 0.96875
2017-09-09T16:17:50.983448: step 9480, loss 0.0171495, acc 0.984375
2017-09-09T16:17:51.331672: step 9481, loss 0.0178664, acc 1
2017-09-09T16:17:51.624320: step 9482, loss 0.0114174, acc 1
2017-09-09T16:17:51.978806: step 9483, loss 0.000206575, acc 1
2017-09-09T16:17:52.320887: step 9484, loss 0.00821127, acc 1
2017-09-09T16:17:52.681650: step 9485, loss 0.0640327, acc 0.953125
2017-09-09T16:17:52.997904: step 9486, loss 0.0054033, acc 1
2017-09-09T16:17:53.277624: step 9487, loss 0.0274293, acc 0.984375
2017-09-09T16:17:53.612431: step 9488, loss 0.00898946, acc 1
2017-09-09T16:17:53.912086: step 9489, loss 0.00164445, acc 1
2017-09-09T16:17:54.251109: step 9490, loss 0.0148619, acc 1
2017-09-09T16:17:54.535930: step 9491, loss 0.0346942, acc 0.96875
2017-09-09T16:17:54.882381: step 9492, loss 0.00497092, acc 1
2017-09-09T16:17:55.258719: step 9493, loss 0.0272029, acc 0.984375
2017-09-09T16:17:55.537417: step 9494, loss 0.0403934, acc 0.984375
2017-09-09T16:17:55.904645: step 9495, loss 0.0028787, acc 1
2017-09-09T16:17:56.189633: step 9496, loss 0.0584997, acc 0.96875
2017-09-09T16:17:56.528051: step 9497, loss 0.0209795, acc 0.984375
2017-09-09T16:17:56.827347: step 9498, loss 0.000286124, acc 1
2017-09-09T16:17:57.103578: step 9499, loss 0.000565879, acc 1
2017-09-09T16:17:57.428027: step 9500, loss 0.0427255, acc 0.96875

Evaluation:
2017-09-09T16:17:57.503099: step 9500, loss 3.0645, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9500

2017-09-09T16:18:00.680865: step 9501, loss 0.0682558, acc 0.984375
2017-09-09T16:18:01.010723: step 9502, loss 0.0153534, acc 1
2017-09-09T16:18:01.304896: step 9503, loss 0.0451779, acc 0.953125
2017-09-09T16:18:01.674979: step 9504, loss 0.0326192, acc 0.984375
2017-09-09T16:18:02.009329: step 9505, loss 0.025272, acc 0.984375
2017-09-09T16:18:02.332921: step 9506, loss 0.0535412, acc 0.960784
2017-09-09T16:18:02.717825: step 9507, loss 0.0282363, acc 0.984375
2017-09-09T16:18:03.005009: step 9508, loss 0.0131026, acc 1
2017-09-09T16:18:03.381488: step 9509, loss 0.0177951, acc 1
2017-09-09T16:18:03.773993: step 9510, loss 3.45516e-05, acc 1
2017-09-09T16:18:04.088301: step 9511, loss 0.0199761, acc 1
2017-09-09T16:18:04.457297: step 9512, loss 0.0452861, acc 0.96875
2017-09-09T16:18:04.769972: step 9513, loss 5.54631e-05, acc 1
2017-09-09T16:18:05.161823: step 9514, loss 0.0440787, acc 0.96875
2017-09-09T16:18:05.508456: step 9515, loss 0.000714848, acc 1
2017-09-09T16:18:05.787793: step 9516, loss 0.0194791, acc 1
2017-09-09T16:18:06.115665: step 9517, loss 0.0400084, acc 0.984375
2017-09-09T16:18:06.392921: step 9518, loss 0.000421825, acc 1
2017-09-09T16:18:06.676444: step 9519, loss 0.0879875, acc 0.96875
2017-09-09T16:18:06.971043: step 9520, loss 0.0746426, acc 0.984375
2017-09-09T16:18:07.220495: step 9521, loss 0.00869338, acc 1
2017-09-09T16:18:07.556762: step 9522, loss 0.0170274, acc 1
2017-09-09T16:18:07.882965: step 9523, loss 0.033084, acc 0.984375
2017-09-09T16:18:08.229423: step 9524, loss 0.0338984, acc 0.96875
2017-09-09T16:18:08.609945: step 9525, loss 0.00310408, acc 1
2017-09-09T16:18:08.896594: step 9526, loss 0.00438834, acc 1
2017-09-09T16:18:09.220768: step 9527, loss 0.0202273, acc 1
2017-09-09T16:18:09.502403: step 9528, loss 0.0165962, acc 0.984375
2017-09-09T16:18:09.831844: step 9529, loss 0.00130269, acc 1
2017-09-09T16:18:10.137826: step 9530, loss 0.0307113, acc 0.984375
2017-09-09T16:18:10.456581: step 9531, loss 0.0188525, acc 1
2017-09-09T16:18:10.750521: step 9532, loss 0.0182014, acc 1
2017-09-09T16:18:11.043394: step 9533, loss 0.0306436, acc 0.984375
2017-09-09T16:18:11.456629: step 9534, loss 0.0126855, acc 1
2017-09-09T16:18:11.791934: step 9535, loss 0.0228527, acc 0.984375
2017-09-09T16:18:12.100676: step 9536, loss 0.00159328, acc 1
2017-09-09T16:18:12.382793: step 9537, loss 0.025499, acc 0.984375
2017-09-09T16:18:12.704736: step 9538, loss 0.0205026, acc 0.984375
2017-09-09T16:18:13.091167: step 9539, loss 0.0264651, acc 1
2017-09-09T16:18:13.364104: step 9540, loss 0.000444596, acc 1
2017-09-09T16:18:13.698153: step 9541, loss 0.048289, acc 0.96875
2017-09-09T16:18:13.998864: step 9542, loss 0.0268848, acc 1
2017-09-09T16:18:14.338322: step 9543, loss 0.0583702, acc 0.96875
2017-09-09T16:18:14.631225: step 9544, loss 0.0201359, acc 1
2017-09-09T16:18:14.918912: step 9545, loss 0.0397598, acc 0.96875
2017-09-09T16:18:15.268775: step 9546, loss 0.020601, acc 0.984375
2017-09-09T16:18:15.559493: step 9547, loss 0.028737, acc 0.984375
2017-09-09T16:18:15.881011: step 9548, loss 0.029875, acc 0.984375
2017-09-09T16:18:16.149137: step 9549, loss 0.0566967, acc 0.96875
2017-09-09T16:18:16.507236: step 9550, loss 0.0224801, acc 0.984375

Evaluation:
2017-09-09T16:18:16.572857: step 9550, loss 2.63083, acc 0.339568

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9550

2017-09-09T16:18:19.027938: step 9551, loss 0.0224226, acc 0.984375
2017-09-09T16:18:19.337815: step 9552, loss 0.0132324, acc 1
2017-09-09T16:18:19.666353: step 9553, loss 0.0184524, acc 1
2017-09-09T16:18:20.056713: step 9554, loss 0.00696341, acc 1
2017-09-09T16:18:20.389270: step 9555, loss 0.000594773, acc 1
2017-09-09T16:18:20.769392: step 9556, loss 0.0364215, acc 0.96875
2017-09-09T16:18:21.081739: step 9557, loss 0.00037418, acc 1
2017-09-09T16:18:21.418884: step 9558, loss 0.0257176, acc 0.984375
2017-09-09T16:18:21.760608: step 9559, loss 0.0226298, acc 1
2017-09-09T16:18:22.111032: step 9560, loss 0.00674785, acc 1
2017-09-09T16:18:22.468329: step 9561, loss 0.0226664, acc 0.984375
2017-09-09T16:18:22.802287: step 9562, loss 0.0409414, acc 0.96875
2017-09-09T16:18:23.222146: step 9563, loss 0.0219979, acc 1
2017-09-09T16:18:23.622220: step 9564, loss 0.00188743, acc 1
2017-09-09T16:18:23.913989: step 9565, loss 0.0273392, acc 0.984375
2017-09-09T16:18:24.321129: step 9566, loss 0.0147273, acc 0.984375
2017-09-09T16:18:24.602801: step 9567, loss 0.00304203, acc 1
2017-09-09T16:18:24.956032: step 9568, loss 0.00451044, acc 1
2017-09-09T16:18:25.255519: step 9569, loss 0.0107137, acc 1
2017-09-09T16:18:25.566409: step 9570, loss 0.0237343, acc 0.984375
2017-09-09T16:18:25.949228: step 9571, loss 0.0579379, acc 0.984375
2017-09-09T16:18:26.258726: step 9572, loss 0.0259391, acc 0.984375
2017-09-09T16:18:26.599238: step 9573, loss 0.0305008, acc 0.984375
2017-09-09T16:18:26.919863: step 9574, loss 0.0188843, acc 0.984375
2017-09-09T16:18:27.245678: step 9575, loss 0.0494081, acc 0.984375
2017-09-09T16:18:27.571425: step 9576, loss 0.0835387, acc 0.96875
2017-09-09T16:18:27.884115: step 9577, loss 0.0711009, acc 0.953125
2017-09-09T16:18:28.236986: step 9578, loss 0.0177394, acc 1
2017-09-09T16:18:28.534769: step 9579, loss 0.073142, acc 0.953125
2017-09-09T16:18:28.885580: step 9580, loss 0.000559347, acc 1
2017-09-09T16:18:29.199270: step 9581, loss 0.00142182, acc 1
2017-09-09T16:18:29.498333: step 9582, loss 0.0232366, acc 1
2017-09-09T16:18:29.863114: step 9583, loss 0.00980459, acc 1
2017-09-09T16:18:30.199034: step 9584, loss 0.000329119, acc 1
2017-09-09T16:18:30.568708: step 9585, loss 0.00158599, acc 1
2017-09-09T16:18:30.851345: step 9586, loss 0.0243264, acc 0.984375
2017-09-09T16:18:31.179314: step 9587, loss 0.105194, acc 0.9375
2017-09-09T16:18:31.536353: step 9588, loss 0.0502967, acc 0.953125
2017-09-09T16:18:31.819506: step 9589, loss 0.0467862, acc 0.96875
2017-09-09T16:18:32.162643: step 9590, loss 0.0466984, acc 0.984375
2017-09-09T16:18:32.445408: step 9591, loss 0.04052, acc 0.984375
2017-09-09T16:18:32.813586: step 9592, loss 0.0184319, acc 0.984375
2017-09-09T16:18:33.152215: step 9593, loss 0.0181659, acc 1
2017-09-09T16:18:33.460792: step 9594, loss 0.00462735, acc 1
2017-09-09T16:18:33.783563: step 9595, loss 0.024788, acc 0.984375
2017-09-09T16:18:34.085444: step 9596, loss 0.00806603, acc 1
2017-09-09T16:18:34.485137: step 9597, loss 0.0216971, acc 0.984375
2017-09-09T16:18:34.854321: step 9598, loss 0.00993318, acc 1
2017-09-09T16:18:35.127544: step 9599, loss 0.0128381, acc 1
2017-09-09T16:18:35.453327: step 9600, loss 0.0688385, acc 0.96875

Evaluation:
2017-09-09T16:18:35.519988: step 9600, loss 2.19361, acc 0.313669

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9600

2017-09-09T16:18:38.991303: step 9601, loss 0.0141793, acc 1
2017-09-09T16:18:39.369515: step 9602, loss 0.0152074, acc 0.984375
2017-09-09T16:18:39.704402: step 9603, loss 0.0788983, acc 0.953125
2017-09-09T16:18:39.980172: step 9604, loss 0.0197388, acc 0.980392
2017-09-09T16:18:40.321642: step 9605, loss 0.0013602, acc 1
2017-09-09T16:18:40.600504: step 9606, loss 0.0806056, acc 0.96875
2017-09-09T16:18:40.910381: step 9607, loss 0.0244767, acc 0.984375
2017-09-09T16:18:41.199293: step 9608, loss 0.0131678, acc 1
2017-09-09T16:18:41.543655: step 9609, loss 0.0100949, acc 1
2017-09-09T16:18:41.869054: step 9610, loss 0.0458676, acc 0.96875
2017-09-09T16:18:42.218785: step 9611, loss 0.00160605, acc 1
2017-09-09T16:18:42.607464: step 9612, loss 0.028505, acc 0.984375
2017-09-09T16:18:42.925200: step 9613, loss 0.000336537, acc 1
2017-09-09T16:18:43.243927: step 9614, loss 0.0437626, acc 0.984375
2017-09-09T16:18:43.587786: step 9615, loss 0.019136, acc 0.984375
2017-09-09T16:18:43.931942: step 9616, loss 0.0299154, acc 0.984375
2017-09-09T16:18:44.282768: step 9617, loss 0.0145122, acc 1
2017-09-09T16:18:44.577830: step 9618, loss 0.00582409, acc 1
2017-09-09T16:18:44.882768: step 9619, loss 0.0217948, acc 0.984375
2017-09-09T16:18:45.213156: step 9620, loss 0.0445336, acc 0.96875
2017-09-09T16:18:45.503579: step 9621, loss 0.00126881, acc 1
2017-09-09T16:18:45.909667: step 9622, loss 0.044614, acc 0.984375
2017-09-09T16:18:46.180272: step 9623, loss 0.0391501, acc 0.984375
2017-09-09T16:18:46.654366: step 9624, loss 0.00380649, acc 1
2017-09-09T16:18:46.960837: step 9625, loss 0.0124538, acc 1
2017-09-09T16:18:47.266085: step 9626, loss 0.00276618, acc 1
2017-09-09T16:18:47.546211: step 9627, loss 0.00669302, acc 1
2017-09-09T16:18:47.884807: step 9628, loss 0.0643663, acc 0.953125
2017-09-09T16:18:48.175818: step 9629, loss 0.0142123, acc 0.984375
2017-09-09T16:18:48.442920: step 9630, loss 0.00220253, acc 1
2017-09-09T16:18:48.758006: step 9631, loss 0.0306339, acc 0.984375
2017-09-09T16:18:49.088172: step 9632, loss 0.00206785, acc 1
2017-09-09T16:18:49.420891: step 9633, loss 0.00806641, acc 1
2017-09-09T16:18:49.776091: step 9634, loss 0.0402116, acc 0.984375
2017-09-09T16:18:50.108512: step 9635, loss 0.0106026, acc 1
2017-09-09T16:18:50.452932: step 9636, loss 0.0271716, acc 1
2017-09-09T16:18:50.785342: step 9637, loss 0.0920094, acc 0.96875
2017-09-09T16:18:51.172365: step 9638, loss 0.0763169, acc 0.96875
2017-09-09T16:18:51.538870: step 9639, loss 0.00127121, acc 1
2017-09-09T16:18:51.849939: step 9640, loss 0.00465987, acc 1
2017-09-09T16:18:52.266974: step 9641, loss 0.0605266, acc 0.96875
2017-09-09T16:18:52.615460: step 9642, loss 0.0153962, acc 1
2017-09-09T16:18:52.904867: step 9643, loss 0.0480381, acc 0.984375
2017-09-09T16:18:53.302612: step 9644, loss 0.00334362, acc 1
2017-09-09T16:18:53.590167: step 9645, loss 0.0307126, acc 0.96875
2017-09-09T16:18:53.968248: step 9646, loss 0.0089885, acc 1
2017-09-09T16:18:54.280173: step 9647, loss 0.0224629, acc 0.984375
2017-09-09T16:18:54.555140: step 9648, loss 0.010587, acc 1
2017-09-09T16:18:54.915177: step 9649, loss 0.0506544, acc 0.96875
2017-09-09T16:18:55.208704: step 9650, loss 0.0231425, acc 0.984375

Evaluation:
2017-09-09T16:18:55.298500: step 9650, loss 3.05894, acc 0.34964

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9650

2017-09-09T16:18:57.519776: step 9651, loss 0.105819, acc 0.953125
2017-09-09T16:18:57.894037: step 9652, loss 0.00978355, acc 1
2017-09-09T16:18:58.207980: step 9653, loss 0.0275656, acc 0.96875
2017-09-09T16:18:58.557513: step 9654, loss 0.00849287, acc 1
2017-09-09T16:18:58.835219: step 9655, loss 0.0319526, acc 1
2017-09-09T16:18:59.180764: step 9656, loss 0.0458316, acc 0.984375
2017-09-09T16:18:59.495162: step 9657, loss 0.00413999, acc 1
2017-09-09T16:18:59.797574: step 9658, loss 0.002174, acc 1
2017-09-09T16:19:00.186935: step 9659, loss 0.019884, acc 0.984375
2017-09-09T16:19:00.520879: step 9660, loss 0.027352, acc 0.984375
2017-09-09T16:19:00.816076: step 9661, loss 0.0163175, acc 0.984375
2017-09-09T16:19:01.133072: step 9662, loss 0.0444035, acc 0.96875
2017-09-09T16:19:01.474142: step 9663, loss 0.000122991, acc 1
2017-09-09T16:19:01.819603: step 9664, loss 0.102032, acc 0.96875
2017-09-09T16:19:02.099457: step 9665, loss 0.0351718, acc 0.984375
2017-09-09T16:19:02.487470: step 9666, loss 0.0275427, acc 0.984375
2017-09-09T16:19:02.800022: step 9667, loss 0.0700359, acc 0.984375
2017-09-09T16:19:03.090609: step 9668, loss 0.00156415, acc 1
2017-09-09T16:19:03.434504: step 9669, loss 0.0185472, acc 1
2017-09-09T16:19:03.724607: step 9670, loss 0.0344865, acc 0.984375
2017-09-09T16:19:04.119365: step 9671, loss 0.0212221, acc 1
2017-09-09T16:19:04.440898: step 9672, loss 0.0124796, acc 1
2017-09-09T16:19:04.723448: step 9673, loss 0.016307, acc 0.984375
2017-09-09T16:19:05.084509: step 9674, loss 0.0228228, acc 0.984375
2017-09-09T16:19:05.360376: step 9675, loss 0.0186026, acc 1
2017-09-09T16:19:05.663057: step 9676, loss 0.0110667, acc 1
2017-09-09T16:19:06.017267: step 9677, loss 0.0036753, acc 1
2017-09-09T16:19:06.333392: step 9678, loss 0.000373, acc 1
2017-09-09T16:19:06.690102: step 9679, loss 0.0323317, acc 0.96875
2017-09-09T16:19:07.040557: step 9680, loss 0.029146, acc 0.984375
2017-09-09T16:19:07.436114: step 9681, loss 0.108934, acc 0.953125
2017-09-09T16:19:07.779220: step 9682, loss 0.0129734, acc 1
2017-09-09T16:19:08.141789: step 9683, loss 0.0163622, acc 0.984375
2017-09-09T16:19:08.509094: step 9684, loss 0.0208016, acc 0.984375
2017-09-09T16:19:08.876460: step 9685, loss 0.0356273, acc 0.984375
2017-09-09T16:19:09.174253: step 9686, loss 0.0297535, acc 0.984375
2017-09-09T16:19:09.503393: step 9687, loss 0.0749165, acc 0.953125
2017-09-09T16:19:09.794143: step 9688, loss 0.0290803, acc 0.984375
2017-09-09T16:19:10.169684: step 9689, loss 0.000284993, acc 1
2017-09-09T16:19:10.544371: step 9690, loss 0.0381281, acc 0.984375
2017-09-09T16:19:10.893989: step 9691, loss 0.0275248, acc 0.96875
2017-09-09T16:19:11.248141: step 9692, loss 0.0282075, acc 0.984375
2017-09-09T16:19:11.540784: step 9693, loss 0.0653709, acc 0.953125
2017-09-09T16:19:11.897361: step 9694, loss 0.0589977, acc 0.953125
2017-09-09T16:19:12.219857: step 9695, loss 0.0413541, acc 0.984375
2017-09-09T16:19:12.494277: step 9696, loss 0.0292107, acc 0.984375
2017-09-09T16:19:12.872782: step 9697, loss 0.00175954, acc 1
2017-09-09T16:19:13.186376: step 9698, loss 0.0538473, acc 0.96875
2017-09-09T16:19:13.537323: step 9699, loss 0.0415361, acc 0.984375
2017-09-09T16:19:13.830572: step 9700, loss 0.0351654, acc 0.96875

Evaluation:
2017-09-09T16:19:13.934065: step 9700, loss 1.83283, acc 0.244604

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9700

2017-09-09T16:19:16.807110: step 9701, loss 0.0368782, acc 0.984375
2017-09-09T16:19:17.084631: step 9702, loss 0.0831083, acc 0.960784
2017-09-09T16:19:17.404086: step 9703, loss 0.0104546, acc 1
2017-09-09T16:19:17.745084: step 9704, loss 0.0165421, acc 0.984375
2017-09-09T16:19:18.041592: step 9705, loss 0.0407341, acc 0.96875
2017-09-09T16:19:18.398147: step 9706, loss 0.0431832, acc 0.984375
2017-09-09T16:19:18.789811: step 9707, loss 0.0633197, acc 0.96875
2017-09-09T16:19:19.124144: step 9708, loss 0.0029652, acc 1
2017-09-09T16:19:19.457014: step 9709, loss 0.0466246, acc 0.984375
2017-09-09T16:19:19.821333: step 9710, loss 0.0236993, acc 1
2017-09-09T16:19:20.100290: step 9711, loss 0.0207668, acc 0.984375
2017-09-09T16:19:20.495140: step 9712, loss 4.51284e-05, acc 1
2017-09-09T16:19:20.799584: step 9713, loss 0.0459781, acc 0.96875
2017-09-09T16:19:21.161819: step 9714, loss 0.00898342, acc 1
2017-09-09T16:19:21.538773: step 9715, loss 0.0127656, acc 1
2017-09-09T16:19:21.827623: step 9716, loss 0.0258401, acc 0.984375
2017-09-09T16:19:22.177630: step 9717, loss 0.0125803, acc 0.984375
2017-09-09T16:19:22.500445: step 9718, loss 0.0289307, acc 0.984375
2017-09-09T16:19:22.792272: step 9719, loss 0.0052121, acc 1
2017-09-09T16:19:23.145566: step 9720, loss 0.0191541, acc 0.984375
2017-09-09T16:19:23.441520: step 9721, loss 0.0822177, acc 0.96875
2017-09-09T16:19:23.807023: step 9722, loss 0.00221666, acc 1
2017-09-09T16:19:24.103594: step 9723, loss 0.0515733, acc 0.984375
2017-09-09T16:19:24.425087: step 9724, loss 0.0219326, acc 0.984375
2017-09-09T16:19:24.755688: step 9725, loss 0.00949411, acc 1
2017-09-09T16:19:25.047456: step 9726, loss 0.0185934, acc 1
2017-09-09T16:19:25.363094: step 9727, loss 0.0203518, acc 1
2017-09-09T16:19:25.664517: step 9728, loss 0.0263205, acc 0.984375
2017-09-09T16:19:26.003000: step 9729, loss 0.0198479, acc 0.984375
2017-09-09T16:19:26.305030: step 9730, loss 0.00107216, acc 1
2017-09-09T16:19:26.630965: step 9731, loss 0.019263, acc 1
2017-09-09T16:19:26.965597: step 9732, loss 0.0031019, acc 1
2017-09-09T16:19:27.257066: step 9733, loss 0.00523894, acc 1
2017-09-09T16:19:27.641000: step 9734, loss 0.00119798, acc 1
2017-09-09T16:19:27.986030: step 9735, loss 0.0107138, acc 1
2017-09-09T16:19:28.295768: step 9736, loss 0.020075, acc 1
2017-09-09T16:19:28.613489: step 9737, loss 0.0471436, acc 0.984375
2017-09-09T16:19:28.923599: step 9738, loss 0.000338597, acc 1
2017-09-09T16:19:29.368877: step 9739, loss 0.00618571, acc 1
2017-09-09T16:19:29.713124: step 9740, loss 0.0255922, acc 0.984375
2017-09-09T16:19:30.028667: step 9741, loss 0.00251316, acc 1
2017-09-09T16:19:30.383361: step 9742, loss 0.0365152, acc 0.984375
2017-09-09T16:19:30.728745: step 9743, loss 0.0276568, acc 0.984375
2017-09-09T16:19:31.082798: step 9744, loss 0.00561853, acc 1
2017-09-09T16:19:31.440286: step 9745, loss 0.0361368, acc 0.984375
2017-09-09T16:19:31.809575: step 9746, loss 0.00509209, acc 1
2017-09-09T16:19:32.159178: step 9747, loss 0.0432301, acc 0.984375
2017-09-09T16:19:32.453725: step 9748, loss 0.00620259, acc 1
2017-09-09T16:19:32.877901: step 9749, loss 0.0473437, acc 0.96875
2017-09-09T16:19:33.210041: step 9750, loss 0.00390501, acc 1

Evaluation:
2017-09-09T16:19:33.282607: step 9750, loss 3.10977, acc 0.338129

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9750

2017-09-09T16:19:36.003792: step 9751, loss 0.0532643, acc 0.984375
2017-09-09T16:19:36.329288: step 9752, loss 0.0214648, acc 0.984375
2017-09-09T16:19:36.646635: step 9753, loss 0.00554585, acc 1
2017-09-09T16:19:36.972211: step 9754, loss 0.0357351, acc 0.984375
2017-09-09T16:19:37.305844: step 9755, loss 0.0333407, acc 1
2017-09-09T16:19:37.608126: step 9756, loss 0.0378471, acc 0.984375
2017-09-09T16:19:37.946761: step 9757, loss 0.00303204, acc 1
2017-09-09T16:19:38.298898: step 9758, loss 0.0181431, acc 0.984375
2017-09-09T16:19:38.602357: step 9759, loss 0.000512963, acc 1
2017-09-09T16:19:38.948294: step 9760, loss 0.00825575, acc 1
2017-09-09T16:19:39.263369: step 9761, loss 0.0354986, acc 0.984375
2017-09-09T16:19:40.005174: step 9762, loss 0.0806396, acc 0.96875
2017-09-09T16:19:40.351658: step 9763, loss 0.00278468, acc 1
2017-09-09T16:19:40.686986: step 9764, loss 0.0687324, acc 0.953125
2017-09-09T16:19:41.089106: step 9765, loss 0.0499473, acc 0.96875
2017-09-09T16:19:41.390271: step 9766, loss 0.0770568, acc 0.96875
2017-09-09T16:19:41.732467: step 9767, loss 0.0057388, acc 1
2017-09-09T16:19:42.055957: step 9768, loss 0.012218, acc 1
2017-09-09T16:19:42.364868: step 9769, loss 0.0176335, acc 1
2017-09-09T16:19:42.760529: step 9770, loss 0.000606211, acc 1
2017-09-09T16:19:43.068825: step 9771, loss 0.00640476, acc 1
2017-09-09T16:19:43.414772: step 9772, loss 0.0187496, acc 0.984375
2017-09-09T16:19:43.803880: step 9773, loss 0.049427, acc 0.984375
2017-09-09T16:19:44.198196: step 9774, loss 0.063454, acc 0.96875
2017-09-09T16:19:44.517476: step 9775, loss 0.0233986, acc 0.984375
2017-09-09T16:19:44.844160: step 9776, loss 0.00920744, acc 1
2017-09-09T16:19:45.161559: step 9777, loss 0.0117433, acc 1
2017-09-09T16:19:45.471483: step 9778, loss 0.0676823, acc 0.953125
2017-09-09T16:19:45.836016: step 9779, loss 0.0181539, acc 0.984375
2017-09-09T16:19:46.152094: step 9780, loss 0.0149177, acc 1
2017-09-09T16:19:46.444187: step 9781, loss 0.0368787, acc 0.984375
2017-09-09T16:19:46.796036: step 9782, loss 0.00185309, acc 1
2017-09-09T16:19:47.131592: step 9783, loss 0.0594802, acc 0.953125
2017-09-09T16:19:47.445090: step 9784, loss 0.032316, acc 0.984375
2017-09-09T16:19:47.779676: step 9785, loss 0.0335201, acc 0.984375
2017-09-09T16:19:48.116996: step 9786, loss 0.0379112, acc 0.96875
2017-09-09T16:19:48.407313: step 9787, loss 0.00689762, acc 1
2017-09-09T16:19:48.790788: step 9788, loss 0.0622067, acc 0.96875
2017-09-09T16:19:49.078241: step 9789, loss 0.000833659, acc 1
2017-09-09T16:19:49.407387: step 9790, loss 0.0180112, acc 0.984375
2017-09-09T16:19:49.728330: step 9791, loss 0.0297444, acc 1
2017-09-09T16:19:50.027847: step 9792, loss 0.066397, acc 0.96875
2017-09-09T16:19:50.407009: step 9793, loss 0.00829196, acc 1
2017-09-09T16:19:50.689393: step 9794, loss 0.0278214, acc 0.984375
2017-09-09T16:19:51.021125: step 9795, loss 0.034691, acc 0.984375
2017-09-09T16:19:51.324259: step 9796, loss 0.0455793, acc 0.984375
2017-09-09T16:19:51.608243: step 9797, loss 0.0398673, acc 0.984375
2017-09-09T16:19:51.928524: step 9798, loss 0.0319637, acc 0.984375
2017-09-09T16:19:52.206525: step 9799, loss 0.0497997, acc 0.96875
2017-09-09T16:19:52.613946: step 9800, loss 0.000365164, acc 1

Evaluation:
2017-09-09T16:19:52.680168: step 9800, loss 3.1894, acc 0.341007

Saved model checkpoint to /home/xxliu10/repo/multicnntext/dataalltogether-dqw/cnn-text-classification-tf/runs/1504988440/checkpoints/model-9800

